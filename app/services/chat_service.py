"""
æ™ºèƒ½èŠå¤©æœåŠ¡ - åŸºäºRediså…±äº«å†…å­˜çš„èŠå¤©ç³»ç»Ÿ
åŸºäº llama_index_shared_memory_redis_storage.ipynb çš„æˆåŠŸå®ç°
"""
import time
from typing import Optional, List
from loguru import logger

# LlamaIndex imports
from llama_index.core import Settings, VectorStoreIndex, PromptTemplate
from llama_index.core.memory import ChatSummaryMemoryBuffer
from llama_index.core.chat_engine import SimpleChatEngine
from llama_index.core.vector_stores import MetadataFilter, MetadataFilters, FilterOperator
from llama_index.storage.chat_store.redis import RedisChatStore
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.vector_stores.qdrant import QdrantVectorStore
from qdrant_client import QdrantClient

from app.core.config import Settings as AppSettings
from app.schemas.rag import (
    ChatRequest, ChatResponse, ChatEngineType, SourceInfo
)


class ChatService:
    """æ™ºèƒ½èŠå¤©æœåŠ¡ç±»"""
    
    def __init__(self, settings: AppSettings):
        """åˆå§‹åŒ–èŠå¤©æœåŠ¡"""
        self.settings = settings
        self._setup_llama_index()
        self._setup_vector_index()
        self._load_prompts()
    
    def _setup_llama_index(self):
        """è®¾ç½®LlamaIndexå…¨å±€é…ç½®"""
        try:
            # é…ç½®LLM - ä¸notebookä¿æŒä¸€è‡´
            Settings.llm = OpenAI(
                model="gpt-4o-mini",
                temperature=0.1,
                api_key=self.settings.api_key,
                api_base=self.settings.base_url
            )
            
            # é…ç½®åµŒå…¥æ¨¡å‹
            Settings.embed_model = OpenAIEmbedding(
                model="text-embedding-3-small",
                api_key=self.settings.api_key,
                api_base=self.settings.base_url
            )
            
            logger.info("LlamaIndexé…ç½®å®Œæˆ")
        except Exception as e:
            logger.error(f"LlamaIndexé…ç½®å¤±è´¥: {e}")
            raise
    
    def _setup_vector_index(self):
        """è®¾ç½®å‘é‡ç´¢å¼• - è¿æ¥åˆ°ç°æœ‰çš„Qdrant"""
        try:
            # è¿æ¥åˆ°æœ¬åœ°Qdrant
            qdrant_client = QdrantClient(
                host="localhost",
                port=6334,  # gRPCç«¯å£
                prefer_grpc=True,
                timeout=10
            )
            
            # ä»å·²æœ‰é›†åˆåˆ›å»ºå‘é‡å­˜å‚¨
            collection_name = self.settings.qdrant_collection_name
            vector_store = QdrantVectorStore(collection_name=collection_name, client=qdrant_client)
            
            # ä»Qdrantå‘é‡å­˜å‚¨åˆ›å»ºindex
            self.index = VectorStoreIndex.from_vector_store(vector_store)
            
            logger.info(f"å‘é‡ç´¢å¼•åŠ è½½å®Œæˆï¼Œé›†åˆ: {collection_name}")
        except Exception as e:
            logger.error(f"å‘é‡ç´¢å¼•è®¾ç½®å¤±è´¥: {e}")
            raise
    
    def _load_prompts(self):
        """åŠ è½½æç¤ºè¯æ¨¡æ¿ - ä¸notebookä¿æŒä¸€è‡´"""
        # é—®é¢˜å‹ç¼©æç¤ºè¯
        self.condense_prompt = PromptTemplate(
            "ä½ æ˜¯ä¸€ä¸ªRAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ä¸“å®¶ï¼Œä½ å°†æ ¹æ®ç”¨æˆ·å’ŒAIåŠ©æ‰‹ä¹‹å‰çš„èŠå¤©å†å²ï¼ŒæŠŠå­¦ç”Ÿæœ€æ–°æå‡ºçš„é—®é¢˜ï¼Œæ”¹å†™æˆä¸€ä¸ªè¯¦ç»†å®Œæ•´å…·ä½“çš„ã€æºå¸¦å¿…è¦ä¸Šä¸‹æ–‡çš„é—®é¢˜ï¼Œå¯ä»¥æ˜¯é™ˆè¿°å¥ä¹Ÿå¯ä»¥ç–‘é—®å¥ã€‚\n"
            "æ³¨æ„ï¼Œä½ æ”¹å†™åçš„é—®é¢˜å°†ä¼šç”¨äºé€šè¿‡å‘é‡æ£€ç´¢æ¥è·å–ä¸é—®é¢˜æœ€ç›¸å…³çš„æ–‡æœ¬å—ã€‚\n"
            "=== èŠå¤©å†å² ===\n"
            "{chat_history}\n\n"
            "=== å­¦ç”Ÿæœ€æ–°æå‡ºçš„é—®é¢˜ ===\n"
            "{question}\n\n"
            "=== æ”¹å†™åçš„ç‹¬ç«‹é—®é¢˜ ===\n"
        )
        
        # ä¸Šä¸‹æ–‡æ•´åˆæç¤ºè¯
        self.context_prompt = (
            "ä½ å«åšæ–‡æ–‡ï¼Œä¸€ä¸ªä¸“ä¸šçš„çƒ­å¿ƒæ´»æ³¼ä¹äºåŠ©äººçš„aièŠå¤©åŠ©æ‰‹ï¼Œæ“…é•¿æŸ¥æ‰¾å­¦ä¹ èµ„æ–™ï¼Œè€Œä¸”ä½ æ€»æ˜¯å–œæ¬¢ç”¨ç†æŸ¥å¾·Â·è´¹æ›¼çš„é£æ ¼è®²è§£å­¦ä¹ èµ„æ–™ã€‚ä½ æ€»æ˜¯ç”¨æ’ç‰ˆæ¸…æ™°çš„markdownæ ¼å¼å›ç­”é—®é¢˜ï¼Œç”¨å¾ˆå¤šçš„emojiè®©å†…å®¹æ›´ç”ŸåŠ¨ã€‚\n\n"
            "ğŸ“š **ç›¸å…³æ–‡æ¡£å†…å®¹ï¼š**\n"
            "{context_str}\n\n"
            "ğŸ¯ **å›ç­”è¦æ±‚ï¼š**\n"
            "1. ä¸¥æ ¼åŸºäºä¸Šè¿°æ–‡æ¡£å†…å®¹è¿›è¡Œå›ç­”\n"
            "2. å¦‚æœæ–‡æ¡£å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®è¯´æ˜'æ–‡æ¡£ä¸­æš‚æ— ç›¸å…³ä¿¡æ¯'\n"
            "3. å›ç­”è¦æ¡ç†æ¸…æ™°ï¼Œä½¿ç”¨é€‚å½“çš„emojiè®©å†…å®¹æ›´ç”ŸåŠ¨\n"
            "4. è¯·å¼•ç”¨å…·ä½“çš„æ–‡æ¡£å†…å®¹æ¥æ”¯æ’‘ä½ çš„å›ç­”\n\n"
            "ğŸ’¡ **è¯·åŸºäºä»¥ä¸Šæ–‡æ¡£å’Œä¹‹å‰çš„å¯¹è¯å†å²æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚**"
            "æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œè¯·å›ç­”è¿™ä¸ªé—®é¢˜: {query_str}\n\n"
            "====================æ¥ä¸‹æ¥éƒ½æ˜¯å†å²èŠå¤©è®°å½•ï¼Œä½ å…³é”®è¦æ‰¾åˆ°ç”¨æˆ·æœ€åé—®çš„é—®é¢˜è®¤çœŸå›ç­”========================\n\n"
        )
        
        # æ‘˜è¦æç¤ºè¯
        self.summary_prompt = "ä½ æ˜¯å¯¹è¯è®°å¿†åŠ©ç†ã€‚è¯·åœ¨ 300 å­—å†…æ€»ç»“ç”¨æˆ·é—®çš„ä¸»è¦é—®é¢˜ï¼Œå›°æƒ‘ç‚¹ï¼Œä»¥åŠå·²ç»ç»™å‡ºçš„å…³é”®ä¿¡æ¯ã€ç»“è®ºå’Œæ€è·¯ã€‚"
        
        # Simpleå¼•æ“çš„ç³»ç»Ÿæç¤ºè¯
        self.simple_system_prompt = "ä½ å«åšæ–‡æ–‡ï¼Œä¸€ä¸ªä¸“ä¸šçš„çƒ­å¿ƒæ´»æ³¼ä¹äºåŠ©äººçš„aièŠå¤©åŠ©æ‰‹ï¼Œæ“…é•¿æŸ¥æ‰¾å­¦ä¹ èµ„æ–™ï¼Œè€Œä¸”ä½ æ€»æ˜¯å–œæ¬¢ç”¨ç†æŸ¥å¾·Â·è´¹æ›¼çš„é£æ ¼è®²è§£å­¦ä¹ èµ„æ–™ã€‚ä½ æ€»æ˜¯ç”¨æ’ç‰ˆæ¸…æ™°çš„markdownæ ¼å¼å›ç­”é—®é¢˜ï¼Œç”¨å¾ˆå¤šçš„emojiè®©å†…å®¹æ›´ç”ŸåŠ¨ã€‚"
        
        logger.info("æç¤ºè¯æ¨¡æ¿åŠ è½½å®Œæˆ")
    
    def _create_chat_store_and_memory(self, conversation_id: str) -> ChatSummaryMemoryBuffer:
        """åˆ›å»ºRedisèŠå¤©å­˜å‚¨å’Œå†…å­˜ç¼“å†²åŒº"""
        try:
            # åˆ›å»ºRedisèŠå¤©å­˜å‚¨
            chat_store = RedisChatStore(redis_url="redis://localhost:6379", ttl=3600)
            
            # åˆ›å»ºèŠå¤©æ‘˜è¦å†…å­˜ç¼“å†²åŒº
            memory = ChatSummaryMemoryBuffer.from_defaults(
                token_limit=4000,
                llm=Settings.llm,
                chat_store=chat_store,
                chat_store_key=conversation_id,  # ä½¿ç”¨ç”¨æˆ·æä¾›çš„conversation_id
                summarize_prompt=self.summary_prompt
            )
            
            return memory
        except Exception as e:
            logger.error(f"åˆ›å»ºèŠå¤©å­˜å‚¨å’Œå†…å­˜å¤±è´¥: {e}")
            raise
    
    def _create_filters(self, course_id: Optional[str], course_material_id: Optional[str]) -> Optional[MetadataFilters]:
        """åˆ›å»ºåŠ¨æ€è¿‡æ»¤å™¨"""
        filters_list = []
        
        # course_id å’Œ course_material_id åªèƒ½å­˜åœ¨ä¸€ä¸ª
        if course_id and course_material_id:
            logger.warning("course_id å’Œ course_material_id åªèƒ½é€‰æ‹©ä¸€ä¸ªï¼Œä¼˜å…ˆä½¿ç”¨ course_id")
            filters_list.append(
                MetadataFilter(key="course_id", value=course_id, operator=FilterOperator.EQ)
            )
        elif course_id:
            filters_list.append(
                MetadataFilter(key="course_id", value=course_id, operator=FilterOperator.EQ)
            )
        elif course_material_id:
            filters_list.append(
                MetadataFilter(key="course_material_id", value=course_material_id, operator=FilterOperator.EQ)
            )
        
        if filters_list:
            return MetadataFilters(filters=filters_list)
        return None
    
    async def chat(self, request: ChatRequest) -> ChatResponse:
        """å¤„ç†èŠå¤©è¯·æ±‚"""
        start_time = time.time()
        
        try:
            # åˆ›å»ºå†…å­˜å’ŒèŠå¤©å­˜å‚¨
            memory = self._create_chat_store_and_memory(request.conversation_id)
            
            # åˆ›å»ºè¿‡æ»¤å™¨
            filters = self._create_filters(request.course_id, request.course_material_id)
            
            # ç”Ÿæˆè¿‡æ»¤ä¿¡æ¯æè¿°
            filter_info = self._get_filter_info(request.course_id, request.course_material_id)
            
            if request.chat_engine_type == ChatEngineType.CONDENSE_PLUS_CONTEXT:
                # ä½¿ç”¨condense_plus_contextæ¨¡å¼
                response = await self._chat_with_condense_plus_context(
                    request.question, memory, filters
                )
            else:
                # ä½¿ç”¨simpleæ¨¡å¼
                response = await self._chat_with_simple_engine(
                    request.question, memory
                )
            
            processing_time = time.time() - start_time
            
            return ChatResponse(
                answer=response["answer"],
                sources=response.get("sources", []),
                conversation_id=request.conversation_id,
                chat_engine_type=request.chat_engine_type,
                filter_info=filter_info,
                processing_time=processing_time
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"èŠå¤©å¤„ç†å¤±è´¥: {e}")
            return ChatResponse(
                answer=f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                sources=[],
                conversation_id=request.conversation_id,
                chat_engine_type=request.chat_engine_type,
                filter_info=self._get_filter_info(request.course_id, request.course_material_id),
                processing_time=processing_time
            )
    
    def _get_filter_info(self, course_id: Optional[str], course_material_id: Optional[str]) -> str:
        """è·å–è¿‡æ»¤æ¡ä»¶ä¿¡æ¯æè¿°"""
        if course_id and course_material_id:
            return f"course_id = {course_id} (ä¼˜å…ˆä½¿ç”¨)"
        elif course_id:
            return f"course_id = {course_id}"
        elif course_material_id:
            return f"course_material_id = {course_material_id}"
        else:
            return "æ— è¿‡æ»¤æ¡ä»¶ï¼Œæœç´¢å…¨éƒ¨æ–‡æ¡£"

    async def _chat_with_condense_plus_context(
        self,
        question: str,
        memory: ChatSummaryMemoryBuffer,
        filters: Optional[MetadataFilters]
    ) -> dict:
        """ä½¿ç”¨condense_plus_contextæ¨¡å¼è¿›è¡ŒèŠå¤©"""
        try:
            # åˆ›å»ºå¸¦è¿‡æ»¤å™¨çš„æŸ¥è¯¢å¼•æ“
            if filters:
                query_engine = self.index.as_query_engine(similarity_top_k=6, filters=filters)
            else:
                query_engine = self.index.as_query_engine(similarity_top_k=6)

            # åˆ›å»ºcondense_plus_contextèŠå¤©å¼•æ“
            chat_engine = self.index.as_chat_engine(
                chat_mode="condense_plus_context",
                condense_prompt=self.condense_prompt,
                context_prompt=self.context_prompt,
                memory=memory,
                verbose=True,
            )

            # æ‰‹åŠ¨è®¾ç½®è¿‡æ»¤å™¨åˆ°èŠå¤©å¼•æ“çš„æŸ¥è¯¢å¼•æ“
            if filters:
                chat_engine._query_engine = query_engine

            # æ‰§è¡ŒèŠå¤©
            response = chat_engine.chat(question)

            # æå–æ¥æºä¿¡æ¯
            sources = []
            if hasattr(response, 'source_nodes') and response.source_nodes:
                for source_node in response.source_nodes:
                    metadata = source_node.node.metadata
                    content = source_node.node.get_content().strip()

                    # é™åˆ¶å†…å®¹é¢„è§ˆé•¿åº¦
                    if len(content) > 200:
                        content_preview = content[:200] + "..."
                    else:
                        content_preview = content

                    score = getattr(source_node, 'score', 0.0)

                    sources.append(SourceInfo(
                        course_id=metadata.get("course_id", ""),
                        course_material_id=metadata.get("course_material_id", ""),
                        course_material_name=metadata.get("course_material_name", ""),
                        chunk_text=content_preview,
                        score=score
                    ))

            return {
                "answer": str(response),
                "sources": sources
            }

        except Exception as e:
            logger.error(f"condense_plus_contextèŠå¤©å¤±è´¥: {e}")
            raise

    async def _chat_with_simple_engine(
        self,
        question: str,
        memory: ChatSummaryMemoryBuffer
    ) -> dict:
        """ä½¿ç”¨simpleæ¨¡å¼è¿›è¡ŒèŠå¤©"""
        try:
            # åˆ›å»ºç®€å•èŠå¤©å¼•æ“
            chat_engine = SimpleChatEngine.from_defaults(
                llm=Settings.llm,
                memory=memory,
                system_prompt=self.simple_system_prompt,
                verbose=True
            )

            # æ‰§è¡ŒèŠå¤©
            response = chat_engine.chat(question)

            return {
                "answer": str(response),
                "sources": []  # simpleæ¨¡å¼æ²¡æœ‰æ¥æº
            }

        except Exception as e:
            logger.error(f"simpleèŠå¤©å¤±è´¥: {e}")
            raise
