[
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "rmtree",
        "importPath": "shutil",
        "description": "shutil",
        "isExtraImport": true,
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "rmtree",
        "importPath": "shutil",
        "description": "shutil",
        "isExtraImport": true,
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "sysconfig",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sysconfig",
        "description": "sysconfig",
        "detail": "sysconfig",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "winreg",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "winreg",
        "description": "winreg",
        "detail": "winreg",
        "documentation": {}
    },
    {
        "label": "site",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "site",
        "description": "site",
        "detail": "site",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Depends",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "status",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Depends",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "status",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "UploadFile",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "File",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Form",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Depends",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Depends",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "UploadFile",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "File",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Form",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Depends",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "status",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "UploadFile",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "File",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Form",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Depends",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "status",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "UploadFile",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "File",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Form",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Depends",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "UploadFile",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "UploadFile",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "UploadFile",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Request",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "loguru",
        "description": "loguru",
        "isExtraImport": true,
        "detail": "loguru",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "loguru",
        "description": "loguru",
        "isExtraImport": true,
        "detail": "loguru",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "loguru",
        "description": "loguru",
        "isExtraImport": true,
        "detail": "loguru",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "loguru",
        "description": "loguru",
        "isExtraImport": true,
        "detail": "loguru",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "loguru",
        "description": "loguru",
        "isExtraImport": true,
        "detail": "loguru",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "loguru",
        "description": "loguru",
        "isExtraImport": true,
        "detail": "loguru",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "loguru",
        "description": "loguru",
        "isExtraImport": true,
        "detail": "loguru",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "loguru",
        "description": "loguru",
        "isExtraImport": true,
        "detail": "loguru",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "loguru",
        "description": "loguru",
        "isExtraImport": true,
        "detail": "loguru",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "loguru",
        "description": "loguru",
        "isExtraImport": true,
        "detail": "loguru",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "loguru",
        "description": "loguru",
        "isExtraImport": true,
        "detail": "loguru",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "loguru",
        "description": "loguru",
        "isExtraImport": true,
        "detail": "loguru",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "loguru",
        "description": "loguru",
        "isExtraImport": true,
        "detail": "loguru",
        "documentation": {}
    },
    {
        "label": "get_settings",
        "importPath": "app.core.config",
        "description": "app.core.config",
        "isExtraImport": true,
        "detail": "app.core.config",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "app.core.config",
        "description": "app.core.config",
        "isExtraImport": true,
        "detail": "app.core.config",
        "documentation": {}
    },
    {
        "label": "get_settings",
        "importPath": "app.core.config",
        "description": "app.core.config",
        "isExtraImport": true,
        "detail": "app.core.config",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "app.core.config",
        "description": "app.core.config",
        "isExtraImport": true,
        "detail": "app.core.config",
        "documentation": {}
    },
    {
        "label": "get_settings",
        "importPath": "app.core.config",
        "description": "app.core.config",
        "isExtraImport": true,
        "detail": "app.core.config",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "app.core.config",
        "description": "app.core.config",
        "isExtraImport": true,
        "detail": "app.core.config",
        "documentation": {}
    },
    {
        "label": "get_settings",
        "importPath": "app.core.config",
        "description": "app.core.config",
        "isExtraImport": true,
        "detail": "app.core.config",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "app.core.config",
        "description": "app.core.config",
        "isExtraImport": true,
        "detail": "app.core.config",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "app.core.config",
        "description": "app.core.config",
        "isExtraImport": true,
        "detail": "app.core.config",
        "documentation": {}
    },
    {
        "label": "get_settings",
        "importPath": "app.core.config",
        "description": "app.core.config",
        "isExtraImport": true,
        "detail": "app.core.config",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "app.core.config",
        "description": "app.core.config",
        "isExtraImport": true,
        "detail": "app.core.config",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "app.core.config",
        "description": "app.core.config",
        "isExtraImport": true,
        "detail": "app.core.config",
        "documentation": {}
    },
    {
        "label": "get_settings",
        "importPath": "app.core.config",
        "description": "app.core.config",
        "isExtraImport": true,
        "detail": "app.core.config",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "app.core.config",
        "description": "app.core.config",
        "isExtraImport": true,
        "detail": "app.core.config",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "app.core.config",
        "description": "app.core.config",
        "isExtraImport": true,
        "detail": "app.core.config",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "app.core.config",
        "description": "app.core.config",
        "isExtraImport": true,
        "detail": "app.core.config",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "app.core.config",
        "description": "app.core.config",
        "isExtraImport": true,
        "detail": "app.core.config",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "app.core.config",
        "description": "app.core.config",
        "isExtraImport": true,
        "detail": "app.core.config",
        "documentation": {}
    },
    {
        "label": "get_settings",
        "importPath": "app.core.config",
        "description": "app.core.config",
        "isExtraImport": true,
        "detail": "app.core.config",
        "documentation": {}
    },
    {
        "label": "ChatService",
        "importPath": "app.services.chat_service",
        "description": "app.services.chat_service",
        "isExtraImport": true,
        "detail": "app.services.chat_service",
        "documentation": {}
    },
    {
        "label": "ChatRequest",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "ChatEngineType",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "ChatRequest",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "ChatEngineType",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "IndexRequest",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "IndexResponse",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "QueryRequest",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "QueryResponse",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "CollectionListResponse",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "DeleteCollectionResponse",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "DocumentMetadata",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "IndexRequest",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "IndexResponse",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "CollectionInfo",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "DocumentMetadata",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "DeleteCollectionResponse",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "CollectionInfo",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "ChatRequest",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "ChatEngineType",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "SourceInfo",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "IndexRequest",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "IndexResponse",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "QueryRequest",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "QueryResponse",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "ChatMemory",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "SourceInfo",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "ChatMode",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "ChatRequest",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "ChatEngineType",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "SourceInfo",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "IndexRequest",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "IndexResponse",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "CollectionInfo",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "ChatRequest",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "ChatEngineType",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "SourceInfo",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "IndexRequest",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "IndexResponse",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "QueryRequest",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "QueryResponse",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "ChatMemory",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "SourceInfo",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "ChatMode",
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "isExtraImport": true,
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "typing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "typing",
        "description": "typing",
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Awaitable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Coroutine",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Coroutine",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Awaitable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Annotated",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AsyncGenerator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AsyncGenerator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Coroutine",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "overload",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "ClassVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "ClassVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Coroutine",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "ClassVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Coroutine",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generic",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Protocol",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "runtime_checkable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generic",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generic",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generic",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generic",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generic",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "ClassVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generic",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AsyncGenerator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AsyncGenerator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Protocol",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "runtime_checkable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generic",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypedDict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Protocol",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "runtime_checkable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generic",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generic",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generic",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AsyncGenerator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generic",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AsyncGenerator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AsyncGenerator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AsyncGenerator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "overload",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generic",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Awaitable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Awaitable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "get_origin",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Awaitable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "get_origin",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "get_args",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Protocol",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "runtime_checkable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Mapping",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Coroutine",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Annotated",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AsyncGenerator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generic",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TYPE_CHECKING",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AsyncGenerator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Protocol",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "runtime_checkable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "AsyncGenerator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Coroutine",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "ConversationService",
        "importPath": "app.services.rag.conversation_service",
        "description": "app.services.rag.conversation_service",
        "isExtraImport": true,
        "detail": "app.services.rag.conversation_service",
        "documentation": {}
    },
    {
        "label": "get_rag_config_manager",
        "importPath": "app.services.rag.rag_settings",
        "description": "app.services.rag.rag_settings",
        "isExtraImport": true,
        "detail": "app.services.rag.rag_settings",
        "documentation": {}
    },
    {
        "label": "RAGConfigManager",
        "importPath": "app.services.rag.rag_settings",
        "description": "app.services.rag.rag_settings",
        "isExtraImport": true,
        "detail": "app.services.rag.rag_settings",
        "documentation": {}
    },
    {
        "label": "get_rag_config_manager",
        "importPath": "app.services.rag.rag_settings",
        "description": "app.services.rag.rag_settings",
        "isExtraImport": true,
        "detail": "app.services.rag.rag_settings",
        "documentation": {}
    },
    {
        "label": "RAGConfigManager",
        "importPath": "app.services.rag.rag_settings",
        "description": "app.services.rag.rag_settings",
        "isExtraImport": true,
        "detail": "app.services.rag.rag_settings",
        "documentation": {}
    },
    {
        "label": "RAGConfigManager",
        "importPath": "app.services.rag.rag_settings",
        "description": "app.services.rag.rag_settings",
        "isExtraImport": true,
        "detail": "app.services.rag.rag_settings",
        "documentation": {}
    },
    {
        "label": "RAGConfigManager",
        "importPath": "app.services.rag.rag_settings",
        "description": "app.services.rag.rag_settings",
        "isExtraImport": true,
        "detail": "app.services.rag.rag_settings",
        "documentation": {}
    },
    {
        "label": "JSONResponse",
        "importPath": "fastapi.responses",
        "description": "fastapi.responses",
        "isExtraImport": true,
        "detail": "fastapi.responses",
        "documentation": {}
    },
    {
        "label": "JSONResponse",
        "importPath": "fastapi.responses",
        "description": "fastapi.responses",
        "isExtraImport": true,
        "detail": "fastapi.responses",
        "documentation": {}
    },
    {
        "label": "JSONResponse",
        "importPath": "fastapi.responses",
        "description": "fastapi.responses",
        "isExtraImport": true,
        "detail": "fastapi.responses",
        "documentation": {}
    },
    {
        "label": "JSONResponse",
        "importPath": "fastapi.responses",
        "description": "fastapi.responses",
        "isExtraImport": true,
        "detail": "fastapi.responses",
        "documentation": {}
    },
    {
        "label": "JSONResponse",
        "importPath": "fastapi.responses",
        "description": "fastapi.responses",
        "isExtraImport": true,
        "detail": "fastapi.responses",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "PurePosixPath",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "timezone",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "timezone",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "RAGService",
        "importPath": "app.services.rag_service",
        "description": "app.services.rag_service",
        "isExtraImport": true,
        "detail": "app.services.rag_service",
        "documentation": {}
    },
    {
        "label": "DocumentIndexingService",
        "importPath": "app.services.rag.document_indexing_service",
        "description": "app.services.rag.document_indexing_service",
        "isExtraImport": true,
        "detail": "app.services.rag.document_indexing_service",
        "documentation": {}
    },
    {
        "label": "pydantic",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pydantic",
        "description": "pydantic",
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "validator",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "validator",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "validator",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "AnyUrl",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BeforeValidator",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "FilePath",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "GetCoreSchemaHandler",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "GetJsonSchemaHandler",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "PlainSerializer",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "PrivateAttr",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Secret",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "SecretStr",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "SerializationInfo",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "SerializeAsAny",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "SerializerFunctionWrapHandler",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "StrictFloat",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "StrictInt",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "StrictStr",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "TypeAdapter",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "ValidationError",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "ValidationInfo",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "WithJsonSchema",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "WrapSerializer",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "create_model",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "field_serializer",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "field_validator",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "model_serializer",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "model_validator",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "ValidationError",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "AnyUrl",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "pydantic_settings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pydantic_settings",
        "description": "pydantic_settings",
        "detail": "pydantic_settings",
        "documentation": {}
    },
    {
        "label": "BaseSettings",
        "importPath": "pydantic_settings",
        "description": "pydantic_settings",
        "isExtraImport": true,
        "detail": "pydantic_settings",
        "documentation": {}
    },
    {
        "label": "BaseSettings",
        "importPath": "pydantic_settings",
        "description": "pydantic_settings",
        "isExtraImport": true,
        "detail": "pydantic_settings",
        "documentation": {}
    },
    {
        "label": "BaseSettings",
        "importPath": "pydantic_settings",
        "description": "pydantic_settings",
        "isExtraImport": true,
        "detail": "pydantic_settings",
        "documentation": {}
    },
    {
        "label": "SettingsConfigDict",
        "importPath": "pydantic_settings",
        "description": "pydantic_settings",
        "isExtraImport": true,
        "detail": "pydantic_settings",
        "documentation": {}
    },
    {
        "label": "aiofiles",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "aiofiles",
        "description": "aiofiles",
        "detail": "aiofiles",
        "documentation": {}
    },
    {
        "label": "uuid",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uuid",
        "description": "uuid",
        "detail": "uuid",
        "documentation": {}
    },
    {
        "label": "UUID",
        "importPath": "uuid",
        "description": "uuid",
        "isExtraImport": true,
        "detail": "uuid",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "QdrantClient",
        "importPath": "qdrant_client",
        "description": "qdrant_client",
        "isExtraImport": true,
        "detail": "qdrant_client",
        "documentation": {}
    },
    {
        "label": "QdrantClient",
        "importPath": "qdrant_client",
        "description": "qdrant_client",
        "isExtraImport": true,
        "detail": "qdrant_client",
        "documentation": {}
    },
    {
        "label": "QdrantClient",
        "importPath": "qdrant_client",
        "description": "qdrant_client",
        "isExtraImport": true,
        "detail": "qdrant_client",
        "documentation": {}
    },
    {
        "label": "QdrantClient",
        "importPath": "qdrant_client",
        "description": "qdrant_client",
        "isExtraImport": true,
        "detail": "qdrant_client",
        "documentation": {}
    },
    {
        "label": "models",
        "importPath": "qdrant_client.http",
        "description": "qdrant_client.http",
        "isExtraImport": true,
        "detail": "qdrant_client.http",
        "documentation": {}
    },
    {
        "label": "Distance",
        "importPath": "qdrant_client.http.models",
        "description": "qdrant_client.http.models",
        "isExtraImport": true,
        "detail": "qdrant_client.http.models",
        "documentation": {}
    },
    {
        "label": "VectorParams",
        "importPath": "qdrant_client.http.models",
        "description": "qdrant_client.http.models",
        "isExtraImport": true,
        "detail": "qdrant_client.http.models",
        "documentation": {}
    },
    {
        "label": "PointStruct",
        "importPath": "qdrant_client.http.models",
        "description": "qdrant_client.http.models",
        "isExtraImport": true,
        "detail": "qdrant_client.http.models",
        "documentation": {}
    },
    {
        "label": "PointStruct",
        "importPath": "qdrant_client.http.models",
        "description": "qdrant_client.http.models",
        "isExtraImport": true,
        "detail": "qdrant_client.http.models",
        "documentation": {}
    },
    {
        "label": "PointStruct",
        "importPath": "qdrant_client.http.models",
        "description": "qdrant_client.http.models",
        "isExtraImport": true,
        "detail": "qdrant_client.http.models",
        "documentation": {}
    },
    {
        "label": "PointStruct",
        "importPath": "qdrant_client.http.models",
        "description": "qdrant_client.http.models",
        "isExtraImport": true,
        "detail": "qdrant_client.http.models",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "auto",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "SummaryIndex",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "MockEmbedding",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "MockEmbedding",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "MockEmbedding",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "PropertyGraphIndex",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "MockEmbedding",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "set_global_tokenizer",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "SQLDatabase",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "ChatSummaryMemoryBuffer",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "ChatSummaryMemoryBuffer",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "ChatSummaryMemoryBuffer",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "ChatSummaryMemoryBuffer",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "ChatSummaryMemoryBuffer",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "BaseMemory",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "ChatMemoryBuffer",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "BaseMemory",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "BaseMemory",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "BaseMemory",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "ChatMemoryBuffer",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "BaseMemory",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "BaseMemory",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "ChatMemoryBuffer",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "BaseMemory",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "ChatMemoryBuffer",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "BaseMemory",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "ChatMemoryBuffer",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "BaseMemory",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "ChatMemoryBuffer",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "BaseMemory",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "ChatMemoryBuffer",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "VectorMemory",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "SimpleComposableMemory",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "ChatMemoryBuffer",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "VectorMemory",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "BaseMemory",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "Memory",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "BaseMemory",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "ChatMemoryBuffer",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "ChatMemoryBuffer",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "ChatMemoryBuffer",
        "importPath": "llama_index.core.memory",
        "description": "llama_index.core.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory",
        "documentation": {}
    },
    {
        "label": "SimpleChatEngine",
        "importPath": "llama_index.core.chat_engine",
        "description": "llama_index.core.chat_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine",
        "documentation": {}
    },
    {
        "label": "SimpleChatEngine",
        "importPath": "llama_index.core.chat_engine",
        "description": "llama_index.core.chat_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine",
        "documentation": {}
    },
    {
        "label": "CondensePlusContextChatEngine",
        "importPath": "llama_index.core.chat_engine",
        "description": "llama_index.core.chat_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine",
        "documentation": {}
    },
    {
        "label": "SimpleChatEngine",
        "importPath": "llama_index.core.chat_engine",
        "description": "llama_index.core.chat_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine",
        "documentation": {}
    },
    {
        "label": "SimpleChatEngine",
        "importPath": "llama_index.core.chat_engine",
        "description": "llama_index.core.chat_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine",
        "documentation": {}
    },
    {
        "label": "SimpleChatEngine",
        "importPath": "llama_index.core.chat_engine",
        "description": "llama_index.core.chat_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine",
        "documentation": {}
    },
    {
        "label": "CondensePlusContextChatEngine",
        "importPath": "llama_index.core.chat_engine",
        "description": "llama_index.core.chat_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine",
        "documentation": {}
    },
    {
        "label": "MetadataFilter",
        "importPath": "llama_index.core.vector_stores",
        "description": "llama_index.core.vector_stores",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores",
        "documentation": {}
    },
    {
        "label": "MetadataFilters",
        "importPath": "llama_index.core.vector_stores",
        "description": "llama_index.core.vector_stores",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores",
        "documentation": {}
    },
    {
        "label": "FilterOperator",
        "importPath": "llama_index.core.vector_stores",
        "description": "llama_index.core.vector_stores",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores",
        "documentation": {}
    },
    {
        "label": "MetadataFilter",
        "importPath": "llama_index.core.vector_stores",
        "description": "llama_index.core.vector_stores",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores",
        "documentation": {}
    },
    {
        "label": "MetadataFilters",
        "importPath": "llama_index.core.vector_stores",
        "description": "llama_index.core.vector_stores",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores",
        "documentation": {}
    },
    {
        "label": "FilterOperator",
        "importPath": "llama_index.core.vector_stores",
        "description": "llama_index.core.vector_stores",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores",
        "documentation": {}
    },
    {
        "label": "MetadataFilter",
        "importPath": "llama_index.core.vector_stores",
        "description": "llama_index.core.vector_stores",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores",
        "documentation": {}
    },
    {
        "label": "MetadataFilters",
        "importPath": "llama_index.core.vector_stores",
        "description": "llama_index.core.vector_stores",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores",
        "documentation": {}
    },
    {
        "label": "FilterOperator",
        "importPath": "llama_index.core.vector_stores",
        "description": "llama_index.core.vector_stores",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores",
        "documentation": {}
    },
    {
        "label": "RedisChatStore",
        "importPath": "llama_index.storage.chat_store.redis",
        "description": "llama_index.storage.chat_store.redis",
        "isExtraImport": true,
        "detail": "llama_index.storage.chat_store.redis",
        "documentation": {}
    },
    {
        "label": "RedisChatStore",
        "importPath": "llama_index.storage.chat_store.redis",
        "description": "llama_index.storage.chat_store.redis",
        "isExtraImport": true,
        "detail": "llama_index.storage.chat_store.redis",
        "documentation": {}
    },
    {
        "label": "RedisChatStore",
        "importPath": "llama_index.storage.chat_store.redis",
        "description": "llama_index.storage.chat_store.redis",
        "isExtraImport": true,
        "detail": "llama_index.storage.chat_store.redis",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "llama_index.llms.openai",
        "description": "llama_index.llms.openai",
        "isExtraImport": true,
        "detail": "llama_index.llms.openai",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "llama_index.llms.openai",
        "description": "llama_index.llms.openai",
        "isExtraImport": true,
        "detail": "llama_index.llms.openai",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "llama_index.llms.openai",
        "description": "llama_index.llms.openai",
        "isExtraImport": true,
        "detail": "llama_index.llms.openai",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "llama_index.llms.openai",
        "description": "llama_index.llms.openai",
        "isExtraImport": true,
        "detail": "llama_index.llms.openai",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "llama_index.llms.openai",
        "description": "llama_index.llms.openai",
        "isExtraImport": true,
        "detail": "llama_index.llms.openai",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbedding",
        "importPath": "llama_index.embeddings.openai",
        "description": "llama_index.embeddings.openai",
        "isExtraImport": true,
        "detail": "llama_index.embeddings.openai",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbedding",
        "importPath": "llama_index.embeddings.openai",
        "description": "llama_index.embeddings.openai",
        "isExtraImport": true,
        "detail": "llama_index.embeddings.openai",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbedding",
        "importPath": "llama_index.embeddings.openai",
        "description": "llama_index.embeddings.openai",
        "isExtraImport": true,
        "detail": "llama_index.embeddings.openai",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbedding",
        "importPath": "llama_index.embeddings.openai",
        "description": "llama_index.embeddings.openai",
        "isExtraImport": true,
        "detail": "llama_index.embeddings.openai",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbedding",
        "importPath": "llama_index.embeddings.openai",
        "description": "llama_index.embeddings.openai",
        "isExtraImport": true,
        "detail": "llama_index.embeddings.openai",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbedding",
        "importPath": "llama_index.embeddings.openai",
        "description": "llama_index.embeddings.openai",
        "isExtraImport": true,
        "detail": "llama_index.embeddings.openai",
        "documentation": {}
    },
    {
        "label": "QdrantVectorStore",
        "importPath": "llama_index.vector_stores.qdrant",
        "description": "llama_index.vector_stores.qdrant",
        "isExtraImport": true,
        "detail": "llama_index.vector_stores.qdrant",
        "documentation": {}
    },
    {
        "label": "QdrantVectorStore",
        "importPath": "llama_index.vector_stores.qdrant",
        "description": "llama_index.vector_stores.qdrant",
        "isExtraImport": true,
        "detail": "llama_index.vector_stores.qdrant",
        "documentation": {}
    },
    {
        "label": "QdrantVectorStore",
        "importPath": "llama_index.vector_stores.qdrant",
        "description": "llama_index.vector_stores.qdrant",
        "isExtraImport": true,
        "detail": "llama_index.vector_stores.qdrant",
        "documentation": {}
    },
    {
        "label": "QdrantVectorStore",
        "importPath": "llama_index.vector_stores.qdrant",
        "description": "llama_index.vector_stores.qdrant",
        "isExtraImport": true,
        "detail": "llama_index.vector_stores.qdrant",
        "documentation": {}
    },
    {
        "label": "QdrantVectorStore",
        "importPath": "llama_index.vector_stores.qdrant",
        "description": "llama_index.vector_stores.qdrant",
        "isExtraImport": true,
        "detail": "llama_index.vector_stores.qdrant",
        "documentation": {}
    },
    {
        "label": "SentenceSplitter",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "SentenceSplitter",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "SentenceSplitter",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "SentenceSplitter",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "CodeSplitter",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "HTMLNodeParser",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "JSONNodeParser",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "MarkdownNodeParser",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "SentenceSplitter",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "SimpleFileNodeParser",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "TokenTextSplitter",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "MarkdownElementNodeParser",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "NodeParser",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "NodeParser",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "SentenceSplitter",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "TextSplitter",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "NodeParser",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "SentenceSplitter",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "SentenceSplitter",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "MarkdownElementNodeParser",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "SentenceSplitter",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "SimpleFileNodeParser",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "HierarchicalNodeParser",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "get_child_nodes",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "get_deeper_nodes",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "get_leaf_nodes",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "get_root_nodes",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "NodeParser",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "QdrantRepository",
        "importPath": "app.repositories.rag_repository",
        "description": "app.repositories.rag_repository",
        "isExtraImport": true,
        "detail": "app.repositories.rag_repository",
        "documentation": {}
    },
    {
        "label": "QdrantRepository",
        "importPath": "app.repositories.rag_repository",
        "description": "app.repositories.rag_repository",
        "isExtraImport": true,
        "detail": "app.repositories.rag_repository",
        "documentation": {}
    },
    {
        "label": "QdrantRepository",
        "importPath": "app.repositories.rag_repository",
        "description": "app.repositories.rag_repository",
        "isExtraImport": true,
        "detail": "app.repositories.rag_repository",
        "documentation": {}
    },
    {
        "label": "AsyncOpenAI",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "sha256",
        "importPath": "hashlib",
        "description": "hashlib",
        "isExtraImport": true,
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "sha256",
        "importPath": "hashlib",
        "description": "hashlib",
        "isExtraImport": true,
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "aiofiles.os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "aiofiles.os",
        "description": "aiofiles.os",
        "detail": "aiofiles.os",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "string",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "string",
        "description": "string",
        "detail": "string",
        "documentation": {}
    },
    {
        "label": "contextlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "contextlib",
        "description": "contextlib",
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "asynccontextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "asynccontextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "contextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "contextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "contextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "CORSMiddleware",
        "importPath": "fastapi.middleware.cors",
        "description": "fastapi.middleware.cors",
        "isExtraImport": true,
        "detail": "fastapi.middleware.cors",
        "documentation": {}
    },
    {
        "label": "RequestValidationError",
        "importPath": "fastapi.exceptions",
        "description": "fastapi.exceptions",
        "isExtraImport": true,
        "detail": "fastapi.exceptions",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "starlette.exceptions",
        "description": "starlette.exceptions",
        "isExtraImport": true,
        "detail": "starlette.exceptions",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABCMeta",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "CONTEXT_REACT_CHAT_SYSTEM_HEADER",
        "importPath": "llama_index.core.agent.react.prompts",
        "description": "llama_index.core.agent.react.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.react.prompts",
        "documentation": {}
    },
    {
        "label": "REACT_CHAT_SYSTEM_HEADER",
        "importPath": "llama_index.core.agent.react.prompts",
        "description": "llama_index.core.agent.react.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.react.prompts",
        "documentation": {}
    },
    {
        "label": "BaseReasoningStep",
        "importPath": "llama_index.core.agent.react.types",
        "description": "llama_index.core.agent.react.types",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.react.types",
        "documentation": {}
    },
    {
        "label": "ObservationReasoningStep",
        "importPath": "llama_index.core.agent.react.types",
        "description": "llama_index.core.agent.react.types",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.react.types",
        "documentation": {}
    },
    {
        "label": "ActionReasoningStep",
        "importPath": "llama_index.core.agent.react.types",
        "description": "llama_index.core.agent.react.types",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.react.types",
        "documentation": {}
    },
    {
        "label": "BaseReasoningStep",
        "importPath": "llama_index.core.agent.react.types",
        "description": "llama_index.core.agent.react.types",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.react.types",
        "documentation": {}
    },
    {
        "label": "ResponseReasoningStep",
        "importPath": "llama_index.core.agent.react.types",
        "description": "llama_index.core.agent.react.types",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.react.types",
        "documentation": {}
    },
    {
        "label": "ActionReasoningStep",
        "importPath": "llama_index.core.agent.react.types",
        "description": "llama_index.core.agent.react.types",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.react.types",
        "documentation": {}
    },
    {
        "label": "BaseReasoningStep",
        "importPath": "llama_index.core.agent.react.types",
        "description": "llama_index.core.agent.react.types",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.react.types",
        "documentation": {}
    },
    {
        "label": "ObservationReasoningStep",
        "importPath": "llama_index.core.agent.react.types",
        "description": "llama_index.core.agent.react.types",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.react.types",
        "documentation": {}
    },
    {
        "label": "ResponseReasoningStep",
        "importPath": "llama_index.core.agent.react.types",
        "description": "llama_index.core.agent.react.types",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.react.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseAsyncGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponseAsyncGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponseGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "LLMMetadata",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "TextBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseAsyncGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponseAsyncGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponseGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ImageBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseAsyncGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseAsyncGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseAsyncGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseAsyncGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "LLMMetadata",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseAsyncGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponseAsyncGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponseGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseAsyncGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponseAsyncGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseAsyncGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseAsyncGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponseAsyncGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponseGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponseGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "LLMMetadata",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseAsyncGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponseGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "LLMMetadata",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "LLMMetadata",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ContentBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "TextBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "TextBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ContentBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "TextBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "AudioBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ImageBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "DocumentBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CachePoint",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CitableBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CitationBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseAsyncGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponseAsyncGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponseGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ImageBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ContentBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "TextBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ContentBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "TextBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ImageBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "AudioBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ContentBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "TextBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "LLMMetadata",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "TextBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ImageBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "AudioBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CitableBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CitationBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ContentBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ContentBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "TextBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "TextBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "LLMMetadata",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseAsyncGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ImageBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "TextBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "DocumentBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "AudioBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CachePoint",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CacheControl",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponseGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponseGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "LLMMetadata",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseGen",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "LLMMetadata",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "TextBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ContentBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ImageBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "AudioBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "TextBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ContentBlock",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "LLMMetadata",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "LLMMetadata",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "LLMMetadata",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.base.llms.types",
        "description": "llama_index.core.base.llms.types",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "field_validator",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "model_validator",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "model_serializer",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ValidationError",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "model_validator",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "model_serializer",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "model_validator",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "AnyUrl",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "FilePath",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "field_serializer",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "field_validator",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "model_validator",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "GetCoreSchemaHandler",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "GetJsonSchemaHandler",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "SerializeAsAny",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "PrivateAttr",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "PrivateAttr",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "SerializeAsAny",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "SerializeAsAny",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "create_model",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "field_validator",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "create_model",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "PrivateAttr",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ValidationError",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ValidationError",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ValidationError",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "SerializeAsAny",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "model_validator",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "field_validator",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "SerializeAsAny",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "PrivateAttr",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "WithJsonSchema",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "field_validator",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "model_validator",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ValidationError",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "SerializeAsAny",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "field_validator",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "field_validator",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "field_validator",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "model_validator",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "PrivateAttr",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "model_validator",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "field_serializer",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "SerializeAsAny",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "model_validator",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "SerializeAsAny",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "field_serializer",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "SerializeAsAny",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "field_validator",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ValidationError",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "PrivateAttr",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "PrivateAttr",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "SerializeAsAny",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "WithJsonSchema",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "PrivateAttr",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "PrivateAttr",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "WithJsonSchema",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BeforeValidator",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "PlainSerializer",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "PrivateAttr",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "SerializeAsAny",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "field_validator",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "SerializeAsAny",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "SerializeAsAny",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "PrivateAttr",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "SerializeAsAny",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "PrivateAttr",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "PrivateAttr",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "SerializeAsAny",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ValidationError",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ValidationError",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "create_model",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "WithJsonSchema",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "PlainSerializer",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "SerializeAsAny",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ValidationError",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "WrapSerializer",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "PrivateAttr",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "model_serializer",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "SerializeAsAny",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "FieldInfo",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "FieldInfo",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "create_model",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "PrivateAttr",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "StrictFloat",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "StrictInt",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "StrictStr",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "AnyUrl",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "GetJsonSchemaHandler",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "JsonSchemaValue",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "PlainSerializer",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "SerializationInfo",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "SerializeAsAny",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "SerializerFunctionWrapHandler",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ValidationInfo",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "field_serializer",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "field_validator",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "model_serializer",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "GetCoreSchemaHandler",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "GetJsonSchemaHandler",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ValidationError",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "ValidationError",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "PrivateAttr",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "llama_index.core.bridge.pydantic",
        "description": "llama_index.core.bridge.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "BaseTool",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "BaseTool",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "AsyncBaseTool",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "FunctionTool",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "ToolOutput",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "ToolSelection",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "adapt_to_async_tool",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "BaseTool",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "FunctionTool",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "AsyncBaseTool",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "BaseTool",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "AsyncBaseTool",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "FunctionTool",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "ToolOutput",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "ToolSelection",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "adapt_to_async_tool",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "AsyncBaseTool",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "ToolSelection",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "ToolOutput",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "ToolOutput",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "ToolOutput",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "BaseTool",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "QueryEngineTool",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "BaseTool",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "ToolSelection",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "ToolSelection",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "ToolOutput",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "ToolSelection",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "ToolOutput",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "FunctionTool",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "ToolSelection",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "FunctionTool",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "ToolOutput",
        "importPath": "llama_index.core.tools",
        "description": "llama_index.core.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.tools",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "extract_json_str",
        "importPath": "llama_index.core.output_parsers.utils",
        "description": "llama_index.core.output_parsers.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.utils",
        "documentation": {}
    },
    {
        "label": "parse_json_markdown",
        "importPath": "llama_index.core.output_parsers.utils",
        "description": "llama_index.core.output_parsers.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.utils",
        "documentation": {}
    },
    {
        "label": "extract_json_str",
        "importPath": "llama_index.core.output_parsers.utils",
        "description": "llama_index.core.output_parsers.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.utils",
        "documentation": {}
    },
    {
        "label": "_marshal_llm_to_json",
        "importPath": "llama_index.core.output_parsers.utils",
        "description": "llama_index.core.output_parsers.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.utils",
        "documentation": {}
    },
    {
        "label": "parse_json_markdown",
        "importPath": "llama_index.core.output_parsers.utils",
        "description": "llama_index.core.output_parsers.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.utils",
        "documentation": {}
    },
    {
        "label": "parse_json_markdown",
        "importPath": "llama_index.core.output_parsers.utils",
        "description": "llama_index.core.output_parsers.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.utils",
        "documentation": {}
    },
    {
        "label": "extract_json_str",
        "importPath": "llama_index.core.output_parsers.utils",
        "description": "llama_index.core.output_parsers.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.utils",
        "documentation": {}
    },
    {
        "label": "BaseOutputParser",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "TokenGen",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "TokenAsyncGen",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "Thread",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "Thread",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "TokenGen",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "TokenAsyncGen",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "BasePydanticProgram",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "Model",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "BaseOutputParser",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "BaseOutputParser",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "PydanticProgramMode",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "TokenAsyncGen",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "TokenGen",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "Model",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "PydanticProgramMode",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "Model",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "BaseOutputParser",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "BasePydanticProgram",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "Model",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "BaseOutputParser",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "BasePydanticProgram",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "Model",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "BasePydanticProgram",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "Model",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "BasePydanticProgram",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "Model",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "BasePydanticProgram",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "Model",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "PydanticProgramMode",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "BaseOutputParser",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "BaseOutputParser",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "BaseOutputParser",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "BaseOutputParser",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "BaseOutputParser",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TEXT_TYPE",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TEXT_TYPE",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TEXT_TYPE",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TEXT_TYPE",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TEXT_TYPE",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "BasePydanticProgram",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TEXT_TYPE",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TEXT_TYPE",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TEXT_TYPE",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "BasePydanticProgram",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TEXT_TYPE",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TEXT_TYPE",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "BaseOutputParser",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "BasePydanticProgram",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "TokenAsyncGen",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "TokenGen",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "PydanticProgramMode",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "Model",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "Model",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "BaseOutputParser",
        "importPath": "llama_index.core.types",
        "description": "llama_index.core.types",
        "isExtraImport": true,
        "detail": "llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "inspect",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "inspect",
        "description": "inspect",
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "iscoroutinefunction",
        "importPath": "inspect",
        "description": "inspect",
        "isExtraImport": true,
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "signature",
        "importPath": "inspect",
        "description": "inspect",
        "isExtraImport": true,
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "signature",
        "importPath": "inspect",
        "description": "inspect",
        "isExtraImport": true,
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "ModelMetaclass",
        "importPath": "pydantic._internal._model_construction",
        "description": "pydantic._internal._model_construction",
        "isExtraImport": true,
        "detail": "pydantic._internal._model_construction",
        "documentation": {}
    },
    {
        "label": "DEFAULT_STATE_PROMPT",
        "importPath": "llama_index.core.agent.workflow.prompts",
        "description": "llama_index.core.agent.workflow.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_HANDOFF_PROMPT",
        "importPath": "llama_index.core.agent.workflow.prompts",
        "description": "llama_index.core.agent.workflow.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_HANDOFF_OUTPUT_PROMPT",
        "importPath": "llama_index.core.agent.workflow.prompts",
        "description": "llama_index.core.agent.workflow.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_STATE_PROMPT",
        "importPath": "llama_index.core.agent.workflow.prompts",
        "description": "llama_index.core.agent.workflow.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.prompts",
        "documentation": {}
    },
    {
        "label": "AgentOutput",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentInput",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentSetup",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentWorkflowStartEvent",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentStreamStructuredOutput",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "ToolCall",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "ToolCallResult",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentInput",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentOutput",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentStream",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "ToolCallResult",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentInput",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentOutput",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentStream",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "ToolCallResult",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "ToolCall",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "ToolCallResult",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentInput",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentSetup",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentOutput",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentWorkflowStartEvent",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentStreamStructuredOutput",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentInput",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentOutput",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentStream",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "ToolCallResult",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentOutput",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "ToolCallResult",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentWorkflowStartEvent",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentOutput",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "PydanticConversionWarning",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentStreamStructuredOutput",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentInput",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentOutput",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "ToolCallResult",
        "importPath": "llama_index.core.agent.workflow.workflow_events",
        "description": "llama_index.core.agent.workflow.workflow_events",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptType",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "SelectorPromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "RichPromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "RichPromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "RichPromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "SelectorPromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "RichPromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "SelectorPromptTemplate",
        "importPath": "llama_index.core.prompts",
        "description": "llama_index.core.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts",
        "documentation": {}
    },
    {
        "label": "generate_structured_response",
        "importPath": "llama_index.core.agent.utils",
        "description": "llama_index.core.agent.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.utils",
        "documentation": {}
    },
    {
        "label": "generate_structured_response",
        "importPath": "llama_index.core.agent.utils",
        "description": "llama_index.core.agent.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.utils",
        "documentation": {}
    },
    {
        "label": "messages_to_xml_format",
        "importPath": "llama_index.core.agent.utils",
        "description": "llama_index.core.agent.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.utils",
        "documentation": {}
    },
    {
        "label": "generate_structured_response",
        "importPath": "llama_index.core.agent.utils",
        "description": "llama_index.core.agent.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.utils",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "TextBlock",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "TextBlock",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "TextBlock",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ImageBlock",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "TextBlock",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ImageBlock",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "TextBlock",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "AudioBlock",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ImageBlock",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "TextBlock",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "DocumentBlock",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ImageBlock",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "TextBlock",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "TextBlock",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ImageBlock",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "TextBlock",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLMMetadata",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatResponseAsyncGen",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLMMetadata",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatResponseAsyncGen",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLMMetadata",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatResponseAsyncGen",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLMMetadata",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "TextBlock",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ImageBlock",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "LLMMetadata",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ImageBlock",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptType",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "llama_index.core.prompts.base",
        "description": "llama_index.core.prompts.base",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptMixin",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixin",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixin",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixin",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixin",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixin",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixin",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixin",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixin",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixin",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixin",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixin",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixin",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixin",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "importPath": "llama_index.core.prompts.mixin",
        "description": "llama_index.core.prompts.mixin",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "Context",
        "importPath": "llama_index.core.workflow",
        "description": "llama_index.core.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow",
        "documentation": {}
    },
    {
        "label": "Context",
        "importPath": "llama_index.core.workflow",
        "description": "llama_index.core.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow",
        "documentation": {}
    },
    {
        "label": "Context",
        "importPath": "llama_index.core.workflow",
        "description": "llama_index.core.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow",
        "documentation": {}
    },
    {
        "label": "Context",
        "importPath": "llama_index.core.workflow",
        "description": "llama_index.core.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow",
        "documentation": {}
    },
    {
        "label": "StartEvent",
        "importPath": "llama_index.core.workflow",
        "description": "llama_index.core.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow",
        "documentation": {}
    },
    {
        "label": "StopEvent",
        "importPath": "llama_index.core.workflow",
        "description": "llama_index.core.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow",
        "documentation": {}
    },
    {
        "label": "Workflow",
        "importPath": "llama_index.core.workflow",
        "description": "llama_index.core.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow",
        "documentation": {}
    },
    {
        "label": "step",
        "importPath": "llama_index.core.workflow",
        "description": "llama_index.core.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow",
        "documentation": {}
    },
    {
        "label": "WorkflowRuntimeError",
        "importPath": "llama_index.core.workflow",
        "description": "llama_index.core.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow",
        "documentation": {}
    },
    {
        "label": "Context",
        "importPath": "llama_index.core.workflow",
        "description": "llama_index.core.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow",
        "documentation": {}
    },
    {
        "label": "Event",
        "importPath": "llama_index.core.workflow",
        "description": "llama_index.core.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow",
        "documentation": {}
    },
    {
        "label": "StartEvent",
        "importPath": "llama_index.core.workflow",
        "description": "llama_index.core.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow",
        "documentation": {}
    },
    {
        "label": "Context",
        "importPath": "llama_index.core.workflow",
        "description": "llama_index.core.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow",
        "documentation": {}
    },
    {
        "label": "Context",
        "importPath": "llama_index.core.workflow",
        "description": "llama_index.core.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow",
        "documentation": {}
    },
    {
        "label": "WorkflowRuntimeError",
        "importPath": "llama_index.core.workflow",
        "description": "llama_index.core.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow",
        "documentation": {}
    },
    {
        "label": "HumanResponseEvent",
        "importPath": "llama_index.core.workflow",
        "description": "llama_index.core.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow",
        "documentation": {}
    },
    {
        "label": "InputRequiredEvent",
        "importPath": "llama_index.core.workflow",
        "description": "llama_index.core.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow",
        "documentation": {}
    },
    {
        "label": "ObjectRetriever",
        "importPath": "llama_index.core.objects",
        "description": "llama_index.core.objects",
        "isExtraImport": true,
        "detail": "llama_index.core.objects",
        "documentation": {}
    },
    {
        "label": "ObjectRetriever",
        "importPath": "llama_index.core.objects",
        "description": "llama_index.core.objects",
        "isExtraImport": true,
        "detail": "llama_index.core.objects",
        "documentation": {}
    },
    {
        "label": "SQLTableNodeMapping",
        "importPath": "llama_index.core.objects",
        "description": "llama_index.core.objects",
        "isExtraImport": true,
        "detail": "llama_index.core.objects",
        "documentation": {}
    },
    {
        "label": "ObjectIndex",
        "importPath": "llama_index.core.objects",
        "description": "llama_index.core.objects",
        "isExtraImport": true,
        "detail": "llama_index.core.objects",
        "documentation": {}
    },
    {
        "label": "SQLTableSchema",
        "importPath": "llama_index.core.objects",
        "description": "llama_index.core.objects",
        "isExtraImport": true,
        "detail": "llama_index.core.objects",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core.settings",
        "description": "llama_index.core.settings",
        "isExtraImport": true,
        "detail": "llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "CheckpointCallback",
        "importPath": "llama_index.core.workflow.checkpointer",
        "description": "llama_index.core.workflow.checkpointer",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow.checkpointer",
        "documentation": {}
    },
    {
        "label": "CheckpointCallback",
        "importPath": "llama_index.core.workflow.checkpointer",
        "description": "llama_index.core.workflow.checkpointer",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow.checkpointer",
        "documentation": {}
    },
    {
        "label": "Context",
        "importPath": "llama_index.core.workflow.context",
        "description": "llama_index.core.workflow.context",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow.context",
        "documentation": {}
    },
    {
        "label": "Context",
        "importPath": "llama_index.core.workflow.context",
        "description": "llama_index.core.workflow.context",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow.context",
        "documentation": {}
    },
    {
        "label": "Context",
        "importPath": "llama_index.core.workflow.context",
        "description": "llama_index.core.workflow.context",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow.context",
        "documentation": {}
    },
    {
        "label": "step",
        "importPath": "llama_index.core.workflow.decorators",
        "description": "llama_index.core.workflow.decorators",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow.decorators",
        "documentation": {}
    },
    {
        "label": "StopEvent",
        "importPath": "llama_index.core.workflow.events",
        "description": "llama_index.core.workflow.events",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow.events",
        "documentation": {}
    },
    {
        "label": "Event",
        "importPath": "llama_index.core.workflow.events",
        "description": "llama_index.core.workflow.events",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow.events",
        "documentation": {}
    },
    {
        "label": "StopEvent",
        "importPath": "llama_index.core.workflow.events",
        "description": "llama_index.core.workflow.events",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow.events",
        "documentation": {}
    },
    {
        "label": "WorkflowRuntimeError",
        "importPath": "llama_index.core.workflow.errors",
        "description": "llama_index.core.workflow.errors",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow.errors",
        "documentation": {}
    },
    {
        "label": "WorkflowRuntimeError",
        "importPath": "llama_index.core.workflow.errors",
        "description": "llama_index.core.workflow.errors",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow.errors",
        "documentation": {}
    },
    {
        "label": "WorkflowHandler",
        "importPath": "llama_index.core.workflow.handler",
        "description": "llama_index.core.workflow.handler",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow.handler",
        "documentation": {}
    },
    {
        "label": "WorkflowHandler",
        "importPath": "llama_index.core.workflow.handler",
        "description": "llama_index.core.workflow.handler",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow.handler",
        "documentation": {}
    },
    {
        "label": "Workflow",
        "importPath": "llama_index.core.workflow.workflow",
        "description": "llama_index.core.workflow.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow.workflow",
        "documentation": {}
    },
    {
        "label": "WorkflowMeta",
        "importPath": "llama_index.core.workflow.workflow",
        "description": "llama_index.core.workflow.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow.workflow",
        "documentation": {}
    },
    {
        "label": "WorkflowMeta",
        "importPath": "llama_index.core.workflow.workflow",
        "description": "llama_index.core.workflow.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.workflow.workflow",
        "documentation": {}
    },
    {
        "label": "BaseWorkflowAgent",
        "importPath": "llama_index.core.agent.workflow.base_agent",
        "description": "llama_index.core.agent.workflow.base_agent",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.base_agent",
        "documentation": {}
    },
    {
        "label": "BaseWorkflowAgent",
        "importPath": "llama_index.core.agent.workflow.base_agent",
        "description": "llama_index.core.agent.workflow.base_agent",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.base_agent",
        "documentation": {}
    },
    {
        "label": "BaseWorkflowAgent",
        "importPath": "llama_index.core.agent.workflow.base_agent",
        "description": "llama_index.core.agent.workflow.base_agent",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.base_agent",
        "documentation": {}
    },
    {
        "label": "DEFAULT_AGENT_NAME",
        "importPath": "llama_index.core.agent.workflow.base_agent",
        "description": "llama_index.core.agent.workflow.base_agent",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.base_agent",
        "documentation": {}
    },
    {
        "label": "DEFAULT_AGENT_DESCRIPTION",
        "importPath": "llama_index.core.agent.workflow.base_agent",
        "description": "llama_index.core.agent.workflow.base_agent",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.base_agent",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MAX_ITERATIONS",
        "importPath": "llama_index.core.agent.workflow.base_agent",
        "description": "llama_index.core.agent.workflow.base_agent",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.base_agent",
        "documentation": {}
    },
    {
        "label": "BaseWorkflowAgent",
        "importPath": "llama_index.core.agent.workflow.base_agent",
        "description": "llama_index.core.agent.workflow.base_agent",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.base_agent",
        "documentation": {}
    },
    {
        "label": "ToolSelection",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "ToolSelection",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "ToolSelection",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "MessagesToPromptType",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "CompletionToPromptType",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "ToolSelection",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "ToolSelection",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "ToolSelection",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "ToolSelection",
        "importPath": "llama_index.core.llms.llm",
        "description": "llama_index.core.llms.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "FunctionCallingLLM",
        "importPath": "llama_index.core.llms.function_calling",
        "description": "llama_index.core.llms.function_calling",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.function_calling",
        "documentation": {}
    },
    {
        "label": "FunctionCallingLLM",
        "importPath": "llama_index.core.llms.function_calling",
        "description": "llama_index.core.llms.function_calling",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.function_calling",
        "documentation": {}
    },
    {
        "label": "FunctionCallingLLM",
        "importPath": "llama_index.core.llms.function_calling",
        "description": "llama_index.core.llms.function_calling",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.function_calling",
        "documentation": {}
    },
    {
        "label": "FunctionCallingLLM",
        "importPath": "llama_index.core.llms.function_calling",
        "description": "llama_index.core.llms.function_calling",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.function_calling",
        "documentation": {}
    },
    {
        "label": "FunctionCallingLLM",
        "importPath": "llama_index.core.llms.function_calling",
        "description": "llama_index.core.llms.function_calling",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.function_calling",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "log2",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "FunctionAgent",
        "importPath": "llama_index.core.agent.workflow.function_agent",
        "description": "llama_index.core.agent.workflow.function_agent",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.function_agent",
        "documentation": {}
    },
    {
        "label": "FunctionAgent",
        "importPath": "llama_index.core.agent.workflow.function_agent",
        "description": "llama_index.core.agent.workflow.function_agent",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.function_agent",
        "documentation": {}
    },
    {
        "label": "ReActAgent",
        "importPath": "llama_index.core.agent.workflow.react_agent",
        "description": "llama_index.core.agent.workflow.react_agent",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.react_agent",
        "documentation": {}
    },
    {
        "label": "ReActAgent",
        "importPath": "llama_index.core.agent.workflow.react_agent",
        "description": "llama_index.core.agent.workflow.react_agent",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.react_agent",
        "documentation": {}
    },
    {
        "label": "ReActChatFormatter",
        "importPath": "llama_index.core.agent.react.formatter",
        "description": "llama_index.core.agent.react.formatter",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.react.formatter",
        "documentation": {}
    },
    {
        "label": "ReActOutputParser",
        "importPath": "llama_index.core.agent.react.output_parser",
        "description": "llama_index.core.agent.react.output_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.react.output_parser",
        "documentation": {}
    },
    {
        "label": "extract_final_response",
        "importPath": "llama_index.core.agent.react.output_parser",
        "description": "llama_index.core.agent.react.output_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.react.output_parser",
        "documentation": {}
    },
    {
        "label": "extract_tool_use",
        "importPath": "llama_index.core.agent.react.output_parser",
        "description": "llama_index.core.agent.react.output_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.react.output_parser",
        "documentation": {}
    },
    {
        "label": "parse_action_reasoning_step",
        "importPath": "llama_index.core.agent.react.output_parser",
        "description": "llama_index.core.agent.react.output_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.react.output_parser",
        "documentation": {}
    },
    {
        "label": "Self",
        "importPath": "typing_extensions",
        "description": "typing_extensions",
        "isExtraImport": true,
        "detail": "typing_extensions",
        "documentation": {}
    },
    {
        "label": "Self",
        "importPath": "typing_extensions",
        "description": "typing_extensions",
        "isExtraImport": true,
        "detail": "typing_extensions",
        "documentation": {}
    },
    {
        "label": "assert_never",
        "importPath": "typing_extensions",
        "description": "typing_extensions",
        "isExtraImport": true,
        "detail": "typing_extensions",
        "documentation": {}
    },
    {
        "label": "TypeGuard",
        "importPath": "typing_extensions",
        "description": "typing_extensions",
        "isExtraImport": true,
        "detail": "typing_extensions",
        "documentation": {}
    },
    {
        "label": "Self",
        "importPath": "typing_extensions",
        "description": "typing_extensions",
        "isExtraImport": true,
        "detail": "typing_extensions",
        "documentation": {}
    },
    {
        "label": "Annotated",
        "importPath": "typing_extensions",
        "description": "typing_extensions",
        "isExtraImport": true,
        "detail": "typing_extensions",
        "documentation": {}
    },
    {
        "label": "Annotated",
        "importPath": "typing_extensions",
        "description": "typing_extensions",
        "isExtraImport": true,
        "detail": "typing_extensions",
        "documentation": {}
    },
    {
        "label": "Annotated",
        "importPath": "typing_extensions",
        "description": "typing_extensions",
        "isExtraImport": true,
        "detail": "typing_extensions",
        "documentation": {}
    },
    {
        "label": "Annotated",
        "importPath": "typing_extensions",
        "description": "typing_extensions",
        "isExtraImport": true,
        "detail": "typing_extensions",
        "documentation": {}
    },
    {
        "label": "Annotated",
        "importPath": "typing_extensions",
        "description": "typing_extensions",
        "isExtraImport": true,
        "detail": "typing_extensions",
        "documentation": {}
    },
    {
        "label": "Self",
        "importPath": "typing_extensions",
        "description": "typing_extensions",
        "isExtraImport": true,
        "detail": "typing_extensions",
        "documentation": {}
    },
    {
        "label": "override",
        "importPath": "typing_extensions",
        "description": "typing_extensions",
        "isExtraImport": true,
        "detail": "typing_extensions",
        "documentation": {}
    },
    {
        "label": "override",
        "importPath": "typing_extensions",
        "description": "typing_extensions",
        "isExtraImport": true,
        "detail": "typing_extensions",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "BaseCallbackHandler",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks.base",
        "description": "llama_index.core.callbacks.base",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "BASE_TRACE_EVENT",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "LEAF_EVENTS",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "BASE_TRACE_EVENT",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "BASE_TRACE_EVENT",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "TIMESTAMP_FORMAT",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEvent",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventStats",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks.schema",
        "description": "llama_index.core.callbacks.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EMBED_BATCH_SIZE",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EMBED_BATCH_SIZE",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CONTEXT_WINDOW",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_NUM_OUTPUTS",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PROJECT_NAME",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "GRAPH_STORE_KEY",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SIMILARITY_TOP_K",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SIMILARITY_TOP_K",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SIMILARITY_TOP_K",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CONTEXT_WINDOW",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_NUM_OUTPUTS",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_APP_URL",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_BASE_URL",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PIPELINE_NAME",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PROJECT_NAME",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CONTEXT_WINDOW",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_NUM_INPUT_FILES",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_NUM_OUTPUTS",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CHUNK_SIZE",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CHUNK_OVERLAP",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CHUNK_SIZE",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SIMILARITY_TOP_K",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DATA_KEY",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "TYPE_KEY",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DATA_KEY",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "TYPE_KEY",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DOC_STORE_KEY",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "GRAPH_STORE_KEY",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "INDEX_STORE_KEY",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "VECTOR_STORE_KEY",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "PG_STORE_KEY",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CONTEXT_WINDOW",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_NUM_OUTPUTS",
        "importPath": "llama_index.core.constants",
        "description": "llama_index.core.constants",
        "isExtraImport": true,
        "detail": "llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "llama_index.core.instrumentation",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "llama_index.core.instrumentation",
        "description": "llama_index.core.instrumentation",
        "detail": "llama_index.core.instrumentation",
        "documentation": {}
    },
    {
        "label": "DispatcherSpanMixin",
        "importPath": "llama_index.core.instrumentation",
        "description": "llama_index.core.instrumentation",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation",
        "documentation": {}
    },
    {
        "label": "DispatcherSpanMixin",
        "importPath": "llama_index.core.instrumentation",
        "description": "llama_index.core.instrumentation",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation",
        "documentation": {}
    },
    {
        "label": "DispatcherSpanMixin",
        "importPath": "llama_index.core.instrumentation",
        "description": "llama_index.core.instrumentation",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation",
        "documentation": {}
    },
    {
        "label": "DispatcherSpanMixin",
        "importPath": "llama_index.core.instrumentation",
        "description": "llama_index.core.instrumentation",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation",
        "documentation": {}
    },
    {
        "label": "DispatcherSpanMixin",
        "importPath": "llama_index.core.instrumentation",
        "description": "llama_index.core.instrumentation",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation",
        "documentation": {}
    },
    {
        "label": "DispatcherSpanMixin",
        "importPath": "llama_index.core.instrumentation",
        "description": "llama_index.core.instrumentation",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation",
        "documentation": {}
    },
    {
        "label": "DispatcherSpanMixin",
        "importPath": "llama_index.core.instrumentation",
        "description": "llama_index.core.instrumentation",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation",
        "documentation": {}
    },
    {
        "label": "DispatcherSpanMixin",
        "importPath": "llama_index.core.instrumentation",
        "description": "llama_index.core.instrumentation",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation",
        "documentation": {}
    },
    {
        "label": "DispatcherSpanMixin",
        "importPath": "llama_index.core.instrumentation",
        "description": "llama_index.core.instrumentation",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation",
        "documentation": {}
    },
    {
        "label": "get_dispatcher",
        "importPath": "llama_index.core.instrumentation",
        "description": "llama_index.core.instrumentation",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation",
        "documentation": {}
    },
    {
        "label": "get_dispatcher",
        "importPath": "llama_index.core.instrumentation",
        "description": "llama_index.core.instrumentation",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation",
        "documentation": {}
    },
    {
        "label": "DispatcherSpanMixin",
        "importPath": "llama_index.core.instrumentation",
        "description": "llama_index.core.instrumentation",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation",
        "documentation": {}
    },
    {
        "label": "get_dispatcher",
        "importPath": "llama_index.core.instrumentation",
        "description": "llama_index.core.instrumentation",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation",
        "documentation": {}
    },
    {
        "label": "DispatcherSpanMixin",
        "importPath": "llama_index.core.instrumentation",
        "description": "llama_index.core.instrumentation",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation",
        "documentation": {}
    },
    {
        "label": "DispatcherSpanMixin",
        "importPath": "llama_index.core.instrumentation",
        "description": "llama_index.core.instrumentation",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation",
        "documentation": {}
    },
    {
        "label": "DispatcherSpanMixin",
        "importPath": "llama_index.core.instrumentation",
        "description": "llama_index.core.instrumentation",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation",
        "documentation": {}
    },
    {
        "label": "DispatcherSpanMixin",
        "importPath": "llama_index.core.instrumentation",
        "description": "llama_index.core.instrumentation",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation",
        "documentation": {}
    },
    {
        "label": "DispatcherSpanMixin",
        "importPath": "llama_index.core.instrumentation",
        "description": "llama_index.core.instrumentation",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation",
        "documentation": {}
    },
    {
        "label": "DispatcherSpanMixin",
        "importPath": "llama_index.core.instrumentation",
        "description": "llama_index.core.instrumentation",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation",
        "documentation": {}
    },
    {
        "label": "DispatcherSpanMixin",
        "importPath": "llama_index.core.instrumentation",
        "description": "llama_index.core.instrumentation",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation",
        "documentation": {}
    },
    {
        "label": "DispatcherSpanMixin",
        "importPath": "llama_index.core.instrumentation",
        "description": "llama_index.core.instrumentation",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TransformComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ImageNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ImageDocument",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryType",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "IndexNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryType",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryType",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TransformComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ImageType",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ImageNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TransformComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Node",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TransformComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "IndexNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeRelationship",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ObjectType",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "RelatedNodeInfo",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "IndexNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeRelationship",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "RelatedNodeInfo",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "IndexNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "IndexNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "IndexNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TransformComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ImageNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ObjectType",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryType",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeRelationship",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "RelatedNodeInfo",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TransformComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TransformComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeRelationship",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TransformComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TransformComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TransformComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryType",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryType",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryType",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryType",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "IndexNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "IndexNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ObjectType",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ImageNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "IndexNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TransformComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "IndexNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TransformComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ImageNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TransformComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryType",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryType",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryType",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryType",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TransformComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ImageNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ImageDocument",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "IndexNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeRelationship",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeRelationship",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeRelationship",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeRelationship",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeRelationship",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TransformComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ImageDocument",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ImageNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeRelationship",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryType",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeRelationship",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ImageNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryType",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "IndexNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ImageNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ImageNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryType",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryType",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "IndexNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "IndexNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "IndexNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "IndexNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ImageDocument",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ImageNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "IndexNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Node",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeRelationship",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "RelatedNodeInfo",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Node",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ImageNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Node",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "IndexNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeRelationship",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "RelatedNodeInfo",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TransformComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ImageDocument",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeRelationship",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeRelationship",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "RelatedNodeInfo",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TransformComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeRelationship",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "RelatedNodeInfo",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeRelationship",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "RelatedNodeInfo",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ImageNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeRelationship",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "RelatedNodeInfo",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TransformComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ImageDocument",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TransformComponent",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "IndexNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeRelationship",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "IndexNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeRelationship",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "RelatedNodeInfo",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "importPath": "llama_index.core.schema",
        "description": "llama_index.core.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "get_tqdm_iterable",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tqdm_iterable",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "resolve_binary",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "truncate_text",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "print_text",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tokenizer",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tqdm_iterable",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_cache_dir",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_cache_dir",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_cache_dir",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "truncate_text",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tqdm_iterable",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tqdm_iterable",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tqdm_iterable",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "truncate_text",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "globals_helper",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tqdm_iterable",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "print_text",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "truncate_text",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tqdm_iterable",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "print_text",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "print_text",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "print_text",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "truncate_text",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "iter_batch",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "globals_helper",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "truncate_text",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "concat_dirs",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_new_id",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tokenizer",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tokenizer",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tokenizer",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tqdm_iterable",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tqdm_iterable",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tqdm_iterable",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tqdm_iterable",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tqdm_iterable",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tqdm_iterable",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tqdm_iterable",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "globals_helper",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tqdm_iterable",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tokenizer",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tqdm_iterable",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tokenizer",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "globals_helper",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tqdm_iterable",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "truncate_text",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "concat_dirs",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_color_mapping",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "print_text",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "globals_helper",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "print_text",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "infer_torch_device",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "print_text",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "print_text",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "print_text",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "print_text",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "print_text",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_color_mapping",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "print_text",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tqdm_iterable",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "truncate_text",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "truncate_text",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "temp_set_attrs",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "print_text",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "concat_dirs",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "concat_dirs",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "concat_dirs",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "print_text",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tokenizer",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "concat_dirs",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "SAMPLE_TEXT",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "truncate_text",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tokenizer",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "set_global_tokenizer",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tokenizer",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tokenizer",
        "importPath": "llama_index.core.utils",
        "description": "llama_index.core.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "run_jobs",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "run_jobs",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "asyncio_run",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "asyncio_run",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "asyncio_run",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "asyncio_module",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "asyncio_run",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "asyncio_run",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "asyncio_module",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "asyncio_run",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "DEFAULT_NUM_WORKERS",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "run_jobs",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "asyncio_run",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "DEFAULT_NUM_WORKERS",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "run_jobs",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "run_async_tasks",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "run_async_tasks",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "run_jobs",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "run_jobs",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "run_jobs",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "asyncio_run",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "run_jobs",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "run_async_tasks",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "asyncio_module",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "DEFAULT_NUM_WORKERS",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "run_jobs",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "asyncio_run",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "asyncio_run",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "asyncio_run",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "DEFAULT_NUM_WORKERS",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "run_jobs",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "asyncio_run",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "run_async_tasks",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "run_async_tasks",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "get_asyncio_module",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "run_jobs",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "run_async_tasks",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "run_async_tasks",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "run_async_tasks",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "asyncio_run",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "asyncio_run",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "asyncio_run",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "asyncio_run",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "asyncio_run",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "asyncio_run",
        "importPath": "llama_index.core.async_utils",
        "description": "llama_index.core.async_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "EmbeddingEndEvent",
        "importPath": "llama_index.core.instrumentation.events.embedding",
        "description": "llama_index.core.instrumentation.events.embedding",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.embedding",
        "documentation": {}
    },
    {
        "label": "EmbeddingStartEvent",
        "importPath": "llama_index.core.instrumentation.events.embedding",
        "description": "llama_index.core.instrumentation.events.embedding",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.embedding",
        "documentation": {}
    },
    {
        "label": "SparseEmbeddingEndEvent",
        "importPath": "llama_index.core.instrumentation.events.embedding",
        "description": "llama_index.core.instrumentation.events.embedding",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.embedding",
        "documentation": {}
    },
    {
        "label": "SparseEmbeddingStartEvent",
        "importPath": "llama_index.core.instrumentation.events.embedding",
        "description": "llama_index.core.instrumentation.events.embedding",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.embedding",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "ChainMap",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "trace_method",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "trace_method",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "trace_method",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "trace_method",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "TokenCountingHandler",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.callbacks",
        "description": "llama_index.core.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks",
        "documentation": {}
    },
    {
        "label": "base64",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "base64",
        "description": "base64",
        "detail": "base64",
        "documentation": {}
    },
    {
        "label": "Error",
        "importPath": "binascii",
        "description": "binascii",
        "isExtraImport": true,
        "detail": "binascii",
        "documentation": {}
    },
    {
        "label": "Error",
        "importPath": "binascii",
        "description": "binascii",
        "isExtraImport": true,
        "detail": "binascii",
        "documentation": {}
    },
    {
        "label": "Error",
        "importPath": "binascii",
        "description": "binascii",
        "isExtraImport": true,
        "detail": "binascii",
        "documentation": {}
    },
    {
        "label": "Error",
        "importPath": "binascii",
        "description": "binascii",
        "isExtraImport": true,
        "detail": "binascii",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "filetype",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "filetype",
        "description": "filetype",
        "detail": "filetype",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "StringIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "dataclasses",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "dataclasses",
        "description": "dataclasses",
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.base.base_retriever",
        "description": "llama_index.core.base.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseImageRetriever",
        "importPath": "llama_index.core.image_retriever",
        "description": "llama_index.core.image_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.image_retriever",
        "documentation": {}
    },
    {
        "label": "QueryType",
        "importPath": "llama_index.core.indices.query.schema",
        "description": "llama_index.core.indices.query.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.indices.query.schema",
        "description": "llama_index.core.indices.query.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.indices.query.schema",
        "description": "llama_index.core.indices.query.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.indices.query.schema",
        "description": "llama_index.core.indices.query.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.indices.query.schema",
        "description": "llama_index.core.indices.query.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.schema",
        "documentation": {}
    },
    {
        "label": "QueryType",
        "importPath": "llama_index.core.indices.query.schema",
        "description": "llama_index.core.indices.query.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.indices.query.schema",
        "description": "llama_index.core.indices.query.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "importPath": "llama_index.core.indices.query.schema",
        "description": "llama_index.core.indices.query.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.schema",
        "documentation": {}
    },
    {
        "label": "QueryType",
        "importPath": "llama_index.core.indices.query.schema",
        "description": "llama_index.core.indices.query.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.schema",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TYPE",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "AsyncStreamingResponse",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "StreamingResponse",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TYPE",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "StreamingResponse",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "AsyncStreamingResponse",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "StreamingResponse",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "AsyncStreamingResponse",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "StreamingResponse",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TYPE",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TYPE",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "AsyncStreamingResponse",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "StreamingResponse",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TYPE",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TYPE",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TYPE",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TYPE",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "PydanticResponse",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TYPE",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TYPE",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TYPE",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TYPE",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TYPE",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TYPE",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TYPE",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TYPE",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TYPE",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "AsyncStreamingResponse",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "AsyncStreamingResponse",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TYPE",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TYPE",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "PydanticResponse",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "StreamingResponse",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "AsyncStreamingResponse",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TYPE",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "StreamingResponse",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TYPE",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TYPE",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TYPE",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "PydanticResponse",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "StreamingResponse",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "AsyncStreamingResponse",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TYPE",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "llama_index.core.base.response.schema",
        "description": "llama_index.core.base.response.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "QueryEndEvent",
        "importPath": "llama_index.core.instrumentation.events.query",
        "description": "llama_index.core.instrumentation.events.query",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.query",
        "documentation": {}
    },
    {
        "label": "QueryStartEvent",
        "importPath": "llama_index.core.instrumentation.events.query",
        "description": "llama_index.core.instrumentation.events.query",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.query",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.base.base_query_engine",
        "description": "llama_index.core.base.base_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "RetrievalEndEvent",
        "importPath": "llama_index.core.instrumentation.events.retrieval",
        "description": "llama_index.core.instrumentation.events.retrieval",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.retrieval",
        "documentation": {}
    },
    {
        "label": "RetrievalStartEvent",
        "importPath": "llama_index.core.instrumentation.events.retrieval",
        "description": "llama_index.core.instrumentation.events.retrieval",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.retrieval",
        "documentation": {}
    },
    {
        "label": "ToolMetadata",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "ToolMetadata",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "BaseTool",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "ToolMetadata",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "ToolMetadata",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "ToolMetadata",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "ToolMetadata",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "ToolMetadata",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "ToolMetadata",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "ToolMetadata",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "ToolMetadata",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "ToolMetadata",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "BaseTool",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "ToolOutput",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "adapt_to_async_tool",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "ToolMetadata",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "ToolOutput",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "AsyncBaseTool",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "ToolMetadata",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "ToolOutput",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "AsyncBaseTool",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "ToolMetadata",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "ToolOutput",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "AsyncBaseTool",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "ToolMetadata",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "ToolOutput",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "BaseTool",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "ToolMetadata",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "ToolOutput",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "AsyncBaseTool",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "ToolMetadata",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "ToolOutput",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "BaseTool",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "BaseTool",
        "importPath": "llama_index.core.tools.types",
        "description": "llama_index.core.tools.types",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "langchain",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "langchain",
        "description": "langchain",
        "detail": "langchain",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "langchain.agents",
        "description": "langchain.agents",
        "isExtraImport": true,
        "detail": "langchain.agents",
        "documentation": {}
    },
    {
        "label": "BaseToolkit",
        "importPath": "langchain.agents.agent_toolkits.base",
        "description": "langchain.agents.agent_toolkits.base",
        "isExtraImport": true,
        "detail": "langchain.agents.agent_toolkits.base",
        "documentation": {}
    },
    {
        "label": "BaseLanguageModel",
        "importPath": "langchain.base_language",
        "description": "langchain.base_language",
        "isExtraImport": true,
        "detail": "langchain.base_language",
        "documentation": {}
    },
    {
        "label": "BaseCallbackHandler",
        "importPath": "langchain.callbacks.base",
        "description": "langchain.callbacks.base",
        "isExtraImport": true,
        "detail": "langchain.callbacks.base",
        "documentation": {}
    },
    {
        "label": "BaseCallbackManager",
        "importPath": "langchain.callbacks.base",
        "description": "langchain.callbacks.base",
        "isExtraImport": true,
        "detail": "langchain.callbacks.base",
        "documentation": {}
    },
    {
        "label": "ConditionalPromptSelector",
        "importPath": "langchain.chains.prompt_selector",
        "description": "langchain.chains.prompt_selector",
        "isExtraImport": true,
        "detail": "langchain.chains.prompt_selector",
        "documentation": {}
    },
    {
        "label": "is_chat_model",
        "importPath": "langchain.chains.prompt_selector",
        "description": "langchain.chains.prompt_selector",
        "isExtraImport": true,
        "detail": "langchain.chains.prompt_selector",
        "documentation": {}
    },
    {
        "label": "BaseChatModel",
        "importPath": "langchain.chat_models.base",
        "description": "langchain.chat_models.base",
        "isExtraImport": true,
        "detail": "langchain.chat_models.base",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain.docstore.document",
        "description": "langchain.docstore.document",
        "isExtraImport": true,
        "detail": "langchain.docstore.document",
        "documentation": {}
    },
    {
        "label": "ConversationBufferMemory",
        "importPath": "langchain.memory",
        "description": "langchain.memory",
        "isExtraImport": true,
        "detail": "langchain.memory",
        "documentation": {}
    },
    {
        "label": "BaseChatMemory",
        "importPath": "langchain.memory.chat_memory",
        "description": "langchain.memory.chat_memory",
        "isExtraImport": true,
        "detail": "langchain.memory.chat_memory",
        "documentation": {}
    },
    {
        "label": "ResponseSchema",
        "importPath": "langchain.output_parsers",
        "description": "langchain.output_parsers",
        "isExtraImport": true,
        "detail": "langchain.output_parsers",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "# pants: no-infer-dep\r\n    AIMessagePromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "# pants: no-infer-dep\r\n    BaseMessagePromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "# pants: no-infer-dep\r\n    ChatPromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "# pants: no-infer-dep\r\n    HumanMessagePromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "# pants: no-infer-dep\r\n    SystemMessagePromptTemplate",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "# pants: no-infer-dep",
        "importPath": "langchain.prompts.chat",
        "description": "langchain.prompts.chat",
        "isExtraImport": true,
        "detail": "langchain.prompts.chat",
        "documentation": {}
    },
    {
        "label": "# pants: no-infer-dep\r\n    AIMessage",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "# pants: no-infer-dep\r\n    BaseMemory",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "# pants: no-infer-dep\r\n    BaseMessage",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "# pants: no-infer-dep\r\n    BaseOutputParser",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "# pants: no-infer-dep\r\n    ChatGeneration",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "# pants: no-infer-dep\r\n    ChatMessage",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "# pants: no-infer-dep\r\n    FunctionMessage",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "# pants: no-infer-dep\r\n    HumanMessage",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "# pants: no-infer-dep\r\n    LLMResult",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "# pants: no-infer-dep\r\n    SystemMessage",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "# pants: no-infer-dep",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "Embeddings",
        "importPath": "langchain.schema.embeddings",
        "description": "langchain.schema.embeddings",
        "isExtraImport": true,
        "detail": "langchain.schema.embeddings",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "importPath": "langchain.schema.prompt_template",
        "description": "langchain.schema.prompt_template",
        "isExtraImport": true,
        "detail": "langchain.schema.prompt_template",
        "documentation": {}
    },
    {
        "label": "RecursiveCharacterTextSplitter",
        "importPath": "langchain.text_splitter",
        "description": "langchain.text_splitter",
        "isExtraImport": true,
        "detail": "langchain.text_splitter",
        "documentation": {}
    },
    {
        "label": "TextSplitter",
        "importPath": "langchain.text_splitter",
        "description": "langchain.text_splitter",
        "isExtraImport": true,
        "detail": "langchain.text_splitter",
        "documentation": {}
    },
    {
        "label": "BaseTool",
        "importPath": "langchain.tools",
        "description": "langchain.tools",
        "isExtraImport": true,
        "detail": "langchain.tools",
        "documentation": {}
    },
    {
        "label": "StructuredTool",
        "importPath": "langchain.tools",
        "description": "langchain.tools",
        "isExtraImport": true,
        "detail": "langchain.tools",
        "documentation": {}
    },
    {
        "label": "Tool",
        "importPath": "langchain.tools",
        "description": "langchain.tools",
        "isExtraImport": true,
        "detail": "langchain.tools",
        "documentation": {}
    },
    {
        "label": "ChatMessageHistory",
        "importPath": "langchain_community.chat_message_histories",
        "description": "langchain_community.chat_message_histories",
        "isExtraImport": true,
        "detail": "langchain_community.chat_message_histories",
        "documentation": {}
    },
    {
        "label": "ChatAnyscale",
        "importPath": "langchain_community.chat_models",
        "description": "langchain_community.chat_models",
        "isExtraImport": true,
        "detail": "langchain_community.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_community.chat_models",
        "description": "langchain_community.chat_models",
        "isExtraImport": true,
        "detail": "langchain_community.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatFireworks",
        "importPath": "langchain_community.chat_models",
        "description": "langchain_community.chat_models",
        "isExtraImport": true,
        "detail": "langchain_community.chat_models",
        "documentation": {}
    },
    {
        "label": "# pants: no-infer-dep\r\n    HuggingFaceBgeEmbeddings",
        "importPath": "langchain_community.embeddings",
        "description": "langchain_community.embeddings",
        "isExtraImport": true,
        "detail": "langchain_community.embeddings",
        "documentation": {}
    },
    {
        "label": "# pants: no-infer-dep\r\n    HuggingFaceEmbeddings",
        "importPath": "langchain_community.embeddings",
        "description": "langchain_community.embeddings",
        "isExtraImport": true,
        "detail": "langchain_community.embeddings",
        "documentation": {}
    },
    {
        "label": "# pants: no-infer-dep",
        "importPath": "langchain_community.embeddings",
        "description": "langchain_community.embeddings",
        "isExtraImport": true,
        "detail": "langchain_community.embeddings",
        "documentation": {}
    },
    {
        "label": "AI21",
        "importPath": "langchain_community.llms",
        "description": "langchain_community.llms",
        "isExtraImport": true,
        "detail": "langchain_community.llms",
        "documentation": {}
    },
    {
        "label": "BaseLLM",
        "importPath": "langchain_community.llms",
        "description": "langchain_community.llms",
        "isExtraImport": true,
        "detail": "langchain_community.llms",
        "documentation": {}
    },
    {
        "label": "Cohere",
        "importPath": "langchain_community.llms",
        "description": "langchain_community.llms",
        "isExtraImport": true,
        "detail": "langchain_community.llms",
        "documentation": {}
    },
    {
        "label": "FakeListLLM",
        "importPath": "langchain_community.llms",
        "description": "langchain_community.llms",
        "isExtraImport": true,
        "detail": "langchain_community.llms",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "langchain_community.llms",
        "description": "langchain_community.llms",
        "isExtraImport": true,
        "detail": "langchain_community.llms",
        "documentation": {}
    },
    {
        "label": "FieldInfo",
        "importPath": "pydantic.fields",
        "description": "pydantic.fields",
        "isExtraImport": true,
        "detail": "pydantic.fields",
        "documentation": {}
    },
    {
        "label": "JsonSchemaValue",
        "importPath": "pydantic.json_schema",
        "description": "pydantic.json_schema",
        "isExtraImport": true,
        "detail": "pydantic.json_schema",
        "documentation": {}
    },
    {
        "label": "pydantic_core",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pydantic_core",
        "description": "pydantic_core",
        "detail": "pydantic_core",
        "documentation": {}
    },
    {
        "label": "CoreSchema",
        "importPath": "pydantic_core",
        "description": "pydantic_core",
        "isExtraImport": true,
        "detail": "pydantic_core",
        "documentation": {}
    },
    {
        "label": "core_schema",
        "importPath": "pydantic_core",
        "description": "pydantic_core",
        "isExtraImport": true,
        "detail": "pydantic_core",
        "documentation": {}
    },
    {
        "label": "ContextVar",
        "importPath": "contextvars",
        "description": "contextvars",
        "isExtraImport": true,
        "detail": "contextvars",
        "documentation": {}
    },
    {
        "label": "ContextVar",
        "importPath": "contextvars",
        "description": "contextvars",
        "isExtraImport": true,
        "detail": "contextvars",
        "documentation": {}
    },
    {
        "label": "copy_context",
        "importPath": "contextvars",
        "description": "contextvars",
        "isExtraImport": true,
        "detail": "contextvars",
        "documentation": {}
    },
    {
        "label": "BaseCallbackHandler",
        "importPath": "llama_index.core.callbacks.base_handler",
        "description": "llama_index.core.callbacks.base_handler",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base_handler",
        "documentation": {}
    },
    {
        "label": "BaseCallbackHandler",
        "importPath": "llama_index.core.callbacks.base_handler",
        "description": "llama_index.core.callbacks.base_handler",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base_handler",
        "documentation": {}
    },
    {
        "label": "BaseCallbackHandler",
        "importPath": "llama_index.core.callbacks.base_handler",
        "description": "llama_index.core.callbacks.base_handler",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.base_handler",
        "documentation": {}
    },
    {
        "label": "CoreSchema",
        "importPath": "llama_index.core.bridge.pydantic_core",
        "description": "llama_index.core.bridge.pydantic_core",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic_core",
        "documentation": {}
    },
    {
        "label": "core_schema",
        "importPath": "llama_index.core.bridge.pydantic_core",
        "description": "llama_index.core.bridge.pydantic_core",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic_core",
        "documentation": {}
    },
    {
        "label": "CoreSchema",
        "importPath": "llama_index.core.bridge.pydantic_core",
        "description": "llama_index.core.bridge.pydantic_core",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic_core",
        "documentation": {}
    },
    {
        "label": "CoreSchema",
        "importPath": "llama_index.core.bridge.pydantic_core",
        "description": "llama_index.core.bridge.pydantic_core",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic_core",
        "documentation": {}
    },
    {
        "label": "core_schema",
        "importPath": "llama_index.core.bridge.pydantic_core",
        "description": "llama_index.core.bridge.pydantic_core",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.pydantic_core",
        "documentation": {}
    },
    {
        "label": "SimpleLLMHandler",
        "importPath": "llama_index.core.callbacks.simple_llm_handler",
        "description": "llama_index.core.callbacks.simple_llm_handler",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.simple_llm_handler",
        "documentation": {}
    },
    {
        "label": "PythonicallyPrintingBaseHandler",
        "importPath": "llama_index.core.callbacks.pythonically_printing_base_handler",
        "description": "llama_index.core.callbacks.pythonically_printing_base_handler",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.pythonically_printing_base_handler",
        "documentation": {}
    },
    {
        "label": "PythonicallyPrintingBaseHandler",
        "importPath": "llama_index.core.callbacks.pythonically_printing_base_handler",
        "description": "llama_index.core.callbacks.pythonically_printing_base_handler",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.pythonically_printing_base_handler",
        "documentation": {}
    },
    {
        "label": "PythonicallyPrintingBaseHandler",
        "importPath": "llama_index.core.callbacks.pythonically_printing_base_handler",
        "description": "llama_index.core.callbacks.pythonically_printing_base_handler",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.pythonically_printing_base_handler",
        "documentation": {}
    },
    {
        "label": "TokenCounter",
        "importPath": "llama_index.core.utilities.token_counting",
        "description": "llama_index.core.utilities.token_counting",
        "isExtraImport": true,
        "detail": "llama_index.core.utilities.token_counting",
        "documentation": {}
    },
    {
        "label": "TokenCounter",
        "importPath": "llama_index.core.utilities.token_counting",
        "description": "llama_index.core.utilities.token_counting",
        "isExtraImport": true,
        "detail": "llama_index.core.utilities.token_counting",
        "documentation": {}
    },
    {
        "label": "TokenCounter",
        "importPath": "llama_index.core.utilities.token_counting",
        "description": "llama_index.core.utilities.token_counting",
        "isExtraImport": true,
        "detail": "llama_index.core.utilities.token_counting",
        "documentation": {}
    },
    {
        "label": "functools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "functools",
        "description": "functools",
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "lru_cache",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "reduce",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "reduce",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "wraps",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "AgentChatResponse",
        "importPath": "llama_index.core.chat_engine.types",
        "description": "llama_index.core.chat_engine.types",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "BaseChatEngine",
        "importPath": "llama_index.core.chat_engine.types",
        "description": "llama_index.core.chat_engine.types",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "StreamingAgentChatResponse",
        "importPath": "llama_index.core.chat_engine.types",
        "description": "llama_index.core.chat_engine.types",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "ToolOutput",
        "importPath": "llama_index.core.chat_engine.types",
        "description": "llama_index.core.chat_engine.types",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "AgentChatResponse",
        "importPath": "llama_index.core.chat_engine.types",
        "description": "llama_index.core.chat_engine.types",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "BaseChatEngine",
        "importPath": "llama_index.core.chat_engine.types",
        "description": "llama_index.core.chat_engine.types",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "StreamingAgentChatResponse",
        "importPath": "llama_index.core.chat_engine.types",
        "description": "llama_index.core.chat_engine.types",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "AgentChatResponse",
        "importPath": "llama_index.core.chat_engine.types",
        "description": "llama_index.core.chat_engine.types",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "BaseChatEngine",
        "importPath": "llama_index.core.chat_engine.types",
        "description": "llama_index.core.chat_engine.types",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "StreamingAgentChatResponse",
        "importPath": "llama_index.core.chat_engine.types",
        "description": "llama_index.core.chat_engine.types",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "ToolOutput",
        "importPath": "llama_index.core.chat_engine.types",
        "description": "llama_index.core.chat_engine.types",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "AgentChatResponse",
        "importPath": "llama_index.core.chat_engine.types",
        "description": "llama_index.core.chat_engine.types",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "BaseChatEngine",
        "importPath": "llama_index.core.chat_engine.types",
        "description": "llama_index.core.chat_engine.types",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "StreamingAgentChatResponse",
        "importPath": "llama_index.core.chat_engine.types",
        "description": "llama_index.core.chat_engine.types",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "BaseChatEngine",
        "importPath": "llama_index.core.chat_engine.types",
        "description": "llama_index.core.chat_engine.types",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "ChatMode",
        "importPath": "llama_index.core.chat_engine.types",
        "description": "llama_index.core.chat_engine.types",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "AGENT_CHAT_RESPONSE_TYPE",
        "importPath": "llama_index.core.chat_engine.types",
        "description": "llama_index.core.chat_engine.types",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "AgentChatResponse",
        "importPath": "llama_index.core.chat_engine.types",
        "description": "llama_index.core.chat_engine.types",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "StreamingAgentChatResponse",
        "importPath": "llama_index.core.chat_engine.types",
        "description": "llama_index.core.chat_engine.types",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "AgentChatResponse",
        "importPath": "llama_index.core.chat_engine.types",
        "description": "llama_index.core.chat_engine.types",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "AgentChatResponse",
        "importPath": "llama_index.core.chat_engine.types",
        "description": "llama_index.core.chat_engine.types",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.indices.base_retriever",
        "description": "llama_index.core.indices.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.base_retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.indices.base_retriever",
        "description": "llama_index.core.indices.base_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.base_retriever",
        "documentation": {}
    },
    {
        "label": "messages_to_history_str",
        "importPath": "llama_index.core.base.llms.generic_utils",
        "description": "llama_index.core.base.llms.generic_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "messages_to_history_str",
        "importPath": "llama_index.core.base.llms.generic_utils",
        "description": "llama_index.core.base.llms.generic_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "completion_response_to_chat_response",
        "importPath": "llama_index.core.base.llms.generic_utils",
        "description": "llama_index.core.base.llms.generic_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "stream_completion_response_to_chat_response",
        "importPath": "llama_index.core.base.llms.generic_utils",
        "description": "llama_index.core.base.llms.generic_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "messages_to_prompt",
        "importPath": "llama_index.core.base.llms.generic_utils",
        "description": "llama_index.core.base.llms.generic_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "achat_to_completion_decorator",
        "importPath": "llama_index.core.base.llms.generic_utils",
        "description": "llama_index.core.base.llms.generic_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "chat_to_completion_decorator",
        "importPath": "llama_index.core.base.llms.generic_utils",
        "description": "llama_index.core.base.llms.generic_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "image_node_to_image_block",
        "importPath": "llama_index.core.base.llms.generic_utils",
        "description": "llama_index.core.base.llms.generic_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "messages_to_prompt",
        "importPath": "llama_index.core.base.llms.generic_utils",
        "description": "llama_index.core.base.llms.generic_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "prompt_to_messages",
        "importPath": "llama_index.core.base.llms.generic_utils",
        "description": "llama_index.core.base.llms.generic_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "messages_to_prompt",
        "importPath": "llama_index.core.base.llms.generic_utils",
        "description": "llama_index.core.base.llms.generic_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "image_node_to_image_block",
        "importPath": "llama_index.core.base.llms.generic_utils",
        "description": "llama_index.core.base.llms.generic_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "BaseNodePostprocessor",
        "importPath": "llama_index.core.postprocessor.types",
        "description": "llama_index.core.postprocessor.types",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.types",
        "documentation": {}
    },
    {
        "label": "BaseNodePostprocessor",
        "importPath": "llama_index.core.postprocessor.types",
        "description": "llama_index.core.postprocessor.types",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.types",
        "documentation": {}
    },
    {
        "label": "BaseNodePostprocessor",
        "importPath": "llama_index.core.postprocessor.types",
        "description": "llama_index.core.postprocessor.types",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.types",
        "documentation": {}
    },
    {
        "label": "BaseNodePostprocessor",
        "importPath": "llama_index.core.postprocessor.types",
        "description": "llama_index.core.postprocessor.types",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.types",
        "documentation": {}
    },
    {
        "label": "BaseNodePostprocessor",
        "importPath": "llama_index.core.postprocessor.types",
        "description": "llama_index.core.postprocessor.types",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.types",
        "documentation": {}
    },
    {
        "label": "BaseNodePostprocessor",
        "importPath": "llama_index.core.postprocessor.types",
        "description": "llama_index.core.postprocessor.types",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.types",
        "documentation": {}
    },
    {
        "label": "BaseNodePostprocessor",
        "importPath": "llama_index.core.postprocessor.types",
        "description": "llama_index.core.postprocessor.types",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.types",
        "documentation": {}
    },
    {
        "label": "BaseNodePostprocessor",
        "importPath": "llama_index.core.postprocessor.types",
        "description": "llama_index.core.postprocessor.types",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.types",
        "documentation": {}
    },
    {
        "label": "BaseNodePostprocessor",
        "importPath": "llama_index.core.postprocessor.types",
        "description": "llama_index.core.postprocessor.types",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.types",
        "documentation": {}
    },
    {
        "label": "BaseNodePostprocessor",
        "importPath": "llama_index.core.postprocessor.types",
        "description": "llama_index.core.postprocessor.types",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.types",
        "documentation": {}
    },
    {
        "label": "BaseNodePostprocessor",
        "importPath": "llama_index.core.postprocessor.types",
        "description": "llama_index.core.postprocessor.types",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.types",
        "documentation": {}
    },
    {
        "label": "BaseNodePostprocessor",
        "importPath": "llama_index.core.postprocessor.types",
        "description": "llama_index.core.postprocessor.types",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.types",
        "documentation": {}
    },
    {
        "label": "BaseNodePostprocessor",
        "importPath": "llama_index.core.postprocessor.types",
        "description": "llama_index.core.postprocessor.types",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.types",
        "documentation": {}
    },
    {
        "label": "BaseNodePostprocessor",
        "importPath": "llama_index.core.postprocessor.types",
        "description": "llama_index.core.postprocessor.types",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.types",
        "documentation": {}
    },
    {
        "label": "BaseNodePostprocessor",
        "importPath": "llama_index.core.postprocessor.types",
        "description": "llama_index.core.postprocessor.types",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.types",
        "documentation": {}
    },
    {
        "label": "BaseNodePostprocessor",
        "importPath": "llama_index.core.postprocessor.types",
        "description": "llama_index.core.postprocessor.types",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.types",
        "documentation": {}
    },
    {
        "label": "BaseNodePostprocessor",
        "importPath": "llama_index.core.postprocessor.types",
        "description": "llama_index.core.postprocessor.types",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.types",
        "documentation": {}
    },
    {
        "label": "BaseNodePostprocessor",
        "importPath": "llama_index.core.postprocessor.types",
        "description": "llama_index.core.postprocessor.types",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.types",
        "documentation": {}
    },
    {
        "label": "BaseNodePostprocessor",
        "importPath": "llama_index.core.postprocessor.types",
        "description": "llama_index.core.postprocessor.types",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.types",
        "documentation": {}
    },
    {
        "label": "BaseNodePostprocessor",
        "importPath": "llama_index.core.postprocessor.types",
        "description": "llama_index.core.postprocessor.types",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.types",
        "documentation": {}
    },
    {
        "label": "BaseNodePostprocessor",
        "importPath": "llama_index.core.postprocessor.types",
        "description": "llama_index.core.postprocessor.types",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.types",
        "documentation": {}
    },
    {
        "label": "CompactAndRefine",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "CompactAndRefine",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "CompactAndRefine",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "get_response_synthesizer",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "BaseSynthesizer",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "ResponseMode",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "get_response_synthesizer",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "get_response_synthesizer",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "get_response_synthesizer",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "ResponseMode",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "get_response_synthesizer",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "BaseSynthesizer",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "ResponseMode",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "get_response_synthesizer",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "BaseSynthesizer",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "get_response_synthesizer",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "BaseSynthesizer",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "get_response_synthesizer",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "BaseSynthesizer",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "ResponseMode",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "get_response_synthesizer",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "TreeSummarize",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "BaseSynthesizer",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "get_response_synthesizer",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "Accumulate",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "BaseSynthesizer",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "get_response_synthesizer",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "get_response_synthesizer",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "ResponseMode",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "get_response_synthesizer",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "TreeSummarize",
        "importPath": "llama_index.core.response_synthesizers",
        "description": "llama_index.core.response_synthesizers",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers",
        "documentation": {}
    },
    {
        "label": "get_prefix_messages_with_context",
        "importPath": "llama_index.core.chat_engine.utils",
        "description": "llama_index.core.chat_engine.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.utils",
        "documentation": {}
    },
    {
        "label": "get_response_synthesizer",
        "importPath": "llama_index.core.chat_engine.utils",
        "description": "llama_index.core.chat_engine.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.utils",
        "documentation": {}
    },
    {
        "label": "response_gen_from_query_engine",
        "importPath": "llama_index.core.chat_engine.utils",
        "description": "llama_index.core.chat_engine.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.utils",
        "documentation": {}
    },
    {
        "label": "aresponse_gen_from_query_engine",
        "importPath": "llama_index.core.chat_engine.utils",
        "description": "llama_index.core.chat_engine.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.utils",
        "documentation": {}
    },
    {
        "label": "get_prefix_messages_with_context",
        "importPath": "llama_index.core.chat_engine.utils",
        "description": "llama_index.core.chat_engine.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.utils",
        "documentation": {}
    },
    {
        "label": "get_response_synthesizer",
        "importPath": "llama_index.core.chat_engine.utils",
        "description": "llama_index.core.chat_engine.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.utils",
        "documentation": {}
    },
    {
        "label": "Queue",
        "importPath": "queue",
        "description": "queue",
        "isExtraImport": true,
        "detail": "queue",
        "documentation": {}
    },
    {
        "label": "Empty",
        "importPath": "queue",
        "description": "queue",
        "isExtraImport": true,
        "detail": "queue",
        "documentation": {}
    },
    {
        "label": "Queue",
        "importPath": "queue",
        "description": "queue",
        "isExtraImport": true,
        "detail": "queue",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "Event",
        "importPath": "threading",
        "description": "threading",
        "isExtraImport": true,
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "Event",
        "importPath": "threading",
        "description": "threading",
        "isExtraImport": true,
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "StreamChatErrorEvent",
        "importPath": "llama_index.core.instrumentation.events.chat_engine",
        "description": "llama_index.core.instrumentation.events.chat_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.chat_engine",
        "documentation": {}
    },
    {
        "label": "StreamChatEndEvent",
        "importPath": "llama_index.core.instrumentation.events.chat_engine",
        "description": "llama_index.core.instrumentation.events.chat_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.chat_engine",
        "documentation": {}
    },
    {
        "label": "StreamChatStartEvent",
        "importPath": "llama_index.core.instrumentation.events.chat_engine",
        "description": "llama_index.core.instrumentation.events.chat_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.chat_engine",
        "documentation": {}
    },
    {
        "label": "StreamChatDeltaReceivedEvent",
        "importPath": "llama_index.core.instrumentation.events.chat_engine",
        "description": "llama_index.core.instrumentation.events.chat_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.chat_engine",
        "documentation": {}
    },
    {
        "label": "Artifact",
        "importPath": "llama_index.core.chat_ui.models.artifact",
        "description": "llama_index.core.chat_ui.models.artifact",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_ui.models.artifact",
        "documentation": {}
    },
    {
        "label": "ComposableGraph",
        "importPath": "llama_index.core.indices.composability.graph",
        "description": "llama_index.core.indices.composability.graph",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.composability.graph",
        "documentation": {}
    },
    {
        "label": "ComposableGraph",
        "importPath": "llama_index.core.indices.composability.graph",
        "description": "llama_index.core.indices.composability.graph",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.composability.graph",
        "documentation": {}
    },
    {
        "label": "ComposableGraph",
        "importPath": "llama_index.core.indices.composability.graph",
        "description": "llama_index.core.indices.composability.graph",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.composability.graph",
        "documentation": {}
    },
    {
        "label": "ComposableGraph",
        "importPath": "llama_index.core.indices.composability.graph",
        "description": "llama_index.core.indices.composability.graph",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.composability.graph",
        "documentation": {}
    },
    {
        "label": "ComposableGraph",
        "importPath": "llama_index.core.indices.composability.graph",
        "description": "llama_index.core.indices.composability.graph",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.composability.graph",
        "documentation": {}
    },
    {
        "label": "ComposableGraph",
        "importPath": "llama_index.core.indices.composability.graph",
        "description": "llama_index.core.indices.composability.graph",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.composability.graph",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "Embedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "SimilarityMode",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "similarity",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "similarity",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "SimilarityMode",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "mean_agg",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base",
        "description": "llama_index.core.base.embeddings.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "SummaryIndex",
        "importPath": "llama_index.core.indices.list.base",
        "description": "llama_index.core.indices.list.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.list.base",
        "documentation": {}
    },
    {
        "label": "SummaryIndex",
        "importPath": "llama_index.core.indices.list.base",
        "description": "llama_index.core.indices.list.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.list.base",
        "documentation": {}
    },
    {
        "label": "SummaryIndex",
        "importPath": "llama_index.core.indices.list.base",
        "description": "llama_index.core.indices.list.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.list.base",
        "documentation": {}
    },
    {
        "label": "ListRetrieverMode",
        "importPath": "llama_index.core.indices.list.base",
        "description": "llama_index.core.indices.list.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.list.base",
        "documentation": {}
    },
    {
        "label": "SummaryIndex",
        "importPath": "llama_index.core.indices.list.base",
        "description": "llama_index.core.indices.list.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.list.base",
        "documentation": {}
    },
    {
        "label": "SummaryIndex",
        "importPath": "llama_index.core.indices.list.base",
        "description": "llama_index.core.indices.list.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.list.base",
        "documentation": {}
    },
    {
        "label": "ListRetrieverMode",
        "importPath": "llama_index.core.indices.list.base",
        "description": "llama_index.core.indices.list.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.list.base",
        "documentation": {}
    },
    {
        "label": "SummaryIndex",
        "importPath": "llama_index.core.indices.list.base",
        "description": "llama_index.core.indices.list.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.list.base",
        "documentation": {}
    },
    {
        "label": "SummaryIndex",
        "importPath": "llama_index.core.indices.list.base",
        "description": "llama_index.core.indices.list.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.list.base",
        "documentation": {}
    },
    {
        "label": "SummaryIndex",
        "importPath": "llama_index.core.indices.list.base",
        "description": "llama_index.core.indices.list.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.list.base",
        "documentation": {}
    },
    {
        "label": "SummaryIndex",
        "importPath": "llama_index.core.indices.list.base",
        "description": "llama_index.core.indices.list.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.list.base",
        "documentation": {}
    },
    {
        "label": "SummaryIndex",
        "importPath": "llama_index.core.indices.list.base",
        "description": "llama_index.core.indices.list.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.list.base",
        "documentation": {}
    },
    {
        "label": "SummaryIndex",
        "importPath": "llama_index.core.indices.list.base",
        "description": "llama_index.core.indices.list.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.list.base",
        "documentation": {}
    },
    {
        "label": "SummaryIndex",
        "importPath": "llama_index.core.indices.list.base",
        "description": "llama_index.core.indices.list.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.list.base",
        "documentation": {}
    },
    {
        "label": "SummaryIndex",
        "importPath": "llama_index.core.indices.list.base",
        "description": "llama_index.core.indices.list.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.list.base",
        "documentation": {}
    },
    {
        "label": "SummaryIndex",
        "importPath": "llama_index.core.indices.list.base",
        "description": "llama_index.core.indices.list.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.list.base",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core.indices.vector_store",
        "description": "llama_index.core.indices.vector_store",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.vector_store",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core.indices.vector_store",
        "description": "llama_index.core.indices.vector_store",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.vector_store",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core.indices.vector_store",
        "description": "llama_index.core.indices.vector_store",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.vector_store",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core.indices.vector_store",
        "description": "llama_index.core.indices.vector_store",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.vector_store",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core.indices.vector_store",
        "description": "llama_index.core.indices.vector_store",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.vector_store",
        "documentation": {}
    },
    {
        "label": "run_transformations",
        "importPath": "llama_index.core.ingestion",
        "description": "llama_index.core.ingestion",
        "isExtraImport": true,
        "detail": "llama_index.core.ingestion",
        "documentation": {}
    },
    {
        "label": "run_transformations",
        "importPath": "llama_index.core.ingestion",
        "description": "llama_index.core.ingestion",
        "isExtraImport": true,
        "detail": "llama_index.core.ingestion",
        "documentation": {}
    },
    {
        "label": "run_transformations",
        "importPath": "llama_index.core.ingestion",
        "description": "llama_index.core.ingestion",
        "isExtraImport": true,
        "detail": "llama_index.core.ingestion",
        "documentation": {}
    },
    {
        "label": "arun_transformations",
        "importPath": "llama_index.core.ingestion",
        "description": "llama_index.core.ingestion",
        "isExtraImport": true,
        "detail": "llama_index.core.ingestion",
        "documentation": {}
    },
    {
        "label": "run_transformations",
        "importPath": "llama_index.core.ingestion",
        "description": "llama_index.core.ingestion",
        "isExtraImport": true,
        "detail": "llama_index.core.ingestion",
        "documentation": {}
    },
    {
        "label": "IngestionCache",
        "importPath": "llama_index.core.ingestion",
        "description": "llama_index.core.ingestion",
        "isExtraImport": true,
        "detail": "llama_index.core.ingestion",
        "documentation": {}
    },
    {
        "label": "run_transformations",
        "importPath": "llama_index.core.ingestion",
        "description": "llama_index.core.ingestion",
        "isExtraImport": true,
        "detail": "llama_index.core.ingestion",
        "documentation": {}
    },
    {
        "label": "RouterQueryEngine",
        "importPath": "llama_index.core.query_engine.router_query_engine",
        "description": "llama_index.core.query_engine.router_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.query_engine.router_query_engine",
        "documentation": {}
    },
    {
        "label": "StorageContext",
        "importPath": "llama_index.core.storage.storage_context",
        "description": "llama_index.core.storage.storage_context",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "StorageContext",
        "importPath": "llama_index.core.storage.storage_context",
        "description": "llama_index.core.storage.storage_context",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "StorageContext",
        "importPath": "llama_index.core.storage.storage_context",
        "description": "llama_index.core.storage.storage_context",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "StorageContext",
        "importPath": "llama_index.core.storage.storage_context",
        "description": "llama_index.core.storage.storage_context",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "StorageContext",
        "importPath": "llama_index.core.storage.storage_context",
        "description": "llama_index.core.storage.storage_context",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "StorageContext",
        "importPath": "llama_index.core.storage.storage_context",
        "description": "llama_index.core.storage.storage_context",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "StorageContext",
        "importPath": "llama_index.core.storage.storage_context",
        "description": "llama_index.core.storage.storage_context",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "StorageContext",
        "importPath": "llama_index.core.storage.storage_context",
        "description": "llama_index.core.storage.storage_context",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "StorageContext",
        "importPath": "llama_index.core.storage.storage_context",
        "description": "llama_index.core.storage.storage_context",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "StorageContext",
        "importPath": "llama_index.core.storage.storage_context",
        "description": "llama_index.core.storage.storage_context",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "StorageContext",
        "importPath": "llama_index.core.storage.storage_context",
        "description": "llama_index.core.storage.storage_context",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "DOCSTORE_FNAME",
        "importPath": "llama_index.core.storage.storage_context",
        "description": "llama_index.core.storage.storage_context",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_DIR",
        "importPath": "llama_index.core.storage.storage_context",
        "description": "llama_index.core.storage.storage_context",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "StorageContext",
        "importPath": "llama_index.core.storage.storage_context",
        "description": "llama_index.core.storage.storage_context",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_DIR",
        "importPath": "llama_index.core.storage.storage_context",
        "description": "llama_index.core.storage.storage_context",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "StorageContext",
        "importPath": "llama_index.core.storage.storage_context",
        "description": "llama_index.core.storage.storage_context",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "StorageContext",
        "importPath": "llama_index.core.storage.storage_context",
        "description": "llama_index.core.storage.storage_context",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "StorageContext",
        "importPath": "llama_index.core.storage.storage_context",
        "description": "llama_index.core.storage.storage_context",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "StorageContext",
        "importPath": "llama_index.core.storage.storage_context",
        "description": "llama_index.core.storage.storage_context",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "StorageContext",
        "importPath": "llama_index.core.storage.storage_context",
        "description": "llama_index.core.storage.storage_context",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "StorageContext",
        "importPath": "llama_index.core.storage.storage_context",
        "description": "llama_index.core.storage.storage_context",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "QueryEngineTool",
        "importPath": "llama_index.core.tools.query_engine",
        "description": "llama_index.core.tools.query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.query_engine",
        "documentation": {}
    },
    {
        "label": "QueryEngineTool",
        "importPath": "llama_index.core.tools.query_engine",
        "description": "llama_index.core.tools.query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.query_engine",
        "documentation": {}
    },
    {
        "label": "QueryEngineTool",
        "importPath": "llama_index.core.tools.query_engine",
        "description": "llama_index.core.tools.query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.query_engine",
        "documentation": {}
    },
    {
        "label": "QueryEngineTool",
        "importPath": "llama_index.core.tools.query_engine",
        "description": "llama_index.core.tools.query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.query_engine",
        "documentation": {}
    },
    {
        "label": "QueryEngineTool",
        "importPath": "llama_index.core.tools.query_engine",
        "description": "llama_index.core.tools.query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.query_engine",
        "documentation": {}
    },
    {
        "label": "QueryEngineTool",
        "importPath": "llama_index.core.tools.query_engine",
        "description": "llama_index.core.tools.query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.query_engine",
        "documentation": {}
    },
    {
        "label": "DataClassJsonMixin",
        "importPath": "dataclasses_json",
        "description": "dataclasses_json",
        "isExtraImport": true,
        "detail": "dataclasses_json",
        "documentation": {}
    },
    {
        "label": "DataClassJsonMixin",
        "importPath": "dataclasses_json",
        "description": "dataclasses_json",
        "isExtraImport": true,
        "detail": "dataclasses_json",
        "documentation": {}
    },
    {
        "label": "DataClassJsonMixin",
        "importPath": "dataclasses_json",
        "description": "dataclasses_json",
        "isExtraImport": true,
        "detail": "dataclasses_json",
        "documentation": {}
    },
    {
        "label": "DataClassJsonMixin",
        "importPath": "dataclasses_json",
        "description": "dataclasses_json",
        "isExtraImport": true,
        "detail": "dataclasses_json",
        "documentation": {}
    },
    {
        "label": "DataClassJsonMixin",
        "importPath": "dataclasses_json",
        "description": "dataclasses_json",
        "isExtraImport": true,
        "detail": "dataclasses_json",
        "documentation": {}
    },
    {
        "label": "DataClassJsonMixin",
        "importPath": "dataclasses_json",
        "description": "dataclasses_json",
        "isExtraImport": true,
        "detail": "dataclasses_json",
        "documentation": {}
    },
    {
        "label": "DataClassJsonMixin",
        "importPath": "dataclasses_json",
        "description": "dataclasses_json",
        "isExtraImport": true,
        "detail": "dataclasses_json",
        "documentation": {}
    },
    {
        "label": "DataClassJsonMixin",
        "importPath": "dataclasses_json",
        "description": "dataclasses_json",
        "isExtraImport": true,
        "detail": "dataclasses_json",
        "documentation": {}
    },
    {
        "label": "IndexStructType",
        "importPath": "llama_index.core.data_structs.struct_type",
        "description": "llama_index.core.data_structs.struct_type",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.struct_type",
        "documentation": {}
    },
    {
        "label": "IndexStructType",
        "importPath": "llama_index.core.data_structs.struct_type",
        "description": "llama_index.core.data_structs.struct_type",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.struct_type",
        "documentation": {}
    },
    {
        "label": "IndexStructType",
        "importPath": "llama_index.core.data_structs.struct_type",
        "description": "llama_index.core.data_structs.struct_type",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.struct_type",
        "documentation": {}
    },
    {
        "label": "IndexStructType",
        "importPath": "llama_index.core.data_structs.struct_type",
        "description": "llama_index.core.data_structs.struct_type",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.struct_type",
        "documentation": {}
    },
    {
        "label": "IndexStructType",
        "importPath": "llama_index.core.data_structs.struct_type",
        "description": "llama_index.core.data_structs.struct_type",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.struct_type",
        "documentation": {}
    },
    {
        "label": "IndexStructType",
        "importPath": "llama_index.core.data_structs.struct_type",
        "description": "llama_index.core.data_structs.struct_type",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.struct_type",
        "documentation": {}
    },
    {
        "label": "IndexStruct",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "KG",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "EmptyIndexStruct",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexDict",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexGraph",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexList",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexLPG",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexStruct",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "KeywordTable",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "MultiModelIndexDict",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexStruct",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexGraph",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexStruct",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "EmptyIndexStruct",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "KeywordTable",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "KG",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexList",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexDict",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexDict",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "MultiModelIndexDict",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexDict",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexGraph",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexGraph",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexGraph",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexDict",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexStruct",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexStruct",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexStruct",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexStruct",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "EmptyIndexStruct",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexStruct",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexGraph",
        "importPath": "llama_index.core.data_structs.data_structs",
        "description": "llama_index.core.data_structs.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexDocumentSummary",
        "importPath": "llama_index.core.data_structs.document_summary",
        "description": "llama_index.core.data_structs.document_summary",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.document_summary",
        "documentation": {}
    },
    {
        "label": "IndexDocumentSummary",
        "importPath": "llama_index.core.data_structs.document_summary",
        "description": "llama_index.core.data_structs.document_summary",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.document_summary",
        "documentation": {}
    },
    {
        "label": "PandasStructTable",
        "importPath": "llama_index.core.data_structs.table",
        "description": "llama_index.core.data_structs.table",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.table",
        "documentation": {}
    },
    {
        "label": "SQLStructTable",
        "importPath": "llama_index.core.data_structs.table",
        "description": "llama_index.core.data_structs.table",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.table",
        "documentation": {}
    },
    {
        "label": "StructDatapoint",
        "importPath": "llama_index.core.data_structs.table",
        "description": "llama_index.core.data_structs.table",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.table",
        "documentation": {}
    },
    {
        "label": "StructDatapoint",
        "importPath": "llama_index.core.data_structs.table",
        "description": "llama_index.core.data_structs.table",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.table",
        "documentation": {}
    },
    {
        "label": "BaseStructTable",
        "importPath": "llama_index.core.data_structs.table",
        "description": "llama_index.core.data_structs.table",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.table",
        "documentation": {}
    },
    {
        "label": "SQLStructTable",
        "importPath": "llama_index.core.data_structs.table",
        "description": "llama_index.core.data_structs.table",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.table",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tqdm",
        "description": "tqdm",
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "get_file_content",
        "importPath": "llama_index.core.download.utils",
        "description": "llama_index.core.download.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.download.utils",
        "documentation": {}
    },
    {
        "label": "get_file_content_bytes",
        "importPath": "llama_index.core.download.utils",
        "description": "llama_index.core.download.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.download.utils",
        "documentation": {}
    },
    {
        "label": "get_source_files_list",
        "importPath": "llama_index.core.download.utils",
        "description": "llama_index.core.download.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.download.utils",
        "documentation": {}
    },
    {
        "label": "initialize_directory",
        "importPath": "llama_index.core.download.utils",
        "description": "llama_index.core.download.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.download.utils",
        "documentation": {}
    },
    {
        "label": "get_exports",
        "importPath": "llama_index.core.download.utils",
        "description": "llama_index.core.download.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.download.utils",
        "documentation": {}
    },
    {
        "label": "get_file_content",
        "importPath": "llama_index.core.download.utils",
        "description": "llama_index.core.download.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.download.utils",
        "documentation": {}
    },
    {
        "label": "initialize_directory",
        "importPath": "llama_index.core.download.utils",
        "description": "llama_index.core.download.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.download.utils",
        "documentation": {}
    },
    {
        "label": "rewrite_exports",
        "importPath": "llama_index.core.download.utils",
        "description": "llama_index.core.download.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.download.utils",
        "documentation": {}
    },
    {
        "label": "ChangeDirectory",
        "importPath": "llama_index.core.download.utils",
        "description": "llama_index.core.download.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.download.utils",
        "documentation": {}
    },
    {
        "label": "get_file_content",
        "importPath": "llama_index.core.download.utils",
        "description": "llama_index.core.download.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.download.utils",
        "documentation": {}
    },
    {
        "label": "initialize_directory",
        "importPath": "llama_index.core.download.utils",
        "description": "llama_index.core.download.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.download.utils",
        "documentation": {}
    },
    {
        "label": "get_source_files_recursive",
        "importPath": "llama_index.core.download.utils",
        "description": "llama_index.core.download.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.download.utils",
        "documentation": {}
    },
    {
        "label": "importlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "importlib",
        "description": "importlib",
        "detail": "importlib",
        "documentation": {}
    },
    {
        "label": "util",
        "importPath": "importlib",
        "description": "importlib",
        "isExtraImport": true,
        "detail": "importlib",
        "documentation": {}
    },
    {
        "label": "util",
        "importPath": "importlib",
        "description": "importlib",
        "isExtraImport": true,
        "detail": "importlib",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "MockEmbedding",
        "importPath": "llama_index.core.embeddings.mock_embed_model",
        "description": "llama_index.core.embeddings.mock_embed_model",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings.mock_embed_model",
        "documentation": {}
    },
    {
        "label": "MockEmbedding",
        "importPath": "llama_index.core.embeddings.mock_embed_model",
        "description": "llama_index.core.embeddings.mock_embed_model",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings.mock_embed_model",
        "documentation": {}
    },
    {
        "label": "MockEmbedding",
        "importPath": "llama_index.core.embeddings.mock_embed_model",
        "description": "llama_index.core.embeddings.mock_embed_model",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings.mock_embed_model",
        "documentation": {}
    },
    {
        "label": "MockEmbedding",
        "importPath": "llama_index.core.embeddings.mock_embed_model",
        "description": "llama_index.core.embeddings.mock_embed_model",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings.mock_embed_model",
        "documentation": {}
    },
    {
        "label": "MockEmbedding",
        "importPath": "llama_index.core.embeddings.mock_embed_model",
        "description": "llama_index.core.embeddings.mock_embed_model",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings.mock_embed_model",
        "documentation": {}
    },
    {
        "label": "MockEmbedding",
        "importPath": "llama_index.core.embeddings.mock_embed_model",
        "description": "llama_index.core.embeddings.mock_embed_model",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings.mock_embed_model",
        "documentation": {}
    },
    {
        "label": "MockEmbedding",
        "importPath": "llama_index.core.embeddings.mock_embed_model",
        "description": "llama_index.core.embeddings.mock_embed_model",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings.mock_embed_model",
        "documentation": {}
    },
    {
        "label": "MockEmbedding",
        "importPath": "llama_index.core.embeddings.mock_embed_model",
        "description": "llama_index.core.embeddings.mock_embed_model",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings.mock_embed_model",
        "documentation": {}
    },
    {
        "label": "MockEmbedding",
        "importPath": "llama_index.core.embeddings.mock_embed_model",
        "description": "llama_index.core.embeddings.mock_embed_model",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings.mock_embed_model",
        "documentation": {}
    },
    {
        "label": "RetrieverQueryEngine",
        "importPath": "llama_index.core.query_engine.retriever_query_engine",
        "description": "llama_index.core.query_engine.retriever_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.query_engine.retriever_query_engine",
        "documentation": {}
    },
    {
        "label": "RetrieverQueryEngine",
        "importPath": "llama_index.core.query_engine.retriever_query_engine",
        "description": "llama_index.core.query_engine.retriever_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.query_engine.retriever_query_engine",
        "documentation": {}
    },
    {
        "label": "RetrieverQueryEngine",
        "importPath": "llama_index.core.query_engine.retriever_query_engine",
        "description": "llama_index.core.query_engine.retriever_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.query_engine.retriever_query_engine",
        "documentation": {}
    },
    {
        "label": "RetrieverQueryEngine",
        "importPath": "llama_index.core.query_engine.retriever_query_engine",
        "description": "llama_index.core.query_engine.retriever_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.query_engine.retriever_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseEvaluator",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "EvaluationResult",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "BaseEvaluator",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "EvaluationResult",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "BaseEvaluator",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "EvaluationResult",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "BaseEvaluator",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "EvaluationResult",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "BaseEvaluator",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "EvaluationResult",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "BaseEvaluator",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "EvaluationResult",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "EvaluationResult",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "BaseEvaluator",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "EvaluationResult",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "BaseEvaluator",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "EvaluationResult",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "BaseEvaluator",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "EvaluationResult",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "BaseEvaluator",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "EvaluationResult",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "BaseEvaluator",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "EvaluationResult",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "Evaluation",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "BaseEvaluator",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "EvaluationResult",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "EvaluationResult",
        "importPath": "llama_index.core.evaluation.base",
        "description": "llama_index.core.evaluation.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "resolve_metrics",
        "importPath": "llama_index.core.evaluation.retrieval.metrics",
        "description": "llama_index.core.evaluation.retrieval.metrics",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.retrieval.metrics",
        "documentation": {}
    },
    {
        "label": "AveragePrecision",
        "importPath": "llama_index.core.evaluation.retrieval.metrics",
        "description": "llama_index.core.evaluation.retrieval.metrics",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.retrieval.metrics",
        "documentation": {}
    },
    {
        "label": "HitRate",
        "importPath": "llama_index.core.evaluation.retrieval.metrics",
        "description": "llama_index.core.evaluation.retrieval.metrics",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.retrieval.metrics",
        "documentation": {}
    },
    {
        "label": "MRR",
        "importPath": "llama_index.core.evaluation.retrieval.metrics",
        "description": "llama_index.core.evaluation.retrieval.metrics",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.retrieval.metrics",
        "documentation": {}
    },
    {
        "label": "NDCG",
        "importPath": "llama_index.core.evaluation.retrieval.metrics",
        "description": "llama_index.core.evaluation.retrieval.metrics",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.retrieval.metrics",
        "documentation": {}
    },
    {
        "label": "Precision",
        "importPath": "llama_index.core.evaluation.retrieval.metrics",
        "description": "llama_index.core.evaluation.retrieval.metrics",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.retrieval.metrics",
        "documentation": {}
    },
    {
        "label": "Recall",
        "importPath": "llama_index.core.evaluation.retrieval.metrics",
        "description": "llama_index.core.evaluation.retrieval.metrics",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.retrieval.metrics",
        "documentation": {}
    },
    {
        "label": "BaseRetrievalMetric",
        "importPath": "llama_index.core.evaluation.retrieval.metrics_base",
        "description": "llama_index.core.evaluation.retrieval.metrics_base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.retrieval.metrics_base",
        "documentation": {}
    },
    {
        "label": "RetrievalMetricResult",
        "importPath": "llama_index.core.evaluation.retrieval.metrics_base",
        "description": "llama_index.core.evaluation.retrieval.metrics_base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.retrieval.metrics_base",
        "documentation": {}
    },
    {
        "label": "BaseRetrievalMetric",
        "importPath": "llama_index.core.evaluation.retrieval.metrics_base",
        "description": "llama_index.core.evaluation.retrieval.metrics_base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.retrieval.metrics_base",
        "documentation": {}
    },
    {
        "label": "RetrievalMetricResult",
        "importPath": "llama_index.core.evaluation.retrieval.metrics_base",
        "description": "llama_index.core.evaluation.retrieval.metrics_base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.retrieval.metrics_base",
        "documentation": {}
    },
    {
        "label": "EmbeddingQAFinetuneDataset",
        "importPath": "llama_index.core.llama_dataset.legacy.embedding",
        "description": "llama_index.core.llama_dataset.legacy.embedding",
        "isExtraImport": true,
        "detail": "llama_index.core.llama_dataset.legacy.embedding",
        "documentation": {}
    },
    {
        "label": "BaseRetrievalEvaluator",
        "importPath": "llama_index.core.evaluation.retrieval.base",
        "description": "llama_index.core.evaluation.retrieval.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.retrieval.base",
        "documentation": {}
    },
    {
        "label": "RetrievalEvalMode",
        "importPath": "llama_index.core.evaluation.retrieval.base",
        "description": "llama_index.core.evaluation.retrieval.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.retrieval.base",
        "documentation": {}
    },
    {
        "label": "RetrievalEvalResult",
        "importPath": "llama_index.core.evaluation.retrieval.base",
        "description": "llama_index.core.evaluation.retrieval.base",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.retrieval.base",
        "documentation": {}
    },
    {
        "label": "retry",
        "importPath": "tenacity",
        "description": "tenacity",
        "isExtraImport": true,
        "detail": "tenacity",
        "documentation": {}
    },
    {
        "label": "stop_after_attempt",
        "importPath": "tenacity",
        "description": "tenacity",
        "isExtraImport": true,
        "detail": "tenacity",
        "documentation": {}
    },
    {
        "label": "wait_exponential",
        "importPath": "tenacity",
        "description": "tenacity",
        "isExtraImport": true,
        "detail": "tenacity",
        "documentation": {}
    },
    {
        "label": "SummaryIndex",
        "importPath": "llama_index.core.indices",
        "description": "llama_index.core.indices",
        "isExtraImport": true,
        "detail": "llama_index.core.indices",
        "documentation": {}
    },
    {
        "label": "SummaryIndex",
        "importPath": "llama_index.core.indices",
        "description": "llama_index.core.indices",
        "isExtraImport": true,
        "detail": "llama_index.core.indices",
        "documentation": {}
    },
    {
        "label": "SummaryIndex",
        "importPath": "llama_index.core.indices",
        "description": "llama_index.core.indices",
        "isExtraImport": true,
        "detail": "llama_index.core.indices",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core.indices",
        "description": "llama_index.core.indices",
        "isExtraImport": true,
        "detail": "llama_index.core.indices",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core.indices",
        "description": "llama_index.core.indices",
        "isExtraImport": true,
        "detail": "llama_index.core.indices",
        "documentation": {}
    },
    {
        "label": "default_parser",
        "importPath": "llama_index.core.evaluation.eval_utils",
        "description": "llama_index.core.evaluation.eval_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.eval_utils",
        "documentation": {}
    },
    {
        "label": "upload_eval_dataset",
        "importPath": "llama_index.core.evaluation.eval_utils",
        "description": "llama_index.core.evaluation.eval_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.eval_utils",
        "documentation": {}
    },
    {
        "label": "deprecated",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "deprecated",
        "description": "deprecated",
        "detail": "deprecated",
        "documentation": {}
    },
    {
        "label": "deprecated",
        "importPath": "deprecated",
        "description": "deprecated",
        "isExtraImport": true,
        "detail": "deprecated",
        "documentation": {}
    },
    {
        "label": "deprecated",
        "importPath": "deprecated",
        "description": "deprecated",
        "isExtraImport": true,
        "detail": "deprecated",
        "documentation": {}
    },
    {
        "label": "deprecated",
        "importPath": "deprecated",
        "description": "deprecated",
        "isExtraImport": true,
        "detail": "deprecated",
        "documentation": {}
    },
    {
        "label": "deprecated",
        "importPath": "deprecated",
        "description": "deprecated",
        "isExtraImport": true,
        "detail": "deprecated",
        "documentation": {}
    },
    {
        "label": "deprecated",
        "importPath": "deprecated",
        "description": "deprecated",
        "isExtraImport": true,
        "detail": "deprecated",
        "documentation": {}
    },
    {
        "label": "deprecated",
        "importPath": "deprecated",
        "description": "deprecated",
        "isExtraImport": true,
        "detail": "deprecated",
        "documentation": {}
    },
    {
        "label": "deprecated",
        "importPath": "deprecated",
        "description": "deprecated",
        "isExtraImport": true,
        "detail": "deprecated",
        "documentation": {}
    },
    {
        "label": "SummaryIndex",
        "importPath": "llama_index.core.indices.list",
        "description": "llama_index.core.indices.list",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.list",
        "documentation": {}
    },
    {
        "label": "KeywordNodePostprocessor",
        "importPath": "llama_index.core.postprocessor.node",
        "description": "llama_index.core.postprocessor.node",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.node",
        "documentation": {}
    },
    {
        "label": "KeywordNodePostprocessor",
        "importPath": "llama_index.core.postprocessor.node",
        "description": "llama_index.core.postprocessor.node",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.node",
        "documentation": {}
    },
    {
        "label": "KeywordNodePostprocessor",
        "importPath": "llama_index.core.postprocessor.node",
        "description": "llama_index.core.postprocessor.node",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.node",
        "documentation": {}
    },
    {
        "label": "PrevNextNodePostprocessor",
        "importPath": "llama_index.core.postprocessor.node",
        "description": "llama_index.core.postprocessor.node",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.node",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TEXT_QA_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TABLE_CONTEXT_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TABLE_CONTEXT_QUERY",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CHOICE_SELECT_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SIMPLE_INPUT_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_KEYWORD_EXTRACT_TEMPLATE",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_KEYWORD_EXTRACT_TEMPLATE",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_KG_TRIPLET_EXTRACT_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CHOICE_SELECT_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_DYNAMIC_EXTRACT_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_DYNAMIC_EXTRACT_PROPS_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_KG_TRIPLET_EXTRACT_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_HYDE_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SCHEMA_EXTRACT_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_JSON_PATH_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TEXT_TO_SQL_PGVECTOR_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TEXT_TO_SQL_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_REFINE_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TEXT_TO_SQL_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_INSERT_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SUMMARY_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_INSERT_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SUMMARY_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_QUERY_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_QUERY_PROMPT_MULTIPLE",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TEXT_QA_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TEXT_QA_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CHOICE_SELECT_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "RANKGPT_RERANK_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "STRUCTURED_CHOICE_SELECT_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_REFINE_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_REFINE_TABLE_CONTEXT_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TEXT_QA_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TREE_SUMMARIZE_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TEXT_QA_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SIMPLE_INPUT_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SIMPLE_INPUT_PROMPT",
        "importPath": "llama_index.core.prompts.default_prompts",
        "description": "llama_index.core.prompts.default_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "get_client",
        "importPath": "llama_index.core.ingestion.api_utils",
        "description": "llama_index.core.ingestion.api_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.ingestion.api_utils",
        "documentation": {}
    },
    {
        "label": "PydanticOutputParser",
        "importPath": "llama_index.core.output_parsers",
        "description": "llama_index.core.output_parsers",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers",
        "documentation": {}
    },
    {
        "label": "BaseOutputParser",
        "importPath": "llama_index.core.output_parsers",
        "description": "llama_index.core.output_parsers",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers",
        "documentation": {}
    },
    {
        "label": "BaseOutputParser",
        "importPath": "llama_index.core.output_parsers",
        "description": "llama_index.core.output_parsers",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers",
        "documentation": {}
    },
    {
        "label": "EvaluationResult",
        "importPath": "llama_index.core.evaluation",
        "description": "llama_index.core.evaluation",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation",
        "documentation": {}
    },
    {
        "label": "BaseEvaluator",
        "importPath": "llama_index.core.evaluation",
        "description": "llama_index.core.evaluation",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation",
        "documentation": {}
    },
    {
        "label": "BaseEvaluator",
        "importPath": "llama_index.core.evaluation",
        "description": "llama_index.core.evaluation",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation",
        "documentation": {}
    },
    {
        "label": "EvaluationResult",
        "importPath": "llama_index.core.evaluation",
        "description": "llama_index.core.evaluation",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation",
        "documentation": {}
    },
    {
        "label": "BaseEvaluator",
        "importPath": "llama_index.core.evaluation",
        "description": "llama_index.core.evaluation",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation",
        "documentation": {}
    },
    {
        "label": "AnswerRelevancyEvaluator",
        "importPath": "llama_index.core.evaluation",
        "description": "llama_index.core.evaluation",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation",
        "documentation": {}
    },
    {
        "label": "BaseEvaluator",
        "importPath": "llama_index.core.evaluation",
        "description": "llama_index.core.evaluation",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation",
        "documentation": {}
    },
    {
        "label": "EvaluationResult",
        "importPath": "llama_index.core.evaluation",
        "description": "llama_index.core.evaluation",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation",
        "documentation": {}
    },
    {
        "label": "BaseEvaluator",
        "importPath": "llama_index.core.evaluation",
        "description": "llama_index.core.evaluation",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation",
        "documentation": {}
    },
    {
        "label": "BaseEvaluator",
        "importPath": "llama_index.core.evaluation",
        "description": "llama_index.core.evaluation",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation",
        "documentation": {}
    },
    {
        "label": "BaseExtractor",
        "importPath": "llama_index.core.extractors",
        "description": "llama_index.core.extractors",
        "isExtraImport": true,
        "detail": "llama_index.core.extractors",
        "documentation": {}
    },
    {
        "label": "DocumentContextExtractor",
        "importPath": "llama_index.core.extractors",
        "description": "llama_index.core.extractors",
        "isExtraImport": true,
        "detail": "llama_index.core.extractors",
        "documentation": {}
    },
    {
        "label": "KeywordExtractor",
        "importPath": "llama_index.core.extractors",
        "description": "llama_index.core.extractors",
        "isExtraImport": true,
        "detail": "llama_index.core.extractors",
        "documentation": {}
    },
    {
        "label": "KeywordExtractor",
        "importPath": "llama_index.core.extractors",
        "description": "llama_index.core.extractors",
        "isExtraImport": true,
        "detail": "llama_index.core.extractors",
        "documentation": {}
    },
    {
        "label": "QuestionsAnsweredExtractor",
        "importPath": "llama_index.core.extractors",
        "description": "llama_index.core.extractors",
        "isExtraImport": true,
        "detail": "llama_index.core.extractors",
        "documentation": {}
    },
    {
        "label": "SummaryExtractor",
        "importPath": "llama_index.core.extractors",
        "description": "llama_index.core.extractors",
        "isExtraImport": true,
        "detail": "llama_index.core.extractors",
        "documentation": {}
    },
    {
        "label": "TitleExtractor",
        "importPath": "llama_index.core.extractors",
        "description": "llama_index.core.extractors",
        "isExtraImport": true,
        "detail": "llama_index.core.extractors",
        "documentation": {}
    },
    {
        "label": "BaseDocumentStore",
        "importPath": "llama_index.core.storage.docstore.types",
        "description": "llama_index.core.storage.docstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "RefDocInfo",
        "importPath": "llama_index.core.storage.docstore.types",
        "description": "llama_index.core.storage.docstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "RefDocInfo",
        "importPath": "llama_index.core.storage.docstore.types",
        "description": "llama_index.core.storage.docstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "RefDocInfo",
        "importPath": "llama_index.core.storage.docstore.types",
        "description": "llama_index.core.storage.docstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "RefDocInfo",
        "importPath": "llama_index.core.storage.docstore.types",
        "description": "llama_index.core.storage.docstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "RefDocInfo",
        "importPath": "llama_index.core.storage.docstore.types",
        "description": "llama_index.core.storage.docstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "RefDocInfo",
        "importPath": "llama_index.core.storage.docstore.types",
        "description": "llama_index.core.storage.docstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "RefDocInfo",
        "importPath": "llama_index.core.storage.docstore.types",
        "description": "llama_index.core.storage.docstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "RefDocInfo",
        "importPath": "llama_index.core.storage.docstore.types",
        "description": "llama_index.core.storage.docstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "RefDocInfo",
        "importPath": "llama_index.core.storage.docstore.types",
        "description": "llama_index.core.storage.docstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "RefDocInfo",
        "importPath": "llama_index.core.storage.docstore.types",
        "description": "llama_index.core.storage.docstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "BaseDocumentStore",
        "importPath": "llama_index.core.storage.docstore.types",
        "description": "llama_index.core.storage.docstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "RefDocInfo",
        "importPath": "llama_index.core.storage.docstore.types",
        "description": "llama_index.core.storage.docstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "BaseDocumentStore",
        "importPath": "llama_index.core.storage.docstore.types",
        "description": "llama_index.core.storage.docstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "RefDocInfo",
        "importPath": "llama_index.core.storage.docstore.types",
        "description": "llama_index.core.storage.docstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "BaseDocumentStore",
        "importPath": "llama_index.core.storage.docstore.types",
        "description": "llama_index.core.storage.docstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_BATCH_SIZE",
        "importPath": "llama_index.core.storage.docstore.types",
        "description": "llama_index.core.storage.docstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_DIR",
        "importPath": "llama_index.core.storage.docstore.types",
        "description": "llama_index.core.storage.docstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_FNAME",
        "importPath": "llama_index.core.storage.docstore.types",
        "description": "llama_index.core.storage.docstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_PATH",
        "importPath": "llama_index.core.storage.docstore.types",
        "description": "llama_index.core.storage.docstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_FNAME",
        "importPath": "llama_index.core.storage.docstore.types",
        "description": "llama_index.core.storage.docstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "BaseDocumentStore",
        "importPath": "llama_index.core.storage.docstore.types",
        "description": "llama_index.core.storage.docstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "BaseExtractor",
        "importPath": "llama_index.core.extractors.metadata_extractors",
        "description": "llama_index.core.extractors.metadata_extractors",
        "isExtraImport": true,
        "detail": "llama_index.core.extractors.metadata_extractors",
        "documentation": {}
    },
    {
        "label": "KeywordExtractor",
        "importPath": "llama_index.core.extractors.metadata_extractors",
        "description": "llama_index.core.extractors.metadata_extractors",
        "isExtraImport": true,
        "detail": "llama_index.core.extractors.metadata_extractors",
        "documentation": {}
    },
    {
        "label": "QuestionsAnsweredExtractor",
        "importPath": "llama_index.core.extractors.metadata_extractors",
        "description": "llama_index.core.extractors.metadata_extractors",
        "isExtraImport": true,
        "detail": "llama_index.core.extractors.metadata_extractors",
        "documentation": {}
    },
    {
        "label": "SummaryExtractor",
        "importPath": "llama_index.core.extractors.metadata_extractors",
        "description": "llama_index.core.extractors.metadata_extractors",
        "isExtraImport": true,
        "detail": "llama_index.core.extractors.metadata_extractors",
        "documentation": {}
    },
    {
        "label": "TitleExtractor",
        "importPath": "llama_index.core.extractors.metadata_extractors",
        "description": "llama_index.core.extractors.metadata_extractors",
        "isExtraImport": true,
        "detail": "llama_index.core.extractors.metadata_extractors",
        "documentation": {}
    },
    {
        "label": "BaseExtractor",
        "importPath": "llama_index.core.extractors.interface",
        "description": "llama_index.core.extractors.interface",
        "isExtraImport": true,
        "detail": "llama_index.core.extractors.interface",
        "documentation": {}
    },
    {
        "label": "fsspec",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fsspec",
        "description": "fsspec",
        "detail": "fsspec",
        "documentation": {}
    },
    {
        "label": "AbstractFileSystem",
        "importPath": "fsspec",
        "description": "fsspec",
        "isExtraImport": true,
        "detail": "fsspec",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_DIR",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_FNAME",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "GraphStore",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "PropertyGraphStore",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "ChunkNode",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "EntityNode",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "Triplet",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "LabelledNode",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "LabelledPropertyGraph",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "Relation",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_DIR",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PG_PERSIST_FNAME",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "GraphStore",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "PropertyGraphStore",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "Triplet",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "PropertyGraphStore",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "PropertyGraphStore",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "PropertyGraphStore",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "KG_SOURCE_REL",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "PropertyGraphStore",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "PropertyGraphStore",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "KG_SOURCE_REL",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "VECTOR_SOURCE_KEY",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "EntityNode",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "Relation",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "KG_NODES_KEY",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "KG_RELATIONS_KEY",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "Relation",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "KG_NODES_KEY",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "KG_RELATIONS_KEY",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "EntityNode",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "Relation",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "Triplet",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "KG_NODES_KEY",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "KG_RELATIONS_KEY",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "EntityNode",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "Relation",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "KG_NODES_KEY",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "KG_RELATIONS_KEY",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "KG_NODES_KEY",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "KG_RELATIONS_KEY",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "VECTOR_SOURCE_KEY",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "LabelledNode",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "Relation",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "PropertyGraphStore",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "TRIPLET_SOURCE_KEY",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PG_PERSIST_FNAME",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "GraphStore",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "PropertyGraphStore",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "EntityNode",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "Relation",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "EntityNode",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "Relation",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "KG_NODES_KEY",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "KG_RELATIONS_KEY",
        "importPath": "llama_index.core.graph_stores.types",
        "description": "llama_index.core.graph_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQuery",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQuery",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "BasePydanticVectorStore",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQuery",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "BasePydanticVectorStore",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "MetadataFilters",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "BasePydanticVectorStore",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQuery",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQueryMode",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQueryResult",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "BasePydanticVectorStore",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQuery",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "MetadataFilters",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "BasePydanticVectorStore",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQueryMode",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "FilterCondition",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "MetadataFilters",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreInfo",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQueryMode",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQuerySpec",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQuerySpec",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "FilterOperator",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "MetadataFilter",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "MetadataInfo",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreInfo",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQuerySpec",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "MetadataFilters",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQuery",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQueryMode",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQueryResult",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "BasePydanticVectorStore",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQueryResult",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "BasePydanticVectorStore",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "BasePydanticVectorStore",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "BasePydanticVectorStore",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "MetadataFilter",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "MetadataFilters",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQuery",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "BasePydanticVectorStore",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "BasePydanticVectorStore",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_DIR",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_FNAME",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "BasePydanticVectorStore",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "MetadataFilters",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQuery",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQueryMode",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQueryResult",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "MetadataFilters",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "FilterOperator",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "FilterCondition",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "ExactMatchFilter",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQuerySpec",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQueryResult",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "BasePydanticVectorStore",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQuery",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQueryResult",
        "importPath": "llama_index.core.vector_stores.types",
        "description": "llama_index.core.vector_stores.types",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CYPHER_TEMPALTE",
        "importPath": "llama_index.core.graph_stores.prompts",
        "description": "llama_index.core.graph_stores.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.prompts",
        "documentation": {}
    },
    {
        "label": "metadata_dict_to_node",
        "importPath": "llama_index.core.vector_stores.utils",
        "description": "llama_index.core.vector_stores.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.utils",
        "documentation": {}
    },
    {
        "label": "node_to_metadata_dict",
        "importPath": "llama_index.core.vector_stores.utils",
        "description": "llama_index.core.vector_stores.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.utils",
        "documentation": {}
    },
    {
        "label": "node_to_metadata_dict",
        "importPath": "llama_index.core.vector_stores.utils",
        "description": "llama_index.core.vector_stores.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.utils",
        "documentation": {}
    },
    {
        "label": "build_metadata_filter_fn",
        "importPath": "llama_index.core.vector_stores.utils",
        "description": "llama_index.core.vector_stores.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.utils",
        "documentation": {}
    },
    {
        "label": "PromptHelper",
        "importPath": "llama_index.core.indices.prompt_helper",
        "description": "llama_index.core.indices.prompt_helper",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.prompt_helper",
        "documentation": {}
    },
    {
        "label": "PromptHelper",
        "importPath": "llama_index.core.indices.prompt_helper",
        "description": "llama_index.core.indices.prompt_helper",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.prompt_helper",
        "documentation": {}
    },
    {
        "label": "PromptHelper",
        "importPath": "llama_index.core.indices.prompt_helper",
        "description": "llama_index.core.indices.prompt_helper",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.prompt_helper",
        "documentation": {}
    },
    {
        "label": "PromptHelper",
        "importPath": "llama_index.core.indices.prompt_helper",
        "description": "llama_index.core.indices.prompt_helper",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.prompt_helper",
        "documentation": {}
    },
    {
        "label": "PromptHelper",
        "importPath": "llama_index.core.indices.prompt_helper",
        "description": "llama_index.core.indices.prompt_helper",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.prompt_helper",
        "documentation": {}
    },
    {
        "label": "PromptHelper",
        "importPath": "llama_index.core.indices.prompt_helper",
        "description": "llama_index.core.indices.prompt_helper",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.prompt_helper",
        "documentation": {}
    },
    {
        "label": "PromptHelper",
        "importPath": "llama_index.core.indices.prompt_helper",
        "description": "llama_index.core.indices.prompt_helper",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.prompt_helper",
        "documentation": {}
    },
    {
        "label": "PromptHelper",
        "importPath": "llama_index.core.indices.prompt_helper",
        "description": "llama_index.core.indices.prompt_helper",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.prompt_helper",
        "documentation": {}
    },
    {
        "label": "PromptHelper",
        "importPath": "llama_index.core.indices.prompt_helper",
        "description": "llama_index.core.indices.prompt_helper",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.prompt_helper",
        "documentation": {}
    },
    {
        "label": "PromptHelper",
        "importPath": "llama_index.core.indices.prompt_helper",
        "description": "llama_index.core.indices.prompt_helper",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.prompt_helper",
        "documentation": {}
    },
    {
        "label": "PromptHelper",
        "importPath": "llama_index.core.indices.prompt_helper",
        "description": "llama_index.core.indices.prompt_helper",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.prompt_helper",
        "documentation": {}
    },
    {
        "label": "PromptHelper",
        "importPath": "llama_index.core.indices.prompt_helper",
        "description": "llama_index.core.indices.prompt_helper",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.prompt_helper",
        "documentation": {}
    },
    {
        "label": "PromptHelper",
        "importPath": "llama_index.core.indices.prompt_helper",
        "description": "llama_index.core.indices.prompt_helper",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.prompt_helper",
        "documentation": {}
    },
    {
        "label": "PromptHelper",
        "importPath": "llama_index.core.indices.prompt_helper",
        "description": "llama_index.core.indices.prompt_helper",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.prompt_helper",
        "documentation": {}
    },
    {
        "label": "PromptHelper",
        "importPath": "llama_index.core.indices.prompt_helper",
        "description": "llama_index.core.indices.prompt_helper",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.prompt_helper",
        "documentation": {}
    },
    {
        "label": "TextSplitter",
        "importPath": "llama_index.core.node_parser.interface",
        "description": "llama_index.core.node_parser.interface",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.interface",
        "documentation": {}
    },
    {
        "label": "NodeParser",
        "importPath": "llama_index.core.node_parser.interface",
        "description": "llama_index.core.node_parser.interface",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.interface",
        "documentation": {}
    },
    {
        "label": "NodeParser",
        "importPath": "llama_index.core.node_parser.interface",
        "description": "llama_index.core.node_parser.interface",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.interface",
        "documentation": {}
    },
    {
        "label": "NodeParser",
        "importPath": "llama_index.core.node_parser.interface",
        "description": "llama_index.core.node_parser.interface",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.interface",
        "documentation": {}
    },
    {
        "label": "NodeParser",
        "importPath": "llama_index.core.node_parser.interface",
        "description": "llama_index.core.node_parser.interface",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.interface",
        "documentation": {}
    },
    {
        "label": "NodeParser",
        "importPath": "llama_index.core.node_parser.interface",
        "description": "llama_index.core.node_parser.interface",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.interface",
        "documentation": {}
    },
    {
        "label": "NodeParser",
        "importPath": "llama_index.core.node_parser.interface",
        "description": "llama_index.core.node_parser.interface",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.interface",
        "documentation": {}
    },
    {
        "label": "TextSplitter",
        "importPath": "llama_index.core.node_parser.interface",
        "description": "llama_index.core.node_parser.interface",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.interface",
        "documentation": {}
    },
    {
        "label": "TextSplitter",
        "importPath": "llama_index.core.node_parser.interface",
        "description": "llama_index.core.node_parser.interface",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.interface",
        "documentation": {}
    },
    {
        "label": "NodeParser",
        "importPath": "llama_index.core.node_parser.interface",
        "description": "llama_index.core.node_parser.interface",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.interface",
        "documentation": {}
    },
    {
        "label": "NodeParser",
        "importPath": "llama_index.core.node_parser.interface",
        "description": "llama_index.core.node_parser.interface",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.interface",
        "documentation": {}
    },
    {
        "label": "NodeParser",
        "importPath": "llama_index.core.node_parser.interface",
        "description": "llama_index.core.node_parser.interface",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.interface",
        "documentation": {}
    },
    {
        "label": "MetadataAwareTextSplitter",
        "importPath": "llama_index.core.node_parser.interface",
        "description": "llama_index.core.node_parser.interface",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.interface",
        "documentation": {}
    },
    {
        "label": "NodeParser",
        "importPath": "llama_index.core.node_parser.interface",
        "description": "llama_index.core.node_parser.interface",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.interface",
        "documentation": {}
    },
    {
        "label": "MetadataAwareTextSplitter",
        "importPath": "llama_index.core.node_parser.interface",
        "description": "llama_index.core.node_parser.interface",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.interface",
        "documentation": {}
    },
    {
        "label": "TextSplitter",
        "importPath": "llama_index.core.node_parser.interface",
        "description": "llama_index.core.node_parser.interface",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.interface",
        "documentation": {}
    },
    {
        "label": "NodeParser",
        "importPath": "llama_index.core.node_parser.interface",
        "description": "llama_index.core.node_parser.interface",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.interface",
        "documentation": {}
    },
    {
        "label": "DEFAULT_REFINE_TABLE_CONTEXT_PROMPT_SEL",
        "importPath": "llama_index.core.prompts.default_prompt_selectors",
        "description": "llama_index.core.prompts.default_prompt_selectors",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompt_selectors",
        "documentation": {}
    },
    {
        "label": "DEFAULT_REFINE_PROMPT_SEL",
        "importPath": "llama_index.core.prompts.default_prompt_selectors",
        "description": "llama_index.core.prompts.default_prompt_selectors",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompt_selectors",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TREE_SUMMARIZE_PROMPT_SEL",
        "importPath": "llama_index.core.prompts.default_prompt_selectors",
        "description": "llama_index.core.prompts.default_prompt_selectors",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompt_selectors",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TEXT_QA_PROMPT_SEL",
        "importPath": "llama_index.core.prompts.default_prompt_selectors",
        "description": "llama_index.core.prompts.default_prompt_selectors",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompt_selectors",
        "documentation": {}
    },
    {
        "label": "DEFAULT_REFINE_PROMPT_SEL",
        "importPath": "llama_index.core.prompts.default_prompt_selectors",
        "description": "llama_index.core.prompts.default_prompt_selectors",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompt_selectors",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TEXT_QA_PROMPT_SEL",
        "importPath": "llama_index.core.prompts.default_prompt_selectors",
        "description": "llama_index.core.prompts.default_prompt_selectors",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompt_selectors",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TREE_SUMMARIZE_PROMPT_SEL",
        "importPath": "llama_index.core.prompts.default_prompt_selectors",
        "description": "llama_index.core.prompts.default_prompt_selectors",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompt_selectors",
        "documentation": {}
    },
    {
        "label": "DEFAULT_REFINE_PROMPT_SEL",
        "importPath": "llama_index.core.prompts.default_prompt_selectors",
        "description": "llama_index.core.prompts.default_prompt_selectors",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompt_selectors",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TEXT_QA_PROMPT_SEL",
        "importPath": "llama_index.core.prompts.default_prompt_selectors",
        "description": "llama_index.core.prompts.default_prompt_selectors",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompt_selectors",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TEXT_QA_PROMPT_SEL",
        "importPath": "llama_index.core.prompts.default_prompt_selectors",
        "description": "llama_index.core.prompts.default_prompt_selectors",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompt_selectors",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TREE_SUMMARIZE_PROMPT_SEL",
        "importPath": "llama_index.core.prompts.default_prompt_selectors",
        "description": "llama_index.core.prompts.default_prompt_selectors",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.default_prompt_selectors",
        "documentation": {}
    },
    {
        "label": "PromptType",
        "importPath": "llama_index.core.prompts.prompt_type",
        "description": "llama_index.core.prompts.prompt_type",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.prompt_type",
        "documentation": {}
    },
    {
        "label": "PromptType",
        "importPath": "llama_index.core.prompts.prompt_type",
        "description": "llama_index.core.prompts.prompt_type",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.prompt_type",
        "documentation": {}
    },
    {
        "label": "PromptType",
        "importPath": "llama_index.core.prompts.prompt_type",
        "description": "llama_index.core.prompts.prompt_type",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.prompt_type",
        "documentation": {}
    },
    {
        "label": "PromptType",
        "importPath": "llama_index.core.prompts.prompt_type",
        "description": "llama_index.core.prompts.prompt_type",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.prompt_type",
        "documentation": {}
    },
    {
        "label": "PromptType",
        "importPath": "llama_index.core.prompts.prompt_type",
        "description": "llama_index.core.prompts.prompt_type",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.prompt_type",
        "documentation": {}
    },
    {
        "label": "PromptType",
        "importPath": "llama_index.core.prompts.prompt_type",
        "description": "llama_index.core.prompts.prompt_type",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.prompt_type",
        "documentation": {}
    },
    {
        "label": "PromptType",
        "importPath": "llama_index.core.prompts.prompt_type",
        "description": "llama_index.core.prompts.prompt_type",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.prompt_type",
        "documentation": {}
    },
    {
        "label": "PromptType",
        "importPath": "llama_index.core.prompts.prompt_type",
        "description": "llama_index.core.prompts.prompt_type",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.prompt_type",
        "documentation": {}
    },
    {
        "label": "PromptType",
        "importPath": "llama_index.core.prompts.prompt_type",
        "description": "llama_index.core.prompts.prompt_type",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.prompt_type",
        "documentation": {}
    },
    {
        "label": "PromptType",
        "importPath": "llama_index.core.prompts.prompt_type",
        "description": "llama_index.core.prompts.prompt_type",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.prompt_type",
        "documentation": {}
    },
    {
        "label": "PromptType",
        "importPath": "llama_index.core.prompts.prompt_type",
        "description": "llama_index.core.prompts.prompt_type",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.prompt_type",
        "documentation": {}
    },
    {
        "label": "PromptType",
        "importPath": "llama_index.core.prompts.prompt_type",
        "description": "llama_index.core.prompts.prompt_type",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.prompt_type",
        "documentation": {}
    },
    {
        "label": "PromptType",
        "importPath": "llama_index.core.prompts.prompt_type",
        "description": "llama_index.core.prompts.prompt_type",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.prompt_type",
        "documentation": {}
    },
    {
        "label": "PromptType",
        "importPath": "llama_index.core.prompts.prompt_type",
        "description": "llama_index.core.prompts.prompt_type",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.prompt_type",
        "documentation": {}
    },
    {
        "label": "PromptType",
        "importPath": "llama_index.core.prompts.prompt_type",
        "description": "llama_index.core.prompts.prompt_type",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.prompt_type",
        "documentation": {}
    },
    {
        "label": "PromptType",
        "importPath": "llama_index.core.prompts.prompt_type",
        "description": "llama_index.core.prompts.prompt_type",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.prompt_type",
        "documentation": {}
    },
    {
        "label": "PromptType",
        "importPath": "llama_index.core.prompts.prompt_type",
        "description": "llama_index.core.prompts.prompt_type",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.prompt_type",
        "documentation": {}
    },
    {
        "label": "SQLDatabase",
        "importPath": "llama_index.core.utilities.sql_wrapper",
        "description": "llama_index.core.utilities.sql_wrapper",
        "isExtraImport": true,
        "detail": "llama_index.core.utilities.sql_wrapper",
        "documentation": {}
    },
    {
        "label": "SQLDatabase",
        "importPath": "llama_index.core.utilities.sql_wrapper",
        "description": "llama_index.core.utilities.sql_wrapper",
        "isExtraImport": true,
        "detail": "llama_index.core.utilities.sql_wrapper",
        "documentation": {}
    },
    {
        "label": "SQLDatabase",
        "importPath": "llama_index.core.utilities.sql_wrapper",
        "description": "llama_index.core.utilities.sql_wrapper",
        "isExtraImport": true,
        "detail": "llama_index.core.utilities.sql_wrapper",
        "documentation": {}
    },
    {
        "label": "SQLDatabase",
        "importPath": "llama_index.core.utilities.sql_wrapper",
        "description": "llama_index.core.utilities.sql_wrapper",
        "isExtraImport": true,
        "detail": "llama_index.core.utilities.sql_wrapper",
        "documentation": {}
    },
    {
        "label": "SQLDatabase",
        "importPath": "llama_index.core.utilities.sql_wrapper",
        "description": "llama_index.core.utilities.sql_wrapper",
        "isExtraImport": true,
        "detail": "llama_index.core.utilities.sql_wrapper",
        "documentation": {}
    },
    {
        "label": "SQLDatabase",
        "importPath": "llama_index.core.utilities.sql_wrapper",
        "description": "llama_index.core.utilities.sql_wrapper",
        "isExtraImport": true,
        "detail": "llama_index.core.utilities.sql_wrapper",
        "documentation": {}
    },
    {
        "label": "SQLDatabase",
        "importPath": "llama_index.core.utilities.sql_wrapper",
        "description": "llama_index.core.utilities.sql_wrapper",
        "isExtraImport": true,
        "detail": "llama_index.core.utilities.sql_wrapper",
        "documentation": {}
    },
    {
        "label": "SQLDatabase",
        "importPath": "llama_index.core.utilities.sql_wrapper",
        "description": "llama_index.core.utilities.sql_wrapper",
        "isExtraImport": true,
        "detail": "llama_index.core.utilities.sql_wrapper",
        "documentation": {}
    },
    {
        "label": "SQLDatabase",
        "importPath": "llama_index.core.utilities.sql_wrapper",
        "description": "llama_index.core.utilities.sql_wrapper",
        "isExtraImport": true,
        "detail": "llama_index.core.utilities.sql_wrapper",
        "documentation": {}
    },
    {
        "label": "OUTPUT_PARSER_TYPE",
        "importPath": "llama_index.core.indices.common.struct_store.base",
        "description": "llama_index.core.indices.common.struct_store.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.common.struct_store.base",
        "documentation": {}
    },
    {
        "label": "BaseStructDatapointExtractor",
        "importPath": "llama_index.core.indices.common.struct_store.base",
        "description": "llama_index.core.indices.common.struct_store.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.common.struct_store.base",
        "documentation": {}
    },
    {
        "label": "SQLDocumentContextBuilder",
        "importPath": "llama_index.core.indices.common.struct_store.base",
        "description": "llama_index.core.indices.common.struct_store.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.common.struct_store.base",
        "documentation": {}
    },
    {
        "label": "Table",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Table",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Table",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Table",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "JSON",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Column",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "BigInteger",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Integer",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "MetaData",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "String",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Table",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "delete",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "select",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "insert",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "update",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "MetaData",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "create_engine",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "insert",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "inspect",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "text",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Column",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Integer",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "MetaData",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "String",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Table",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "create_engine",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "delete",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "select",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Column",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Integer",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "MetaData",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "String",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Table",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "create_engine",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "insert",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "text",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "get_sorted_node_list",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "truncate_text",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "embed_nodes",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "default_format_node_batch_fn",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "default_parse_choice_select_answer_fn",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "expand_tokens_with_subtokens",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "default_format_node_batch_fn",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "default_parse_choice_select_answer_fn",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "async_embed_image_nodes",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "async_embed_nodes",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "embed_image_nodes",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "embed_nodes",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "log_vector_store_query_result",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "get_sorted_node_list",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "extract_numbers_given_response",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "get_sorted_node_list",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "get_sorted_node_list",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "extract_numbers_given_response",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "get_sorted_node_list",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "get_sorted_node_list",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "log_vector_store_query_result",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "async_embed_nodes",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "embed_nodes",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "default_format_node_batch_fn",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "default_parse_choice_select_answer_fn",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "default_format_node_batch_fn",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "truncate_text",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "truncate_text",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "expand_tokens_with_subtokens",
        "importPath": "llama_index.core.indices.utils",
        "description": "llama_index.core.indices.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "BaseDocumentStore",
        "importPath": "llama_index.core.storage.docstore",
        "description": "llama_index.core.storage.docstore",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore",
        "documentation": {}
    },
    {
        "label": "BaseDocumentStore",
        "importPath": "llama_index.core.storage.docstore",
        "description": "llama_index.core.storage.docstore",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore",
        "documentation": {}
    },
    {
        "label": "BaseDocumentStore",
        "importPath": "llama_index.core.storage.docstore",
        "description": "llama_index.core.storage.docstore",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore",
        "documentation": {}
    },
    {
        "label": "SimpleDocumentStore",
        "importPath": "llama_index.core.storage.docstore",
        "description": "llama_index.core.storage.docstore",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore",
        "documentation": {}
    },
    {
        "label": "BaseDocumentStore",
        "importPath": "llama_index.core.storage.docstore",
        "description": "llama_index.core.storage.docstore",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore",
        "documentation": {}
    },
    {
        "label": "BaseDocumentStore",
        "importPath": "llama_index.core.storage.docstore",
        "description": "llama_index.core.storage.docstore",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore",
        "documentation": {}
    },
    {
        "label": "SimpleDocumentStore",
        "importPath": "llama_index.core.storage.docstore",
        "description": "llama_index.core.storage.docstore",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore",
        "documentation": {}
    },
    {
        "label": "get_default_docstore",
        "importPath": "llama_index.core.storage.docstore.registry",
        "description": "llama_index.core.storage.docstore.registry",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.registry",
        "documentation": {}
    },
    {
        "label": "get_default_docstore",
        "importPath": "llama_index.core.storage.docstore.registry",
        "description": "llama_index.core.storage.docstore.registry",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.registry",
        "documentation": {}
    },
    {
        "label": "BaseIndex",
        "importPath": "llama_index.core.indices.base",
        "description": "llama_index.core.indices.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "BaseIndex",
        "importPath": "llama_index.core.indices.base",
        "description": "llama_index.core.indices.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "BaseIndex",
        "importPath": "llama_index.core.indices.base",
        "description": "llama_index.core.indices.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "BaseIndex",
        "importPath": "llama_index.core.indices.base",
        "description": "llama_index.core.indices.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "BaseIndex",
        "importPath": "llama_index.core.indices.base",
        "description": "llama_index.core.indices.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "BaseIndex",
        "importPath": "llama_index.core.indices.base",
        "description": "llama_index.core.indices.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "BaseIndex",
        "importPath": "llama_index.core.indices.base",
        "description": "llama_index.core.indices.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "IndexType",
        "importPath": "llama_index.core.indices.base",
        "description": "llama_index.core.indices.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "BaseIndex",
        "importPath": "llama_index.core.indices.base",
        "description": "llama_index.core.indices.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "BaseIndex",
        "importPath": "llama_index.core.indices.base",
        "description": "llama_index.core.indices.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "BaseIndex",
        "importPath": "llama_index.core.indices.base",
        "description": "llama_index.core.indices.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.indices.base",
        "description": "llama_index.core.indices.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "BaseIndex",
        "importPath": "llama_index.core.indices.base",
        "description": "llama_index.core.indices.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "BaseIndex",
        "importPath": "llama_index.core.indices.base",
        "description": "llama_index.core.indices.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "BaseIndex",
        "importPath": "llama_index.core.indices.base",
        "description": "llama_index.core.indices.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "BaseIndex",
        "importPath": "llama_index.core.indices.base",
        "description": "llama_index.core.indices.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "BaseIndex",
        "importPath": "llama_index.core.indices.base",
        "description": "llama_index.core.indices.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "BaseIndex",
        "importPath": "llama_index.core.indices.base",
        "description": "llama_index.core.indices.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "BaseIndex",
        "importPath": "llama_index.core.indices.base",
        "description": "llama_index.core.indices.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "BaseGPTIndex",
        "importPath": "llama_index.core.indices.base",
        "description": "llama_index.core.indices.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "BaseIndex",
        "importPath": "llama_index.core.indices.base",
        "description": "llama_index.core.indices.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "BaseIndex",
        "importPath": "llama_index.core.indices.base",
        "description": "llama_index.core.indices.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "DocumentSummaryIndex",
        "importPath": "llama_index.core.indices.document_summary.base",
        "description": "llama_index.core.indices.document_summary.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.document_summary.base",
        "documentation": {}
    },
    {
        "label": "DocumentSummaryIndex",
        "importPath": "llama_index.core.indices.document_summary.base",
        "description": "llama_index.core.indices.document_summary.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.document_summary.base",
        "documentation": {}
    },
    {
        "label": "DocumentSummaryIndex",
        "importPath": "llama_index.core.indices.document_summary.base",
        "description": "llama_index.core.indices.document_summary.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.document_summary.base",
        "documentation": {}
    },
    {
        "label": "DocumentSummaryIndex",
        "importPath": "llama_index.core.indices.document_summary.base",
        "description": "llama_index.core.indices.document_summary.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.document_summary.base",
        "documentation": {}
    },
    {
        "label": "DocumentSummaryIndex",
        "importPath": "llama_index.core.indices.document_summary.base",
        "description": "llama_index.core.indices.document_summary.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.document_summary.base",
        "documentation": {}
    },
    {
        "label": "DocumentSummaryRetrieverMode",
        "importPath": "llama_index.core.indices.document_summary.base",
        "description": "llama_index.core.indices.document_summary.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.document_summary.base",
        "documentation": {}
    },
    {
        "label": "LLMType",
        "importPath": "llama_index.core.llms.utils",
        "description": "llama_index.core.llms.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.utils",
        "documentation": {}
    },
    {
        "label": "LLMType",
        "importPath": "llama_index.core.llms.utils",
        "description": "llama_index.core.llms.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.utils",
        "documentation": {}
    },
    {
        "label": "LLMType",
        "importPath": "llama_index.core.llms.utils",
        "description": "llama_index.core.llms.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.utils",
        "documentation": {}
    },
    {
        "label": "LLMType",
        "importPath": "llama_index.core.llms.utils",
        "description": "llama_index.core.llms.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.utils",
        "documentation": {}
    },
    {
        "label": "resolve_llm",
        "importPath": "llama_index.core.llms.utils",
        "description": "llama_index.core.llms.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.utils",
        "documentation": {}
    },
    {
        "label": "LLM",
        "importPath": "llama_index.core.llms.utils",
        "description": "llama_index.core.llms.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.utils",
        "documentation": {}
    },
    {
        "label": "LLMType",
        "importPath": "llama_index.core.llms.utils",
        "description": "llama_index.core.llms.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.utils",
        "documentation": {}
    },
    {
        "label": "resolve_llm",
        "importPath": "llama_index.core.llms.utils",
        "description": "llama_index.core.llms.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.utils",
        "documentation": {}
    },
    {
        "label": "LLMType",
        "importPath": "llama_index.core.llms.utils",
        "description": "llama_index.core.llms.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.utils",
        "documentation": {}
    },
    {
        "label": "resolve_llm",
        "importPath": "llama_index.core.llms.utils",
        "description": "llama_index.core.llms.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.utils",
        "documentation": {}
    },
    {
        "label": "EmptyIndex",
        "importPath": "llama_index.core.indices.empty.base",
        "description": "llama_index.core.indices.empty.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.empty.base",
        "documentation": {}
    },
    {
        "label": "EmptyIndex",
        "importPath": "llama_index.core.indices.empty.base",
        "description": "llama_index.core.indices.empty.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.empty.base",
        "documentation": {}
    },
    {
        "label": "EmptyIndex",
        "importPath": "llama_index.core.indices.empty.base",
        "description": "llama_index.core.indices.empty.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.empty.base",
        "documentation": {}
    },
    {
        "label": "extract_keywords_given_response",
        "importPath": "llama_index.core.indices.keyword_table.utils",
        "description": "llama_index.core.indices.keyword_table.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.keyword_table.utils",
        "documentation": {}
    },
    {
        "label": "rake_extract_keywords",
        "importPath": "llama_index.core.indices.keyword_table.utils",
        "description": "llama_index.core.indices.keyword_table.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.keyword_table.utils",
        "documentation": {}
    },
    {
        "label": "extract_keywords_given_response",
        "importPath": "llama_index.core.indices.keyword_table.utils",
        "description": "llama_index.core.indices.keyword_table.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.keyword_table.utils",
        "documentation": {}
    },
    {
        "label": "rake_extract_keywords",
        "importPath": "llama_index.core.indices.keyword_table.utils",
        "description": "llama_index.core.indices.keyword_table.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.keyword_table.utils",
        "documentation": {}
    },
    {
        "label": "simple_extract_keywords",
        "importPath": "llama_index.core.indices.keyword_table.utils",
        "description": "llama_index.core.indices.keyword_table.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.keyword_table.utils",
        "documentation": {}
    },
    {
        "label": "simple_extract_keywords",
        "importPath": "llama_index.core.indices.keyword_table.utils",
        "description": "llama_index.core.indices.keyword_table.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.keyword_table.utils",
        "documentation": {}
    },
    {
        "label": "extract_keywords_given_response",
        "importPath": "llama_index.core.indices.keyword_table.utils",
        "description": "llama_index.core.indices.keyword_table.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.keyword_table.utils",
        "documentation": {}
    },
    {
        "label": "extract_keywords_given_response",
        "importPath": "llama_index.core.indices.keyword_table.utils",
        "description": "llama_index.core.indices.keyword_table.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.keyword_table.utils",
        "documentation": {}
    },
    {
        "label": "simple_extract_keywords",
        "importPath": "llama_index.core.indices.keyword_table.utils",
        "description": "llama_index.core.indices.keyword_table.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.keyword_table.utils",
        "documentation": {}
    },
    {
        "label": "BaseKeywordTableIndex",
        "importPath": "llama_index.core.indices.keyword_table.base",
        "description": "llama_index.core.indices.keyword_table.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.keyword_table.base",
        "documentation": {}
    },
    {
        "label": "KeywordTableRetrieverMode",
        "importPath": "llama_index.core.indices.keyword_table.base",
        "description": "llama_index.core.indices.keyword_table.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.keyword_table.base",
        "documentation": {}
    },
    {
        "label": "BaseKeywordTableIndex",
        "importPath": "llama_index.core.indices.keyword_table.base",
        "description": "llama_index.core.indices.keyword_table.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.keyword_table.base",
        "documentation": {}
    },
    {
        "label": "BaseKeywordTableIndex",
        "importPath": "llama_index.core.indices.keyword_table.base",
        "description": "llama_index.core.indices.keyword_table.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.keyword_table.base",
        "documentation": {}
    },
    {
        "label": "KeywordTableRetrieverMode",
        "importPath": "llama_index.core.indices.keyword_table.base",
        "description": "llama_index.core.indices.keyword_table.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.keyword_table.base",
        "documentation": {}
    },
    {
        "label": "KeywordTableIndex",
        "importPath": "llama_index.core.indices.keyword_table.base",
        "description": "llama_index.core.indices.keyword_table.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.keyword_table.base",
        "documentation": {}
    },
    {
        "label": "SimpleGraphStore",
        "importPath": "llama_index.core.graph_stores.simple",
        "description": "llama_index.core.graph_stores.simple",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.simple",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_FNAME",
        "importPath": "llama_index.core.graph_stores.simple",
        "description": "llama_index.core.graph_stores.simple",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.simple",
        "documentation": {}
    },
    {
        "label": "SimpleGraphStore",
        "importPath": "llama_index.core.graph_stores.simple",
        "description": "llama_index.core.graph_stores.simple",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.simple",
        "documentation": {}
    },
    {
        "label": "KnowledgeGraphIndex",
        "importPath": "llama_index.core.indices.knowledge_graph.base",
        "description": "llama_index.core.indices.knowledge_graph.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.knowledge_graph.base",
        "documentation": {}
    },
    {
        "label": "KnowledgeGraphIndex",
        "importPath": "llama_index.core.indices.knowledge_graph.base",
        "description": "llama_index.core.indices.knowledge_graph.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.knowledge_graph.base",
        "documentation": {}
    },
    {
        "label": "KnowledgeGraphIndex",
        "importPath": "llama_index.core.indices.knowledge_graph.base",
        "description": "llama_index.core.indices.knowledge_graph.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.knowledge_graph.base",
        "documentation": {}
    },
    {
        "label": "KnowledgeGraphIndex",
        "importPath": "llama_index.core.indices.knowledge_graph.base",
        "description": "llama_index.core.indices.knowledge_graph.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.knowledge_graph.base",
        "documentation": {}
    },
    {
        "label": "get_top_k_embeddings",
        "importPath": "llama_index.core.indices.query.embedding_utils",
        "description": "llama_index.core.indices.query.embedding_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.embedding_utils",
        "documentation": {}
    },
    {
        "label": "get_top_k_embeddings",
        "importPath": "llama_index.core.indices.query.embedding_utils",
        "description": "llama_index.core.indices.query.embedding_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.embedding_utils",
        "documentation": {}
    },
    {
        "label": "get_top_k_embeddings",
        "importPath": "llama_index.core.indices.query.embedding_utils",
        "description": "llama_index.core.indices.query.embedding_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.embedding_utils",
        "documentation": {}
    },
    {
        "label": "get_top_k_embeddings",
        "importPath": "llama_index.core.indices.query.embedding_utils",
        "description": "llama_index.core.indices.query.embedding_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.embedding_utils",
        "documentation": {}
    },
    {
        "label": "get_top_k_embeddings",
        "importPath": "llama_index.core.indices.query.embedding_utils",
        "description": "llama_index.core.indices.query.embedding_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.embedding_utils",
        "documentation": {}
    },
    {
        "label": "get_top_k_embeddings_learner",
        "importPath": "llama_index.core.indices.query.embedding_utils",
        "description": "llama_index.core.indices.query.embedding_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.embedding_utils",
        "documentation": {}
    },
    {
        "label": "get_top_k_mmr_embeddings",
        "importPath": "llama_index.core.indices.query.embedding_utils",
        "description": "llama_index.core.indices.query.embedding_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.embedding_utils",
        "documentation": {}
    },
    {
        "label": "get_top_k_embeddings",
        "importPath": "llama_index.core.indices.query.embedding_utils",
        "description": "llama_index.core.indices.query.embedding_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.embedding_utils",
        "documentation": {}
    },
    {
        "label": "get_top_k_mmr_embeddings",
        "importPath": "llama_index.core.indices.query.embedding_utils",
        "description": "llama_index.core.indices.query.embedding_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.embedding_utils",
        "documentation": {}
    },
    {
        "label": "BaseLLM",
        "importPath": "llama_index.core.base.llms.base",
        "description": "llama_index.core.base.llms.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.base",
        "documentation": {}
    },
    {
        "label": "BaseLLM",
        "importPath": "llama_index.core.base.llms.base",
        "description": "llama_index.core.base.llms.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.base",
        "documentation": {}
    },
    {
        "label": "BaseLLM",
        "importPath": "llama_index.core.base.llms.base",
        "description": "llama_index.core.base.llms.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.base",
        "documentation": {}
    },
    {
        "label": "BaseLLM",
        "importPath": "llama_index.core.base.llms.base",
        "description": "llama_index.core.base.llms.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.base",
        "documentation": {}
    },
    {
        "label": "BaseLLM",
        "importPath": "llama_index.core.base.llms.base",
        "description": "llama_index.core.base.llms.base",
        "isExtraImport": true,
        "detail": "llama_index.core.base.llms.base",
        "documentation": {}
    },
    {
        "label": "MultiModalEmbedding",
        "importPath": "llama_index.core.embeddings.multi_modal_base",
        "description": "llama_index.core.embeddings.multi_modal_base",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings.multi_modal_base",
        "documentation": {}
    },
    {
        "label": "MultiModalEmbedding",
        "importPath": "llama_index.core.embeddings.multi_modal_base",
        "description": "llama_index.core.embeddings.multi_modal_base",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings.multi_modal_base",
        "documentation": {}
    },
    {
        "label": "MultiModalEmbedding",
        "importPath": "llama_index.core.embeddings.multi_modal_base",
        "description": "llama_index.core.embeddings.multi_modal_base",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings.multi_modal_base",
        "documentation": {}
    },
    {
        "label": "EmbedType",
        "importPath": "llama_index.core.embeddings.utils",
        "description": "llama_index.core.embeddings.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings.utils",
        "documentation": {}
    },
    {
        "label": "resolve_embed_model",
        "importPath": "llama_index.core.embeddings.utils",
        "description": "llama_index.core.embeddings.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings.utils",
        "documentation": {}
    },
    {
        "label": "EmbedType",
        "importPath": "llama_index.core.embeddings.utils",
        "description": "llama_index.core.embeddings.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings.utils",
        "documentation": {}
    },
    {
        "label": "resolve_embed_model",
        "importPath": "llama_index.core.embeddings.utils",
        "description": "llama_index.core.embeddings.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings.utils",
        "documentation": {}
    },
    {
        "label": "EmbedType",
        "importPath": "llama_index.core.embeddings.utils",
        "description": "llama_index.core.embeddings.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings.utils",
        "documentation": {}
    },
    {
        "label": "resolve_embed_model",
        "importPath": "llama_index.core.embeddings.utils",
        "description": "llama_index.core.embeddings.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings.utils",
        "documentation": {}
    },
    {
        "label": "EmbedType",
        "importPath": "llama_index.core.embeddings.utils",
        "description": "llama_index.core.embeddings.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings.utils",
        "documentation": {}
    },
    {
        "label": "EmbedType",
        "importPath": "llama_index.core.embeddings.utils",
        "description": "llama_index.core.embeddings.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings.utils",
        "documentation": {}
    },
    {
        "label": "resolve_embed_model",
        "importPath": "llama_index.core.embeddings.utils",
        "description": "llama_index.core.embeddings.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings.utils",
        "documentation": {}
    },
    {
        "label": "resolve_embed_model",
        "importPath": "llama_index.core.embeddings.utils",
        "description": "llama_index.core.embeddings.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings.utils",
        "documentation": {}
    },
    {
        "label": "resolve_embed_model",
        "importPath": "llama_index.core.embeddings.utils",
        "description": "llama_index.core.embeddings.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings.utils",
        "documentation": {}
    },
    {
        "label": "MultiModalVectorIndexRetriever",
        "importPath": "llama_index.core.indices.multi_modal.retriever",
        "description": "llama_index.core.indices.multi_modal.retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.multi_modal.retriever",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core.indices.vector_store.base",
        "description": "llama_index.core.indices.vector_store.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.vector_store.base",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core.indices.vector_store.base",
        "description": "llama_index.core.indices.vector_store.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.vector_store.base",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core.indices.vector_store.base",
        "description": "llama_index.core.indices.vector_store.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.vector_store.base",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core.indices.vector_store.base",
        "description": "llama_index.core.indices.vector_store.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.vector_store.base",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core.indices.vector_store.base",
        "description": "llama_index.core.indices.vector_store.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.vector_store.base",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core.indices.vector_store.base",
        "description": "llama_index.core.indices.vector_store.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.vector_store.base",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core.indices.vector_store.base",
        "description": "llama_index.core.indices.vector_store.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.vector_store.base",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core.indices.vector_store.base",
        "description": "llama_index.core.indices.vector_store.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.vector_store.base",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core.indices.vector_store.base",
        "description": "llama_index.core.indices.vector_store.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.vector_store.base",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core.indices.vector_store.base",
        "description": "llama_index.core.indices.vector_store.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.vector_store.base",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core.indices.vector_store.base",
        "description": "llama_index.core.indices.vector_store.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.vector_store.base",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core.indices.vector_store.base",
        "description": "llama_index.core.indices.vector_store.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.vector_store.base",
        "documentation": {}
    },
    {
        "label": "MultiModalLLM",
        "importPath": "llama_index.core.multi_modal_llms.base",
        "description": "llama_index.core.multi_modal_llms.base",
        "isExtraImport": true,
        "detail": "llama_index.core.multi_modal_llms.base",
        "documentation": {}
    },
    {
        "label": "SimpleMultiModalQueryEngine",
        "importPath": "llama_index.core.query_engine.multi_modal",
        "description": "llama_index.core.query_engine.multi_modal",
        "isExtraImport": true,
        "detail": "llama_index.core.query_engine.multi_modal",
        "documentation": {}
    },
    {
        "label": "DEFAULT_VECTOR_STORE",
        "importPath": "llama_index.core.vector_stores.simple",
        "description": "llama_index.core.vector_stores.simple",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.simple",
        "documentation": {}
    },
    {
        "label": "SimpleVectorStore",
        "importPath": "llama_index.core.vector_stores.simple",
        "description": "llama_index.core.vector_stores.simple",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.simple",
        "documentation": {}
    },
    {
        "label": "DEFAULT_VECTOR_STORE",
        "importPath": "llama_index.core.vector_stores.simple",
        "description": "llama_index.core.vector_stores.simple",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.simple",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_FNAME",
        "importPath": "llama_index.core.vector_stores.simple",
        "description": "llama_index.core.vector_stores.simple",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.simple",
        "documentation": {}
    },
    {
        "label": "DEFAULT_VECTOR_STORE",
        "importPath": "llama_index.core.vector_stores.simple",
        "description": "llama_index.core.vector_stores.simple",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.simple",
        "documentation": {}
    },
    {
        "label": "NAMESPACE_SEP",
        "importPath": "llama_index.core.vector_stores.simple",
        "description": "llama_index.core.vector_stores.simple",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.simple",
        "documentation": {}
    },
    {
        "label": "SimpleVectorStore",
        "importPath": "llama_index.core.vector_stores.simple",
        "description": "llama_index.core.vector_stores.simple",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.simple",
        "documentation": {}
    },
    {
        "label": "SimpleVectorStore",
        "importPath": "llama_index.core.vector_stores.simple",
        "description": "llama_index.core.vector_stores.simple",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.simple",
        "documentation": {}
    },
    {
        "label": "SimpleVectorStore",
        "importPath": "llama_index.core.vector_stores.simple",
        "description": "llama_index.core.vector_stores.simple",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.simple",
        "documentation": {}
    },
    {
        "label": "SimpleVectorStore",
        "importPath": "llama_index.core.vector_stores.simple",
        "description": "llama_index.core.vector_stores.simple",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.simple",
        "documentation": {}
    },
    {
        "label": "SimpleVectorStore",
        "importPath": "llama_index.core.vector_stores.simple",
        "description": "llama_index.core.vector_stores.simple",
        "isExtraImport": true,
        "detail": "llama_index.core.vector_stores.simple",
        "documentation": {}
    },
    {
        "label": "MultiModalRetriever",
        "importPath": "llama_index.core.base.base_multi_modal_retriever",
        "description": "llama_index.core.base.base_multi_modal_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_multi_modal_retriever",
        "documentation": {}
    },
    {
        "label": "TRIPLET_SOURCE_KEY",
        "importPath": "llama_index.core.indices.property_graph.base",
        "description": "llama_index.core.indices.property_graph.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.property_graph.base",
        "documentation": {}
    },
    {
        "label": "BasePGRetriever",
        "importPath": "llama_index.core.indices.property_graph.sub_retrievers.base",
        "description": "llama_index.core.indices.property_graph.sub_retrievers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.property_graph.sub_retrievers.base",
        "documentation": {}
    },
    {
        "label": "BasePGRetriever",
        "importPath": "llama_index.core.indices.property_graph.sub_retrievers.base",
        "description": "llama_index.core.indices.property_graph.sub_retrievers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.property_graph.sub_retrievers.base",
        "documentation": {}
    },
    {
        "label": "BasePGRetriever",
        "importPath": "llama_index.core.indices.property_graph.sub_retrievers.base",
        "description": "llama_index.core.indices.property_graph.sub_retrievers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.property_graph.sub_retrievers.base",
        "documentation": {}
    },
    {
        "label": "BasePGRetriever",
        "importPath": "llama_index.core.indices.property_graph.sub_retrievers.base",
        "description": "llama_index.core.indices.property_graph.sub_retrievers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.property_graph.sub_retrievers.base",
        "documentation": {}
    },
    {
        "label": "BasePGRetriever",
        "importPath": "llama_index.core.indices.property_graph.sub_retrievers.base",
        "description": "llama_index.core.indices.property_graph.sub_retrievers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.property_graph.sub_retrievers.base",
        "documentation": {}
    },
    {
        "label": "BasePGRetriever",
        "importPath": "llama_index.core.indices.property_graph.sub_retrievers.base",
        "description": "llama_index.core.indices.property_graph.sub_retrievers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.property_graph.sub_retrievers.base",
        "documentation": {}
    },
    {
        "label": "get_entity_class",
        "importPath": "llama_index.core.indices.property_graph.transformations.utils",
        "description": "llama_index.core.indices.property_graph.transformations.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.property_graph.transformations.utils",
        "documentation": {}
    },
    {
        "label": "get_relation_class",
        "importPath": "llama_index.core.indices.property_graph.transformations.utils",
        "description": "llama_index.core.indices.property_graph.transformations.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.property_graph.transformations.utils",
        "documentation": {}
    },
    {
        "label": "default_parse_triplets_fn",
        "importPath": "llama_index.core.indices.property_graph.utils",
        "description": "llama_index.core.indices.property_graph.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.property_graph.utils",
        "documentation": {}
    },
    {
        "label": "IndexLPG",
        "importPath": "llama_index.core.data_structs",
        "description": "llama_index.core.data_structs",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs",
        "documentation": {}
    },
    {
        "label": "SimplePropertyGraphStore",
        "importPath": "llama_index.core.graph_stores.simple_labelled",
        "description": "llama_index.core.graph_stores.simple_labelled",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.simple_labelled",
        "documentation": {}
    },
    {
        "label": "SimplePropertyGraphStore",
        "importPath": "llama_index.core.graph_stores.simple_labelled",
        "description": "llama_index.core.graph_stores.simple_labelled",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.simple_labelled",
        "documentation": {}
    },
    {
        "label": "SimplePropertyGraphStore",
        "importPath": "llama_index.core.graph_stores.simple_labelled",
        "description": "llama_index.core.graph_stores.simple_labelled",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.simple_labelled",
        "documentation": {}
    },
    {
        "label": "SimplePropertyGraphStore",
        "importPath": "llama_index.core.graph_stores.simple_labelled",
        "description": "llama_index.core.graph_stores.simple_labelled",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores.simple_labelled",
        "documentation": {}
    },
    {
        "label": "SimpleLLMPathExtractor",
        "importPath": "llama_index.core.indices.property_graph.transformations",
        "description": "llama_index.core.indices.property_graph.transformations",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.property_graph.transformations",
        "documentation": {}
    },
    {
        "label": "ImplicitPathExtractor",
        "importPath": "llama_index.core.indices.property_graph.transformations",
        "description": "llama_index.core.indices.property_graph.transformations",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.property_graph.transformations",
        "documentation": {}
    },
    {
        "label": "run_transformations",
        "importPath": "llama_index.core.ingestion.pipeline",
        "description": "llama_index.core.ingestion.pipeline",
        "isExtraImport": true,
        "detail": "llama_index.core.ingestion.pipeline",
        "documentation": {}
    },
    {
        "label": "arun_transformations",
        "importPath": "llama_index.core.ingestion.pipeline",
        "description": "llama_index.core.ingestion.pipeline",
        "isExtraImport": true,
        "detail": "llama_index.core.ingestion.pipeline",
        "documentation": {}
    },
    {
        "label": "get_transformation_hash",
        "importPath": "llama_index.core.ingestion.pipeline",
        "description": "llama_index.core.ingestion.pipeline",
        "isExtraImport": true,
        "detail": "llama_index.core.ingestion.pipeline",
        "documentation": {}
    },
    {
        "label": "IngestionPipeline",
        "importPath": "llama_index.core.ingestion.pipeline",
        "description": "llama_index.core.ingestion.pipeline",
        "isExtraImport": true,
        "detail": "llama_index.core.ingestion.pipeline",
        "documentation": {}
    },
    {
        "label": "DEFAULT_DECOMPOSE_QUERY_TRANSFORM_PROMPT",
        "importPath": "llama_index.core.indices.query.query_transform.prompts",
        "description": "llama_index.core.indices.query.query_transform.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.query_transform.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_IMAGE_OUTPUT_PROMPT",
        "importPath": "llama_index.core.indices.query.query_transform.prompts",
        "description": "llama_index.core.indices.query.query_transform.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.query_transform.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_STEP_DECOMPOSE_QUERY_TRANSFORM_PROMPT",
        "importPath": "llama_index.core.indices.query.query_transform.prompts",
        "description": "llama_index.core.indices.query.query_transform.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.query_transform.prompts",
        "documentation": {}
    },
    {
        "label": "DecomposeQueryTransformPrompt",
        "importPath": "llama_index.core.indices.query.query_transform.prompts",
        "description": "llama_index.core.indices.query.query_transform.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.query_transform.prompts",
        "documentation": {}
    },
    {
        "label": "ImageOutputQueryTransformPrompt",
        "importPath": "llama_index.core.indices.query.query_transform.prompts",
        "description": "llama_index.core.indices.query.query_transform.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.query_transform.prompts",
        "documentation": {}
    },
    {
        "label": "StepDecomposeQueryTransformPrompt",
        "importPath": "llama_index.core.indices.query.query_transform.prompts",
        "description": "llama_index.core.indices.query.query_transform.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.query_transform.prompts",
        "documentation": {}
    },
    {
        "label": "DecomposeQueryTransformPrompt",
        "importPath": "llama_index.core.indices.query.query_transform.prompts",
        "description": "llama_index.core.indices.query.query_transform.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.query_transform.prompts",
        "documentation": {}
    },
    {
        "label": "BaseQueryTransform",
        "importPath": "llama_index.core.indices.query.query_transform.base",
        "description": "llama_index.core.indices.query.query_transform.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.query_transform.base",
        "documentation": {}
    },
    {
        "label": "StepDecomposeQueryTransform",
        "importPath": "llama_index.core.indices.query.query_transform.base",
        "description": "llama_index.core.indices.query.query_transform.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.query_transform.base",
        "documentation": {}
    },
    {
        "label": "BaseQueryTransform",
        "importPath": "llama_index.core.indices.query.query_transform.base",
        "description": "llama_index.core.indices.query.query_transform.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.query_transform.base",
        "documentation": {}
    },
    {
        "label": "BaseQueryTransform",
        "importPath": "llama_index.core.indices.query.query_transform.base",
        "description": "llama_index.core.indices.query.query_transform.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.query_transform.base",
        "documentation": {}
    },
    {
        "label": "BaseQueryTransform",
        "importPath": "llama_index.core.indices.query.query_transform.base",
        "description": "llama_index.core.indices.query.query_transform.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.query_transform.base",
        "documentation": {}
    },
    {
        "label": "DecomposeQueryTransform",
        "importPath": "llama_index.core.indices.query.query_transform.base",
        "description": "llama_index.core.indices.query.query_transform.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.query_transform.base",
        "documentation": {}
    },
    {
        "label": "heapq",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "heapq",
        "description": "heapq",
        "detail": "heapq",
        "documentation": {}
    },
    {
        "label": "SQLContextContainer",
        "importPath": "llama_index.core.indices.common.struct_store.schema",
        "description": "llama_index.core.indices.common.struct_store.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.common.struct_store.schema",
        "documentation": {}
    },
    {
        "label": "SQLContextContainer",
        "importPath": "llama_index.core.indices.common.struct_store.schema",
        "description": "llama_index.core.indices.common.struct_store.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.common.struct_store.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "llama_index.core.readers.base",
        "description": "llama_index.core.readers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.readers.base",
        "documentation": {}
    },
    {
        "label": "BasePydanticReader",
        "importPath": "llama_index.core.readers.base",
        "description": "llama_index.core.readers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.readers.base",
        "documentation": {}
    },
    {
        "label": "ReaderConfig",
        "importPath": "llama_index.core.readers.base",
        "description": "llama_index.core.readers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.readers.base",
        "documentation": {}
    },
    {
        "label": "ReaderConfig",
        "importPath": "llama_index.core.readers.base",
        "description": "llama_index.core.readers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.readers.base",
        "documentation": {}
    },
    {
        "label": "BaseReader",
        "importPath": "llama_index.core.readers.base",
        "description": "llama_index.core.readers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.readers.base",
        "documentation": {}
    },
    {
        "label": "ResourcesReaderMixin",
        "importPath": "llama_index.core.readers.base",
        "description": "llama_index.core.readers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.readers.base",
        "documentation": {}
    },
    {
        "label": "BaseReader",
        "importPath": "llama_index.core.readers.base",
        "description": "llama_index.core.readers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.readers.base",
        "documentation": {}
    },
    {
        "label": "BaseReader",
        "importPath": "llama_index.core.readers.base",
        "description": "llama_index.core.readers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.readers.base",
        "documentation": {}
    },
    {
        "label": "BasePydanticReader",
        "importPath": "llama_index.core.readers.base",
        "description": "llama_index.core.readers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.readers.base",
        "documentation": {}
    },
    {
        "label": "BasePydanticReader",
        "importPath": "llama_index.core.readers.base",
        "description": "llama_index.core.readers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.readers.base",
        "documentation": {}
    },
    {
        "label": "BaseReader",
        "importPath": "llama_index.core.readers.base",
        "description": "llama_index.core.readers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.readers.base",
        "documentation": {}
    },
    {
        "label": "SQLStructDatapointExtractor",
        "importPath": "llama_index.core.indices.common.struct_store.sql",
        "description": "llama_index.core.indices.common.struct_store.sql",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.common.struct_store.sql",
        "documentation": {}
    },
    {
        "label": "BaseStructStoreIndex",
        "importPath": "llama_index.core.indices.struct_store.base",
        "description": "llama_index.core.indices.struct_store.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.struct_store.base",
        "documentation": {}
    },
    {
        "label": "default_output_parser",
        "importPath": "llama_index.core.indices.struct_store.base",
        "description": "llama_index.core.indices.struct_store.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.struct_store.base",
        "documentation": {}
    },
    {
        "label": "SQLContextContainerBuilder",
        "importPath": "llama_index.core.indices.struct_store.container_builder",
        "description": "llama_index.core.indices.struct_store.container_builder",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.struct_store.container_builder",
        "documentation": {}
    },
    {
        "label": "SQLContextContainerBuilder",
        "importPath": "llama_index.core.indices.struct_store.container_builder",
        "description": "llama_index.core.indices.struct_store.container_builder",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.struct_store.container_builder",
        "documentation": {}
    },
    {
        "label": "SQLStructStoreIndex",
        "importPath": "llama_index.core.indices.struct_store.sql",
        "description": "llama_index.core.indices.struct_store.sql",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.struct_store.sql",
        "documentation": {}
    },
    {
        "label": "SQLStructStoreIndex",
        "importPath": "llama_index.core.indices.struct_store.sql",
        "description": "llama_index.core.indices.struct_store.sql",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.struct_store.sql",
        "documentation": {}
    },
    {
        "label": "SQLContextContainerBuilder",
        "importPath": "llama_index.core.indices.struct_store.sql",
        "description": "llama_index.core.indices.struct_store.sql",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.struct_store.sql",
        "documentation": {}
    },
    {
        "label": "SQLStructStoreIndex",
        "importPath": "llama_index.core.indices.struct_store.sql",
        "description": "llama_index.core.indices.struct_store.sql",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.struct_store.sql",
        "documentation": {}
    },
    {
        "label": "SQLStructStoreIndex",
        "importPath": "llama_index.core.indices.struct_store.sql",
        "description": "llama_index.core.indices.struct_store.sql",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.struct_store.sql",
        "documentation": {}
    },
    {
        "label": "NLSQLRetriever",
        "importPath": "llama_index.core.indices.struct_store.sql_retriever",
        "description": "llama_index.core.indices.struct_store.sql_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.struct_store.sql_retriever",
        "documentation": {}
    },
    {
        "label": "SQLParserMode",
        "importPath": "llama_index.core.indices.struct_store.sql_retriever",
        "description": "llama_index.core.indices.struct_store.sql_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.struct_store.sql_retriever",
        "documentation": {}
    },
    {
        "label": "ObjectRetriever",
        "importPath": "llama_index.core.objects.base",
        "description": "llama_index.core.objects.base",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.base",
        "documentation": {}
    },
    {
        "label": "ObjectRetriever",
        "importPath": "llama_index.core.objects.base",
        "description": "llama_index.core.objects.base",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.base",
        "documentation": {}
    },
    {
        "label": "SimpleObjectNodeMapping",
        "importPath": "llama_index.core.objects.base",
        "description": "llama_index.core.objects.base",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.base",
        "documentation": {}
    },
    {
        "label": "ObjectRetriever",
        "importPath": "llama_index.core.objects.base",
        "description": "llama_index.core.objects.base",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.base",
        "documentation": {}
    },
    {
        "label": "ObjectIndex",
        "importPath": "llama_index.core.objects.base",
        "description": "llama_index.core.objects.base",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.base",
        "documentation": {}
    },
    {
        "label": "SQLTableSchema",
        "importPath": "llama_index.core.objects.table_node_mapping",
        "description": "llama_index.core.objects.table_node_mapping",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.table_node_mapping",
        "documentation": {}
    },
    {
        "label": "SQLTableSchema",
        "importPath": "llama_index.core.objects.table_node_mapping",
        "description": "llama_index.core.objects.table_node_mapping",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.table_node_mapping",
        "documentation": {}
    },
    {
        "label": "SQLTableNodeMapping",
        "importPath": "llama_index.core.objects.table_node_mapping",
        "description": "llama_index.core.objects.table_node_mapping",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.table_node_mapping",
        "documentation": {}
    },
    {
        "label": "SQLTableSchema",
        "importPath": "llama_index.core.objects.table_node_mapping",
        "description": "llama_index.core.objects.table_node_mapping",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.table_node_mapping",
        "documentation": {}
    },
    {
        "label": "TreeIndex",
        "importPath": "llama_index.core.indices.tree.base",
        "description": "llama_index.core.indices.tree.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.tree.base",
        "documentation": {}
    },
    {
        "label": "TreeIndex",
        "importPath": "llama_index.core.indices.tree.base",
        "description": "llama_index.core.indices.tree.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.tree.base",
        "documentation": {}
    },
    {
        "label": "TreeIndex",
        "importPath": "llama_index.core.indices.tree.base",
        "description": "llama_index.core.indices.tree.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.tree.base",
        "documentation": {}
    },
    {
        "label": "TreeIndex",
        "importPath": "llama_index.core.indices.tree.base",
        "description": "llama_index.core.indices.tree.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.tree.base",
        "documentation": {}
    },
    {
        "label": "TreeIndex",
        "importPath": "llama_index.core.indices.tree.base",
        "description": "llama_index.core.indices.tree.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.tree.base",
        "documentation": {}
    },
    {
        "label": "TreeIndex",
        "importPath": "llama_index.core.indices.tree.base",
        "description": "llama_index.core.indices.tree.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.tree.base",
        "documentation": {}
    },
    {
        "label": "TreeRetrieverMode",
        "importPath": "llama_index.core.indices.tree.base",
        "description": "llama_index.core.indices.tree.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.tree.base",
        "documentation": {}
    },
    {
        "label": "TreeIndex",
        "importPath": "llama_index.core.indices.tree.base",
        "description": "llama_index.core.indices.tree.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.tree.base",
        "documentation": {}
    },
    {
        "label": "TreeIndex",
        "importPath": "llama_index.core.indices.tree.base",
        "description": "llama_index.core.indices.tree.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.tree.base",
        "documentation": {}
    },
    {
        "label": "TreeIndex",
        "importPath": "llama_index.core.indices.tree.base",
        "description": "llama_index.core.indices.tree.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.tree.base",
        "documentation": {}
    },
    {
        "label": "TreeIndex",
        "importPath": "llama_index.core.indices.tree.base",
        "description": "llama_index.core.indices.tree.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.tree.base",
        "documentation": {}
    },
    {
        "label": "TreeIndex",
        "importPath": "llama_index.core.indices.tree.base",
        "description": "llama_index.core.indices.tree.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.tree.base",
        "documentation": {}
    },
    {
        "label": "GPTTreeIndexBuilder",
        "importPath": "llama_index.core.indices.common_tree.base",
        "description": "llama_index.core.indices.common_tree.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.common_tree.base",
        "documentation": {}
    },
    {
        "label": "TreeIndexInserter",
        "importPath": "llama_index.core.indices.tree.inserter",
        "description": "llama_index.core.indices.tree.inserter",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.tree.inserter",
        "documentation": {}
    },
    {
        "label": "get_numbered_text_from_nodes",
        "importPath": "llama_index.core.indices.tree.utils",
        "description": "llama_index.core.indices.tree.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.tree.utils",
        "documentation": {}
    },
    {
        "label": "get_numbered_text_from_nodes",
        "importPath": "llama_index.core.indices.tree.utils",
        "description": "llama_index.core.indices.tree.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.tree.utils",
        "documentation": {}
    },
    {
        "label": "get_numbered_text_from_nodes",
        "importPath": "llama_index.core.indices.tree.utils",
        "description": "llama_index.core.indices.tree.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.tree.utils",
        "documentation": {}
    },
    {
        "label": "TreeSelectLeafRetriever",
        "importPath": "llama_index.core.indices.tree.select_leaf_retriever",
        "description": "llama_index.core.indices.tree.select_leaf_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.tree.select_leaf_retriever",
        "documentation": {}
    },
    {
        "label": "TokenTextSplitter",
        "importPath": "llama_index.core.node_parser.text",
        "description": "llama_index.core.node_parser.text",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.text",
        "documentation": {}
    },
    {
        "label": "SentenceSplitter",
        "importPath": "llama_index.core.node_parser.text",
        "description": "llama_index.core.node_parser.text",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.text",
        "documentation": {}
    },
    {
        "label": "TokenTextSplitter",
        "importPath": "llama_index.core.node_parser.text",
        "description": "llama_index.core.node_parser.text",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.text",
        "documentation": {}
    },
    {
        "label": "truncate_text",
        "importPath": "llama_index.core.node_parser.text.utils",
        "description": "llama_index.core.node_parser.text.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.text.utils",
        "documentation": {}
    },
    {
        "label": "truncate_text",
        "importPath": "llama_index.core.node_parser.text.utils",
        "description": "llama_index.core.node_parser.text.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.text.utils",
        "documentation": {}
    },
    {
        "label": "split_by_sentence_tokenizer",
        "importPath": "llama_index.core.node_parser.text.utils",
        "description": "llama_index.core.node_parser.text.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.text.utils",
        "documentation": {}
    },
    {
        "label": "split_by_sentence_tokenizer",
        "importPath": "llama_index.core.node_parser.text.utils",
        "description": "llama_index.core.node_parser.text.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.text.utils",
        "documentation": {}
    },
    {
        "label": "split_by_char",
        "importPath": "llama_index.core.node_parser.text.utils",
        "description": "llama_index.core.node_parser.text.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.text.utils",
        "documentation": {}
    },
    {
        "label": "split_by_regex",
        "importPath": "llama_index.core.node_parser.text.utils",
        "description": "llama_index.core.node_parser.text.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.text.utils",
        "documentation": {}
    },
    {
        "label": "split_by_sentence_tokenizer",
        "importPath": "llama_index.core.node_parser.text.utils",
        "description": "llama_index.core.node_parser.text.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.text.utils",
        "documentation": {}
    },
    {
        "label": "split_by_sep",
        "importPath": "llama_index.core.node_parser.text.utils",
        "description": "llama_index.core.node_parser.text.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.text.utils",
        "documentation": {}
    },
    {
        "label": "split_by_sentence_tokenizer",
        "importPath": "llama_index.core.node_parser.text.utils",
        "description": "llama_index.core.node_parser.text.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.text.utils",
        "documentation": {}
    },
    {
        "label": "split_by_char",
        "importPath": "llama_index.core.node_parser.text.utils",
        "description": "llama_index.core.node_parser.text.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.text.utils",
        "documentation": {}
    },
    {
        "label": "split_by_sep",
        "importPath": "llama_index.core.node_parser.text.utils",
        "description": "llama_index.core.node_parser.text.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.text.utils",
        "documentation": {}
    },
    {
        "label": "truncate_text",
        "importPath": "llama_index.core.node_parser.text.utils",
        "description": "llama_index.core.node_parser.text.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.text.utils",
        "documentation": {}
    },
    {
        "label": "BaseAutoRetriever",
        "importPath": "llama_index.core.base.base_auto_retriever",
        "description": "llama_index.core.base.base_auto_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_auto_retriever",
        "documentation": {}
    },
    {
        "label": "VectorIndexRetriever",
        "importPath": "llama_index.core.indices.vector_store.retrievers",
        "description": "llama_index.core.indices.vector_store.retrievers",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.vector_store.retrievers",
        "documentation": {}
    },
    {
        "label": "VectorStoreQueryOutputParser",
        "importPath": "llama_index.core.indices.vector_store.retrievers.auto_retriever.output_parser",
        "description": "llama_index.core.indices.vector_store.retrievers.auto_retriever.output_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.vector_store.retrievers.auto_retriever.output_parser",
        "documentation": {}
    },
    {
        "label": "VectorStoreQueryOutputParser",
        "importPath": "llama_index.core.indices.vector_store.retrievers.auto_retriever.output_parser",
        "description": "llama_index.core.indices.vector_store.retrievers.auto_retriever.output_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.vector_store.retrievers.auto_retriever.output_parser",
        "documentation": {}
    },
    {
        "label": "DEFAULT_VECTOR_STORE_QUERY_PROMPT_TMPL",
        "importPath": "llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "description": "llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "documentation": {}
    },
    {
        "label": "OutputParserException",
        "importPath": "llama_index.core.output_parsers.base",
        "description": "llama_index.core.output_parsers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.base",
        "documentation": {}
    },
    {
        "label": "StructuredOutput",
        "importPath": "llama_index.core.output_parsers.base",
        "description": "llama_index.core.output_parsers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.base",
        "documentation": {}
    },
    {
        "label": "StructuredOutput",
        "importPath": "llama_index.core.output_parsers.base",
        "description": "llama_index.core.output_parsers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.base",
        "documentation": {}
    },
    {
        "label": "OutputParserException",
        "importPath": "llama_index.core.output_parsers.base",
        "description": "llama_index.core.output_parsers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.base",
        "documentation": {}
    },
    {
        "label": "StructuredOutput",
        "importPath": "llama_index.core.output_parsers.base",
        "description": "llama_index.core.output_parsers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.base",
        "documentation": {}
    },
    {
        "label": "OutputParserException",
        "importPath": "llama_index.core.output_parsers.base",
        "description": "llama_index.core.output_parsers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.base",
        "documentation": {}
    },
    {
        "label": "OutputParserException",
        "importPath": "llama_index.core.output_parsers.base",
        "description": "llama_index.core.output_parsers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.base",
        "documentation": {}
    },
    {
        "label": "StructuredOutput",
        "importPath": "llama_index.core.output_parsers.base",
        "description": "llama_index.core.output_parsers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.base",
        "documentation": {}
    },
    {
        "label": "StructuredOutput",
        "importPath": "llama_index.core.output_parsers.base",
        "description": "llama_index.core.output_parsers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.base",
        "documentation": {}
    },
    {
        "label": "StructuredOutput",
        "importPath": "llama_index.core.output_parsers.base",
        "description": "llama_index.core.output_parsers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.base",
        "documentation": {}
    },
    {
        "label": "StructuredOutput",
        "importPath": "llama_index.core.output_parsers.base",
        "description": "llama_index.core.output_parsers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.base",
        "documentation": {}
    },
    {
        "label": "StructuredOutput",
        "importPath": "llama_index.core.output_parsers.base",
        "description": "llama_index.core.output_parsers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.base",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "collections.abc",
        "description": "collections.abc",
        "isExtraImport": true,
        "detail": "collections.abc",
        "documentation": {}
    },
    {
        "label": "MutableMapping",
        "importPath": "collections.abc",
        "description": "collections.abc",
        "isExtraImport": true,
        "detail": "collections.abc",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "collections.abc",
        "description": "collections.abc",
        "isExtraImport": true,
        "detail": "collections.abc",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "collections.abc",
        "description": "collections.abc",
        "isExtraImport": true,
        "detail": "collections.abc",
        "documentation": {}
    },
    {
        "label": "INDEX_STRUCT_TYPE_TO_INDEX_CLASS",
        "importPath": "llama_index.core.indices.registry",
        "description": "llama_index.core.indices.registry",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.registry",
        "documentation": {}
    },
    {
        "label": "AutoPrevNextNodePostprocessor",
        "importPath": "llama_index.core.postprocessor",
        "description": "llama_index.core.postprocessor",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor",
        "documentation": {}
    },
    {
        "label": "EmbeddingRecencyPostprocessor",
        "importPath": "llama_index.core.postprocessor",
        "description": "llama_index.core.postprocessor",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor",
        "documentation": {}
    },
    {
        "label": "FixedRecencyPostprocessor",
        "importPath": "llama_index.core.postprocessor",
        "description": "llama_index.core.postprocessor",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor",
        "documentation": {}
    },
    {
        "label": "KeywordNodePostprocessor",
        "importPath": "llama_index.core.postprocessor",
        "description": "llama_index.core.postprocessor",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor",
        "documentation": {}
    },
    {
        "label": "LLMRerank",
        "importPath": "llama_index.core.postprocessor",
        "description": "llama_index.core.postprocessor",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor",
        "documentation": {}
    },
    {
        "label": "LongContextReorder",
        "importPath": "llama_index.core.postprocessor",
        "description": "llama_index.core.postprocessor",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor",
        "documentation": {}
    },
    {
        "label": "MetadataReplacementPostProcessor",
        "importPath": "llama_index.core.postprocessor",
        "description": "llama_index.core.postprocessor",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor",
        "documentation": {}
    },
    {
        "label": "NERPIINodePostprocessor",
        "importPath": "llama_index.core.postprocessor",
        "description": "llama_index.core.postprocessor",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor",
        "documentation": {}
    },
    {
        "label": "PIINodePostprocessor",
        "importPath": "llama_index.core.postprocessor",
        "description": "llama_index.core.postprocessor",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor",
        "documentation": {}
    },
    {
        "label": "PrevNextNodePostprocessor",
        "importPath": "llama_index.core.postprocessor",
        "description": "llama_index.core.postprocessor",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor",
        "documentation": {}
    },
    {
        "label": "SentenceEmbeddingOptimizer",
        "importPath": "llama_index.core.postprocessor",
        "description": "llama_index.core.postprocessor",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor",
        "documentation": {}
    },
    {
        "label": "SentenceTransformerRerank",
        "importPath": "llama_index.core.postprocessor",
        "description": "llama_index.core.postprocessor",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor",
        "documentation": {}
    },
    {
        "label": "SimilarityPostprocessor",
        "importPath": "llama_index.core.postprocessor",
        "description": "llama_index.core.postprocessor",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor",
        "documentation": {}
    },
    {
        "label": "TimeWeightedPostprocessor",
        "importPath": "llama_index.core.postprocessor",
        "description": "llama_index.core.postprocessor",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor",
        "documentation": {}
    },
    {
        "label": "MetadataReplacementPostProcessor",
        "importPath": "llama_index.core.postprocessor",
        "description": "llama_index.core.postprocessor",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor",
        "documentation": {}
    },
    {
        "label": "RankGPTRerank",
        "importPath": "llama_index.core.postprocessor.rankGPT_rerank",
        "description": "llama_index.core.postprocessor.rankGPT_rerank",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.rankGPT_rerank",
        "documentation": {}
    },
    {
        "label": "RankGPTRerank",
        "importPath": "llama_index.core.postprocessor.rankGPT_rerank",
        "description": "llama_index.core.postprocessor.rankGPT_rerank",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.rankGPT_rerank",
        "documentation": {}
    },
    {
        "label": "SentenceTransformerRerank",
        "importPath": "llama_index.core.postprocessor.sbert_rerank",
        "description": "llama_index.core.postprocessor.sbert_rerank",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.sbert_rerank",
        "documentation": {}
    },
    {
        "label": "StructuredLLM",
        "importPath": "llama_index.core.llms.structured_llm",
        "description": "llama_index.core.llms.structured_llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.structured_llm",
        "documentation": {}
    },
    {
        "label": "StructuredLLM",
        "importPath": "llama_index.core.llms.structured_llm",
        "description": "llama_index.core.llms.structured_llm",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.structured_llm",
        "documentation": {}
    },
    {
        "label": "TokenTextSplitter",
        "importPath": "llama_index.core.node_parser.text.token",
        "description": "llama_index.core.node_parser.text.token",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.text.token",
        "documentation": {}
    },
    {
        "label": "TokenTextSplitter",
        "importPath": "llama_index.core.node_parser.text.token",
        "description": "llama_index.core.node_parser.text.token",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.text.token",
        "documentation": {}
    },
    {
        "label": "get_empty_prompt_txt",
        "importPath": "llama_index.core.prompts.prompt_utils",
        "description": "llama_index.core.prompts.prompt_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.prompt_utils",
        "documentation": {}
    },
    {
        "label": "get_biggest_prompt",
        "importPath": "llama_index.core.prompts.prompt_utils",
        "description": "llama_index.core.prompts.prompt_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.prompt_utils",
        "documentation": {}
    },
    {
        "label": "get_biggest_prompt",
        "importPath": "llama_index.core.prompts.prompt_utils",
        "description": "llama_index.core.prompts.prompt_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.prompt_utils",
        "documentation": {}
    },
    {
        "label": "get_empty_prompt_txt",
        "importPath": "llama_index.core.prompts.prompt_utils",
        "description": "llama_index.core.prompts.prompt_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.prompt_utils",
        "documentation": {}
    },
    {
        "label": "format_content_blocks",
        "importPath": "llama_index.core.prompts.utils",
        "description": "llama_index.core.prompts.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.utils",
        "documentation": {}
    },
    {
        "label": "SafeFormatter",
        "importPath": "llama_index.core.prompts.utils",
        "description": "llama_index.core.prompts.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.utils",
        "documentation": {}
    },
    {
        "label": "get_template_vars",
        "importPath": "llama_index.core.prompts.utils",
        "description": "llama_index.core.prompts.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.utils",
        "documentation": {}
    },
    {
        "label": "format_string",
        "importPath": "llama_index.core.prompts.utils",
        "description": "llama_index.core.prompts.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.utils",
        "documentation": {}
    },
    {
        "label": "is_chat_model",
        "importPath": "llama_index.core.prompts.utils",
        "description": "llama_index.core.prompts.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.utils",
        "documentation": {}
    },
    {
        "label": "MultiModalVectorStoreIndex",
        "importPath": "llama_index.core.indices.multi_modal",
        "description": "llama_index.core.indices.multi_modal",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.multi_modal",
        "documentation": {}
    },
    {
        "label": "PropertyGraphIndex",
        "importPath": "llama_index.core.indices.property_graph",
        "description": "llama_index.core.indices.property_graph",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.property_graph",
        "documentation": {}
    },
    {
        "label": "PandasIndex",
        "importPath": "llama_index.core.indices.struct_store.pandas",
        "description": "llama_index.core.indices.struct_store.pandas",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.struct_store.pandas",
        "documentation": {}
    },
    {
        "label": "httpx",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "httpx",
        "description": "httpx",
        "detail": "httpx",
        "documentation": {}
    },
    {
        "label": "doc_to_json",
        "importPath": "llama_index.core.storage.docstore.utils",
        "description": "llama_index.core.storage.docstore.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.utils",
        "documentation": {}
    },
    {
        "label": "json_to_doc",
        "importPath": "llama_index.core.storage.docstore.utils",
        "description": "llama_index.core.storage.docstore.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.utils",
        "documentation": {}
    },
    {
        "label": "doc_to_json",
        "importPath": "llama_index.core.storage.docstore.utils",
        "description": "llama_index.core.storage.docstore.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.utils",
        "documentation": {}
    },
    {
        "label": "json_to_doc",
        "importPath": "llama_index.core.storage.docstore.utils",
        "description": "llama_index.core.storage.docstore.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.utils",
        "documentation": {}
    },
    {
        "label": "SimpleKVStore",
        "importPath": "llama_index.core.storage.kvstore",
        "description": "llama_index.core.storage.kvstore",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.kvstore",
        "documentation": {}
    },
    {
        "label": "SimpleKVStore",
        "importPath": "llama_index.core.storage.kvstore",
        "description": "llama_index.core.storage.kvstore",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.kvstore",
        "documentation": {}
    },
    {
        "label": "BaseKVStore",
        "importPath": "llama_index.core.storage.kvstore.types",
        "description": "llama_index.core.storage.kvstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.kvstore.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_BATCH_SIZE",
        "importPath": "llama_index.core.storage.kvstore.types",
        "description": "llama_index.core.storage.kvstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.kvstore.types",
        "documentation": {}
    },
    {
        "label": "BaseKVStore",
        "importPath": "llama_index.core.storage.kvstore.types",
        "description": "llama_index.core.storage.kvstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.kvstore.types",
        "documentation": {}
    },
    {
        "label": "MutableMappingKVStore",
        "importPath": "llama_index.core.storage.kvstore.types",
        "description": "llama_index.core.storage.kvstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.kvstore.types",
        "documentation": {}
    },
    {
        "label": "BaseInMemoryKVStore",
        "importPath": "llama_index.core.storage.kvstore.types",
        "description": "llama_index.core.storage.kvstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.kvstore.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_BATCH_SIZE",
        "importPath": "llama_index.core.storage.kvstore.types",
        "description": "llama_index.core.storage.kvstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.kvstore.types",
        "documentation": {}
    },
    {
        "label": "BaseKVStore",
        "importPath": "llama_index.core.storage.kvstore.types",
        "description": "llama_index.core.storage.kvstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.kvstore.types",
        "documentation": {}
    },
    {
        "label": "MutableMappingKVStore",
        "importPath": "llama_index.core.storage.kvstore.types",
        "description": "llama_index.core.storage.kvstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.kvstore.types",
        "documentation": {}
    },
    {
        "label": "BaseInMemoryKVStore",
        "importPath": "llama_index.core.storage.kvstore.types",
        "description": "llama_index.core.storage.kvstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.kvstore.types",
        "documentation": {}
    },
    {
        "label": "MutableMappingKVStore",
        "importPath": "llama_index.core.storage.kvstore.types",
        "description": "llama_index.core.storage.kvstore.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.kvstore.types",
        "documentation": {}
    },
    {
        "label": "multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "cpu_count",
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "isExtraImport": true,
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "concurrent.futures",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ProcessPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "zip_longest",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "islice",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CACHE_NAME",
        "importPath": "llama_index.core.ingestion.cache",
        "description": "llama_index.core.ingestion.cache",
        "isExtraImport": true,
        "detail": "llama_index.core.ingestion.cache",
        "documentation": {}
    },
    {
        "label": "IngestionCache",
        "importPath": "llama_index.core.ingestion.cache",
        "description": "llama_index.core.ingestion.cache",
        "isExtraImport": true,
        "detail": "llama_index.core.ingestion.cache",
        "documentation": {}
    },
    {
        "label": "BaseEvent",
        "importPath": "llama_index.core.instrumentation.events.base",
        "description": "llama_index.core.instrumentation.events.base",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.base",
        "documentation": {}
    },
    {
        "label": "BaseEvent",
        "importPath": "llama_index.core.instrumentation.events.base",
        "description": "llama_index.core.instrumentation.events.base",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.base",
        "documentation": {}
    },
    {
        "label": "BaseEvent",
        "importPath": "llama_index.core.instrumentation.events.base",
        "description": "llama_index.core.instrumentation.events.base",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.base",
        "documentation": {}
    },
    {
        "label": "BaseEvent",
        "importPath": "llama_index.core.instrumentation.events.base",
        "description": "llama_index.core.instrumentation.events.base",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.base",
        "documentation": {}
    },
    {
        "label": "BaseEvent",
        "importPath": "llama_index.core.instrumentation.events.base",
        "description": "llama_index.core.instrumentation.events.base",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.base",
        "documentation": {}
    },
    {
        "label": "BaseEvent",
        "importPath": "llama_index.core.instrumentation.events.base",
        "description": "llama_index.core.instrumentation.events.base",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.base",
        "documentation": {}
    },
    {
        "label": "BaseEvent",
        "importPath": "llama_index.core.instrumentation.events.base",
        "description": "llama_index.core.instrumentation.events.base",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.base",
        "documentation": {}
    },
    {
        "label": "BaseEvent",
        "importPath": "llama_index.core.instrumentation.events.base",
        "description": "llama_index.core.instrumentation.events.base",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.base",
        "documentation": {}
    },
    {
        "label": "BaseEvent",
        "importPath": "llama_index_instrumentation.base",
        "description": "llama_index_instrumentation.base",
        "isExtraImport": true,
        "detail": "llama_index_instrumentation.base",
        "documentation": {}
    },
    {
        "label": "BaseInstrumentationHandler",
        "importPath": "llama_index_instrumentation.base",
        "description": "llama_index_instrumentation.base",
        "isExtraImport": true,
        "detail": "llama_index_instrumentation.base",
        "documentation": {}
    },
    {
        "label": "BaseEvent",
        "importPath": "llama_index.core.instrumentation.events",
        "description": "llama_index.core.instrumentation.events",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events",
        "documentation": {}
    },
    {
        "label": "SpanDropEvent",
        "importPath": "llama_index_instrumentation.events.span",
        "description": "llama_index_instrumentation.events.span",
        "isExtraImport": true,
        "detail": "llama_index_instrumentation.events.span",
        "documentation": {}
    },
    {
        "label": "BaseEventHandler",
        "importPath": "llama_index_instrumentation.event_handlers.base",
        "description": "llama_index_instrumentation.event_handlers.base",
        "isExtraImport": true,
        "detail": "llama_index_instrumentation.event_handlers.base",
        "documentation": {}
    },
    {
        "label": "NullEventHandler",
        "importPath": "llama_index_instrumentation.event_handlers.null",
        "description": "llama_index_instrumentation.event_handlers.null",
        "isExtraImport": true,
        "detail": "llama_index_instrumentation.event_handlers.null",
        "documentation": {}
    },
    {
        "label": "BaseSpan",
        "importPath": "llama_index_instrumentation.span.base",
        "description": "llama_index_instrumentation.span.base",
        "isExtraImport": true,
        "detail": "llama_index_instrumentation.span.base",
        "documentation": {}
    },
    {
        "label": "SimpleSpan",
        "importPath": "llama_index_instrumentation.span.simple",
        "description": "llama_index_instrumentation.span.simple",
        "isExtraImport": true,
        "detail": "llama_index_instrumentation.span.simple",
        "documentation": {}
    },
    {
        "label": "BaseSpanHandler",
        "importPath": "llama_index_instrumentation.span_handlers.base",
        "description": "llama_index_instrumentation.span_handlers.base",
        "isExtraImport": true,
        "detail": "llama_index_instrumentation.span_handlers.base",
        "documentation": {}
    },
    {
        "label": "T",
        "importPath": "llama_index_instrumentation.span_handlers.base",
        "description": "llama_index_instrumentation.span_handlers.base",
        "isExtraImport": true,
        "detail": "llama_index_instrumentation.span_handlers.base",
        "documentation": {}
    },
    {
        "label": "NullSpanHandler",
        "importPath": "llama_index_instrumentation.span_handlers.null",
        "description": "llama_index_instrumentation.span_handlers.null",
        "isExtraImport": true,
        "detail": "llama_index_instrumentation.span_handlers.null",
        "documentation": {}
    },
    {
        "label": "SimpleSpanHandler",
        "importPath": "llama_index_instrumentation.span_handlers.simple",
        "description": "llama_index_instrumentation.span_handlers.simple",
        "isExtraImport": true,
        "detail": "llama_index_instrumentation.span_handlers.simple",
        "documentation": {}
    },
    {
        "label": "DISPATCHER_SPAN_DECORATED_ATTR",
        "importPath": "llama_index_instrumentation.dispatcher",
        "description": "llama_index_instrumentation.dispatcher",
        "isExtraImport": true,
        "detail": "llama_index_instrumentation.dispatcher",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    Dispatcher",
        "importPath": "llama_index_instrumentation.dispatcher",
        "description": "llama_index_instrumentation.dispatcher",
        "isExtraImport": true,
        "detail": "llama_index_instrumentation.dispatcher",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    EventDispatcher",
        "importPath": "llama_index_instrumentation.dispatcher",
        "description": "llama_index_instrumentation.dispatcher",
        "isExtraImport": true,
        "detail": "llama_index_instrumentation.dispatcher",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    Manager",
        "importPath": "llama_index_instrumentation.dispatcher",
        "description": "llama_index_instrumentation.dispatcher",
        "isExtraImport": true,
        "detail": "llama_index_instrumentation.dispatcher",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    active_instrument_tags",
        "importPath": "llama_index_instrumentation.dispatcher",
        "description": "llama_index_instrumentation.dispatcher",
        "isExtraImport": true,
        "detail": "llama_index_instrumentation.dispatcher",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    instrument_tags",
        "importPath": "llama_index_instrumentation.dispatcher",
        "description": "llama_index_instrumentation.dispatcher",
        "isExtraImport": true,
        "detail": "llama_index_instrumentation.dispatcher",
        "documentation": {}
    },
    {
        "label": "# noqa",
        "importPath": "llama_index_instrumentation.dispatcher",
        "description": "llama_index_instrumentation.dispatcher",
        "isExtraImport": true,
        "detail": "llama_index_instrumentation.dispatcher",
        "documentation": {}
    },
    {
        "label": "AgentExecutor",
        "importPath": "llama_index.core.bridge.langchain",
        "description": "llama_index.core.bridge.langchain",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.langchain",
        "documentation": {}
    },
    {
        "label": "AgentType",
        "importPath": "llama_index.core.bridge.langchain",
        "description": "llama_index.core.bridge.langchain",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.langchain",
        "documentation": {}
    },
    {
        "label": "BaseCallbackManager",
        "importPath": "llama_index.core.bridge.langchain",
        "description": "llama_index.core.bridge.langchain",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.langchain",
        "documentation": {}
    },
    {
        "label": "BaseLLM",
        "importPath": "llama_index.core.bridge.langchain",
        "description": "llama_index.core.bridge.langchain",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.langchain",
        "documentation": {}
    },
    {
        "label": "initialize_agent",
        "importPath": "llama_index.core.bridge.langchain",
        "description": "llama_index.core.bridge.langchain",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.langchain",
        "documentation": {}
    },
    {
        "label": "BaseTool",
        "importPath": "llama_index.core.bridge.langchain",
        "description": "llama_index.core.bridge.langchain",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.langchain",
        "documentation": {}
    },
    {
        "label": "BaseToolkit",
        "importPath": "llama_index.core.bridge.langchain",
        "description": "llama_index.core.bridge.langchain",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.langchain",
        "documentation": {}
    },
    {
        "label": "BaseTool",
        "importPath": "llama_index.core.bridge.langchain",
        "description": "llama_index.core.bridge.langchain",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.langchain",
        "documentation": {}
    },
    {
        "label": "AIMessage",
        "importPath": "llama_index.core.bridge.langchain",
        "description": "llama_index.core.bridge.langchain",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.langchain",
        "documentation": {}
    },
    {
        "label": "BaseChatMemory",
        "importPath": "llama_index.core.bridge.langchain",
        "description": "llama_index.core.bridge.langchain",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.langchain",
        "documentation": {}
    },
    {
        "label": "BaseMessage",
        "importPath": "llama_index.core.bridge.langchain",
        "description": "llama_index.core.bridge.langchain",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.langchain",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "llama_index.core.bridge.langchain",
        "description": "llama_index.core.bridge.langchain",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.langchain",
        "documentation": {}
    },
    {
        "label": "BaseMemory",
        "importPath": "llama_index.core.bridge.langchain",
        "description": "llama_index.core.bridge.langchain",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.langchain",
        "documentation": {}
    },
    {
        "label": "BaseCallbackHandler",
        "importPath": "llama_index.core.bridge.langchain",
        "description": "llama_index.core.bridge.langchain",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.langchain",
        "documentation": {}
    },
    {
        "label": "LLMResult",
        "importPath": "llama_index.core.bridge.langchain",
        "description": "llama_index.core.bridge.langchain",
        "isExtraImport": true,
        "detail": "llama_index.core.bridge.langchain",
        "documentation": {}
    },
    {
        "label": "LlamaToolkit",
        "importPath": "llama_index.core.langchain_helpers.agents.toolkits",
        "description": "llama_index.core.langchain_helpers.agents.toolkits",
        "isExtraImport": true,
        "detail": "llama_index.core.langchain_helpers.agents.toolkits",
        "documentation": {}
    },
    {
        "label": "IndexToolConfig",
        "importPath": "llama_index.core.langchain_helpers.agents.tools",
        "description": "llama_index.core.langchain_helpers.agents.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.langchain_helpers.agents.tools",
        "documentation": {}
    },
    {
        "label": "LlamaIndexTool",
        "importPath": "llama_index.core.langchain_helpers.agents.tools",
        "description": "llama_index.core.langchain_helpers.agents.tools",
        "isExtraImport": true,
        "detail": "llama_index.core.langchain_helpers.agents.tools",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "llama_index.core.text_splitter",
        "description": "llama_index.core.text_splitter",
        "isExtraImport": true,
        "detail": "llama_index.core.text_splitter",
        "documentation": {}
    },
    {
        "label": "LLAMA_DATASETS_LFS_URL",
        "importPath": "llama_index.core.download.dataset",
        "description": "llama_index.core.download.dataset",
        "isExtraImport": true,
        "detail": "llama_index.core.download.dataset",
        "documentation": {}
    },
    {
        "label": "LLAMA_DATASETS_SOURCE_FILES_GITHUB_TREE_URL",
        "importPath": "llama_index.core.download.dataset",
        "description": "llama_index.core.download.dataset",
        "isExtraImport": true,
        "detail": "llama_index.core.download.dataset",
        "documentation": {}
    },
    {
        "label": "LLAMA_DATASETS_URL",
        "importPath": "llama_index.core.download.dataset",
        "description": "llama_index.core.download.dataset",
        "isExtraImport": true,
        "detail": "llama_index.core.download.dataset",
        "documentation": {}
    },
    {
        "label": "download_llama_dataset",
        "importPath": "llama_index.core.download.dataset",
        "description": "llama_index.core.download.dataset",
        "isExtraImport": true,
        "detail": "llama_index.core.download.dataset",
        "documentation": {}
    },
    {
        "label": "MODULE_TYPE",
        "importPath": "llama_index.core.download.module",
        "description": "llama_index.core.download.module",
        "isExtraImport": true,
        "detail": "llama_index.core.download.module",
        "documentation": {}
    },
    {
        "label": "track_download",
        "importPath": "llama_index.core.download.module",
        "description": "llama_index.core.download.module",
        "isExtraImport": true,
        "detail": "llama_index.core.download.module",
        "documentation": {}
    },
    {
        "label": "BaseLlamaDataset",
        "importPath": "llama_index.core.llama_dataset.base",
        "description": "llama_index.core.llama_dataset.base",
        "isExtraImport": true,
        "detail": "llama_index.core.llama_dataset.base",
        "documentation": {}
    },
    {
        "label": "BaseLlamaDataExample",
        "importPath": "llama_index.core.llama_dataset.base",
        "description": "llama_index.core.llama_dataset.base",
        "isExtraImport": true,
        "detail": "llama_index.core.llama_dataset.base",
        "documentation": {}
    },
    {
        "label": "BaseLlamaDataset",
        "importPath": "llama_index.core.llama_dataset.base",
        "description": "llama_index.core.llama_dataset.base",
        "isExtraImport": true,
        "detail": "llama_index.core.llama_dataset.base",
        "documentation": {}
    },
    {
        "label": "BaseLlamaExamplePrediction",
        "importPath": "llama_index.core.llama_dataset.base",
        "description": "llama_index.core.llama_dataset.base",
        "isExtraImport": true,
        "detail": "llama_index.core.llama_dataset.base",
        "documentation": {}
    },
    {
        "label": "BaseLlamaPredictionDataset",
        "importPath": "llama_index.core.llama_dataset.base",
        "description": "llama_index.core.llama_dataset.base",
        "isExtraImport": true,
        "detail": "llama_index.core.llama_dataset.base",
        "documentation": {}
    },
    {
        "label": "CreatedBy",
        "importPath": "llama_index.core.llama_dataset.base",
        "description": "llama_index.core.llama_dataset.base",
        "isExtraImport": true,
        "detail": "llama_index.core.llama_dataset.base",
        "documentation": {}
    },
    {
        "label": "BaseLlamaDataExample",
        "importPath": "llama_index.core.llama_dataset.base",
        "description": "llama_index.core.llama_dataset.base",
        "isExtraImport": true,
        "detail": "llama_index.core.llama_dataset.base",
        "documentation": {}
    },
    {
        "label": "BaseLlamaDataset",
        "importPath": "llama_index.core.llama_dataset.base",
        "description": "llama_index.core.llama_dataset.base",
        "isExtraImport": true,
        "detail": "llama_index.core.llama_dataset.base",
        "documentation": {}
    },
    {
        "label": "BaseLlamaExamplePrediction",
        "importPath": "llama_index.core.llama_dataset.base",
        "description": "llama_index.core.llama_dataset.base",
        "isExtraImport": true,
        "detail": "llama_index.core.llama_dataset.base",
        "documentation": {}
    },
    {
        "label": "BaseLlamaPredictionDataset",
        "importPath": "llama_index.core.llama_dataset.base",
        "description": "llama_index.core.llama_dataset.base",
        "isExtraImport": true,
        "detail": "llama_index.core.llama_dataset.base",
        "documentation": {}
    },
    {
        "label": "CreatedBy",
        "importPath": "llama_index.core.llama_dataset.base",
        "description": "llama_index.core.llama_dataset.base",
        "isExtraImport": true,
        "detail": "llama_index.core.llama_dataset.base",
        "documentation": {}
    },
    {
        "label": "BaseLlamaDataExample",
        "importPath": "llama_index.core.llama_dataset.base",
        "description": "llama_index.core.llama_dataset.base",
        "isExtraImport": true,
        "detail": "llama_index.core.llama_dataset.base",
        "documentation": {}
    },
    {
        "label": "BaseLlamaDataset",
        "importPath": "llama_index.core.llama_dataset.base",
        "description": "llama_index.core.llama_dataset.base",
        "isExtraImport": true,
        "detail": "llama_index.core.llama_dataset.base",
        "documentation": {}
    },
    {
        "label": "CreatedBy",
        "importPath": "llama_index.core.llama_dataset.base",
        "description": "llama_index.core.llama_dataset.base",
        "isExtraImport": true,
        "detail": "llama_index.core.llama_dataset.base",
        "documentation": {}
    },
    {
        "label": "BaseLlamaExamplePrediction",
        "importPath": "llama_index.core.llama_dataset.base",
        "description": "llama_index.core.llama_dataset.base",
        "isExtraImport": true,
        "detail": "llama_index.core.llama_dataset.base",
        "documentation": {}
    },
    {
        "label": "BaseLlamaPredictionDataset",
        "importPath": "llama_index.core.llama_dataset.base",
        "description": "llama_index.core.llama_dataset.base",
        "isExtraImport": true,
        "detail": "llama_index.core.llama_dataset.base",
        "documentation": {}
    },
    {
        "label": "LabelledEvaluatorDataset",
        "importPath": "llama_index.core.llama_dataset.evaluator_evaluation",
        "description": "llama_index.core.llama_dataset.evaluator_evaluation",
        "isExtraImport": true,
        "detail": "llama_index.core.llama_dataset.evaluator_evaluation",
        "documentation": {}
    },
    {
        "label": "LabelledPairwiseEvaluatorDataset",
        "importPath": "llama_index.core.llama_dataset.evaluator_evaluation",
        "description": "llama_index.core.llama_dataset.evaluator_evaluation",
        "isExtraImport": true,
        "detail": "llama_index.core.llama_dataset.evaluator_evaluation",
        "documentation": {}
    },
    {
        "label": "LabelledRagDataset",
        "importPath": "llama_index.core.llama_dataset.rag",
        "description": "llama_index.core.llama_dataset.rag",
        "isExtraImport": true,
        "detail": "llama_index.core.llama_dataset.rag",
        "documentation": {}
    },
    {
        "label": "SimpleDirectoryReader",
        "importPath": "llama_index.core.readers",
        "description": "llama_index.core.readers",
        "isExtraImport": true,
        "detail": "llama_index.core.readers",
        "documentation": {}
    },
    {
        "label": "ReaderConfig",
        "importPath": "llama_index.core.readers",
        "description": "llama_index.core.readers",
        "isExtraImport": true,
        "detail": "llama_index.core.readers",
        "documentation": {}
    },
    {
        "label": "StringIterableReader",
        "importPath": "llama_index.core.readers",
        "description": "llama_index.core.readers",
        "isExtraImport": true,
        "detail": "llama_index.core.readers",
        "documentation": {}
    },
    {
        "label": "EvaluationSource",
        "importPath": "llama_index.core.evaluation.pairwise",
        "description": "llama_index.core.evaluation.pairwise",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.pairwise",
        "documentation": {}
    },
    {
        "label": "CreatedBy",
        "importPath": "llama_index.core.llama_dataset",
        "description": "llama_index.core.llama_dataset",
        "isExtraImport": true,
        "detail": "llama_index.core.llama_dataset",
        "documentation": {}
    },
    {
        "label": "CreatedByType",
        "importPath": "llama_index.core.llama_dataset",
        "description": "llama_index.core.llama_dataset",
        "isExtraImport": true,
        "detail": "llama_index.core.llama_dataset",
        "documentation": {}
    },
    {
        "label": "LabelledRagDataExample",
        "importPath": "llama_index.core.llama_dataset",
        "description": "llama_index.core.llama_dataset",
        "isExtraImport": true,
        "detail": "llama_index.core.llama_dataset",
        "documentation": {}
    },
    {
        "label": "LabelledRagDataset",
        "importPath": "llama_index.core.llama_dataset",
        "description": "llama_index.core.llama_dataset",
        "isExtraImport": true,
        "detail": "llama_index.core.llama_dataset",
        "documentation": {}
    },
    {
        "label": "download_integration",
        "importPath": "llama_index.core.download.integration",
        "description": "llama_index.core.download.integration",
        "isExtraImport": true,
        "detail": "llama_index.core.download.integration",
        "documentation": {}
    },
    {
        "label": "download_integration",
        "importPath": "llama_index.core.download.integration",
        "description": "llama_index.core.download.integration",
        "isExtraImport": true,
        "detail": "llama_index.core.download.integration",
        "documentation": {}
    },
    {
        "label": "download_integration",
        "importPath": "llama_index.core.download.integration",
        "description": "llama_index.core.download.integration",
        "isExtraImport": true,
        "detail": "llama_index.core.download.integration",
        "documentation": {}
    },
    {
        "label": "LLAMA_PACKS_CONTENTS_URL",
        "importPath": "llama_index.core.download.pack",
        "description": "llama_index.core.download.pack",
        "isExtraImport": true,
        "detail": "llama_index.core.download.pack",
        "documentation": {}
    },
    {
        "label": "download_llama_pack_template",
        "importPath": "llama_index.core.download.pack",
        "description": "llama_index.core.download.pack",
        "isExtraImport": true,
        "detail": "llama_index.core.download.pack",
        "documentation": {}
    },
    {
        "label": "track_download",
        "importPath": "llama_index.core.download.pack",
        "description": "llama_index.core.download.pack",
        "isExtraImport": true,
        "detail": "llama_index.core.download.pack",
        "documentation": {}
    },
    {
        "label": "BaseLlamaPack",
        "importPath": "llama_index.core.llama_pack.base",
        "description": "llama_index.core.llama_pack.base",
        "isExtraImport": true,
        "detail": "llama_index.core.llama_pack.base",
        "documentation": {}
    },
    {
        "label": "ExceptionEvent",
        "importPath": "llama_index.core.instrumentation.events.exception",
        "description": "llama_index.core.instrumentation.events.exception",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.exception",
        "documentation": {}
    },
    {
        "label": "active_span_id",
        "importPath": "llama_index.core.instrumentation.span",
        "description": "llama_index.core.instrumentation.span",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.span",
        "documentation": {}
    },
    {
        "label": "LLMCompletionEndEvent",
        "importPath": "llama_index.core.instrumentation.events.llm",
        "description": "llama_index.core.instrumentation.events.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.llm",
        "documentation": {}
    },
    {
        "label": "LLMCompletionStartEvent",
        "importPath": "llama_index.core.instrumentation.events.llm",
        "description": "llama_index.core.instrumentation.events.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.llm",
        "documentation": {}
    },
    {
        "label": "LLMChatEndEvent",
        "importPath": "llama_index.core.instrumentation.events.llm",
        "description": "llama_index.core.instrumentation.events.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.llm",
        "documentation": {}
    },
    {
        "label": "LLMChatStartEvent",
        "importPath": "llama_index.core.instrumentation.events.llm",
        "description": "llama_index.core.instrumentation.events.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.llm",
        "documentation": {}
    },
    {
        "label": "LLMChatInProgressEvent",
        "importPath": "llama_index.core.instrumentation.events.llm",
        "description": "llama_index.core.instrumentation.events.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.llm",
        "documentation": {}
    },
    {
        "label": "LLMCompletionInProgressEvent",
        "importPath": "llama_index.core.instrumentation.events.llm",
        "description": "llama_index.core.instrumentation.events.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.llm",
        "documentation": {}
    },
    {
        "label": "LLMPredictEndEvent",
        "importPath": "llama_index.core.instrumentation.events.llm",
        "description": "llama_index.core.instrumentation.events.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.llm",
        "documentation": {}
    },
    {
        "label": "LLMPredictStartEvent",
        "importPath": "llama_index.core.instrumentation.events.llm",
        "description": "llama_index.core.instrumentation.events.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.llm",
        "documentation": {}
    },
    {
        "label": "LLMStructuredPredictInProgressEvent",
        "importPath": "llama_index.core.instrumentation.events.llm",
        "description": "llama_index.core.instrumentation.events.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.llm",
        "documentation": {}
    },
    {
        "label": "LLMStructuredPredictEndEvent",
        "importPath": "llama_index.core.instrumentation.events.llm",
        "description": "llama_index.core.instrumentation.events.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.llm",
        "documentation": {}
    },
    {
        "label": "LLMStructuredPredictStartEvent",
        "importPath": "llama_index.core.instrumentation.events.llm",
        "description": "llama_index.core.instrumentation.events.llm",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.llm",
        "documentation": {}
    },
    {
        "label": "llm_chat_callback",
        "importPath": "llama_index.core.llms.callbacks",
        "description": "llama_index.core.llms.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.callbacks",
        "documentation": {}
    },
    {
        "label": "llm_completion_callback",
        "importPath": "llama_index.core.llms.callbacks",
        "description": "llama_index.core.llms.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.callbacks",
        "documentation": {}
    },
    {
        "label": "llm_chat_callback",
        "importPath": "llama_index.core.llms.callbacks",
        "description": "llama_index.core.llms.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.callbacks",
        "documentation": {}
    },
    {
        "label": "llm_completion_callback",
        "importPath": "llama_index.core.llms.callbacks",
        "description": "llama_index.core.llms.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.callbacks",
        "documentation": {}
    },
    {
        "label": "llm_chat_callback",
        "importPath": "llama_index.core.llms.callbacks",
        "description": "llama_index.core.llms.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.callbacks",
        "documentation": {}
    },
    {
        "label": "llm_completion_callback",
        "importPath": "llama_index.core.llms.callbacks",
        "description": "llama_index.core.llms.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.callbacks",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "importPath": "llama_index.core.llms.callbacks",
        "description": "llama_index.core.llms.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.callbacks",
        "documentation": {}
    },
    {
        "label": "llm_chat_callback",
        "importPath": "llama_index.core.llms.callbacks",
        "description": "llama_index.core.llms.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.callbacks",
        "documentation": {}
    },
    {
        "label": "llm_completion_callback",
        "importPath": "llama_index.core.llms.callbacks",
        "description": "llama_index.core.llms.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.callbacks",
        "documentation": {}
    },
    {
        "label": "llm_completion_callback",
        "importPath": "llama_index.core.llms.callbacks",
        "description": "llama_index.core.llms.callbacks",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.callbacks",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "importPath": "llama_index.core.llms.mock",
        "description": "llama_index.core.llms.mock",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.mock",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "importPath": "llama_index.core.llms.mock",
        "description": "llama_index.core.llms.mock",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.mock",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "importPath": "llama_index.core.llms.mock",
        "description": "llama_index.core.llms.mock",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.mock",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "importPath": "llama_index.core.llms.mock",
        "description": "llama_index.core.llms.mock",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.mock",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "importPath": "llama_index.core.llms.mock",
        "description": "llama_index.core.llms.mock",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.mock",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "importPath": "llama_index.core.llms.mock",
        "description": "llama_index.core.llms.mock",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.mock",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "importPath": "llama_index.core.llms.mock",
        "description": "llama_index.core.llms.mock",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.mock",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "importPath": "llama_index.core.llms.mock",
        "description": "llama_index.core.llms.mock",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.mock",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "importPath": "llama_index.core.llms.mock",
        "description": "llama_index.core.llms.mock",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.mock",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "importPath": "llama_index.core.llms.mock",
        "description": "llama_index.core.llms.mock",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.mock",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "importPath": "llama_index.core.llms.mock",
        "description": "llama_index.core.llms.mock",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.mock",
        "documentation": {}
    },
    {
        "label": "MockLLMWithNonyieldingChatStream",
        "importPath": "llama_index.core.llms.mock",
        "description": "llama_index.core.llms.mock",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.mock",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "importPath": "llama_index.core.llms.mock",
        "description": "llama_index.core.llms.mock",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.mock",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "importPath": "llama_index.core.llms.mock",
        "description": "llama_index.core.llms.mock",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.mock",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "importPath": "llama_index.core.llms.mock",
        "description": "llama_index.core.llms.mock",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.mock",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "importPath": "llama_index.core.llms.mock",
        "description": "llama_index.core.llms.mock",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.mock",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "importPath": "llama_index.core.llms.mock",
        "description": "llama_index.core.llms.mock",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.mock",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "importPath": "llama_index.core.llms.mock",
        "description": "llama_index.core.llms.mock",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.mock",
        "documentation": {}
    },
    {
        "label": "CustomLLM",
        "importPath": "llama_index.core.llms.custom",
        "description": "llama_index.core.llms.custom",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.custom",
        "documentation": {}
    },
    {
        "label": "CustomLLM",
        "importPath": "llama_index.core.llms.custom",
        "description": "llama_index.core.llms.custom",
        "isExtraImport": true,
        "detail": "llama_index.core.llms.custom",
        "documentation": {}
    },
    {
        "label": "BaseMemoryBlock",
        "importPath": "llama_index.core.memory.memory",
        "description": "llama_index.core.memory.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory.memory",
        "documentation": {}
    },
    {
        "label": "BaseMemoryBlock",
        "importPath": "llama_index.core.memory.memory",
        "description": "llama_index.core.memory.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory.memory",
        "documentation": {}
    },
    {
        "label": "BaseMemoryBlock",
        "importPath": "llama_index.core.memory.memory",
        "description": "llama_index.core.memory.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory.memory",
        "documentation": {}
    },
    {
        "label": "Memory",
        "importPath": "llama_index.core.memory.memory",
        "description": "llama_index.core.memory.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory.memory",
        "documentation": {}
    },
    {
        "label": "Memory",
        "importPath": "llama_index.core.memory.memory",
        "description": "llama_index.core.memory.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory.memory",
        "documentation": {}
    },
    {
        "label": "BaseMemoryBlock",
        "importPath": "llama_index.core.memory.memory",
        "description": "llama_index.core.memory.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory.memory",
        "documentation": {}
    },
    {
        "label": "InsertMethod",
        "importPath": "llama_index.core.memory.memory",
        "description": "llama_index.core.memory.memory",
        "isExtraImport": true,
        "detail": "llama_index.core.memory.memory",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CHAT_STORE_KEY",
        "importPath": "llama_index.core.memory.types",
        "description": "llama_index.core.memory.types",
        "isExtraImport": true,
        "detail": "llama_index.core.memory.types",
        "documentation": {}
    },
    {
        "label": "BaseChatStoreMemory",
        "importPath": "llama_index.core.memory.types",
        "description": "llama_index.core.memory.types",
        "isExtraImport": true,
        "detail": "llama_index.core.memory.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CHAT_STORE_KEY",
        "importPath": "llama_index.core.memory.types",
        "description": "llama_index.core.memory.types",
        "isExtraImport": true,
        "detail": "llama_index.core.memory.types",
        "documentation": {}
    },
    {
        "label": "BaseMemory",
        "importPath": "llama_index.core.memory.types",
        "description": "llama_index.core.memory.types",
        "isExtraImport": true,
        "detail": "llama_index.core.memory.types",
        "documentation": {}
    },
    {
        "label": "BaseMemory",
        "importPath": "llama_index.core.memory.types",
        "description": "llama_index.core.memory.types",
        "isExtraImport": true,
        "detail": "llama_index.core.memory.types",
        "documentation": {}
    },
    {
        "label": "BaseMemory",
        "importPath": "llama_index.core.memory.types",
        "description": "llama_index.core.memory.types",
        "isExtraImport": true,
        "detail": "llama_index.core.memory.types",
        "documentation": {}
    },
    {
        "label": "BaseMemory",
        "importPath": "llama_index.core.memory.types",
        "description": "llama_index.core.memory.types",
        "isExtraImport": true,
        "detail": "llama_index.core.memory.types",
        "documentation": {}
    },
    {
        "label": "BaseChatStore",
        "importPath": "llama_index.core.storage.chat_store",
        "description": "llama_index.core.storage.chat_store",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.chat_store",
        "documentation": {}
    },
    {
        "label": "SimpleChatStore",
        "importPath": "llama_index.core.storage.chat_store",
        "description": "llama_index.core.storage.chat_store",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.chat_store",
        "documentation": {}
    },
    {
        "label": "BaseChatStore",
        "importPath": "llama_index.core.storage.chat_store",
        "description": "llama_index.core.storage.chat_store",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.chat_store",
        "documentation": {}
    },
    {
        "label": "SimpleChatStore",
        "importPath": "llama_index.core.storage.chat_store",
        "description": "llama_index.core.storage.chat_store",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.chat_store",
        "documentation": {}
    },
    {
        "label": "BaseChatStore",
        "importPath": "llama_index.core.storage.chat_store",
        "description": "llama_index.core.storage.chat_store",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.chat_store",
        "documentation": {}
    },
    {
        "label": "SimpleChatStore",
        "importPath": "llama_index.core.storage.chat_store",
        "description": "llama_index.core.storage.chat_store",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.chat_store",
        "documentation": {}
    },
    {
        "label": "AsyncEngine",
        "importPath": "sqlalchemy.ext.asyncio",
        "description": "sqlalchemy.ext.asyncio",
        "isExtraImport": true,
        "detail": "sqlalchemy.ext.asyncio",
        "documentation": {}
    },
    {
        "label": "AsyncEngine",
        "importPath": "sqlalchemy.ext.asyncio",
        "description": "sqlalchemy.ext.asyncio",
        "isExtraImport": true,
        "detail": "sqlalchemy.ext.asyncio",
        "documentation": {}
    },
    {
        "label": "AsyncSession",
        "importPath": "sqlalchemy.ext.asyncio",
        "description": "sqlalchemy.ext.asyncio",
        "isExtraImport": true,
        "detail": "sqlalchemy.ext.asyncio",
        "documentation": {}
    },
    {
        "label": "create_async_engine",
        "importPath": "sqlalchemy.ext.asyncio",
        "description": "sqlalchemy.ext.asyncio",
        "isExtraImport": true,
        "detail": "sqlalchemy.ext.asyncio",
        "documentation": {}
    },
    {
        "label": "SQLAlchemyChatStore",
        "importPath": "llama_index.core.storage.chat_store.sql",
        "description": "llama_index.core.storage.chat_store.sql",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.chat_store.sql",
        "documentation": {}
    },
    {
        "label": "MessageStatus",
        "importPath": "llama_index.core.storage.chat_store.sql",
        "description": "llama_index.core.storage.chat_store.sql",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.chat_store.sql",
        "documentation": {}
    },
    {
        "label": "MessageStatus",
        "importPath": "llama_index.core.storage.chat_store.sql",
        "description": "llama_index.core.storage.chat_store.sql",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.chat_store.sql",
        "documentation": {}
    },
    {
        "label": "build_nodes_from_splits",
        "importPath": "llama_index.core.node_parser.node_utils",
        "description": "llama_index.core.node_parser.node_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.node_utils",
        "documentation": {}
    },
    {
        "label": "build_nodes_from_splits",
        "importPath": "llama_index.core.node_parser.node_utils",
        "description": "llama_index.core.node_parser.node_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.node_utils",
        "documentation": {}
    },
    {
        "label": "build_nodes_from_splits",
        "importPath": "llama_index.core.node_parser.node_utils",
        "description": "llama_index.core.node_parser.node_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.node_utils",
        "documentation": {}
    },
    {
        "label": "build_nodes_from_splits",
        "importPath": "llama_index.core.node_parser.node_utils",
        "description": "llama_index.core.node_parser.node_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.node_utils",
        "documentation": {}
    },
    {
        "label": "default_id_func",
        "importPath": "llama_index.core.node_parser.node_utils",
        "description": "llama_index.core.node_parser.node_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.node_utils",
        "documentation": {}
    },
    {
        "label": "default_id_func",
        "importPath": "llama_index.core.node_parser.node_utils",
        "description": "llama_index.core.node_parser.node_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.node_utils",
        "documentation": {}
    },
    {
        "label": "build_nodes_from_splits",
        "importPath": "llama_index.core.node_parser.node_utils",
        "description": "llama_index.core.node_parser.node_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.node_utils",
        "documentation": {}
    },
    {
        "label": "default_id_func",
        "importPath": "llama_index.core.node_parser.node_utils",
        "description": "llama_index.core.node_parser.node_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.node_utils",
        "documentation": {}
    },
    {
        "label": "build_nodes_from_splits",
        "importPath": "llama_index.core.node_parser.node_utils",
        "description": "llama_index.core.node_parser.node_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.node_utils",
        "documentation": {}
    },
    {
        "label": "default_id_func",
        "importPath": "llama_index.core.node_parser.node_utils",
        "description": "llama_index.core.node_parser.node_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.node_utils",
        "documentation": {}
    },
    {
        "label": "default_id_func",
        "importPath": "llama_index.core.node_parser.node_utils",
        "description": "llama_index.core.node_parser.node_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.node_utils",
        "documentation": {}
    },
    {
        "label": "build_nodes_from_splits",
        "importPath": "llama_index.core.node_parser.node_utils",
        "description": "llama_index.core.node_parser.node_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.node_utils",
        "documentation": {}
    },
    {
        "label": "default_id_func",
        "importPath": "llama_index.core.node_parser.node_utils",
        "description": "llama_index.core.node_parser.node_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.node_utils",
        "documentation": {}
    },
    {
        "label": "default_id_func",
        "importPath": "llama_index.core.node_parser.node_utils",
        "description": "llama_index.core.node_parser.node_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.node_utils",
        "documentation": {}
    },
    {
        "label": "build_nodes_from_splits",
        "importPath": "llama_index.core.node_parser.node_utils",
        "description": "llama_index.core.node_parser.node_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.node_utils",
        "documentation": {}
    },
    {
        "label": "default_id_func",
        "importPath": "llama_index.core.node_parser.node_utils",
        "description": "llama_index.core.node_parser.node_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.node_utils",
        "documentation": {}
    },
    {
        "label": "HTMLNodeParser",
        "importPath": "llama_index.core.node_parser.file.html",
        "description": "llama_index.core.node_parser.file.html",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.file.html",
        "documentation": {}
    },
    {
        "label": "HTMLNodeParser",
        "importPath": "llama_index.core.node_parser.file.html",
        "description": "llama_index.core.node_parser.file.html",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.file.html",
        "documentation": {}
    },
    {
        "label": "HTMLNodeParser",
        "importPath": "llama_index.core.node_parser.file.html",
        "description": "llama_index.core.node_parser.file.html",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.file.html",
        "documentation": {}
    },
    {
        "label": "JSONNodeParser",
        "importPath": "llama_index.core.node_parser.file.json",
        "description": "llama_index.core.node_parser.file.json",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.file.json",
        "documentation": {}
    },
    {
        "label": "JSONNodeParser",
        "importPath": "llama_index.core.node_parser.file.json",
        "description": "llama_index.core.node_parser.file.json",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.file.json",
        "documentation": {}
    },
    {
        "label": "JSONNodeParser",
        "importPath": "llama_index.core.node_parser.file.json",
        "description": "llama_index.core.node_parser.file.json",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.file.json",
        "documentation": {}
    },
    {
        "label": "MarkdownNodeParser",
        "importPath": "llama_index.core.node_parser.file.markdown",
        "description": "llama_index.core.node_parser.file.markdown",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.file.markdown",
        "documentation": {}
    },
    {
        "label": "MarkdownNodeParser",
        "importPath": "llama_index.core.node_parser.file.markdown",
        "description": "llama_index.core.node_parser.file.markdown",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.file.markdown",
        "documentation": {}
    },
    {
        "label": "MarkdownNodeParser",
        "importPath": "llama_index.core.node_parser.file.markdown",
        "description": "llama_index.core.node_parser.file.markdown",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.file.markdown",
        "documentation": {}
    },
    {
        "label": "SentenceSplitter",
        "importPath": "llama_index.core.node_parser.text.sentence",
        "description": "llama_index.core.node_parser.text.sentence",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.text.sentence",
        "documentation": {}
    },
    {
        "label": "SentenceSplitter",
        "importPath": "llama_index.core.node_parser.text.sentence",
        "description": "llama_index.core.node_parser.text.sentence",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.text.sentence",
        "documentation": {}
    },
    {
        "label": "BaseElementNodeParser",
        "importPath": "llama_index.core.node_parser.relational.base_element",
        "description": "llama_index.core.node_parser.relational.base_element",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.relational.base_element",
        "documentation": {}
    },
    {
        "label": "Element",
        "importPath": "llama_index.core.node_parser.relational.base_element",
        "description": "llama_index.core.node_parser.relational.base_element",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.relational.base_element",
        "documentation": {}
    },
    {
        "label": "BaseElementNodeParser",
        "importPath": "llama_index.core.node_parser.relational.base_element",
        "description": "llama_index.core.node_parser.relational.base_element",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.relational.base_element",
        "documentation": {}
    },
    {
        "label": "Element",
        "importPath": "llama_index.core.node_parser.relational.base_element",
        "description": "llama_index.core.node_parser.relational.base_element",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.relational.base_element",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SUMMARY_QUERY_STR",
        "importPath": "llama_index.core.node_parser.relational.base_element",
        "description": "llama_index.core.node_parser.relational.base_element",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.relational.base_element",
        "documentation": {}
    },
    {
        "label": "BaseElementNodeParser",
        "importPath": "llama_index.core.node_parser.relational.base_element",
        "description": "llama_index.core.node_parser.relational.base_element",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.relational.base_element",
        "documentation": {}
    },
    {
        "label": "Element",
        "importPath": "llama_index.core.node_parser.relational.base_element",
        "description": "llama_index.core.node_parser.relational.base_element",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.relational.base_element",
        "documentation": {}
    },
    {
        "label": "md_to_df",
        "importPath": "llama_index.core.node_parser.relational.utils",
        "description": "llama_index.core.node_parser.relational.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.relational.utils",
        "documentation": {}
    },
    {
        "label": "md_to_df",
        "importPath": "llama_index.core.node_parser.relational.utils",
        "description": "llama_index.core.node_parser.relational.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.relational.utils",
        "documentation": {}
    },
    {
        "label": "html_to_df",
        "importPath": "llama_index.core.node_parser.relational.utils",
        "description": "llama_index.core.node_parser.relational.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.relational.utils",
        "documentation": {}
    },
    {
        "label": "SimpleFileNodeParser",
        "importPath": "llama_index.core.node_parser.file.simple_file",
        "description": "llama_index.core.node_parser.file.simple_file",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.file.simple_file",
        "documentation": {}
    },
    {
        "label": "HierarchicalNodeParser",
        "importPath": "llama_index.core.node_parser.relational.hierarchical",
        "description": "llama_index.core.node_parser.relational.hierarchical",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.relational.hierarchical",
        "documentation": {}
    },
    {
        "label": "CodeSplitter",
        "importPath": "llama_index.core.node_parser.text.code",
        "description": "llama_index.core.node_parser.text.code",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.text.code",
        "documentation": {}
    },
    {
        "label": "SentenceWindowNodeParser",
        "importPath": "llama_index.core.node_parser.text.sentence_window",
        "description": "llama_index.core.node_parser.text.sentence_window",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.text.sentence_window",
        "documentation": {}
    },
    {
        "label": "SentenceWindowNodeParser",
        "importPath": "llama_index.core.node_parser.text.sentence_window",
        "description": "llama_index.core.node_parser.text.sentence_window",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.text.sentence_window",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_FNAME",
        "importPath": "llama_index.core.objects.base_node_mapping",
        "description": "llama_index.core.objects.base_node_mapping",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.base_node_mapping",
        "documentation": {}
    },
    {
        "label": "BaseObjectNodeMapping",
        "importPath": "llama_index.core.objects.base_node_mapping",
        "description": "llama_index.core.objects.base_node_mapping",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.base_node_mapping",
        "documentation": {}
    },
    {
        "label": "SimpleObjectNodeMapping",
        "importPath": "llama_index.core.objects.base_node_mapping",
        "description": "llama_index.core.objects.base_node_mapping",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.base_node_mapping",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_DIR",
        "importPath": "llama_index.core.objects.base_node_mapping",
        "description": "llama_index.core.objects.base_node_mapping",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.base_node_mapping",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_FNAME",
        "importPath": "llama_index.core.objects.base_node_mapping",
        "description": "llama_index.core.objects.base_node_mapping",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.base_node_mapping",
        "documentation": {}
    },
    {
        "label": "BaseObjectNodeMapping",
        "importPath": "llama_index.core.objects.base_node_mapping",
        "description": "llama_index.core.objects.base_node_mapping",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.base_node_mapping",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_DIR",
        "importPath": "llama_index.core.objects.base_node_mapping",
        "description": "llama_index.core.objects.base_node_mapping",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.base_node_mapping",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_FNAME",
        "importPath": "llama_index.core.objects.base_node_mapping",
        "description": "llama_index.core.objects.base_node_mapping",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.base_node_mapping",
        "documentation": {}
    },
    {
        "label": "BaseObjectNodeMapping",
        "importPath": "llama_index.core.objects.base_node_mapping",
        "description": "llama_index.core.objects.base_node_mapping",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.base_node_mapping",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_DIR",
        "importPath": "llama_index.core.objects.base_node_mapping",
        "description": "llama_index.core.objects.base_node_mapping",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.base_node_mapping",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_FNAME",
        "importPath": "llama_index.core.objects.base_node_mapping",
        "description": "llama_index.core.objects.base_node_mapping",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.base_node_mapping",
        "documentation": {}
    },
    {
        "label": "BaseObjectNodeMapping",
        "importPath": "llama_index.core.objects.base_node_mapping",
        "description": "llama_index.core.objects.base_node_mapping",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.base_node_mapping",
        "documentation": {}
    },
    {
        "label": "BaseObjectNodeMapping",
        "importPath": "llama_index.core.objects.base_node_mapping",
        "description": "llama_index.core.objects.base_node_mapping",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.base_node_mapping",
        "documentation": {}
    },
    {
        "label": "SimpleObjectNodeMapping",
        "importPath": "llama_index.core.objects.base_node_mapping",
        "description": "llama_index.core.objects.base_node_mapping",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.base_node_mapping",
        "documentation": {}
    },
    {
        "label": "SimpleObjectNodeMapping",
        "importPath": "llama_index.core.objects.base_node_mapping",
        "description": "llama_index.core.objects.base_node_mapping",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.base_node_mapping",
        "documentation": {}
    },
    {
        "label": "FnNodeMapping",
        "importPath": "llama_index.core.objects.fn_node_mapping",
        "description": "llama_index.core.objects.fn_node_mapping",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.fn_node_mapping",
        "documentation": {}
    },
    {
        "label": "SimpleToolNodeMapping",
        "importPath": "llama_index.core.objects.tool_node_mapping",
        "description": "llama_index.core.objects.tool_node_mapping",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.tool_node_mapping",
        "documentation": {}
    },
    {
        "label": "SimpleToolNodeMapping",
        "importPath": "llama_index.core.objects.tool_node_mapping",
        "description": "llama_index.core.objects.tool_node_mapping",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.tool_node_mapping",
        "documentation": {}
    },
    {
        "label": "SimpleToolNodeMapping",
        "importPath": "llama_index.core.objects.tool_node_mapping",
        "description": "llama_index.core.objects.tool_node_mapping",
        "isExtraImport": true,
        "detail": "llama_index.core.objects.tool_node_mapping",
        "documentation": {}
    },
    {
        "label": "ReRankEndEvent",
        "importPath": "llama_index.core.instrumentation.events.rerank",
        "description": "llama_index.core.instrumentation.events.rerank",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.rerank",
        "documentation": {}
    },
    {
        "label": "ReRankStartEvent",
        "importPath": "llama_index.core.instrumentation.events.rerank",
        "description": "llama_index.core.instrumentation.events.rerank",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.rerank",
        "documentation": {}
    },
    {
        "label": "FunctionTool",
        "importPath": "llama_index.core.tools.function_tool",
        "description": "llama_index.core.tools.function_tool",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.function_tool",
        "documentation": {}
    },
    {
        "label": "FunctionTool",
        "importPath": "llama_index.core.tools.function_tool",
        "description": "llama_index.core.tools.function_tool",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.function_tool",
        "documentation": {}
    },
    {
        "label": "FunctionTool",
        "importPath": "llama_index.core.tools.function_tool",
        "description": "llama_index.core.tools.function_tool",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.function_tool",
        "documentation": {}
    },
    {
        "label": "FunctionTool",
        "importPath": "llama_index.core.tools.function_tool",
        "description": "llama_index.core.tools.function_tool",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.function_tool",
        "documentation": {}
    },
    {
        "label": "FunctionTool",
        "importPath": "llama_index.core.tools.function_tool",
        "description": "llama_index.core.tools.function_tool",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.function_tool",
        "documentation": {}
    },
    {
        "label": "FunctionTool",
        "importPath": "llama_index.core.tools.function_tool",
        "description": "llama_index.core.tools.function_tool",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.function_tool",
        "documentation": {}
    },
    {
        "label": "FlexibleModel",
        "importPath": "llama_index.core.program.utils",
        "description": "llama_index.core.program.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.program.utils",
        "documentation": {}
    },
    {
        "label": "process_streaming_objects",
        "importPath": "llama_index.core.program.utils",
        "description": "llama_index.core.program.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.program.utils",
        "documentation": {}
    },
    {
        "label": "num_valid_fields",
        "importPath": "llama_index.core.program.utils",
        "description": "llama_index.core.program.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.program.utils",
        "documentation": {}
    },
    {
        "label": "FlexibleModel",
        "importPath": "llama_index.core.program.utils",
        "description": "llama_index.core.program.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.program.utils",
        "documentation": {}
    },
    {
        "label": "_repair_incomplete_json",
        "importPath": "llama_index.core.program.utils",
        "description": "llama_index.core.program.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.program.utils",
        "documentation": {}
    },
    {
        "label": "create_flexible_model",
        "importPath": "llama_index.core.program.utils",
        "description": "llama_index.core.program.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.program.utils",
        "documentation": {}
    },
    {
        "label": "_repair_incomplete_json",
        "importPath": "llama_index.core.program.utils",
        "description": "llama_index.core.program.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.program.utils",
        "documentation": {}
    },
    {
        "label": "process_streaming_objects",
        "importPath": "llama_index.core.program.utils",
        "description": "llama_index.core.program.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.program.utils",
        "documentation": {}
    },
    {
        "label": "num_valid_fields",
        "importPath": "llama_index.core.program.utils",
        "description": "llama_index.core.program.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.program.utils",
        "documentation": {}
    },
    {
        "label": "create_flexible_model",
        "importPath": "llama_index.core.program.utils",
        "description": "llama_index.core.program.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.program.utils",
        "documentation": {}
    },
    {
        "label": "PydanticOutputParser",
        "importPath": "llama_index.core.output_parsers.pydantic",
        "description": "llama_index.core.output_parsers.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.pydantic",
        "documentation": {}
    },
    {
        "label": "PydanticOutputParser",
        "importPath": "llama_index.core.output_parsers.pydantic",
        "description": "llama_index.core.output_parsers.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.pydantic",
        "documentation": {}
    },
    {
        "label": "PydanticOutputParser",
        "importPath": "llama_index.core.output_parsers.pydantic",
        "description": "llama_index.core.output_parsers.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.pydantic",
        "documentation": {}
    },
    {
        "label": "PydanticOutputParser",
        "importPath": "llama_index.core.output_parsers.pydantic",
        "description": "llama_index.core.output_parsers.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.pydantic",
        "documentation": {}
    },
    {
        "label": "PydanticOutputParser",
        "importPath": "llama_index.core.output_parsers.pydantic",
        "description": "llama_index.core.output_parsers.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.pydantic",
        "documentation": {}
    },
    {
        "label": "PydanticOutputParser",
        "importPath": "llama_index.core.output_parsers.pydantic",
        "description": "llama_index.core.output_parsers.pydantic",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.pydantic",
        "documentation": {}
    },
    {
        "label": "CHAT_REFINE_PROMPT",
        "importPath": "llama_index.core.prompts.chat_prompts",
        "description": "llama_index.core.prompts.chat_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.chat_prompts",
        "documentation": {}
    },
    {
        "label": "CHAT_REFINE_TABLE_CONTEXT_PROMPT",
        "importPath": "llama_index.core.prompts.chat_prompts",
        "description": "llama_index.core.prompts.chat_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.chat_prompts",
        "documentation": {}
    },
    {
        "label": "CHAT_TEXT_QA_PROMPT",
        "importPath": "llama_index.core.prompts.chat_prompts",
        "description": "llama_index.core.prompts.chat_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.chat_prompts",
        "documentation": {}
    },
    {
        "label": "CHAT_TREE_SUMMARIZE_PROMPT",
        "importPath": "llama_index.core.prompts.chat_prompts",
        "description": "llama_index.core.prompts.chat_prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.prompts.chat_prompts",
        "documentation": {}
    },
    {
        "label": "Prompt",
        "importPath": "banks",
        "description": "banks",
        "isExtraImport": true,
        "detail": "banks",
        "documentation": {}
    },
    {
        "label": "ContentBlockType",
        "importPath": "banks.types",
        "description": "banks.types",
        "isExtraImport": true,
        "detail": "banks.types",
        "documentation": {}
    },
    {
        "label": "QueryTask",
        "importPath": "llama_index.core.query_engine.flare.schema",
        "description": "llama_index.core.query_engine.flare.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.query_engine.flare.schema",
        "documentation": {}
    },
    {
        "label": "QueryTask",
        "importPath": "llama_index.core.query_engine.flare.schema",
        "description": "llama_index.core.query_engine.flare.schema",
        "isExtraImport": true,
        "detail": "llama_index.core.query_engine.flare.schema",
        "documentation": {}
    },
    {
        "label": "BaseLookaheadAnswerInserter",
        "importPath": "llama_index.core.query_engine.flare.answer_inserter",
        "description": "llama_index.core.query_engine.flare.answer_inserter",
        "isExtraImport": true,
        "detail": "llama_index.core.query_engine.flare.answer_inserter",
        "documentation": {}
    },
    {
        "label": "LLMLookaheadAnswerInserter",
        "importPath": "llama_index.core.query_engine.flare.answer_inserter",
        "description": "llama_index.core.query_engine.flare.answer_inserter",
        "isExtraImport": true,
        "detail": "llama_index.core.query_engine.flare.answer_inserter",
        "documentation": {}
    },
    {
        "label": "IsDoneOutputParser",
        "importPath": "llama_index.core.query_engine.flare.output_parser",
        "description": "llama_index.core.query_engine.flare.output_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.query_engine.flare.output_parser",
        "documentation": {}
    },
    {
        "label": "QueryTaskOutputParser",
        "importPath": "llama_index.core.query_engine.flare.output_parser",
        "description": "llama_index.core.query_engine.flare.output_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.query_engine.flare.output_parser",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "importPath": "llama_index.core.indices.query.base",
        "description": "llama_index.core.indices.query.base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.base",
        "documentation": {}
    },
    {
        "label": "GuidelineEvaluator",
        "importPath": "llama_index.core.evaluation.guideline",
        "description": "llama_index.core.evaluation.guideline",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.guideline",
        "documentation": {}
    },
    {
        "label": "FeedbackQueryTransformation",
        "importPath": "llama_index.core.indices.query.query_transform.feedback_transform",
        "description": "llama_index.core.indices.query.query_transform.feedback_transform",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.query.query_transform.feedback_transform",
        "documentation": {}
    },
    {
        "label": "BaseSelector",
        "importPath": "llama_index.core.base.base_selector",
        "description": "llama_index.core.base.base_selector",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_selector",
        "documentation": {}
    },
    {
        "label": "BaseSelector",
        "importPath": "llama_index.core.base.base_selector",
        "description": "llama_index.core.base.base_selector",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_selector",
        "documentation": {}
    },
    {
        "label": "BaseSelector",
        "importPath": "llama_index.core.base.base_selector",
        "description": "llama_index.core.base.base_selector",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_selector",
        "documentation": {}
    },
    {
        "label": "SelectorResult",
        "importPath": "llama_index.core.base.base_selector",
        "description": "llama_index.core.base.base_selector",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_selector",
        "documentation": {}
    },
    {
        "label": "SingleSelection",
        "importPath": "llama_index.core.base.base_selector",
        "description": "llama_index.core.base.base_selector",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_selector",
        "documentation": {}
    },
    {
        "label": "BaseSelector",
        "importPath": "llama_index.core.base.base_selector",
        "description": "llama_index.core.base.base_selector",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_selector",
        "documentation": {}
    },
    {
        "label": "SelectorResult",
        "importPath": "llama_index.core.base.base_selector",
        "description": "llama_index.core.base.base_selector",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_selector",
        "documentation": {}
    },
    {
        "label": "SingleSelection",
        "importPath": "llama_index.core.base.base_selector",
        "description": "llama_index.core.base.base_selector",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_selector",
        "documentation": {}
    },
    {
        "label": "BaseSelector",
        "importPath": "llama_index.core.base.base_selector",
        "description": "llama_index.core.base.base_selector",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_selector",
        "documentation": {}
    },
    {
        "label": "MultiSelection",
        "importPath": "llama_index.core.base.base_selector",
        "description": "llama_index.core.base.base_selector",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_selector",
        "documentation": {}
    },
    {
        "label": "SelectorResult",
        "importPath": "llama_index.core.base.base_selector",
        "description": "llama_index.core.base.base_selector",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_selector",
        "documentation": {}
    },
    {
        "label": "SingleSelection",
        "importPath": "llama_index.core.base.base_selector",
        "description": "llama_index.core.base.base_selector",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_selector",
        "documentation": {}
    },
    {
        "label": "BaseSelector",
        "importPath": "llama_index.core.base.base_selector",
        "description": "llama_index.core.base.base_selector",
        "isExtraImport": true,
        "detail": "llama_index.core.base.base_selector",
        "documentation": {}
    },
    {
        "label": "get_selector_from_llm",
        "importPath": "llama_index.core.selectors.utils",
        "description": "llama_index.core.selectors.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.selectors.utils",
        "documentation": {}
    },
    {
        "label": "get_selector_from_llm",
        "importPath": "llama_index.core.selectors.utils",
        "description": "llama_index.core.selectors.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.selectors.utils",
        "documentation": {}
    },
    {
        "label": "get_selector_from_llm",
        "importPath": "llama_index.core.selectors.utils",
        "description": "llama_index.core.selectors.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.selectors.utils",
        "documentation": {}
    },
    {
        "label": "BaseSQLTableQueryEngine",
        "importPath": "llama_index.core.indices.struct_store.sql_query",
        "description": "llama_index.core.indices.struct_store.sql_query",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.struct_store.sql_query",
        "documentation": {}
    },
    {
        "label": "NLSQLTableQueryEngine",
        "importPath": "llama_index.core.indices.struct_store.sql_query",
        "description": "llama_index.core.indices.struct_store.sql_query",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.struct_store.sql_query",
        "documentation": {}
    },
    {
        "label": "BaseSQLTableQueryEngine",
        "importPath": "llama_index.core.indices.struct_store.sql_query",
        "description": "llama_index.core.indices.struct_store.sql_query",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.struct_store.sql_query",
        "documentation": {}
    },
    {
        "label": "NLSQLTableQueryEngine",
        "importPath": "llama_index.core.indices.struct_store.sql_query",
        "description": "llama_index.core.indices.struct_store.sql_query",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.struct_store.sql_query",
        "documentation": {}
    },
    {
        "label": "NLStructStoreQueryEngine",
        "importPath": "llama_index.core.indices.struct_store.sql_query",
        "description": "llama_index.core.indices.struct_store.sql_query",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.struct_store.sql_query",
        "documentation": {}
    },
    {
        "label": "NLSQLTableQueryEngine",
        "importPath": "llama_index.core.indices.struct_store.sql_query",
        "description": "llama_index.core.indices.struct_store.sql_query",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.struct_store.sql_query",
        "documentation": {}
    },
    {
        "label": "NLStructStoreQueryEngine",
        "importPath": "llama_index.core.indices.struct_store.sql_query",
        "description": "llama_index.core.indices.struct_store.sql_query",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.struct_store.sql_query",
        "documentation": {}
    },
    {
        "label": "SQLStructStoreQueryEngine",
        "importPath": "llama_index.core.indices.struct_store.sql_query",
        "description": "llama_index.core.indices.struct_store.sql_query",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.struct_store.sql_query",
        "documentation": {}
    },
    {
        "label": "SQLTableRetrieverQueryEngine",
        "importPath": "llama_index.core.indices.struct_store.sql_query",
        "description": "llama_index.core.indices.struct_store.sql_query",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.struct_store.sql_query",
        "documentation": {}
    },
    {
        "label": "LLMSingleSelector",
        "importPath": "llama_index.core.selectors.llm_selectors",
        "description": "llama_index.core.selectors.llm_selectors",
        "isExtraImport": true,
        "detail": "llama_index.core.selectors.llm_selectors",
        "documentation": {}
    },
    {
        "label": "LLMSingleSelector",
        "importPath": "llama_index.core.selectors.llm_selectors",
        "description": "llama_index.core.selectors.llm_selectors",
        "isExtraImport": true,
        "detail": "llama_index.core.selectors.llm_selectors",
        "documentation": {}
    },
    {
        "label": "_build_choices_text",
        "importPath": "llama_index.core.selectors.llm_selectors",
        "description": "llama_index.core.selectors.llm_selectors",
        "isExtraImport": true,
        "detail": "llama_index.core.selectors.llm_selectors",
        "documentation": {}
    },
    {
        "label": "LLMMultiSelector",
        "importPath": "llama_index.core.selectors.llm_selectors",
        "description": "llama_index.core.selectors.llm_selectors",
        "isExtraImport": true,
        "detail": "llama_index.core.selectors.llm_selectors",
        "documentation": {}
    },
    {
        "label": "LLMSingleSelector",
        "importPath": "llama_index.core.selectors.llm_selectors",
        "description": "llama_index.core.selectors.llm_selectors",
        "isExtraImport": true,
        "detail": "llama_index.core.selectors.llm_selectors",
        "documentation": {}
    },
    {
        "label": "PydanticSingleSelector",
        "importPath": "llama_index.core.selectors.pydantic_selectors",
        "description": "llama_index.core.selectors.pydantic_selectors",
        "isExtraImport": true,
        "detail": "llama_index.core.selectors.pydantic_selectors",
        "documentation": {}
    },
    {
        "label": "PydanticSingleSelector",
        "importPath": "llama_index.core.selectors.pydantic_selectors",
        "description": "llama_index.core.selectors.pydantic_selectors",
        "isExtraImport": true,
        "detail": "llama_index.core.selectors.pydantic_selectors",
        "documentation": {}
    },
    {
        "label": "PydanticMultiSelector",
        "importPath": "llama_index.core.selectors.pydantic_selectors",
        "description": "llama_index.core.selectors.pydantic_selectors",
        "isExtraImport": true,
        "detail": "llama_index.core.selectors.pydantic_selectors",
        "documentation": {}
    },
    {
        "label": "PydanticSingleSelector",
        "importPath": "llama_index.core.selectors.pydantic_selectors",
        "description": "llama_index.core.selectors.pydantic_selectors",
        "isExtraImport": true,
        "detail": "llama_index.core.selectors.pydantic_selectors",
        "documentation": {}
    },
    {
        "label": "VectorIndexAutoRetriever",
        "importPath": "llama_index.core.indices.vector_store.retrievers.auto_retriever",
        "description": "llama_index.core.indices.vector_store.retrievers.auto_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.vector_store.retrievers.auto_retriever",
        "documentation": {}
    },
    {
        "label": "SQLAugmentQueryTransform",
        "importPath": "llama_index.core.query_engine.sql_join_query_engine",
        "description": "llama_index.core.query_engine.sql_join_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.query_engine.sql_join_query_engine",
        "documentation": {}
    },
    {
        "label": "SQLJoinQueryEngine",
        "importPath": "llama_index.core.query_engine.sql_join_query_engine",
        "description": "llama_index.core.query_engine.sql_join_query_engine",
        "isExtraImport": true,
        "detail": "llama_index.core.query_engine.sql_join_query_engine",
        "documentation": {}
    },
    {
        "label": "LLMQuestionGenerator",
        "importPath": "llama_index.core.question_gen.llm_generators",
        "description": "llama_index.core.question_gen.llm_generators",
        "isExtraImport": true,
        "detail": "llama_index.core.question_gen.llm_generators",
        "documentation": {}
    },
    {
        "label": "BaseQuestionGenerator",
        "importPath": "llama_index.core.question_gen.types",
        "description": "llama_index.core.question_gen.types",
        "isExtraImport": true,
        "detail": "llama_index.core.question_gen.types",
        "documentation": {}
    },
    {
        "label": "SubQuestion",
        "importPath": "llama_index.core.question_gen.types",
        "description": "llama_index.core.question_gen.types",
        "isExtraImport": true,
        "detail": "llama_index.core.question_gen.types",
        "documentation": {}
    },
    {
        "label": "BaseQuestionGenerator",
        "importPath": "llama_index.core.question_gen.types",
        "description": "llama_index.core.question_gen.types",
        "isExtraImport": true,
        "detail": "llama_index.core.question_gen.types",
        "documentation": {}
    },
    {
        "label": "SubQuestion",
        "importPath": "llama_index.core.question_gen.types",
        "description": "llama_index.core.question_gen.types",
        "isExtraImport": true,
        "detail": "llama_index.core.question_gen.types",
        "documentation": {}
    },
    {
        "label": "SubQuestion",
        "importPath": "llama_index.core.question_gen.types",
        "description": "llama_index.core.question_gen.types",
        "isExtraImport": true,
        "detail": "llama_index.core.question_gen.types",
        "documentation": {}
    },
    {
        "label": "SubQuestion",
        "importPath": "llama_index.core.question_gen.types",
        "description": "llama_index.core.question_gen.types",
        "isExtraImport": true,
        "detail": "llama_index.core.question_gen.types",
        "documentation": {}
    },
    {
        "label": "SubQuestionOutputParser",
        "importPath": "llama_index.core.question_gen.output_parser",
        "description": "llama_index.core.question_gen.output_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.question_gen.output_parser",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SUB_QUESTION_PROMPT_TMPL",
        "importPath": "llama_index.core.question_gen.prompts",
        "description": "llama_index.core.question_gen.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.question_gen.prompts",
        "documentation": {}
    },
    {
        "label": "build_tools_text",
        "importPath": "llama_index.core.question_gen.prompts",
        "description": "llama_index.core.question_gen.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.question_gen.prompts",
        "documentation": {}
    },
    {
        "label": "mimetypes",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "mimetypes",
        "description": "mimetypes",
        "detail": "mimetypes",
        "documentation": {}
    },
    {
        "label": "LocalFileSystem",
        "importPath": "fsspec.implementations.local",
        "description": "fsspec.implementations.local",
        "isExtraImport": true,
        "detail": "fsspec.implementations.local",
        "documentation": {}
    },
    {
        "label": "StringIterableReader",
        "importPath": "llama_index.core.readers.string_iterable",
        "description": "llama_index.core.readers.string_iterable",
        "isExtraImport": true,
        "detail": "llama_index.core.readers.string_iterable",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "Markdown",
        "importPath": "IPython.display",
        "description": "IPython.display",
        "isExtraImport": true,
        "detail": "IPython.display",
        "documentation": {}
    },
    {
        "label": "display",
        "importPath": "IPython.display",
        "description": "IPython.display",
        "isExtraImport": true,
        "detail": "IPython.display",
        "documentation": {}
    },
    {
        "label": "b64_2_img",
        "importPath": "llama_index.core.img_utils",
        "description": "llama_index.core.img_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.img_utils",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "textwrap",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "textwrap",
        "description": "textwrap",
        "detail": "textwrap",
        "documentation": {}
    },
    {
        "label": "dedent",
        "importPath": "textwrap",
        "description": "textwrap",
        "isExtraImport": true,
        "detail": "textwrap",
        "documentation": {}
    },
    {
        "label": "pprint",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "BaseSynthesizer",
        "importPath": "llama_index.core.response_synthesizers.base",
        "description": "llama_index.core.response_synthesizers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers.base",
        "documentation": {}
    },
    {
        "label": "BaseSynthesizer",
        "importPath": "llama_index.core.response_synthesizers.base",
        "description": "llama_index.core.response_synthesizers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers.base",
        "documentation": {}
    },
    {
        "label": "BaseSynthesizer",
        "importPath": "llama_index.core.response_synthesizers.base",
        "description": "llama_index.core.response_synthesizers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers.base",
        "documentation": {}
    },
    {
        "label": "BaseSynthesizer",
        "importPath": "llama_index.core.response_synthesizers.base",
        "description": "llama_index.core.response_synthesizers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers.base",
        "documentation": {}
    },
    {
        "label": "BaseSynthesizer",
        "importPath": "llama_index.core.response_synthesizers.base",
        "description": "llama_index.core.response_synthesizers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers.base",
        "documentation": {}
    },
    {
        "label": "BaseSynthesizer",
        "importPath": "llama_index.core.response_synthesizers.base",
        "description": "llama_index.core.response_synthesizers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers.base",
        "documentation": {}
    },
    {
        "label": "BaseSynthesizer",
        "importPath": "llama_index.core.response_synthesizers.base",
        "description": "llama_index.core.response_synthesizers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers.base",
        "documentation": {}
    },
    {
        "label": "BaseSynthesizer",
        "importPath": "llama_index.core.response_synthesizers.base",
        "description": "llama_index.core.response_synthesizers.base",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers.base",
        "documentation": {}
    },
    {
        "label": "SynthesizeStartEvent",
        "importPath": "llama_index.core.instrumentation.events.synthesis",
        "description": "llama_index.core.instrumentation.events.synthesis",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.synthesis",
        "documentation": {}
    },
    {
        "label": "SynthesizeEndEvent",
        "importPath": "llama_index.core.instrumentation.events.synthesis",
        "description": "llama_index.core.instrumentation.events.synthesis",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.synthesis",
        "documentation": {}
    },
    {
        "label": "SynthesizeStartEvent",
        "importPath": "llama_index.core.instrumentation.events.synthesis",
        "description": "llama_index.core.instrumentation.events.synthesis",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.synthesis",
        "documentation": {}
    },
    {
        "label": "SynthesizeEndEvent",
        "importPath": "llama_index.core.instrumentation.events.synthesis",
        "description": "llama_index.core.instrumentation.events.synthesis",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.synthesis",
        "documentation": {}
    },
    {
        "label": "GetResponseEndEvent",
        "importPath": "llama_index.core.instrumentation.events.synthesis",
        "description": "llama_index.core.instrumentation.events.synthesis",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.synthesis",
        "documentation": {}
    },
    {
        "label": "GetResponseStartEvent",
        "importPath": "llama_index.core.instrumentation.events.synthesis",
        "description": "llama_index.core.instrumentation.events.synthesis",
        "isExtraImport": true,
        "detail": "llama_index.core.instrumentation.events.synthesis",
        "documentation": {}
    },
    {
        "label": "Refine",
        "importPath": "llama_index.core.response_synthesizers.refine",
        "description": "llama_index.core.response_synthesizers.refine",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers.refine",
        "documentation": {}
    },
    {
        "label": "Refine",
        "importPath": "llama_index.core.response_synthesizers.refine",
        "description": "llama_index.core.response_synthesizers.refine",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers.refine",
        "documentation": {}
    },
    {
        "label": "Accumulate",
        "importPath": "llama_index.core.response_synthesizers.accumulate",
        "description": "llama_index.core.response_synthesizers.accumulate",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers.accumulate",
        "documentation": {}
    },
    {
        "label": "CompactAndAccumulate",
        "importPath": "llama_index.core.response_synthesizers.compact_and_accumulate",
        "description": "llama_index.core.response_synthesizers.compact_and_accumulate",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers.compact_and_accumulate",
        "documentation": {}
    },
    {
        "label": "CompactAndRefine",
        "importPath": "llama_index.core.response_synthesizers.compact_and_refine",
        "description": "llama_index.core.response_synthesizers.compact_and_refine",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers.compact_and_refine",
        "documentation": {}
    },
    {
        "label": "ContextOnly",
        "importPath": "llama_index.core.response_synthesizers.context_only",
        "description": "llama_index.core.response_synthesizers.context_only",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers.context_only",
        "documentation": {}
    },
    {
        "label": "Generation",
        "importPath": "llama_index.core.response_synthesizers.generation",
        "description": "llama_index.core.response_synthesizers.generation",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers.generation",
        "documentation": {}
    },
    {
        "label": "NoText",
        "importPath": "llama_index.core.response_synthesizers.no_text",
        "description": "llama_index.core.response_synthesizers.no_text",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers.no_text",
        "documentation": {}
    },
    {
        "label": "SimpleSummarize",
        "importPath": "llama_index.core.response_synthesizers.simple_summarize",
        "description": "llama_index.core.response_synthesizers.simple_summarize",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers.simple_summarize",
        "documentation": {}
    },
    {
        "label": "TreeSummarize",
        "importPath": "llama_index.core.response_synthesizers.tree_summarize",
        "description": "llama_index.core.response_synthesizers.tree_summarize",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers.tree_summarize",
        "documentation": {}
    },
    {
        "label": "ResponseMode",
        "importPath": "llama_index.core.response_synthesizers.type",
        "description": "llama_index.core.response_synthesizers.type",
        "isExtraImport": true,
        "detail": "llama_index.core.response_synthesizers.type",
        "documentation": {}
    },
    {
        "label": "get_response_text",
        "importPath": "llama_index.core.response.utils",
        "description": "llama_index.core.response.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.response.utils",
        "documentation": {}
    },
    {
        "label": "aget_response_text",
        "importPath": "llama_index.core.response.utils",
        "description": "llama_index.core.response.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.response.utils",
        "documentation": {}
    },
    {
        "label": "VectorIndexRetriever",
        "importPath": "llama_index.core.indices.vector_store.retrievers.retriever",
        "description": "llama_index.core.indices.vector_store.retrievers.retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.vector_store.retrievers.retriever",
        "documentation": {}
    },
    {
        "label": "VectorIndexRetriever",
        "importPath": "llama_index.core.indices.vector_store.retrievers.retriever",
        "description": "llama_index.core.indices.vector_store.retrievers.retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.vector_store.retrievers.retriever",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "importPath": "llama_index.core.retrievers",
        "description": "llama_index.core.retrievers",
        "isExtraImport": true,
        "detail": "llama_index.core.retrievers",
        "documentation": {}
    },
    {
        "label": "RetrieverTool",
        "importPath": "llama_index.core.tools.retriever_tool",
        "description": "llama_index.core.tools.retriever_tool",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.retriever_tool",
        "documentation": {}
    },
    {
        "label": "Answer",
        "importPath": "llama_index.core.output_parsers.selection",
        "description": "llama_index.core.output_parsers.selection",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.selection",
        "documentation": {}
    },
    {
        "label": "SelectionOutputParser",
        "importPath": "llama_index.core.output_parsers.selection",
        "description": "llama_index.core.output_parsers.selection",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.selection",
        "documentation": {}
    },
    {
        "label": "SelectionOutputParser",
        "importPath": "llama_index.core.output_parsers.selection",
        "description": "llama_index.core.output_parsers.selection",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.selection",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MULTI_SELECT_PROMPT_TMPL",
        "importPath": "llama_index.core.selectors.prompts",
        "description": "llama_index.core.selectors.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.selectors.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SINGLE_SELECT_PROMPT_TMPL",
        "importPath": "llama_index.core.selectors.prompts",
        "description": "llama_index.core.selectors.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.selectors.prompts",
        "documentation": {}
    },
    {
        "label": "MultiSelectPrompt",
        "importPath": "llama_index.core.selectors.prompts",
        "description": "llama_index.core.selectors.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.selectors.prompts",
        "documentation": {}
    },
    {
        "label": "SingleSelectPrompt",
        "importPath": "llama_index.core.selectors.prompts",
        "description": "llama_index.core.selectors.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.selectors.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MULTI_PYD_SELECT_PROMPT_TMPL",
        "importPath": "llama_index.core.selectors.prompts",
        "description": "llama_index.core.selectors.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.selectors.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SINGLE_PYD_SELECT_PROMPT_TMPL",
        "importPath": "llama_index.core.selectors.prompts",
        "description": "llama_index.core.selectors.prompts",
        "isExtraImport": true,
        "detail": "llama_index.core.selectors.prompts",
        "documentation": {}
    },
    {
        "label": "BaseSparseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base_sparse",
        "description": "llama_index.core.base.embeddings.base_sparse",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base_sparse",
        "documentation": {}
    },
    {
        "label": "SparseEmbedding",
        "importPath": "llama_index.core.base.embeddings.base_sparse",
        "description": "llama_index.core.base.embeddings.base_sparse",
        "isExtraImport": true,
        "detail": "llama_index.core.base.embeddings.base_sparse",
        "documentation": {}
    },
    {
        "label": "BaseChatStore",
        "importPath": "llama_index.core.storage.chat_store.base",
        "description": "llama_index.core.storage.chat_store.base",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.chat_store.base",
        "documentation": {}
    },
    {
        "label": "BaseChatStore",
        "importPath": "llama_index.core.storage.chat_store.base",
        "description": "llama_index.core.storage.chat_store.base",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.chat_store.base",
        "documentation": {}
    },
    {
        "label": "SimpleChatStore",
        "importPath": "llama_index.core.storage.chat_store.simple_chat_store",
        "description": "llama_index.core.storage.chat_store.simple_chat_store",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.chat_store.simple_chat_store",
        "documentation": {}
    },
    {
        "label": "declarative_base",
        "importPath": "sqlalchemy.orm",
        "description": "sqlalchemy.orm",
        "isExtraImport": true,
        "detail": "sqlalchemy.orm",
        "documentation": {}
    },
    {
        "label": "sessionmaker",
        "importPath": "sqlalchemy.orm",
        "description": "sqlalchemy.orm",
        "isExtraImport": true,
        "detail": "sqlalchemy.orm",
        "documentation": {}
    },
    {
        "label": "AsyncDBChatStore",
        "importPath": "llama_index.core.storage.chat_store.base_db",
        "description": "llama_index.core.storage.chat_store.base_db",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.chat_store.base_db",
        "documentation": {}
    },
    {
        "label": "MessageStatus",
        "importPath": "llama_index.core.storage.chat_store.base_db",
        "description": "llama_index.core.storage.chat_store.base_db",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.chat_store.base_db",
        "documentation": {}
    },
    {
        "label": "SimpleDocumentStore",
        "importPath": "llama_index.core.storage.docstore.simple_docstore",
        "description": "llama_index.core.storage.docstore.simple_docstore",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.simple_docstore",
        "documentation": {}
    },
    {
        "label": "SimpleDocumentStore",
        "importPath": "llama_index.core.storage.docstore.simple_docstore",
        "description": "llama_index.core.storage.docstore.simple_docstore",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.simple_docstore",
        "documentation": {}
    },
    {
        "label": "SimpleDocumentStore",
        "importPath": "llama_index.core.storage.docstore.simple_docstore",
        "description": "llama_index.core.storage.docstore.simple_docstore",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.simple_docstore",
        "documentation": {}
    },
    {
        "label": "SimpleDocumentStore",
        "importPath": "llama_index.core.storage.docstore.simple_docstore",
        "description": "llama_index.core.storage.docstore.simple_docstore",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.simple_docstore",
        "documentation": {}
    },
    {
        "label": "KVDocumentStore",
        "importPath": "llama_index.core.storage.docstore.keyval_docstore",
        "description": "llama_index.core.storage.docstore.keyval_docstore",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.docstore.keyval_docstore",
        "documentation": {}
    },
    {
        "label": "SimpleKVStore",
        "importPath": "llama_index.core.storage.kvstore.simple_kvstore",
        "description": "llama_index.core.storage.kvstore.simple_kvstore",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.kvstore.simple_kvstore",
        "documentation": {}
    },
    {
        "label": "SimpleKVStore",
        "importPath": "llama_index.core.storage.kvstore.simple_kvstore",
        "description": "llama_index.core.storage.kvstore.simple_kvstore",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.kvstore.simple_kvstore",
        "documentation": {}
    },
    {
        "label": "BaseIndexStore",
        "importPath": "llama_index.core.storage.index_store.types",
        "description": "llama_index.core.storage.index_store.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.index_store.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_DIR",
        "importPath": "llama_index.core.storage.index_store.types",
        "description": "llama_index.core.storage.index_store.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.index_store.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_FNAME",
        "importPath": "llama_index.core.storage.index_store.types",
        "description": "llama_index.core.storage.index_store.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.index_store.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_PATH",
        "importPath": "llama_index.core.storage.index_store.types",
        "description": "llama_index.core.storage.index_store.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.index_store.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_FNAME",
        "importPath": "llama_index.core.storage.index_store.types",
        "description": "llama_index.core.storage.index_store.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.index_store.types",
        "documentation": {}
    },
    {
        "label": "BaseIndexStore",
        "importPath": "llama_index.core.storage.index_store.types",
        "description": "llama_index.core.storage.index_store.types",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.index_store.types",
        "documentation": {}
    },
    {
        "label": "index_struct_to_json",
        "importPath": "llama_index.core.storage.index_store.utils",
        "description": "llama_index.core.storage.index_store.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.index_store.utils",
        "documentation": {}
    },
    {
        "label": "json_to_index_struct",
        "importPath": "llama_index.core.storage.index_store.utils",
        "description": "llama_index.core.storage.index_store.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.index_store.utils",
        "documentation": {}
    },
    {
        "label": "KVIndexStore",
        "importPath": "llama_index.core.storage.index_store.keyval_index_store",
        "description": "llama_index.core.storage.index_store.keyval_index_store",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.index_store.keyval_index_store",
        "documentation": {}
    },
    {
        "label": "INDEX_STRUCT_TYPE_TO_INDEX_STRUCT_CLASS",
        "importPath": "llama_index.core.data_structs.registry",
        "description": "llama_index.core.data_structs.registry",
        "isExtraImport": true,
        "detail": "llama_index.core.data_structs.registry",
        "documentation": {}
    },
    {
        "label": "SimpleIndexStore",
        "importPath": "llama_index.core.storage.index_store.simple_index_store",
        "description": "llama_index.core.storage.index_store.simple_index_store",
        "isExtraImport": true,
        "detail": "llama_index.core.storage.index_store.simple_index_store",
        "documentation": {}
    },
    {
        "label": "SPEC_FUNCTION_TYPE",
        "importPath": "llama_index.core.tools.tool_spec.base",
        "description": "llama_index.core.tools.tool_spec.base",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.tool_spec.base",
        "documentation": {}
    },
    {
        "label": "BaseToolSpec",
        "importPath": "llama_index.core.tools.tool_spec.base",
        "description": "llama_index.core.tools.tool_spec.base",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.tool_spec.base",
        "documentation": {}
    },
    {
        "label": "BaseToolSpec",
        "importPath": "llama_index.core.tools.tool_spec.base",
        "description": "llama_index.core.tools.tool_spec.base",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.tool_spec.base",
        "documentation": {}
    },
    {
        "label": "create_schema_from_function",
        "importPath": "llama_index.core.tools.utils",
        "description": "llama_index.core.tools.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.utils",
        "documentation": {}
    },
    {
        "label": "create_schema_from_function",
        "importPath": "llama_index.core.tools.utils",
        "description": "llama_index.core.tools.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.utils",
        "documentation": {}
    },
    {
        "label": "create_schema_from_function",
        "importPath": "llama_index.core.tools.utils",
        "description": "llama_index.core.tools.utils",
        "isExtraImport": true,
        "detail": "llama_index.core.tools.utils",
        "documentation": {}
    },
    {
        "label": "Engine",
        "importPath": "sqlalchemy.engine",
        "description": "sqlalchemy.engine",
        "isExtraImport": true,
        "detail": "sqlalchemy.engine",
        "documentation": {}
    },
    {
        "label": "OperationalError",
        "importPath": "sqlalchemy.exc",
        "description": "sqlalchemy.exc",
        "isExtraImport": true,
        "detail": "sqlalchemy.exc",
        "documentation": {}
    },
    {
        "label": "ProgrammingError",
        "importPath": "sqlalchemy.exc",
        "description": "sqlalchemy.exc",
        "isExtraImport": true,
        "detail": "sqlalchemy.exc",
        "documentation": {}
    },
    {
        "label": "OperationalError",
        "importPath": "sqlalchemy.exc",
        "description": "sqlalchemy.exc",
        "isExtraImport": true,
        "detail": "sqlalchemy.exc",
        "documentation": {}
    },
    {
        "label": "CheckpointCallback",
        "importPath": "workflows.checkpointer",
        "description": "workflows.checkpointer",
        "isExtraImport": true,
        "detail": "workflows.checkpointer",
        "documentation": {}
    },
    {
        "label": "Checkpoint",
        "importPath": "workflows.checkpointer",
        "description": "workflows.checkpointer",
        "isExtraImport": true,
        "detail": "workflows.checkpointer",
        "documentation": {}
    },
    {
        "label": "WorkflowCheckpointer",
        "importPath": "workflows.checkpointer",
        "description": "workflows.checkpointer",
        "isExtraImport": true,
        "detail": "workflows.checkpointer",
        "documentation": {}
    },
    {
        "label": "Context",
        "importPath": "workflows.context",
        "description": "workflows.context",
        "isExtraImport": true,
        "detail": "workflows.context",
        "documentation": {}
    },
    {
        "label": "BaseSerializer",
        "importPath": "workflows.context.serializers",
        "description": "workflows.context.serializers",
        "isExtraImport": true,
        "detail": "workflows.context.serializers",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    JsonSerializer",
        "importPath": "workflows.context.serializers",
        "description": "workflows.context.serializers",
        "isExtraImport": true,
        "detail": "workflows.context.serializers",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    PickleSerializer",
        "importPath": "workflows.context.serializers",
        "description": "workflows.context.serializers",
        "isExtraImport": true,
        "detail": "workflows.context.serializers",
        "documentation": {}
    },
    {
        "label": "StepConfig",
        "importPath": "workflows.decorators",
        "description": "workflows.decorators",
        "isExtraImport": true,
        "detail": "workflows.decorators",
        "documentation": {}
    },
    {
        "label": "step",
        "importPath": "workflows.decorators",
        "description": "workflows.decorators",
        "isExtraImport": true,
        "detail": "workflows.decorators",
        "documentation": {}
    },
    {
        "label": "ContextSerdeError",
        "importPath": "workflows.errors",
        "description": "workflows.errors",
        "isExtraImport": true,
        "detail": "workflows.errors",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    WorkflowCancelledByUser",
        "importPath": "workflows.errors",
        "description": "workflows.errors",
        "isExtraImport": true,
        "detail": "workflows.errors",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    WorkflowConfigurationError",
        "importPath": "workflows.errors",
        "description": "workflows.errors",
        "isExtraImport": true,
        "detail": "workflows.errors",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    WorkflowDone",
        "importPath": "workflows.errors",
        "description": "workflows.errors",
        "isExtraImport": true,
        "detail": "workflows.errors",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    WorkflowRuntimeError",
        "importPath": "workflows.errors",
        "description": "workflows.errors",
        "isExtraImport": true,
        "detail": "workflows.errors",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    WorkflowStepDoesNotExistError",
        "importPath": "workflows.errors",
        "description": "workflows.errors",
        "isExtraImport": true,
        "detail": "workflows.errors",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    WorkflowTimeoutError",
        "importPath": "workflows.errors",
        "description": "workflows.errors",
        "isExtraImport": true,
        "detail": "workflows.errors",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    WorkflowValidationError",
        "importPath": "workflows.errors",
        "description": "workflows.errors",
        "isExtraImport": true,
        "detail": "workflows.errors",
        "documentation": {}
    },
    {
        "label": "# noqa",
        "importPath": "workflows.errors",
        "description": "workflows.errors",
        "isExtraImport": true,
        "detail": "workflows.errors",
        "documentation": {}
    },
    {
        "label": "Event",
        "importPath": "workflows.events",
        "description": "workflows.events",
        "isExtraImport": true,
        "detail": "workflows.events",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    EventType",
        "importPath": "workflows.events",
        "description": "workflows.events",
        "isExtraImport": true,
        "detail": "workflows.events",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    HumanResponseEvent",
        "importPath": "workflows.events",
        "description": "workflows.events",
        "isExtraImport": true,
        "detail": "workflows.events",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    InputRequiredEvent",
        "importPath": "workflows.events",
        "description": "workflows.events",
        "isExtraImport": true,
        "detail": "workflows.events",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    StartEvent",
        "importPath": "workflows.events",
        "description": "workflows.events",
        "isExtraImport": true,
        "detail": "workflows.events",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    StopEvent",
        "importPath": "workflows.events",
        "description": "workflows.events",
        "isExtraImport": true,
        "detail": "workflows.events",
        "documentation": {}
    },
    {
        "label": "# noqa",
        "importPath": "workflows.events",
        "description": "workflows.events",
        "isExtraImport": true,
        "detail": "workflows.events",
        "documentation": {}
    },
    {
        "label": "WorkflowHandler",
        "importPath": "workflows.handler",
        "description": "workflows.handler",
        "isExtraImport": true,
        "detail": "workflows.handler",
        "documentation": {}
    },
    {
        "label": "Resource",
        "importPath": "workflows.resource",
        "description": "workflows.resource",
        "isExtraImport": true,
        "detail": "workflows.resource",
        "documentation": {}
    },
    {
        "label": "ResourceDefinition",
        "importPath": "workflows.resource",
        "description": "workflows.resource",
        "isExtraImport": true,
        "detail": "workflows.resource",
        "documentation": {}
    },
    {
        "label": "ResourceManager",
        "importPath": "workflows.resource",
        "description": "workflows.resource",
        "isExtraImport": true,
        "detail": "workflows.resource",
        "documentation": {}
    },
    {
        "label": "RetryPolicy",
        "importPath": "workflows.retry_policy",
        "description": "workflows.retry_policy",
        "isExtraImport": true,
        "detail": "workflows.retry_policy",
        "documentation": {}
    },
    {
        "label": "ConstantDelayRetryPolicy",
        "importPath": "workflows.retry_policy",
        "description": "workflows.retry_policy",
        "isExtraImport": true,
        "detail": "workflows.retry_policy",
        "documentation": {}
    },
    {
        "label": "ServiceManager",
        "importPath": "workflows.service",
        "description": "workflows.service",
        "isExtraImport": true,
        "detail": "workflows.service",
        "documentation": {}
    },
    {
        "label": "ServiceNotFoundError",
        "importPath": "workflows.service",
        "description": "workflows.service",
        "isExtraImport": true,
        "detail": "workflows.service",
        "documentation": {}
    },
    {
        "label": "StopEventT",
        "importPath": "workflows.types",
        "description": "workflows.types",
        "isExtraImport": true,
        "detail": "workflows.types",
        "documentation": {}
    },
    {
        "label": "RunResultT",
        "importPath": "workflows.types",
        "description": "workflows.types",
        "isExtraImport": true,
        "detail": "workflows.types",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    get_qualified_name",
        "importPath": "workflows.context.utils",
        "description": "workflows.context.utils",
        "isExtraImport": true,
        "detail": "workflows.context.utils",
        "documentation": {}
    },
    {
        "label": "import_module_from_qualified_name",
        "importPath": "workflows.context.utils",
        "description": "workflows.context.utils",
        "isExtraImport": true,
        "detail": "workflows.context.utils",
        "documentation": {}
    },
    {
        "label": "BUSY_WAIT_DELAY",
        "importPath": "workflows.utils",
        "description": "workflows.utils",
        "isExtraImport": true,
        "detail": "workflows.utils",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    ServiceDefinition",
        "importPath": "workflows.utils",
        "description": "workflows.utils",
        "isExtraImport": true,
        "detail": "workflows.utils",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    StepSignatureSpec",
        "importPath": "workflows.utils",
        "description": "workflows.utils",
        "isExtraImport": true,
        "detail": "workflows.utils",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    get_steps_from_class",
        "importPath": "workflows.utils",
        "description": "workflows.utils",
        "isExtraImport": true,
        "detail": "workflows.utils",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    get_steps_from_instance",
        "importPath": "workflows.utils",
        "description": "workflows.utils",
        "isExtraImport": true,
        "detail": "workflows.utils",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    inspect_signature",
        "importPath": "workflows.utils",
        "description": "workflows.utils",
        "isExtraImport": true,
        "detail": "workflows.utils",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    is_free_function",
        "importPath": "workflows.utils",
        "description": "workflows.utils",
        "isExtraImport": true,
        "detail": "workflows.utils",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    validate_step_signature",
        "importPath": "workflows.utils",
        "description": "workflows.utils",
        "isExtraImport": true,
        "detail": "workflows.utils",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    _get_param_types",
        "importPath": "workflows.utils",
        "description": "workflows.utils",
        "isExtraImport": true,
        "detail": "workflows.utils",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    _get_return_types",
        "importPath": "workflows.utils",
        "description": "workflows.utils",
        "isExtraImport": true,
        "detail": "workflows.utils",
        "documentation": {}
    },
    {
        "label": "# noqa",
        "importPath": "workflows.utils",
        "description": "workflows.utils",
        "isExtraImport": true,
        "detail": "workflows.utils",
        "documentation": {}
    },
    {
        "label": "Workflow",
        "importPath": "workflows.workflow",
        "description": "workflows.workflow",
        "isExtraImport": true,
        "detail": "workflows.workflow",
        "documentation": {}
    },
    {
        "label": "WorkflowMeta",
        "importPath": "workflows.workflow",
        "description": "workflows.workflow",
        "isExtraImport": true,
        "detail": "workflows.workflow",
        "documentation": {}
    },
    {
        "label": "ImageFile",
        "importPath": "PIL.ImageFile",
        "description": "PIL.ImageFile",
        "isExtraImport": true,
        "detail": "PIL.ImageFile",
        "documentation": {}
    },
    {
        "label": "traceback",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "traceback",
        "description": "traceback",
        "detail": "traceback",
        "documentation": {}
    },
    {
        "label": "platformdirs",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "platformdirs",
        "description": "platformdirs",
        "detail": "platformdirs",
        "documentation": {}
    },
    {
        "label": "pytest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pytest",
        "description": "pytest",
        "detail": "pytest",
        "documentation": {}
    },
    {
        "label": "MonkeyPatch",
        "importPath": "pytest",
        "description": "pytest",
        "isExtraImport": true,
        "detail": "pytest",
        "documentation": {}
    },
    {
        "label": "MonkeyPatch",
        "importPath": "pytest",
        "description": "pytest",
        "isExtraImport": true,
        "detail": "pytest",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "AsyncMock",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "MagicMock",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "AsyncMock",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "MagicMock",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "Mock",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "MagicMock",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "Mock",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "MagicMock",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "mock_open",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "MagicMock",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "MagicMock",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "MagicMock",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "MagicMock",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "ReActAgent",
        "importPath": "llama_index.core.agent.workflow",
        "description": "llama_index.core.agent.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow",
        "documentation": {}
    },
    {
        "label": "AgentWorkflow",
        "importPath": "llama_index.core.agent.workflow",
        "description": "llama_index.core.agent.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow",
        "documentation": {}
    },
    {
        "label": "AgentOutput",
        "importPath": "llama_index.core.agent.workflow",
        "description": "llama_index.core.agent.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow",
        "documentation": {}
    },
    {
        "label": "AgentStreamStructuredOutput",
        "importPath": "llama_index.core.agent.workflow",
        "description": "llama_index.core.agent.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow",
        "documentation": {}
    },
    {
        "label": "FunctionAgent",
        "importPath": "llama_index.core.agent.workflow",
        "description": "llama_index.core.agent.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow",
        "documentation": {}
    },
    {
        "label": "BaseWorkflowAgent",
        "importPath": "llama_index.core.agent.workflow",
        "description": "llama_index.core.agent.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow",
        "documentation": {}
    },
    {
        "label": "AgentInput",
        "importPath": "llama_index.core.agent.workflow",
        "description": "llama_index.core.agent.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow",
        "documentation": {}
    },
    {
        "label": "ReActAgent",
        "importPath": "llama_index.core.agent.workflow",
        "description": "llama_index.core.agent.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow",
        "documentation": {}
    },
    {
        "label": "FunctionAgent",
        "importPath": "llama_index.core.agent.workflow",
        "description": "llama_index.core.agent.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow",
        "documentation": {}
    },
    {
        "label": "ReActAgent",
        "importPath": "llama_index.core.agent.workflow",
        "description": "llama_index.core.agent.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow",
        "documentation": {}
    },
    {
        "label": "AgentInput",
        "importPath": "llama_index.core.agent.workflow",
        "description": "llama_index.core.agent.workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow",
        "documentation": {}
    },
    {
        "label": "CodeActAgent",
        "importPath": "llama_index.core.agent.workflow.codeact_agent",
        "description": "llama_index.core.agent.workflow.codeact_agent",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.codeact_agent",
        "documentation": {}
    },
    {
        "label": "AgentWorkflow",
        "importPath": "llama_index.core.agent.workflow.multi_agent_workflow",
        "description": "llama_index.core.agent.workflow.multi_agent_workflow",
        "isExtraImport": true,
        "detail": "llama_index.core.agent.workflow.multi_agent_workflow",
        "documentation": {}
    },
    {
        "label": "mock",
        "importPath": "unittest",
        "description": "unittest",
        "isExtraImport": true,
        "detail": "unittest",
        "documentation": {}
    },
    {
        "label": "LlamaDebugHandler",
        "importPath": "llama_index.core.callbacks.llama_debug",
        "description": "llama_index.core.callbacks.llama_debug",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.llama_debug",
        "documentation": {}
    },
    {
        "label": "TokenCountingHandler",
        "importPath": "llama_index.core.callbacks.token_counting",
        "description": "llama_index.core.callbacks.token_counting",
        "isExtraImport": true,
        "detail": "llama_index.core.callbacks.token_counting",
        "documentation": {}
    },
    {
        "label": "CondensePlusContextChatEngine",
        "importPath": "llama_index.core.chat_engine.condense_plus_context",
        "description": "llama_index.core.chat_engine.condense_plus_context",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.condense_plus_context",
        "documentation": {}
    },
    {
        "label": "CondenseQuestionChatEngine",
        "importPath": "llama_index.core.chat_engine.condense_question",
        "description": "llama_index.core.chat_engine.condense_question",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.condense_question",
        "documentation": {}
    },
    {
        "label": "ContextChatEngine",
        "importPath": "llama_index.core.chat_engine.context",
        "description": "llama_index.core.chat_engine.context",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.context",
        "documentation": {}
    },
    {
        "label": "gc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gc",
        "description": "gc",
        "detail": "gc",
        "documentation": {}
    },
    {
        "label": "SimpleChatEngine",
        "importPath": "llama_index.core.chat_engine.simple",
        "description": "llama_index.core.chat_engine.simple",
        "isExtraImport": true,
        "detail": "llama_index.core.chat_engine.simple",
        "documentation": {}
    },
    {
        "label": "HuggingFaceEmbedding",
        "importPath": "llama_index.embeddings.huggingface",
        "description": "llama_index.embeddings.huggingface",
        "isExtraImport": true,
        "detail": "llama_index.embeddings.huggingface",
        "documentation": {}
    },
    {
        "label": "BatchEvalRunner",
        "importPath": "llama_index.core.evaluation.batch_runner",
        "description": "llama_index.core.evaluation.batch_runner",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.batch_runner",
        "documentation": {}
    },
    {
        "label": "DatasetGenerator",
        "importPath": "llama_index.core.evaluation.dataset_generation",
        "description": "llama_index.core.evaluation.dataset_generation",
        "isExtraImport": true,
        "detail": "llama_index.core.evaluation.dataset_generation",
        "documentation": {}
    },
    {
        "label": "MOCK_REFINE_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_TEXT_QA_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_KG_TRIPLET_EXTRACT_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_QUERY_KEYWORD_EXTRACT_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_QUERY_KEYWORD_EXTRACT_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_INSERT_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_KEYWORD_EXTRACT_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_QUERY_KEYWORD_EXTRACT_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_QUERY_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_REFINE_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_SUMMARY_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_TEXT_QA_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_QUERY_KEYWORD_EXTRACT_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_REFINE_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_TEXT_QA_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_REFINE_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_SCHEMA_EXTRACT_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_TEXT_QA_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_TABLE_CONTEXT_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_INSERT_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_QUERY_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_REFINE_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_SUMMARY_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_TEXT_QA_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_INSERT_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_SUMMARY_PROMPT",
        "importPath": "tests.mock_utils.mock_prompts",
        "description": "tests.mock_utils.mock_prompts",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "DocumentSummaryIndexEmbeddingRetriever",
        "importPath": "llama_index.core.indices.document_summary.retrievers",
        "description": "llama_index.core.indices.document_summary.retrievers",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.document_summary.retrievers",
        "documentation": {}
    },
    {
        "label": "DocumentSummaryIndexLLMRetriever",
        "importPath": "llama_index.core.indices.document_summary.retrievers",
        "description": "llama_index.core.indices.document_summary.retrievers",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.document_summary.retrievers",
        "documentation": {}
    },
    {
        "label": "SimpleKeywordTableIndex",
        "importPath": "llama_index.core.indices.keyword_table.simple_base",
        "description": "llama_index.core.indices.keyword_table.simple_base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.keyword_table.simple_base",
        "documentation": {}
    },
    {
        "label": "SimpleKeywordTableIndex",
        "importPath": "llama_index.core.indices.keyword_table.simple_base",
        "description": "llama_index.core.indices.keyword_table.simple_base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.keyword_table.simple_base",
        "documentation": {}
    },
    {
        "label": "SimpleKeywordTableIndex",
        "importPath": "llama_index.core.indices.keyword_table.simple_base",
        "description": "llama_index.core.indices.keyword_table.simple_base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.keyword_table.simple_base",
        "documentation": {}
    },
    {
        "label": "SimpleKeywordTableIndex",
        "importPath": "llama_index.core.indices.keyword_table.simple_base",
        "description": "llama_index.core.indices.keyword_table.simple_base",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.keyword_table.simple_base",
        "documentation": {}
    },
    {
        "label": "mock_extract_keywords",
        "importPath": "tests.mock_utils.mock_utils",
        "description": "tests.mock_utils.mock_utils",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_utils",
        "documentation": {}
    },
    {
        "label": "mock_extract_keywords",
        "importPath": "tests.mock_utils.mock_utils",
        "description": "tests.mock_utils.mock_utils",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_utils",
        "documentation": {}
    },
    {
        "label": "mock_tokenizer",
        "importPath": "tests.mock_utils.mock_utils",
        "description": "tests.mock_utils.mock_utils",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_utils",
        "documentation": {}
    },
    {
        "label": "mock_tokenizer",
        "importPath": "tests.mock_utils.mock_utils",
        "description": "tests.mock_utils.mock_utils",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_utils",
        "documentation": {}
    },
    {
        "label": "mock_extract_keywords_response",
        "importPath": "tests.mock_utils.mock_utils",
        "description": "tests.mock_utils.mock_utils",
        "isExtraImport": true,
        "detail": "tests.mock_utils.mock_utils",
        "documentation": {}
    },
    {
        "label": "SimpleGraphStore",
        "importPath": "llama_index.core.graph_stores",
        "description": "llama_index.core.graph_stores",
        "isExtraImport": true,
        "detail": "llama_index.core.graph_stores",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "importPath": "llama_index.core.embeddings",
        "description": "llama_index.core.embeddings",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings",
        "documentation": {}
    },
    {
        "label": "MockEmbedding",
        "importPath": "llama_index.core.embeddings",
        "description": "llama_index.core.embeddings",
        "isExtraImport": true,
        "detail": "llama_index.core.embeddings",
        "documentation": {}
    },
    {
        "label": "KGTableRetriever",
        "importPath": "llama_index.core.indices.knowledge_graph.retrievers",
        "description": "llama_index.core.indices.knowledge_graph.retrievers",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.knowledge_graph.retrievers",
        "documentation": {}
    },
    {
        "label": "SummaryIndexEmbeddingRetriever",
        "importPath": "llama_index.core.indices.list.retrievers",
        "description": "llama_index.core.indices.list.retrievers",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.list.retrievers",
        "documentation": {}
    },
    {
        "label": "MOCK_DECOMPOSE_PROMPT",
        "importPath": "tests.indices.query.query_transform.mock_utils",
        "description": "tests.indices.query.query_transform.mock_utils",
        "isExtraImport": true,
        "detail": "tests.indices.query.query_transform.mock_utils",
        "documentation": {}
    },
    {
        "label": "JSONQueryEngine",
        "importPath": "llama_index.core.indices.struct_store.json_query",
        "description": "llama_index.core.indices.struct_store.json_query",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.struct_store.json_query",
        "documentation": {}
    },
    {
        "label": "JSONType",
        "importPath": "llama_index.core.indices.struct_store.json_query",
        "description": "llama_index.core.indices.struct_store.json_query",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.struct_store.json_query",
        "documentation": {}
    },
    {
        "label": "TreeSelectLeafEmbeddingRetriever",
        "importPath": "llama_index.core.indices.tree.select_leaf_embedding_retriever",
        "description": "llama_index.core.indices.tree.select_leaf_embedding_retriever",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.tree.select_leaf_embedding_retriever",
        "documentation": {}
    },
    {
        "label": "load_index_from_storage",
        "importPath": "llama_index.core.indices.loading",
        "description": "llama_index.core.indices.loading",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.loading",
        "documentation": {}
    },
    {
        "label": "load_index_from_storage",
        "importPath": "llama_index.core.indices.loading",
        "description": "llama_index.core.indices.loading",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.loading",
        "documentation": {}
    },
    {
        "label": "load_indices_from_storage",
        "importPath": "llama_index.core.indices.loading",
        "description": "llama_index.core.indices.loading",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.loading",
        "documentation": {}
    },
    {
        "label": "load_graph_from_storage",
        "importPath": "llama_index.core.indices.loading",
        "description": "llama_index.core.indices.loading",
        "isExtraImport": true,
        "detail": "llama_index.core.indices.loading",
        "documentation": {}
    },
    {
        "label": "ConfigurableDataSinks",
        "importPath": "llama_index.core.ingestion.data_sinks",
        "description": "llama_index.core.ingestion.data_sinks",
        "isExtraImport": true,
        "detail": "llama_index.core.ingestion.data_sinks",
        "documentation": {}
    },
    {
        "label": "ConfiguredDataSink",
        "importPath": "llama_index.core.ingestion.data_sinks",
        "description": "llama_index.core.ingestion.data_sinks",
        "isExtraImport": true,
        "detail": "llama_index.core.ingestion.data_sinks",
        "documentation": {}
    },
    {
        "label": "ConfigurableDataSources",
        "importPath": "llama_index.core.ingestion.data_sources",
        "description": "llama_index.core.ingestion.data_sources",
        "isExtraImport": true,
        "detail": "llama_index.core.ingestion.data_sources",
        "documentation": {}
    },
    {
        "label": "ConfiguredDataSource",
        "importPath": "llama_index.core.ingestion.data_sources",
        "description": "llama_index.core.ingestion.data_sources",
        "isExtraImport": true,
        "detail": "llama_index.core.ingestion.data_sources",
        "documentation": {}
    },
    {
        "label": "ConfigurableTransformations",
        "importPath": "llama_index.core.ingestion.transformations",
        "description": "llama_index.core.ingestion.transformations",
        "isExtraImport": true,
        "detail": "llama_index.core.ingestion.transformations",
        "documentation": {}
    },
    {
        "label": "ConfiguredTransformation",
        "importPath": "llama_index.core.ingestion.transformations",
        "description": "llama_index.core.ingestion.transformations",
        "isExtraImport": true,
        "detail": "llama_index.core.ingestion.transformations",
        "documentation": {}
    },
    {
        "label": "FunctionTool",
        "importPath": "llama_index.core.program.function_program",
        "description": "llama_index.core.program.function_program",
        "isExtraImport": true,
        "detail": "llama_index.core.program.function_program",
        "documentation": {}
    },
    {
        "label": "get_function_tool",
        "importPath": "llama_index.core.program.function_program",
        "description": "llama_index.core.program.function_program",
        "isExtraImport": true,
        "detail": "llama_index.core.program.function_program",
        "documentation": {}
    },
    {
        "label": "FactExtractionMemoryBlock",
        "importPath": "llama_index.core.memory.memory_blocks.fact",
        "description": "llama_index.core.memory.memory_blocks.fact",
        "isExtraImport": true,
        "detail": "llama_index.core.memory.memory_blocks.fact",
        "documentation": {}
    },
    {
        "label": "DEFAULT_FACT_EXTRACT_PROMPT",
        "importPath": "llama_index.core.memory.memory_blocks.fact",
        "description": "llama_index.core.memory.memory_blocks.fact",
        "isExtraImport": true,
        "detail": "llama_index.core.memory.memory_blocks.fact",
        "documentation": {}
    },
    {
        "label": "DEFAULT_FACT_CONDENSE_PROMPT",
        "importPath": "llama_index.core.memory.memory_blocks.fact",
        "description": "llama_index.core.memory.memory_blocks.fact",
        "isExtraImport": true,
        "detail": "llama_index.core.memory.memory_blocks.fact",
        "documentation": {}
    },
    {
        "label": "StaticMemoryBlock",
        "importPath": "llama_index.core.memory.memory_blocks.static",
        "description": "llama_index.core.memory.memory_blocks.static",
        "isExtraImport": true,
        "detail": "llama_index.core.memory.memory_blocks.static",
        "documentation": {}
    },
    {
        "label": "VectorMemoryBlock",
        "importPath": "llama_index.core.memory.memory_blocks.vector",
        "description": "llama_index.core.memory.memory_blocks.vector",
        "isExtraImport": true,
        "detail": "llama_index.core.memory.memory_blocks.vector",
        "documentation": {}
    },
    {
        "label": "ChatMemoryBuffer",
        "importPath": "llama_index.core.memory.chat_memory_buffer",
        "description": "llama_index.core.memory.chat_memory_buffer",
        "isExtraImport": true,
        "detail": "llama_index.core.memory.chat_memory_buffer",
        "documentation": {}
    },
    {
        "label": "ChatSummaryMemoryBuffer",
        "importPath": "llama_index.core.memory.chat_summary_memory_buffer",
        "description": "llama_index.core.memory.chat_summary_memory_buffer",
        "isExtraImport": true,
        "detail": "llama_index.core.memory.chat_summary_memory_buffer",
        "documentation": {}
    },
    {
        "label": "MultiModalLLMMetadata",
        "importPath": "llama_index.core.multi_modal_llms",
        "description": "llama_index.core.multi_modal_llms",
        "isExtraImport": true,
        "detail": "llama_index.core.multi_modal_llms",
        "documentation": {}
    },
    {
        "label": "load_image_urls",
        "importPath": "llama_index.core.multi_modal_llms.generic_utils",
        "description": "llama_index.core.multi_modal_llms.generic_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.multi_modal_llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "encode_image",
        "importPath": "llama_index.core.multi_modal_llms.generic_utils",
        "description": "llama_index.core.multi_modal_llms.generic_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.multi_modal_llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "image_documents_to_base64",
        "importPath": "llama_index.core.multi_modal_llms.generic_utils",
        "description": "llama_index.core.multi_modal_llms.generic_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.multi_modal_llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "infer_image_mimetype_from_base64",
        "importPath": "llama_index.core.multi_modal_llms.generic_utils",
        "description": "llama_index.core.multi_modal_llms.generic_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.multi_modal_llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "infer_image_mimetype_from_file_path",
        "importPath": "llama_index.core.multi_modal_llms.generic_utils",
        "description": "llama_index.core.multi_modal_llms.generic_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.multi_modal_llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "set_base64_and_mimetype_for_image_docs",
        "importPath": "llama_index.core.multi_modal_llms.generic_utils",
        "description": "llama_index.core.multi_modal_llms.generic_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.multi_modal_llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "importlib.util",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "importlib.util",
        "description": "importlib.util",
        "detail": "importlib.util",
        "documentation": {}
    },
    {
        "label": "find_spec",
        "importPath": "importlib.util",
        "description": "importlib.util",
        "isExtraImport": true,
        "detail": "importlib.util",
        "documentation": {}
    },
    {
        "label": "MarkdownElementNodeParser",
        "importPath": "llama_index.core.node_parser.relational.markdown_element",
        "description": "llama_index.core.node_parser.relational.markdown_element",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.relational.markdown_element",
        "documentation": {}
    },
    {
        "label": "tiktoken",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tiktoken",
        "description": "tiktoken",
        "detail": "tiktoken",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "SemanticDoubleMergingSplitterNodeParser",
        "importPath": "llama_index.core.node_parser.text.semantic_double_merging_splitter",
        "description": "llama_index.core.node_parser.text.semantic_double_merging_splitter",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.text.semantic_double_merging_splitter",
        "documentation": {}
    },
    {
        "label": "LanguageConfig",
        "importPath": "llama_index.core.node_parser.text.semantic_double_merging_splitter",
        "description": "llama_index.core.node_parser.text.semantic_double_merging_splitter",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.text.semantic_double_merging_splitter",
        "documentation": {}
    },
    {
        "label": "SemanticSplitterNodeParser",
        "importPath": "llama_index.core.node_parser.text.semantic_splitter",
        "description": "llama_index.core.node_parser.text.semantic_splitter",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.text.semantic_splitter",
        "documentation": {}
    },
    {
        "label": "UnstructuredElementNodeParser",
        "importPath": "llama_index.core.node_parser.relational.unstructured_element",
        "description": "llama_index.core.node_parser.relational.unstructured_element",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser.relational.unstructured_element",
        "documentation": {}
    },
    {
        "label": "MockerFixture",
        "importPath": "pytest_mock",
        "description": "pytest_mock",
        "isExtraImport": true,
        "detail": "pytest_mock",
        "documentation": {}
    },
    {
        "label": "LangchainOutputParser",
        "importPath": "llama_index.core.output_parsers.langchain",
        "description": "llama_index.core.output_parsers.langchain",
        "isExtraImport": true,
        "detail": "llama_index.core.output_parsers.langchain",
        "documentation": {}
    },
    {
        "label": "DEFAULT_INDEX_CLASSES",
        "importPath": "llama_index.core.playground",
        "description": "llama_index.core.playground",
        "isExtraImport": true,
        "detail": "llama_index.core.playground",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MODES",
        "importPath": "llama_index.core.playground",
        "description": "llama_index.core.playground",
        "isExtraImport": true,
        "detail": "llama_index.core.playground",
        "documentation": {}
    },
    {
        "label": "Playground",
        "importPath": "llama_index.core.playground",
        "description": "llama_index.core.playground",
        "isExtraImport": true,
        "detail": "llama_index.core.playground",
        "documentation": {}
    },
    {
        "label": "EmbeddingRecencyPostprocessor",
        "importPath": "llama_index.core.postprocessor.node_recency",
        "description": "llama_index.core.postprocessor.node_recency",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.node_recency",
        "documentation": {}
    },
    {
        "label": "FixedRecencyPostprocessor",
        "importPath": "llama_index.core.postprocessor.node_recency",
        "description": "llama_index.core.postprocessor.node_recency",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.node_recency",
        "documentation": {}
    },
    {
        "label": "TimeWeightedPostprocessor",
        "importPath": "llama_index.core.postprocessor.node_recency",
        "description": "llama_index.core.postprocessor.node_recency",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.node_recency",
        "documentation": {}
    },
    {
        "label": "LLMRerank",
        "importPath": "llama_index.core.postprocessor.llm_rerank",
        "description": "llama_index.core.postprocessor.llm_rerank",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.llm_rerank",
        "documentation": {}
    },
    {
        "label": "SentenceEmbeddingOptimizer",
        "importPath": "llama_index.core.postprocessor.optimizer",
        "description": "llama_index.core.postprocessor.optimizer",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.optimizer",
        "documentation": {}
    },
    {
        "label": "StructuredLLMRerank",
        "importPath": "llama_index.core.postprocessor.structured_llm_rerank",
        "description": "llama_index.core.postprocessor.structured_llm_rerank",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.structured_llm_rerank",
        "documentation": {}
    },
    {
        "label": "DocumentWithRelevance",
        "importPath": "llama_index.core.postprocessor.structured_llm_rerank",
        "description": "llama_index.core.postprocessor.structured_llm_rerank",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.structured_llm_rerank",
        "documentation": {}
    },
    {
        "label": "DocumentRelevanceList",
        "importPath": "llama_index.core.postprocessor.structured_llm_rerank",
        "description": "llama_index.core.postprocessor.structured_llm_rerank",
        "isExtraImport": true,
        "detail": "llama_index.core.postprocessor.structured_llm_rerank",
        "documentation": {}
    },
    {
        "label": "FunctionCallingProgram",
        "importPath": "llama_index.core.program",
        "description": "llama_index.core.program",
        "isExtraImport": true,
        "detail": "llama_index.core.program",
        "documentation": {}
    },
    {
        "label": "MultiModalLLMCompletionProgram",
        "importPath": "llama_index.core.program",
        "description": "llama_index.core.program",
        "isExtraImport": true,
        "detail": "llama_index.core.program",
        "documentation": {}
    },
    {
        "label": "LLMTextCompletionProgram",
        "importPath": "llama_index.core.program.llm_program",
        "description": "llama_index.core.program.llm_program",
        "isExtraImport": true,
        "detail": "llama_index.core.program.llm_program",
        "documentation": {}
    },
    {
        "label": "process_streaming_content_incremental",
        "importPath": "llama_index.core.program.streaming_utils",
        "description": "llama_index.core.program.streaming_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.program.streaming_utils",
        "documentation": {}
    },
    {
        "label": "_extract_partial_list_progress",
        "importPath": "llama_index.core.program.streaming_utils",
        "description": "llama_index.core.program.streaming_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.program.streaming_utils",
        "documentation": {}
    },
    {
        "label": "_parse_partial_list_items",
        "importPath": "llama_index.core.program.streaming_utils",
        "description": "llama_index.core.program.streaming_utils",
        "isExtraImport": true,
        "detail": "llama_index.core.program.streaming_utils",
        "documentation": {}
    },
    {
        "label": "Tee",
        "kind": 6,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "class Tee:\n    def __init__(self, file):\n        self.f = file\n    def write(self, what):\n        if self.f is not None:\n            try:\n                self.f.write(what.replace(\"\\n\", \"\\r\\n\"))\n            except OSError:\n                pass\n        tee_f.write(what)",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "get_root_hkey",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def get_root_hkey():\n    try:\n        winreg.OpenKey(\n            winreg.HKEY_LOCAL_MACHINE, root_key_name, 0, winreg.KEY_CREATE_SUB_KEY\n        )\n        return winreg.HKEY_LOCAL_MACHINE\n    except OSError:\n        # Either not exist, or no permissions to create subkey means\n        # must be HKCU\n        return winreg.HKEY_CURRENT_USER",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "create_shortcut",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def create_shortcut(\n    path, description, filename, arguments=\"\", workdir=\"\", iconpath=\"\", iconindex=0\n):\n    import pythoncom\n    from win32com.shell import shell\n    ilink = pythoncom.CoCreateInstance(\n        shell.CLSID_ShellLink,\n        None,\n        pythoncom.CLSCTX_INPROC_SERVER,\n        shell.IID_IShellLink,",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "get_special_folder_path",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def get_special_folder_path(path_name):\n    from win32com.shell import shell, shellcon\n    for maybe in \"\"\"\n        CSIDL_COMMON_STARTMENU CSIDL_STARTMENU CSIDL_COMMON_APPDATA\n        CSIDL_LOCAL_APPDATA CSIDL_APPDATA CSIDL_COMMON_DESKTOPDIRECTORY\n        CSIDL_DESKTOPDIRECTORY CSIDL_COMMON_STARTUP CSIDL_STARTUP\n        CSIDL_COMMON_PROGRAMS CSIDL_PROGRAMS CSIDL_PROGRAM_FILES_COMMON\n        CSIDL_PROGRAM_FILES CSIDL_FONTS\"\"\".split():\n        if maybe == path_name:\n            csidl = getattr(shellcon, maybe)",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "CopyTo",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def CopyTo(desc, src, dest):\n    import win32api\n    import win32con\n    while 1:\n        try:\n            win32api.CopyFile(src, dest, 0)\n            return\n        except win32api.error as details:\n            if details.winerror == 5:  # access denied - user not admin.\n                raise",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "LoadSystemModule",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def LoadSystemModule(lib_dir, modname):\n    # See if this is a debug build.\n    import importlib.machinery\n    import importlib.util\n    suffix = \"_d\" if \"_d.pyd\" in importlib.machinery.EXTENSION_SUFFIXES else \"\"\n    filename = \"%s%d%d%s.dll\" % (\n        modname,\n        sys.version_info.major,\n        sys.version_info.minor,\n        suffix,",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "SetPyKeyVal",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def SetPyKeyVal(key_name, value_name, value):\n    root_hkey = get_root_hkey()\n    root_key = winreg.OpenKey(root_hkey, root_key_name)\n    try:\n        my_key = winreg.CreateKey(root_key, key_name)\n        try:\n            winreg.SetValueEx(my_key, value_name, 0, winreg.REG_SZ, value)\n            if verbose:\n                print(f\"-> {root_key_name}\\\\{key_name}[{value_name}]={value!r}\")\n        finally:",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "UnsetPyKeyVal",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def UnsetPyKeyVal(key_name, value_name, delete_key=False):\n    root_hkey = get_root_hkey()\n    root_key = winreg.OpenKey(root_hkey, root_key_name)\n    try:\n        my_key = winreg.OpenKey(root_key, key_name, 0, winreg.KEY_SET_VALUE)\n        try:\n            winreg.DeleteValue(my_key, value_name)\n            if verbose:\n                print(f\"-> DELETE {root_key_name}\\\\{key_name}[{value_name}]\")\n        finally:",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "RegisterCOMObjects",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def RegisterCOMObjects(register=True):\n    import win32com.server.register\n    if register:\n        func = win32com.server.register.RegisterClasses\n    else:\n        func = win32com.server.register.UnregisterClasses\n    flags = {}\n    if not verbose:\n        flags[\"quiet\"] = 1\n    for module, klass_name in com_modules:",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "RegisterHelpFile",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def RegisterHelpFile(register=True, lib_dir=None):\n    if lib_dir is None:\n        lib_dir = sysconfig.get_paths()[\"platlib\"]\n    if register:\n        # Register the .chm help file.\n        chm_file = os.path.join(lib_dir, \"PyWin32.chm\")\n        if os.path.isfile(chm_file):\n            # This isn't recursive, so if 'Help' doesn't exist, we croak\n            SetPyKeyVal(\"Help\", None, None)\n            SetPyKeyVal(\"Help\\\\Pythonwin Reference\", None, chm_file)",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "RegisterPythonwin",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def RegisterPythonwin(register=True, lib_dir=None):\n    \"\"\"Add (or remove) Pythonwin to context menu for python scripts.\n    ??? Should probably also add Edit command for pys files also.\n    Also need to remove these keys on uninstall, but there's no function\n    to add registry entries to uninstall log ???\n    \"\"\"\n    import os\n    if lib_dir is None:\n        lib_dir = sysconfig.get_paths()[\"platlib\"]\n    classes_root = get_root_hkey()",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "get_shortcuts_folder",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def get_shortcuts_folder():\n    if get_root_hkey() == winreg.HKEY_LOCAL_MACHINE:\n        try:\n            fldr = get_special_folder_path(\"CSIDL_COMMON_PROGRAMS\")\n        except OSError:\n            # No CSIDL_COMMON_PROGRAMS on this platform\n            fldr = get_special_folder_path(\"CSIDL_PROGRAMS\")\n    else:\n        # non-admin install - always goes in this user's start menu.\n        fldr = get_special_folder_path(\"CSIDL_PROGRAMS\")",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "get_system_dir",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def get_system_dir():\n    import win32api  # we assume this exists.\n    try:\n        import pythoncom\n        import win32process\n        from win32com.shell import shell, shellcon\n        try:\n            if win32process.IsWow64Process():\n                return shell.SHGetSpecialFolderPath(0, shellcon.CSIDL_SYSTEMX86)\n            return shell.SHGetSpecialFolderPath(0, shellcon.CSIDL_SYSTEM)",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "fixup_dbi",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def fixup_dbi():\n    # We used to have a dbi.pyd with our .pyd files, but now have a .py file.\n    # If the user didn't uninstall, they will find the .pyd which will cause\n    # problems - so handle that.\n    import win32api\n    import win32con\n    pyd_name = os.path.join(os.path.dirname(win32api.__file__), \"dbi.pyd\")\n    pyd_d_name = os.path.join(os.path.dirname(win32api.__file__), \"dbi_d.pyd\")\n    py_name = os.path.join(os.path.dirname(win32con.__file__), \"dbi.py\")\n    for this_pyd in (pyd_name, pyd_d_name):",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "install",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def install(lib_dir):\n    import traceback\n    # The .pth file is now installed as a regular file.\n    # Create the .pth file in the site-packages dir, and use only relative paths\n    # We used to write a .pth directly to sys.prefix - clobber it.\n    if os.path.isfile(os.path.join(sys.prefix, \"pywin32.pth\")):\n        os.unlink(os.path.join(sys.prefix, \"pywin32.pth\"))\n    # The .pth may be new and therefore not loaded in this session.\n    # Setup the paths just in case.\n    for name in \"win32 win32\\\\lib Pythonwin\".split():",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "uninstall",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def uninstall(lib_dir):\n    # First ensure our system modules are loaded from pywin32_system, so\n    # we can remove the ones we copied...\n    LoadSystemModule(lib_dir, \"pywintypes\")\n    LoadSystemModule(lib_dir, \"pythoncom\")\n    try:\n        RegisterCOMObjects(False)\n    except Exception as why:\n        print(f\"Failed to unregister COM objects: {why}\")\n    try:",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "verify_destination",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def verify_destination(location: str) -> str:\n    location = os.path.abspath(location)\n    if not os.path.isdir(location):\n        raise argparse.ArgumentTypeError(\n            f'Path \"{location}\" is not an existing directory!'\n        )\n    return location\ndef main():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.RawDescriptionHelpFormatter,",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        description=\"\"\"A post-install script for the pywin32 extensions.\n    * Typical usage:\n    > python -m pywin32_postinstall -install\n    * or (shorter but you don't have control over which python environment is used)\n    > pywin32_postinstall -install\n    You need to execute this script, with a '-install' parameter,\n    to ensure the environment is setup correctly to install COM objects, services, etc.",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "tee_f",
        "kind": 5,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "tee_f = open(\n    os.path.join(\n        tempfile.gettempdir(),  # Send output somewhere so it can be found if necessary...\n        \"pywin32_postinstall.log\",\n    ),\n    \"w\",\n)\nclass Tee:\n    def __init__(self, file):\n        self.f = file",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "sys.stderr",
        "kind": 5,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "sys.stderr = Tee(sys.stderr)\nsys.stdout = Tee(sys.stdout)\ncom_modules = [\n    # module_name,                      class_names\n    (\"win32com.servers.interp\", \"Interpreter\"),\n    (\"win32com.servers.dictionary\", \"DictionaryPolicy\"),\n    (\"win32com.axscript.client.pyscript\", \"PyScript\"),\n]\n# Is this a 'silent' install - ie, avoid all dialogs.\n# Different than 'verbose'",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "sys.stdout",
        "kind": 5,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "sys.stdout = Tee(sys.stdout)\ncom_modules = [\n    # module_name,                      class_names\n    (\"win32com.servers.interp\", \"Interpreter\"),\n    (\"win32com.servers.dictionary\", \"DictionaryPolicy\"),\n    (\"win32com.axscript.client.pyscript\", \"PyScript\"),\n]\n# Is this a 'silent' install - ie, avoid all dialogs.\n# Different than 'verbose'\nsilent = 0",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "com_modules",
        "kind": 5,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "com_modules = [\n    # module_name,                      class_names\n    (\"win32com.servers.interp\", \"Interpreter\"),\n    (\"win32com.servers.dictionary\", \"DictionaryPolicy\"),\n    (\"win32com.axscript.client.pyscript\", \"PyScript\"),\n]\n# Is this a 'silent' install - ie, avoid all dialogs.\n# Different than 'verbose'\nsilent = 0\n# Verbosity of output messages.",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "silent",
        "kind": 5,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "silent = 0\n# Verbosity of output messages.\nverbose = 1\nroot_key_name = \"Software\\\\Python\\\\PythonCore\\\\\" + sys.winver\ndef get_root_hkey():\n    try:\n        winreg.OpenKey(\n            winreg.HKEY_LOCAL_MACHINE, root_key_name, 0, winreg.KEY_CREATE_SUB_KEY\n        )\n        return winreg.HKEY_LOCAL_MACHINE",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "verbose",
        "kind": 5,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "verbose = 1\nroot_key_name = \"Software\\\\Python\\\\PythonCore\\\\\" + sys.winver\ndef get_root_hkey():\n    try:\n        winreg.OpenKey(\n            winreg.HKEY_LOCAL_MACHINE, root_key_name, 0, winreg.KEY_CREATE_SUB_KEY\n        )\n        return winreg.HKEY_LOCAL_MACHINE\n    except OSError:\n        # Either not exist, or no permissions to create subkey means",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "root_key_name",
        "kind": 5,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "root_key_name = \"Software\\\\Python\\\\PythonCore\\\\\" + sys.winver\ndef get_root_hkey():\n    try:\n        winreg.OpenKey(\n            winreg.HKEY_LOCAL_MACHINE, root_key_name, 0, winreg.KEY_CREATE_SUB_KEY\n        )\n        return winreg.HKEY_LOCAL_MACHINE\n    except OSError:\n        # Either not exist, or no permissions to create subkey means\n        # must be HKCU",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "run_test",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_testall",
        "description": ".venv.Scripts.pywin32_testall",
        "peekOfCode": "def run_test(script, cmdline_extras):\n    dirname, scriptname = os.path.split(script)\n    # some tests prefer to be run from their directory.\n    cmd = [sys.executable, \"-u\", scriptname] + cmdline_extras\n    print(\"--- Running '%s' ---\" % script)\n    sys.stdout.flush()\n    result = subprocess.run(cmd, check=False, cwd=dirname)\n    print(f\"*** Test script '{script}' exited with {result.returncode}\")\n    sys.stdout.flush()\n    if result.returncode:",
        "detail": ".venv.Scripts.pywin32_testall",
        "documentation": {}
    },
    {
        "label": "find_and_run",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_testall",
        "description": ".venv.Scripts.pywin32_testall",
        "peekOfCode": "def find_and_run(possible_locations, extras):\n    for maybe in possible_locations:\n        if os.path.isfile(maybe):\n            run_test(maybe, extras)\n            break\n    else:\n        raise RuntimeError(\n            \"Failed to locate a test script in one of %s\" % possible_locations\n        )\ndef main():",
        "detail": ".venv.Scripts.pywin32_testall",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_testall",
        "description": ".venv.Scripts.pywin32_testall",
        "peekOfCode": "def main():\n    import argparse\n    code_directories = [project_root] + site_packages\n    parser = argparse.ArgumentParser(\n        description=\"A script to trigger tests in all subprojects of PyWin32.\"\n    )\n    parser.add_argument(\n        \"-no-user-interaction\",\n        default=False,\n        action=\"store_true\",",
        "detail": ".venv.Scripts.pywin32_testall",
        "documentation": {}
    },
    {
        "label": "project_root",
        "kind": 5,
        "importPath": ".venv.Scripts.pywin32_testall",
        "description": ".venv.Scripts.pywin32_testall",
        "peekOfCode": "project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))\nsite_packages = [site.getusersitepackages()] + site.getsitepackages()\nfailures = []\n# Run a test using subprocess and wait for the result.\n# If we get an returncode != 0, we know that there was an error, but we don't\n# abort immediately - we run as many tests as we can.\ndef run_test(script, cmdline_extras):\n    dirname, scriptname = os.path.split(script)\n    # some tests prefer to be run from their directory.\n    cmd = [sys.executable, \"-u\", scriptname] + cmdline_extras",
        "detail": ".venv.Scripts.pywin32_testall",
        "documentation": {}
    },
    {
        "label": "site_packages",
        "kind": 5,
        "importPath": ".venv.Scripts.pywin32_testall",
        "description": ".venv.Scripts.pywin32_testall",
        "peekOfCode": "site_packages = [site.getusersitepackages()] + site.getsitepackages()\nfailures = []\n# Run a test using subprocess and wait for the result.\n# If we get an returncode != 0, we know that there was an error, but we don't\n# abort immediately - we run as many tests as we can.\ndef run_test(script, cmdline_extras):\n    dirname, scriptname = os.path.split(script)\n    # some tests prefer to be run from their directory.\n    cmd = [sys.executable, \"-u\", scriptname] + cmdline_extras\n    print(\"--- Running '%s' ---\" % script)",
        "detail": ".venv.Scripts.pywin32_testall",
        "documentation": {}
    },
    {
        "label": "failures",
        "kind": 5,
        "importPath": ".venv.Scripts.pywin32_testall",
        "description": ".venv.Scripts.pywin32_testall",
        "peekOfCode": "failures = []\n# Run a test using subprocess and wait for the result.\n# If we get an returncode != 0, we know that there was an error, but we don't\n# abort immediately - we run as many tests as we can.\ndef run_test(script, cmdline_extras):\n    dirname, scriptname = os.path.split(script)\n    # some tests prefer to be run from their directory.\n    cmd = [sys.executable, \"-u\", scriptname] + cmdline_extras\n    print(\"--- Running '%s' ---\" % script)\n    sys.stdout.flush()",
        "detail": ".venv.Scripts.pywin32_testall",
        "documentation": {}
    },
    {
        "label": "get_chat_service",
        "kind": 2,
        "importPath": "app.api.v1.chat",
        "description": "app.api.v1.chat",
        "peekOfCode": "def get_chat_service(settings: Settings = Depends(get_settings)) -> ChatService:\n    \"\"\"\"\"\"\n    return ChatService(settings)\n@router.post(\"/\", response_model=ChatResponse, deprecated=True)\nasync def intelligent_chat(\n    request: ChatRequest,\n    chat_service: ChatService = Depends(get_chat_service)\n):\n    \"\"\"\n    ",
        "detail": "app.api.v1.chat",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "app.api.v1.chat",
        "description": "app.api.v1.chat",
        "peekOfCode": "router = APIRouter(prefix=\"/chat\", tags=[\" (Deprecated)\"])\ndef _log_deprecated_warning(endpoint_name: str):\n    \"\"\"deprecated\"\"\"\n    logger.warning(f\" DEPRECATED API: {endpoint_name} -  /api/v1/conversation/v2 API\")\ndef get_chat_service(settings: Settings = Depends(get_settings)) -> ChatService:\n    \"\"\"\"\"\"\n    return ChatService(settings)\n@router.post(\"/\", response_model=ChatResponse, deprecated=True)\nasync def intelligent_chat(\n    request: ChatRequest,",
        "detail": "app.api.v1.chat",
        "documentation": {}
    },
    {
        "label": "get_conversation_service",
        "kind": 2,
        "importPath": "app.api.v1.conversation_v2",
        "description": "app.api.v1.conversation_v2",
        "peekOfCode": "def get_conversation_service(\n    settings: Settings = Depends(get_settings),\n    rag_config_manager: RAGConfigManager = Depends(get_rag_config_manager)\n) -> ConversationService:\n    \"\"\"\"\"\"\n    return ConversationService(settings, rag_config_manager)\n@router.post(\"/chat\", response_model=ChatResponse)\nasync def intelligent_chat(\n    request: ChatRequest,\n    conv_service: ConversationService = Depends(get_conversation_service)",
        "detail": "app.api.v1.conversation_v2",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "app.api.v1.conversation_v2",
        "description": "app.api.v1.conversation_v2",
        "peekOfCode": "router = APIRouter(prefix=\"/conversation/v2\", tags=[\" v2\"])\ndef get_conversation_service(\n    settings: Settings = Depends(get_settings),\n    rag_config_manager: RAGConfigManager = Depends(get_rag_config_manager)\n) -> ConversationService:\n    \"\"\"\"\"\"\n    return ConversationService(settings, rag_config_manager)\n@router.post(\"/chat\", response_model=ChatResponse)\nasync def intelligent_chat(\n    request: ChatRequest,",
        "detail": "app.api.v1.conversation_v2",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app.api.v1.course_materials",
        "description": "app.api.v1.course_materials",
        "peekOfCode": "logger = get_logger(\"course_materials_api\")\nrouter = APIRouter(prefix=\"/course-materials\", tags=[\"\"])\n@router.post(\n    \"/process\",\n    response_model=CourseProcessResponse,\n    summary=\"\",\n    description=\"RAG\"\n)\nasync def process_course_material(\n    file: UploadFile = File(..., description=\".md.txt\"),",
        "detail": "app.api.v1.course_materials",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "app.api.v1.course_materials",
        "description": "app.api.v1.course_materials",
        "peekOfCode": "router = APIRouter(prefix=\"/course-materials\", tags=[\"\"])\n@router.post(\n    \"/process\",\n    response_model=CourseProcessResponse,\n    summary=\"\",\n    description=\"RAG\"\n)\nasync def process_course_material(\n    file: UploadFile = File(..., description=\".md.txt\"),\n    course_id: str = Form(..., description=\"ID\"),",
        "detail": "app.api.v1.course_materials",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app.api.v1.outline",
        "description": "app.api.v1.outline",
        "peekOfCode": "logger = get_logger(\"outline_api\")\nrouter = APIRouter(prefix=\"/outline\", tags=[\"\"])\n# \ntask_storage = {}\n@router.post(\n    \"/generate\",\n    response_model=OutlineGenerateResponse,\n    summary=\"\",\n    description=\"Markdown\"\n)",
        "detail": "app.api.v1.outline",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "app.api.v1.outline",
        "description": "app.api.v1.outline",
        "peekOfCode": "router = APIRouter(prefix=\"/outline\", tags=[\"\"])\n# \ntask_storage = {}\n@router.post(\n    \"/generate\",\n    response_model=OutlineGenerateResponse,\n    summary=\"\",\n    description=\"Markdown\"\n)\nasync def generate_outline(",
        "detail": "app.api.v1.outline",
        "documentation": {}
    },
    {
        "label": "task_storage",
        "kind": 5,
        "importPath": "app.api.v1.outline",
        "description": "app.api.v1.outline",
        "peekOfCode": "task_storage = {}\n@router.post(\n    \"/generate\",\n    response_model=OutlineGenerateResponse,\n    summary=\"\",\n    description=\"Markdown\"\n)\nasync def generate_outline(\n    file: UploadFile = File(..., description=\"Markdown(.md/.txt)\"),\n    course_id: str = Form(..., description=\"ID\"),",
        "detail": "app.api.v1.outline",
        "documentation": {}
    },
    {
        "label": "get_rag_service",
        "kind": 2,
        "importPath": "app.api.v1.rag",
        "description": "app.api.v1.rag",
        "peekOfCode": "def get_rag_service(settings: Settings = Depends(get_settings)) -> RAGService:\n    \"\"\"RAG\"\"\"\n    return RAGService(settings)\n@router.post(\"/index\", response_model=IndexResponse, deprecated=True)\nasync def build_index(\n    file: UploadFile = File(...),\n    course_id: str = Form(...),\n    course_material_id: str = Form(...),\n    course_material_name: str = Form(...),\n    collection_name: str = Form(None),",
        "detail": "app.api.v1.rag",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "app.api.v1.rag",
        "description": "app.api.v1.rag",
        "peekOfCode": "router = APIRouter(prefix=\"/rag\", tags=[\"RAG (Deprecated)\"])\ndef _log_deprecated_warning(endpoint_name: str):\n    \"\"\"deprecated\"\"\"\n    logger.warning(f\" DEPRECATED API: {endpoint_name} -  /api/v1/rag/v2 API\")\ndef get_rag_service(settings: Settings = Depends(get_settings)) -> RAGService:\n    \"\"\"RAG\"\"\"\n    return RAGService(settings)\n@router.post(\"/index\", response_model=IndexResponse, deprecated=True)\nasync def build_index(\n    file: UploadFile = File(...),",
        "detail": "app.api.v1.rag",
        "documentation": {}
    },
    {
        "label": "get_document_indexing_service",
        "kind": 2,
        "importPath": "app.api.v1.rag_v2",
        "description": "app.api.v1.rag_v2",
        "peekOfCode": "def get_document_indexing_service(\n    settings: Settings = Depends(get_settings),\n    rag_config_manager: RAGConfigManager = Depends(get_rag_config_manager)\n) -> DocumentIndexingService:\n    \"\"\"\"\"\"\n    return DocumentIndexingService(settings, rag_config_manager)\n@router.post(\"/index\", response_model=IndexResponse)\nasync def build_index(\n    file: UploadFile = File(...),\n    course_id: str = Form(...),",
        "detail": "app.api.v1.rag_v2",
        "documentation": {}
    },
    {
        "label": "get_collections",
        "kind": 2,
        "importPath": "app.api.v1.rag_v2",
        "description": "app.api.v1.rag_v2",
        "peekOfCode": "def get_collections(\n    doc_service: DocumentIndexingService = Depends(get_document_indexing_service)\n):\n    \"\"\"\n     v2\n    \n    \"\"\"\n    try:\n        collections = doc_service.get_collections()\n        logger.info(f\" v2: {len(collections)} \")",
        "detail": "app.api.v1.rag_v2",
        "documentation": {}
    },
    {
        "label": "get_collection_info",
        "kind": 2,
        "importPath": "app.api.v1.rag_v2",
        "description": "app.api.v1.rag_v2",
        "peekOfCode": "def get_collection_info(\n    collection_name: str,\n    doc_service: DocumentIndexingService = Depends(get_document_indexing_service)\n):\n    \"\"\"\n     v2\n    - **collection_name**: \n    \"\"\"\n    try:\n        collection_info = doc_service.get_collection_info(collection_name)",
        "detail": "app.api.v1.rag_v2",
        "documentation": {}
    },
    {
        "label": "delete_collection",
        "kind": 2,
        "importPath": "app.api.v1.rag_v2",
        "description": "app.api.v1.rag_v2",
        "peekOfCode": "def delete_collection(\n    collection_name: str,\n    doc_service: DocumentIndexingService = Depends(get_document_indexing_service)\n):\n    \"\"\"\n     v2\n    - **collection_name**: \n    \"\"\"\n    try:\n        success = doc_service.delete_collection(collection_name)",
        "detail": "app.api.v1.rag_v2",
        "documentation": {}
    },
    {
        "label": "delete_documents_by_course",
        "kind": 2,
        "importPath": "app.api.v1.rag_v2",
        "description": "app.api.v1.rag_v2",
        "peekOfCode": "def delete_documents_by_course(\n    course_id: str,\n    collection_name: Optional[str] = None,\n    doc_service: DocumentIndexingService = Depends(get_document_indexing_service)\n):\n    \"\"\"\n     v2\n    - **course_id**: ID\n    - **collection_name**: \n    \"\"\"",
        "detail": "app.api.v1.rag_v2",
        "documentation": {}
    },
    {
        "label": "delete_documents_by_material",
        "kind": 2,
        "importPath": "app.api.v1.rag_v2",
        "description": "app.api.v1.rag_v2",
        "peekOfCode": "def delete_documents_by_material(\n    course_id: str,\n    course_material_id: str,\n    collection_name: Optional[str] = None,\n    doc_service: DocumentIndexingService = Depends(get_document_indexing_service)\n):\n    \"\"\"\n     v2\n    - **course_id**: ID\n    - **course_material_id**: ID",
        "detail": "app.api.v1.rag_v2",
        "documentation": {}
    },
    {
        "label": "count_documents",
        "kind": 2,
        "importPath": "app.api.v1.rag_v2",
        "description": "app.api.v1.rag_v2",
        "peekOfCode": "def count_documents(\n    collection_name: str,\n    doc_service: DocumentIndexingService = Depends(get_document_indexing_service)\n):\n    \"\"\"\n     v2\n    - **collection_name**: \n    \"\"\"\n    try:\n        count = doc_service.count_documents(collection_name)",
        "detail": "app.api.v1.rag_v2",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "app.api.v1.rag_v2",
        "description": "app.api.v1.rag_v2",
        "peekOfCode": "router = APIRouter(prefix=\"/rag/v2\", tags=[\"RAG v2\"])\ndef get_document_indexing_service(\n    settings: Settings = Depends(get_settings),\n    rag_config_manager: RAGConfigManager = Depends(get_rag_config_manager)\n) -> DocumentIndexingService:\n    \"\"\"\"\"\"\n    return DocumentIndexingService(settings, rag_config_manager)\n@router.post(\"/index\", response_model=IndexResponse)\nasync def build_index(\n    file: UploadFile = File(...),",
        "detail": "app.api.v1.rag_v2",
        "documentation": {}
    },
    {
        "label": "ensure_directories",
        "kind": 2,
        "importPath": "app.constants.paths",
        "description": "app.constants.paths",
        "peekOfCode": "def ensure_directories():\n    \"\"\"\"\"\"\n    directories = [\n        DATA_DIR,\n        UPLOADS_DIR,\n        OUTPUTS_DIR,\n        TEMP_DIR,\n        OUTLINES_DIR,\n        RAG_DIR,\n        GRAPHRAG_DIR,",
        "detail": "app.constants.paths",
        "documentation": {}
    },
    {
        "label": "PROJECT_ROOT",
        "kind": 5,
        "importPath": "app.constants.paths",
        "description": "app.constants.paths",
        "peekOfCode": "PROJECT_ROOT = Path(__file__).parent.parent.parent\n# \nDATA_DIR = PROJECT_ROOT / \"data\"\nUPLOADS_DIR = DATA_DIR / \"uploads\"\nOUTPUTS_DIR = DATA_DIR / \"outputs\"\nTEMP_DIR = DATA_DIR / \"tmp\"\n# \nOUTLINES_DIR = OUTPUTS_DIR / \"outlines\"\nRAG_DIR = OUTPUTS_DIR / \"rag\"\nGRAPHRAG_DIR = OUTPUTS_DIR / \"graphrag\"",
        "detail": "app.constants.paths",
        "documentation": {}
    },
    {
        "label": "DATA_DIR",
        "kind": 5,
        "importPath": "app.constants.paths",
        "description": "app.constants.paths",
        "peekOfCode": "DATA_DIR = PROJECT_ROOT / \"data\"\nUPLOADS_DIR = DATA_DIR / \"uploads\"\nOUTPUTS_DIR = DATA_DIR / \"outputs\"\nTEMP_DIR = DATA_DIR / \"tmp\"\n# \nOUTLINES_DIR = OUTPUTS_DIR / \"outlines\"\nRAG_DIR = OUTPUTS_DIR / \"rag\"\nGRAPHRAG_DIR = OUTPUTS_DIR / \"graphrag\"\n# \nSCRIPTS_DIR = PROJECT_ROOT / \"scripts\"",
        "detail": "app.constants.paths",
        "documentation": {}
    },
    {
        "label": "UPLOADS_DIR",
        "kind": 5,
        "importPath": "app.constants.paths",
        "description": "app.constants.paths",
        "peekOfCode": "UPLOADS_DIR = DATA_DIR / \"uploads\"\nOUTPUTS_DIR = DATA_DIR / \"outputs\"\nTEMP_DIR = DATA_DIR / \"tmp\"\n# \nOUTLINES_DIR = OUTPUTS_DIR / \"outlines\"\nRAG_DIR = OUTPUTS_DIR / \"rag\"\nGRAPHRAG_DIR = OUTPUTS_DIR / \"graphrag\"\n# \nSCRIPTS_DIR = PROJECT_ROOT / \"scripts\"\n# ",
        "detail": "app.constants.paths",
        "documentation": {}
    },
    {
        "label": "OUTPUTS_DIR",
        "kind": 5,
        "importPath": "app.constants.paths",
        "description": "app.constants.paths",
        "peekOfCode": "OUTPUTS_DIR = DATA_DIR / \"outputs\"\nTEMP_DIR = DATA_DIR / \"tmp\"\n# \nOUTLINES_DIR = OUTPUTS_DIR / \"outlines\"\nRAG_DIR = OUTPUTS_DIR / \"rag\"\nGRAPHRAG_DIR = OUTPUTS_DIR / \"graphrag\"\n# \nSCRIPTS_DIR = PROJECT_ROOT / \"scripts\"\n# \nTESTS_DIR = PROJECT_ROOT / \"tests\"",
        "detail": "app.constants.paths",
        "documentation": {}
    },
    {
        "label": "TEMP_DIR",
        "kind": 5,
        "importPath": "app.constants.paths",
        "description": "app.constants.paths",
        "peekOfCode": "TEMP_DIR = DATA_DIR / \"tmp\"\n# \nOUTLINES_DIR = OUTPUTS_DIR / \"outlines\"\nRAG_DIR = OUTPUTS_DIR / \"rag\"\nGRAPHRAG_DIR = OUTPUTS_DIR / \"graphrag\"\n# \nSCRIPTS_DIR = PROJECT_ROOT / \"scripts\"\n# \nTESTS_DIR = PROJECT_ROOT / \"tests\"\n# ",
        "detail": "app.constants.paths",
        "documentation": {}
    },
    {
        "label": "OUTLINES_DIR",
        "kind": 5,
        "importPath": "app.constants.paths",
        "description": "app.constants.paths",
        "peekOfCode": "OUTLINES_DIR = OUTPUTS_DIR / \"outlines\"\nRAG_DIR = OUTPUTS_DIR / \"rag\"\nGRAPHRAG_DIR = OUTPUTS_DIR / \"graphrag\"\n# \nSCRIPTS_DIR = PROJECT_ROOT / \"scripts\"\n# \nTESTS_DIR = PROJECT_ROOT / \"tests\"\n# \nAPP_DIR = PROJECT_ROOT / \"app\"\n# ",
        "detail": "app.constants.paths",
        "documentation": {}
    },
    {
        "label": "RAG_DIR",
        "kind": 5,
        "importPath": "app.constants.paths",
        "description": "app.constants.paths",
        "peekOfCode": "RAG_DIR = OUTPUTS_DIR / \"rag\"\nGRAPHRAG_DIR = OUTPUTS_DIR / \"graphrag\"\n# \nSCRIPTS_DIR = PROJECT_ROOT / \"scripts\"\n# \nTESTS_DIR = PROJECT_ROOT / \"tests\"\n# \nAPP_DIR = PROJECT_ROOT / \"app\"\n# \nMARKDOWN_EXTENSIONS = [\".md\", \".markdown\"]",
        "detail": "app.constants.paths",
        "documentation": {}
    },
    {
        "label": "GRAPHRAG_DIR",
        "kind": 5,
        "importPath": "app.constants.paths",
        "description": "app.constants.paths",
        "peekOfCode": "GRAPHRAG_DIR = OUTPUTS_DIR / \"graphrag\"\n# \nSCRIPTS_DIR = PROJECT_ROOT / \"scripts\"\n# \nTESTS_DIR = PROJECT_ROOT / \"tests\"\n# \nAPP_DIR = PROJECT_ROOT / \"app\"\n# \nMARKDOWN_EXTENSIONS = [\".md\", \".markdown\"]\nTEXT_EXTENSIONS = [\".txt\"]",
        "detail": "app.constants.paths",
        "documentation": {}
    },
    {
        "label": "SCRIPTS_DIR",
        "kind": 5,
        "importPath": "app.constants.paths",
        "description": "app.constants.paths",
        "peekOfCode": "SCRIPTS_DIR = PROJECT_ROOT / \"scripts\"\n# \nTESTS_DIR = PROJECT_ROOT / \"tests\"\n# \nAPP_DIR = PROJECT_ROOT / \"app\"\n# \nMARKDOWN_EXTENSIONS = [\".md\", \".markdown\"]\nTEXT_EXTENSIONS = [\".txt\"]\nALLOWED_EXTENSIONS = MARKDOWN_EXTENSIONS + TEXT_EXTENSIONS\n#  ()",
        "detail": "app.constants.paths",
        "documentation": {}
    },
    {
        "label": "TESTS_DIR",
        "kind": 5,
        "importPath": "app.constants.paths",
        "description": "app.constants.paths",
        "peekOfCode": "TESTS_DIR = PROJECT_ROOT / \"tests\"\n# \nAPP_DIR = PROJECT_ROOT / \"app\"\n# \nMARKDOWN_EXTENSIONS = [\".md\", \".markdown\"]\nTEXT_EXTENSIONS = [\".txt\"]\nALLOWED_EXTENSIONS = MARKDOWN_EXTENSIONS + TEXT_EXTENSIONS\n#  ()\nMAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB\nCHUNK_SIZE = 8192  # 8KB",
        "detail": "app.constants.paths",
        "documentation": {}
    },
    {
        "label": "APP_DIR",
        "kind": 5,
        "importPath": "app.constants.paths",
        "description": "app.constants.paths",
        "peekOfCode": "APP_DIR = PROJECT_ROOT / \"app\"\n# \nMARKDOWN_EXTENSIONS = [\".md\", \".markdown\"]\nTEXT_EXTENSIONS = [\".txt\"]\nALLOWED_EXTENSIONS = MARKDOWN_EXTENSIONS + TEXT_EXTENSIONS\n#  ()\nMAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB\nCHUNK_SIZE = 8192  # 8KB\n# API \nAPI_V1_PREFIX = \"/api/v1\"",
        "detail": "app.constants.paths",
        "documentation": {}
    },
    {
        "label": "MARKDOWN_EXTENSIONS",
        "kind": 5,
        "importPath": "app.constants.paths",
        "description": "app.constants.paths",
        "peekOfCode": "MARKDOWN_EXTENSIONS = [\".md\", \".markdown\"]\nTEXT_EXTENSIONS = [\".txt\"]\nALLOWED_EXTENSIONS = MARKDOWN_EXTENSIONS + TEXT_EXTENSIONS\n#  ()\nMAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB\nCHUNK_SIZE = 8192  # 8KB\n# API \nAPI_V1_PREFIX = \"/api/v1\"\n# \nPROMPTS_DIR = APP_DIR / \"prompts\"",
        "detail": "app.constants.paths",
        "documentation": {}
    },
    {
        "label": "TEXT_EXTENSIONS",
        "kind": 5,
        "importPath": "app.constants.paths",
        "description": "app.constants.paths",
        "peekOfCode": "TEXT_EXTENSIONS = [\".txt\"]\nALLOWED_EXTENSIONS = MARKDOWN_EXTENSIONS + TEXT_EXTENSIONS\n#  ()\nMAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB\nCHUNK_SIZE = 8192  # 8KB\n# API \nAPI_V1_PREFIX = \"/api/v1\"\n# \nPROMPTS_DIR = APP_DIR / \"prompts\"\nOUTLINE_PROMPT_FILE = PROMPTS_DIR / \"outline_generation.txt\"",
        "detail": "app.constants.paths",
        "documentation": {}
    },
    {
        "label": "ALLOWED_EXTENSIONS",
        "kind": 5,
        "importPath": "app.constants.paths",
        "description": "app.constants.paths",
        "peekOfCode": "ALLOWED_EXTENSIONS = MARKDOWN_EXTENSIONS + TEXT_EXTENSIONS\n#  ()\nMAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB\nCHUNK_SIZE = 8192  # 8KB\n# API \nAPI_V1_PREFIX = \"/api/v1\"\n# \nPROMPTS_DIR = APP_DIR / \"prompts\"\nOUTLINE_PROMPT_FILE = PROMPTS_DIR / \"outline_generation.txt\"\nREFINE_PROMPT_FILE = PROMPTS_DIR / \"outline_refine.txt\"",
        "detail": "app.constants.paths",
        "documentation": {}
    },
    {
        "label": "MAX_FILE_SIZE",
        "kind": 5,
        "importPath": "app.constants.paths",
        "description": "app.constants.paths",
        "peekOfCode": "MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB\nCHUNK_SIZE = 8192  # 8KB\n# API \nAPI_V1_PREFIX = \"/api/v1\"\n# \nPROMPTS_DIR = APP_DIR / \"prompts\"\nOUTLINE_PROMPT_FILE = PROMPTS_DIR / \"outline_generation.txt\"\nREFINE_PROMPT_FILE = PROMPTS_DIR / \"outline_refine.txt\"\ndef ensure_directories():\n    \"\"\"\"\"\"",
        "detail": "app.constants.paths",
        "documentation": {}
    },
    {
        "label": "CHUNK_SIZE",
        "kind": 5,
        "importPath": "app.constants.paths",
        "description": "app.constants.paths",
        "peekOfCode": "CHUNK_SIZE = 8192  # 8KB\n# API \nAPI_V1_PREFIX = \"/api/v1\"\n# \nPROMPTS_DIR = APP_DIR / \"prompts\"\nOUTLINE_PROMPT_FILE = PROMPTS_DIR / \"outline_generation.txt\"\nREFINE_PROMPT_FILE = PROMPTS_DIR / \"outline_refine.txt\"\ndef ensure_directories():\n    \"\"\"\"\"\"\n    directories = [",
        "detail": "app.constants.paths",
        "documentation": {}
    },
    {
        "label": "API_V1_PREFIX",
        "kind": 5,
        "importPath": "app.constants.paths",
        "description": "app.constants.paths",
        "peekOfCode": "API_V1_PREFIX = \"/api/v1\"\n# \nPROMPTS_DIR = APP_DIR / \"prompts\"\nOUTLINE_PROMPT_FILE = PROMPTS_DIR / \"outline_generation.txt\"\nREFINE_PROMPT_FILE = PROMPTS_DIR / \"outline_refine.txt\"\ndef ensure_directories():\n    \"\"\"\"\"\"\n    directories = [\n        DATA_DIR,\n        UPLOADS_DIR,",
        "detail": "app.constants.paths",
        "documentation": {}
    },
    {
        "label": "PROMPTS_DIR",
        "kind": 5,
        "importPath": "app.constants.paths",
        "description": "app.constants.paths",
        "peekOfCode": "PROMPTS_DIR = APP_DIR / \"prompts\"\nOUTLINE_PROMPT_FILE = PROMPTS_DIR / \"outline_generation.txt\"\nREFINE_PROMPT_FILE = PROMPTS_DIR / \"outline_refine.txt\"\ndef ensure_directories():\n    \"\"\"\"\"\"\n    directories = [\n        DATA_DIR,\n        UPLOADS_DIR,\n        OUTPUTS_DIR,\n        TEMP_DIR,",
        "detail": "app.constants.paths",
        "documentation": {}
    },
    {
        "label": "OUTLINE_PROMPT_FILE",
        "kind": 5,
        "importPath": "app.constants.paths",
        "description": "app.constants.paths",
        "peekOfCode": "OUTLINE_PROMPT_FILE = PROMPTS_DIR / \"outline_generation.txt\"\nREFINE_PROMPT_FILE = PROMPTS_DIR / \"outline_refine.txt\"\ndef ensure_directories():\n    \"\"\"\"\"\"\n    directories = [\n        DATA_DIR,\n        UPLOADS_DIR,\n        OUTPUTS_DIR,\n        TEMP_DIR,\n        OUTLINES_DIR,",
        "detail": "app.constants.paths",
        "documentation": {}
    },
    {
        "label": "REFINE_PROMPT_FILE",
        "kind": 5,
        "importPath": "app.constants.paths",
        "description": "app.constants.paths",
        "peekOfCode": "REFINE_PROMPT_FILE = PROMPTS_DIR / \"outline_refine.txt\"\ndef ensure_directories():\n    \"\"\"\"\"\"\n    directories = [\n        DATA_DIR,\n        UPLOADS_DIR,\n        OUTPUTS_DIR,\n        TEMP_DIR,\n        OUTLINES_DIR,\n        RAG_DIR,",
        "detail": "app.constants.paths",
        "documentation": {}
    },
    {
        "label": "Settings",
        "kind": 6,
        "importPath": "app.core.config",
        "description": "app.core.config",
        "peekOfCode": "class Settings(BaseSettings):\n    \"\"\"\"\"\"\n    # OpenAI API \n    api_key: str = Field(..., description=\"OpenAI API\")\n    base_url: str = Field(default=\"https://api.openai.com/v1\", description=\"OpenAI APIURL\")\n    outline_model: str = Field(default=\"gpt-4o-mini\", description=\"\")\n    refine_model: str = Field(default=\"gpt-4o-mini\", description=\"\")\n    # RAG \n    rag_embed_model: str = Field(default=\"text-embedding-3-small\", description=\"RAG\")\n    rag_llm_model: str = Field(default=\"gpt-4o-mini\", description=\"RAG\")",
        "detail": "app.core.config",
        "documentation": {}
    },
    {
        "label": "get_settings",
        "kind": 2,
        "importPath": "app.core.config",
        "description": "app.core.config",
        "peekOfCode": "def get_settings() -> Settings:\n    \"\"\" - \"\"\"\n    return settings",
        "detail": "app.core.config",
        "documentation": {}
    },
    {
        "label": "settings",
        "kind": 5,
        "importPath": "app.core.config",
        "description": "app.core.config",
        "peekOfCode": "settings = Settings()\ndef get_settings() -> Settings:\n    \"\"\" - \"\"\"\n    return settings",
        "detail": "app.core.config",
        "documentation": {}
    },
    {
        "label": "get_current_settings",
        "kind": 2,
        "importPath": "app.core.deps",
        "description": "app.core.deps",
        "peekOfCode": "def get_current_settings() -> Settings:\n    \"\"\" - \"\"\"\n    return get_settings()\nasync def validate_upload_file(\n    file: UploadFile,\n    settings: Settings = Depends(get_current_settings)\n) -> UploadFile:\n    \"\"\"\"\"\"\n    # \n    if not file:",
        "detail": "app.core.deps",
        "documentation": {}
    },
    {
        "label": "generate_task_id",
        "kind": 2,
        "importPath": "app.core.deps",
        "description": "app.core.deps",
        "peekOfCode": "def generate_task_id() -> str:\n    \"\"\"ID\"\"\"\n    return str(uuid.uuid4())\ndef generate_filename(original_filename: str, task_id: str = None) -> str:\n    \"\"\"\"\"\"\n    if not task_id:\n        task_id = generate_task_id()\n    # \n    file_ext = Path(original_filename).suffix\n    # ",
        "detail": "app.core.deps",
        "documentation": {}
    },
    {
        "label": "generate_filename",
        "kind": 2,
        "importPath": "app.core.deps",
        "description": "app.core.deps",
        "peekOfCode": "def generate_filename(original_filename: str, task_id: str = None) -> str:\n    \"\"\"\"\"\"\n    if not task_id:\n        task_id = generate_task_id()\n    # \n    file_ext = Path(original_filename).suffix\n    # \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    # : timestamp_taskid_original.ext\n    safe_original = Path(original_filename).stem[:50]  # ",
        "detail": "app.core.deps",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app.core.deps",
        "description": "app.core.deps",
        "peekOfCode": "logger = get_logger(\"deps\")\ndef get_current_settings() -> Settings:\n    \"\"\" - \"\"\"\n    return get_settings()\nasync def validate_upload_file(\n    file: UploadFile,\n    settings: Settings = Depends(get_current_settings)\n) -> UploadFile:\n    \"\"\"\"\"\"\n    # ",
        "detail": "app.core.deps",
        "documentation": {}
    },
    {
        "label": "json_formatter",
        "kind": 2,
        "importPath": "app.core.logging",
        "description": "app.core.logging",
        "peekOfCode": "def json_formatter(record: Dict[str, Any]) -> str:\n    \"\"\"JSON\"\"\"\n    log_entry = {\n        \"time\": record[\"time\"].isoformat(),\n        \"level\": record[\"level\"].name,\n        \"message\": record[\"message\"],\n        \"module\": record[\"name\"],\n        \"function\": record[\"function\"],\n        \"line\": record[\"line\"],\n    }",
        "detail": "app.core.logging",
        "documentation": {}
    },
    {
        "label": "text_formatter",
        "kind": 2,
        "importPath": "app.core.logging",
        "description": "app.core.logging",
        "peekOfCode": "def text_formatter(record: Dict[str, Any]) -> str:\n    \"\"\"\"\"\"\n    return (\n        \"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | \"\n        \"<level>{level: <8}</level> | \"\n        \"<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> | \"\n        \"<level>{message}</level>\\n\"\n    )\ndef setup_logging():\n    \"\"\"\"\"\"",
        "detail": "app.core.logging",
        "documentation": {}
    },
    {
        "label": "setup_logging",
        "kind": 2,
        "importPath": "app.core.logging",
        "description": "app.core.logging",
        "peekOfCode": "def setup_logging():\n    \"\"\"\"\"\"\n    settings = get_settings()\n    # \n    logger.remove()\n    #  -  JSON \n    formatter = text_formatter\n    # \n    logger.add(\n        sys.stdout,",
        "detail": "app.core.logging",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "kind": 2,
        "importPath": "app.core.logging",
        "description": "app.core.logging",
        "peekOfCode": "def get_logger(name: str = None):\n    \"\"\"\"\"\"\n    if name:\n        return logger.bind(name=name)\n    return logger\n# \napp_logger = get_logger(\"app\")",
        "detail": "app.core.logging",
        "documentation": {}
    },
    {
        "label": "app_logger",
        "kind": 5,
        "importPath": "app.core.logging",
        "description": "app.core.logging",
        "peekOfCode": "app_logger = get_logger(\"app\")",
        "detail": "app.core.logging",
        "documentation": {}
    },
    {
        "label": "QdrantRepository",
        "kind": 6,
        "importPath": "app.repositories.rag_repository",
        "description": "app.repositories.rag_repository",
        "peekOfCode": "class QdrantRepository:\n    \"\"\"Qdrant\"\"\"\n    def __init__(self, settings: Settings):\n        \"\"\"Qdrant\"\"\"\n        self.settings = settings\n        self.client = None\n        self._initialize_client()\n    def _initialize_client(self):\n        \"\"\"Qdrant\"\"\"\n        try:",
        "detail": "app.repositories.rag_repository",
        "documentation": {}
    },
    {
        "label": "rag_repository",
        "kind": 5,
        "importPath": "app.repositories.rag_repository",
        "description": "app.repositories.rag_repository",
        "peekOfCode": "rag_repository = QdrantRepository(get_settings())",
        "detail": "app.repositories.rag_repository",
        "documentation": {}
    },
    {
        "label": "ProcessingStatus",
        "kind": 6,
        "importPath": "app.schemas.course_materials",
        "description": "app.schemas.course_materials",
        "peekOfCode": "class ProcessingStatus(str, Enum):\n    \"\"\"\"\"\"\n    UPLOADING = \"uploading\"                    # \n    OUTLINE_GENERATING = \"outline_generating\"  # \n    RAG_INDEXING = \"rag_indexing\"             # RAG\n    COMPLETED = \"completed\"                    # \n    FAILED = \"failed\"                         # \nclass ProcessingStep(BaseModel):\n    \"\"\"\"\"\"\n    step_name: str = Field(..., description=\"\")",
        "detail": "app.schemas.course_materials",
        "documentation": {}
    },
    {
        "label": "ProcessingStep",
        "kind": 6,
        "importPath": "app.schemas.course_materials",
        "description": "app.schemas.course_materials",
        "peekOfCode": "class ProcessingStep(BaseModel):\n    \"\"\"\"\"\"\n    step_name: str = Field(..., description=\"\")\n    status: ProcessingStatus = Field(..., description=\"\")\n    message: str = Field(..., description=\"\")\n    start_time: Optional[datetime] = Field(None, description=\"\")\n    end_time: Optional[datetime] = Field(None, description=\"\")\n    error_message: Optional[str] = Field(None, description=\"\")\nclass CourseProcessRequest(BaseModel):\n    \"\"\"\"\"\"",
        "detail": "app.schemas.course_materials",
        "documentation": {}
    },
    {
        "label": "CourseProcessRequest",
        "kind": 6,
        "importPath": "app.schemas.course_materials",
        "description": "app.schemas.course_materials",
        "peekOfCode": "class CourseProcessRequest(BaseModel):\n    \"\"\"\"\"\"\n    # \n    course_id: str = Field(\n        ...,\n        description=\"ID\",\n        min_length=1,\n        max_length=50\n    )\n    course_material_id: str = Field(",
        "detail": "app.schemas.course_materials",
        "documentation": {}
    },
    {
        "label": "CourseProcessResponse",
        "kind": 6,
        "importPath": "app.schemas.course_materials",
        "description": "app.schemas.course_materials",
        "peekOfCode": "class CourseProcessResponse(BaseModel):\n    \"\"\"\"\"\"\n    # \n    task_id: str = Field(..., description=\"ID\")\n    status: ProcessingStatus = Field(..., description=\"\")\n    message: str = Field(..., description=\"\")\n    # \n    current_step: str = Field(..., description=\"\")\n    completed_steps: int = Field(default=0, description=\"\")\n    total_steps: int = Field(default=3, description=\"\")",
        "detail": "app.schemas.course_materials",
        "documentation": {}
    },
    {
        "label": "TaskStatusQuery",
        "kind": 6,
        "importPath": "app.schemas.course_materials",
        "description": "app.schemas.course_materials",
        "peekOfCode": "class TaskStatusQuery(BaseModel):\n    \"\"\"\"\"\"\n    task_id: str = Field(..., description=\"ID\")\n    status: ProcessingStatus = Field(..., description=\"\")\n    message: str = Field(..., description=\"\")\n    # \n    current_step: str = Field(..., description=\"\")\n    completed_steps: int = Field(..., description=\"\")\n    total_steps: int = Field(..., description=\"\")\n    progress_percentage: float = Field(..., description=\"\")",
        "detail": "app.schemas.course_materials",
        "documentation": {}
    },
    {
        "label": "CleanupOperation",
        "kind": 6,
        "importPath": "app.schemas.course_materials",
        "description": "app.schemas.course_materials",
        "peekOfCode": "class CleanupOperation(BaseModel):\n    \"\"\"\"\"\"\n    operation_type: str = Field(..., description=\"\")\n    target: str = Field(..., description=\"\")\n    success: bool = Field(..., description=\"\")\n    message: str = Field(..., description=\"\")\n    details: Optional[str] = Field(None, description=\"\")\nclass CleanupRequest(BaseModel):\n    \"\"\"\"\"\"\n    course_id: str = Field(..., description=\"ID\")",
        "detail": "app.schemas.course_materials",
        "documentation": {}
    },
    {
        "label": "CleanupRequest",
        "kind": 6,
        "importPath": "app.schemas.course_materials",
        "description": "app.schemas.course_materials",
        "peekOfCode": "class CleanupRequest(BaseModel):\n    \"\"\"\"\"\"\n    course_id: str = Field(..., description=\"ID\")\n    course_material_id: Optional[str] = Field(None, description=\"ID\")\n    cleanup_files: bool = Field(default=True, description=\"\")\n    cleanup_rag_data: bool = Field(default=True, description=\"RAG\")\n    cleanup_task_data: bool = Field(default=True, description=\"\")\n    force_cleanup: bool = Field(default=False, description=\"\")\nclass CleanupResponse(BaseModel):\n    \"\"\"\"\"\"",
        "detail": "app.schemas.course_materials",
        "documentation": {}
    },
    {
        "label": "CleanupResponse",
        "kind": 6,
        "importPath": "app.schemas.course_materials",
        "description": "app.schemas.course_materials",
        "peekOfCode": "class CleanupResponse(BaseModel):\n    \"\"\"\"\"\"\n    success: bool = Field(..., description=\"\")\n    message: str = Field(..., description=\"\")\n    # \n    course_id: str = Field(..., description=\"ID\")\n    course_material_id: Optional[str] = Field(None, description=\"ID\")\n    # \n    operations: List[CleanupOperation] = Field(default_factory=list, description=\"\")\n    # ",
        "detail": "app.schemas.course_materials",
        "documentation": {}
    },
    {
        "label": "TaskStatus",
        "kind": 6,
        "importPath": "app.schemas.outline",
        "description": "app.schemas.outline",
        "peekOfCode": "class TaskStatus(str, Enum):\n    \"\"\"\"\"\"\n    PENDING = \"pending\"      # \n    PROCESSING = \"processing\"  # \n    COMPLETED = \"completed\"   # \n    FAILED = \"failed\"        # \nclass OutlineGenerateRequest(BaseModel):\n    \"\"\"\"\"\"\n    #  FastAPI  UploadFile \n    # ",
        "detail": "app.schemas.outline",
        "documentation": {}
    },
    {
        "label": "OutlineGenerateRequest",
        "kind": 6,
        "importPath": "app.schemas.outline",
        "description": "app.schemas.outline",
        "peekOfCode": "class OutlineGenerateRequest(BaseModel):\n    \"\"\"\"\"\"\n    #  FastAPI  UploadFile \n    # \n    course_id: str = Field(\n        ...,\n        description=\"ID\",\n        min_length=1,\n        max_length=50\n    )",
        "detail": "app.schemas.outline",
        "documentation": {}
    },
    {
        "label": "OutlineGenerateResponse",
        "kind": 6,
        "importPath": "app.schemas.outline",
        "description": "app.schemas.outline",
        "peekOfCode": "class OutlineGenerateResponse(BaseModel):\n    \"\"\"\"\"\"\n    task_id: str = Field(..., description=\"ID\")\n    status: TaskStatus = Field(..., description=\"\")\n    message: str = Field(..., description=\"\")\n    # \n    course_id: Optional[str] = Field(None, description=\"ID\")\n    course_material_id: Optional[str] = Field(None, description=\"ID\")\n    material_name: Optional[str] = Field(None, description=\"\")\n    # ",
        "detail": "app.schemas.outline",
        "documentation": {}
    },
    {
        "label": "OutlineTaskQuery",
        "kind": 6,
        "importPath": "app.schemas.outline",
        "description": "app.schemas.outline",
        "peekOfCode": "class OutlineTaskQuery(BaseModel):\n    \"\"\"\"\"\"\n    task_id: str = Field(..., description=\"ID\")\n    status: TaskStatus = Field(..., description=\"\")\n    message: str = Field(..., description=\"\")\n    # \n    course_id: Optional[str] = Field(None, description=\"ID\")\n    course_material_id: Optional[str] = Field(None, description=\"ID\")\n    material_name: Optional[str] = Field(None, description=\"\")\n    # ",
        "detail": "app.schemas.outline",
        "documentation": {}
    },
    {
        "label": "OutlineFileResponse",
        "kind": 6,
        "importPath": "app.schemas.outline",
        "description": "app.schemas.outline",
        "peekOfCode": "class OutlineFileResponse(BaseModel):\n    \"\"\"outline\"\"\"\n    success: bool = Field(..., description=\"\")\n    message: str = Field(..., description=\"\")\n    # \n    course_id: str = Field(..., description=\"ID\")\n    course_material_id: str = Field(..., description=\"ID\")\n    material_name: Optional[str] = Field(None, description=\"\")\n    # \n    file_path: str = Field(..., description=\"\")",
        "detail": "app.schemas.outline",
        "documentation": {}
    },
    {
        "label": "ErrorResponse",
        "kind": 6,
        "importPath": "app.schemas.outline",
        "description": "app.schemas.outline",
        "peekOfCode": "class ErrorResponse(BaseModel):\n    \"\"\"\"\"\"\n    error: str = Field(..., description=\"\")\n    message: str = Field(..., description=\"\")\n    detail: Optional[str] = Field(None, description=\"\")\n    task_id: Optional[str] = Field(None, description=\"ID\")\n    timestamp: datetime = Field(default_factory=datetime.now, description=\"\")\n    class Config:\n        json_schema_extra = {\n            \"example\": {",
        "detail": "app.schemas.outline",
        "documentation": {}
    },
    {
        "label": "HealthResponse",
        "kind": 6,
        "importPath": "app.schemas.outline",
        "description": "app.schemas.outline",
        "peekOfCode": "class HealthResponse(BaseModel):\n    \"\"\"\"\"\"\n    status: str = Field(..., description=\"\")\n    timestamp: datetime = Field(default_factory=datetime.now, description=\"\")\n    version: str = Field(default=\"0.1.0\", description=\"\")\n    # \n    uptime: Optional[float] = Field(None, description=\"()\")\n    # \n    openai_api: Optional[str] = Field(None, description=\"OpenAI API\")\n    class Config:",
        "detail": "app.schemas.outline",
        "documentation": {}
    },
    {
        "label": "ChatMode",
        "kind": 6,
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "peekOfCode": "class ChatMode(str, Enum):\n    \"\"\"\"\"\"\n    QUERY = \"query\"  # condense_question ChatEngine\n    CHAT = \"chat\"    # simple ChatEngine\nclass ChatEngineType(str, Enum):\n    \"\"\"\"\"\"\n    CONDENSE_PLUS_CONTEXT = \"condense_plus_context\"  # \n    SIMPLE = \"simple\"  # \nclass DocumentMetadata(BaseModel):\n    \"\"\"\"\"\"",
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "ChatEngineType",
        "kind": 6,
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "peekOfCode": "class ChatEngineType(str, Enum):\n    \"\"\"\"\"\"\n    CONDENSE_PLUS_CONTEXT = \"condense_plus_context\"  # \n    SIMPLE = \"simple\"  # \nclass DocumentMetadata(BaseModel):\n    \"\"\"\"\"\"\n    course_id: str = Field(..., description=\"ID\")\n    course_material_id: str = Field(..., description=\"ID\")\n    course_material_name: str = Field(..., description=\"\")\n    file_path: Optional[str] = Field(None, description=\"\")",
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "DocumentMetadata",
        "kind": 6,
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "peekOfCode": "class DocumentMetadata(BaseModel):\n    \"\"\"\"\"\"\n    course_id: str = Field(..., description=\"ID\")\n    course_material_id: str = Field(..., description=\"ID\")\n    course_material_name: str = Field(..., description=\"\")\n    file_path: Optional[str] = Field(None, description=\"\")\n    file_size: Optional[int] = Field(None, description=\"\")\n    upload_time: Optional[str] = Field(None, description=\"\")\nclass IndexRequest(BaseModel):\n    \"\"\"\"\"\"",
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "IndexRequest",
        "kind": 6,
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "peekOfCode": "class IndexRequest(BaseModel):\n    \"\"\"\"\"\"\n    file_content: str = Field(..., description=\"\")\n    metadata: DocumentMetadata = Field(..., description=\"\")\n    collection_name: Optional[str] = Field(None, description=\"\")\nclass ChatMessage(BaseModel):\n    \"\"\"\"\"\"\n    role: str = Field(..., description=\"user/assistant\")\n    content: str = Field(..., description=\"\")\nclass ChatMemory(BaseModel):",
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "kind": 6,
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "peekOfCode": "class ChatMessage(BaseModel):\n    \"\"\"\"\"\"\n    role: str = Field(..., description=\"user/assistant\")\n    content: str = Field(..., description=\"\")\nclass ChatMemory(BaseModel):\n    \"\"\"\"\"\"\n    messages: List[ChatMessage] = Field(default_factory=list, description=\"\")\n    summary: Optional[str] = Field(None, description=\"\")\n    token_count: int = Field(default=0, description=\"Token\")\nclass QueryRequest(BaseModel):",
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "ChatMemory",
        "kind": 6,
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "peekOfCode": "class ChatMemory(BaseModel):\n    \"\"\"\"\"\"\n    messages: List[ChatMessage] = Field(default_factory=list, description=\"\")\n    summary: Optional[str] = Field(None, description=\"\")\n    token_count: int = Field(default=0, description=\"Token\")\nclass QueryRequest(BaseModel):\n    \"\"\"\"\"\"\n    question: str = Field(..., description=\"\")\n    mode: ChatMode = Field(default=ChatMode.QUERY, description=\"\")\n    course_id: Optional[str] = Field(None, description=\"ID\")",
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "QueryRequest",
        "kind": 6,
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "peekOfCode": "class QueryRequest(BaseModel):\n    \"\"\"\"\"\"\n    question: str = Field(..., description=\"\")\n    mode: ChatMode = Field(default=ChatMode.QUERY, description=\"\")\n    course_id: Optional[str] = Field(None, description=\"ID\")\n    chat_memory: Optional[ChatMemory] = Field(None, description=\"\")\n    collection_name: Optional[str] = Field(None, description=\"\")\n    top_k: Optional[int] = Field(None, description=\"Top-K\")\nclass SourceInfo(BaseModel):\n    \"\"\"\"\"\"",
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "SourceInfo",
        "kind": 6,
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "peekOfCode": "class SourceInfo(BaseModel):\n    \"\"\"\"\"\"\n    course_id: str = Field(..., description=\"ID\")\n    course_material_id: str = Field(..., description=\"ID\")\n    course_material_name: str = Field(..., description=\"\")\n    chunk_text: str = Field(..., description=\"\")\n    score: float = Field(..., description=\"\")\nclass QueryResponse(BaseModel):\n    \"\"\"\"\"\"\n    answer: str = Field(..., description=\"\")",
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "QueryResponse",
        "kind": 6,
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "peekOfCode": "class QueryResponse(BaseModel):\n    \"\"\"\"\"\"\n    answer: str = Field(..., description=\"\")\n    sources: List[SourceInfo] = Field(default_factory=list, description=\"\")\n    chat_memory: Optional[ChatMemory] = Field(None, description=\"\")\n    mode: ChatMode = Field(..., description=\"\")\n    processing_time: float = Field(..., description=\"\")\nclass IndexResponse(BaseModel):\n    \"\"\"\"\"\"\n    success: bool = Field(..., description=\"\")",
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "IndexResponse",
        "kind": 6,
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "peekOfCode": "class IndexResponse(BaseModel):\n    \"\"\"\"\"\"\n    success: bool = Field(..., description=\"\")\n    message: str = Field(..., description=\"\")\n    document_count: int = Field(default=0, description=\"\")\n    chunk_count: int = Field(default=0, description=\"\")\n    processing_time: float = Field(..., description=\"\")\n    collection_name: str = Field(..., description=\"\")\nclass CollectionInfo(BaseModel):\n    \"\"\"\"\"\"",
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "CollectionInfo",
        "kind": 6,
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "peekOfCode": "class CollectionInfo(BaseModel):\n    \"\"\"\"\"\"\n    name: str = Field(..., description=\"\")\n    vectors_count: int = Field(..., description=\"\")\n    indexed_only: bool = Field(..., description=\"\")\n    payload_schema: Dict[str, Any] = Field(default_factory=dict, description=\"\")\nclass CollectionListResponse(BaseModel):\n    \"\"\"\"\"\"\n    collections: List[CollectionInfo] = Field(..., description=\"\")\n    total_count: int = Field(..., description=\"\")",
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "CollectionListResponse",
        "kind": 6,
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "peekOfCode": "class CollectionListResponse(BaseModel):\n    \"\"\"\"\"\"\n    collections: List[CollectionInfo] = Field(..., description=\"\")\n    total_count: int = Field(..., description=\"\")\nclass DeleteCollectionResponse(BaseModel):\n    \"\"\"\"\"\"\n    success: bool = Field(..., description=\"\")\n    message: str = Field(..., description=\"\")\n    collection_name: str = Field(..., description=\"\")\n# ",
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "DeleteCollectionResponse",
        "kind": 6,
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "peekOfCode": "class DeleteCollectionResponse(BaseModel):\n    \"\"\"\"\"\"\n    success: bool = Field(..., description=\"\")\n    message: str = Field(..., description=\"\")\n    collection_name: str = Field(..., description=\"\")\n# \nclass ChatRequest(BaseModel):\n    \"\"\"\"\"\"\n    conversation_id: str = Field(..., description=\"IDRedis\")\n    course_id: Optional[str] = Field(None, description=\"IDcourse_material_id\")",
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "ChatRequest",
        "kind": 6,
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "peekOfCode": "class ChatRequest(BaseModel):\n    \"\"\"\"\"\"\n    conversation_id: str = Field(..., description=\"IDRedis\")\n    course_id: Optional[str] = Field(None, description=\"IDcourse_material_id\")\n    course_material_id: Optional[str] = Field(None, description=\"IDcourse_id\")\n    chat_engine_type: ChatEngineType = Field(..., description=\"\")\n    question: str = Field(..., description=\"\")\n    collection_name: Optional[str] = Field(None, description=\"\")\nclass ChatResponse(BaseModel):\n    \"\"\"\"\"\"",
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "kind": 6,
        "importPath": "app.schemas.rag",
        "description": "app.schemas.rag",
        "peekOfCode": "class ChatResponse(BaseModel):\n    \"\"\"\"\"\"\n    answer: str = Field(..., description=\"AI\")\n    sources: List[SourceInfo] = Field(default_factory=list, description=\"condense_plus_context\")\n    conversation_id: str = Field(..., description=\"ID\")\n    chat_engine_type: ChatEngineType = Field(..., description=\"\")\n    filter_info: Optional[str] = Field(None, description=\"\")\n    processing_time: float = Field(..., description=\"\")",
        "detail": "app.schemas.rag",
        "documentation": {}
    },
    {
        "label": "CleanupService",
        "kind": 6,
        "importPath": "app.services.course_material.cleanup_service",
        "description": "app.services.course_material.cleanup_service",
        "peekOfCode": "class CleanupService:\n    \"\"\"\"\"\"\n    def __init__(self):\n        self.settings = get_settings()\n    async def cleanup_course_material(\n        self,\n        request: CleanupRequest\n    ) -> CleanupResponse:\n        \"\"\"\n        ",
        "detail": "app.services.course_material.cleanup_service",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app.services.course_material.cleanup_service",
        "description": "app.services.course_material.cleanup_service",
        "peekOfCode": "logger = get_logger(\"cleanup_service\")\nclass CleanupService:\n    \"\"\"\"\"\"\n    def __init__(self):\n        self.settings = get_settings()\n    async def cleanup_course_material(\n        self,\n        request: CleanupRequest\n    ) -> CleanupResponse:\n        \"\"\"",
        "detail": "app.services.course_material.cleanup_service",
        "documentation": {}
    },
    {
        "label": "cleanup_service",
        "kind": 5,
        "importPath": "app.services.course_material.cleanup_service",
        "description": "app.services.course_material.cleanup_service",
        "peekOfCode": "cleanup_service = CleanupService()",
        "detail": "app.services.course_material.cleanup_service",
        "documentation": {}
    },
    {
        "label": "CourseMaterialProcessService",
        "kind": 6,
        "importPath": "app.services.course_material.course_material_process_service",
        "description": "app.services.course_material.course_material_process_service",
        "peekOfCode": "class CourseMaterialProcessService:\n    \"\"\"\"\"\"\n    def __init__(self):\n        self.settings = get_settings()\n        # \n        self.task_storage: Dict[str, CourseProcessResponse] = {}\n        # \n        rag_config_manager = get_rag_config_manager()\n        self.document_indexing_service = DocumentIndexingService(self.settings, rag_config_manager)\n    async def process_course_material(",
        "detail": "app.services.course_material.course_material_process_service",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app.services.course_material.course_material_process_service",
        "description": "app.services.course_material.course_material_process_service",
        "peekOfCode": "logger = get_logger(\"course_material_process_service\")\nclass CourseMaterialProcessService:\n    \"\"\"\"\"\"\n    def __init__(self):\n        self.settings = get_settings()\n        # \n        self.task_storage: Dict[str, CourseProcessResponse] = {}\n        # \n        rag_config_manager = get_rag_config_manager()\n        self.document_indexing_service = DocumentIndexingService(self.settings, rag_config_manager)",
        "detail": "app.services.course_material.course_material_process_service",
        "documentation": {}
    },
    {
        "label": "course_material_process_service",
        "kind": 5,
        "importPath": "app.services.course_material.course_material_process_service",
        "description": "app.services.course_material.course_material_process_service",
        "peekOfCode": "course_material_process_service = CourseMaterialProcessService()",
        "detail": "app.services.course_material.course_material_process_service",
        "documentation": {}
    },
    {
        "label": "ChatService",
        "kind": 6,
        "importPath": "app.services.legacy.chat_service",
        "description": "app.services.legacy.chat_service",
        "peekOfCode": "class ChatService:\n    \"\"\" - DEPRECATED\"\"\"\n    def __init__(self, settings: AppSettings):\n        \"\"\"\"\"\"\n        self.settings = settings\n        self._setup_llama_index()\n        self._setup_vector_index()\n        self._load_prompts()\n    def _setup_llama_index(self):\n        \"\"\"LlamaIndex\"\"\"",
        "detail": "app.services.legacy.chat_service",
        "documentation": {}
    },
    {
        "label": "RAGService",
        "kind": 6,
        "importPath": "app.services.legacy.rag_service",
        "description": "app.services.legacy.rag_service",
        "peekOfCode": "class RAGService:\n    \"\"\"RAG - DEPRECATED\"\"\"\n    def __init__(self, settings: AppSettings):\n        \"\"\"RAG\"\"\"\n        self.settings = settings\n        self.qdrant_repo = QdrantRepository(settings)\n        self._setup_llama_index()\n        self._load_prompts()\n    def _setup_llama_index(self):\n        \"\"\"LlamaIndex\"\"\"",
        "detail": "app.services.legacy.rag_service",
        "documentation": {}
    },
    {
        "label": "rag_service",
        "kind": 5,
        "importPath": "app.services.legacy.rag_service",
        "description": "app.services.legacy.rag_service",
        "peekOfCode": "rag_service = RAGService(get_settings())",
        "detail": "app.services.legacy.rag_service",
        "documentation": {}
    },
    {
        "label": "ConversationMemoryManager",
        "kind": 6,
        "importPath": "app.services.rag.conversation_service",
        "description": "app.services.rag.conversation_service",
        "peekOfCode": "class ConversationMemoryManager:\n    \"\"\"\"\"\"\n    def __init__(self, rag_config_manager: RAGConfigManager):\n        \"\"\"\n        \n        Args:\n            rag_config_manager: RAG\n        \"\"\"\n        self.rag_config_manager = rag_config_manager\n        self._load_prompts()",
        "detail": "app.services.rag.conversation_service",
        "documentation": {}
    },
    {
        "label": "ChatEngineFactory",
        "kind": 6,
        "importPath": "app.services.rag.conversation_service",
        "description": "app.services.rag.conversation_service",
        "peekOfCode": "class ChatEngineFactory:\n    \"\"\"\"\"\"\n    def __init__(self, app_settings: AppSettings, rag_config_manager: RAGConfigManager):\n        \"\"\"\n        \n        Args:\n            app_settings: \n            rag_config_manager: RAG\n        \"\"\"\n        self.app_settings = app_settings",
        "detail": "app.services.rag.conversation_service",
        "documentation": {}
    },
    {
        "label": "ConversationService",
        "kind": 6,
        "importPath": "app.services.rag.conversation_service",
        "description": "app.services.rag.conversation_service",
        "peekOfCode": "class ConversationService:\n    \"\"\"\"\"\"\n    def __init__(self, app_settings: AppSettings, rag_config_manager: RAGConfigManager):\n        \"\"\"\n        \n        Args:\n            app_settings: \n            rag_config_manager: RAG\n        \"\"\"\n        self.app_settings = app_settings",
        "detail": "app.services.rag.conversation_service",
        "documentation": {}
    },
    {
        "label": "DocumentIndexingService",
        "kind": 6,
        "importPath": "app.services.rag.document_indexing_service",
        "description": "app.services.rag.document_indexing_service",
        "peekOfCode": "class DocumentIndexingService:\n    \"\"\"\"\"\"\n    def __init__(self, app_settings: AppSettings, rag_config_manager: RAGConfigManager):\n        \"\"\"\n        \n        Args:\n            app_settings: \n            rag_config_manager: RAG\n        \"\"\"\n        self.app_settings = app_settings",
        "detail": "app.services.rag.document_indexing_service",
        "documentation": {}
    },
    {
        "label": "RAGSettings",
        "kind": 6,
        "importPath": "app.services.rag.rag_settings",
        "description": "app.services.rag.rag_settings",
        "peekOfCode": "class RAGSettings(BaseSettings):\n    \"\"\"RAG\"\"\"\n    # Redis \n    redis_url: str = Field(default=\"redis://localhost:6379\", description=\"RedisURL\")\n    redis_ttl: int = Field(default=3600, description=\"RedisTTL\")\n    # \n    conversation_token_limit: int = Field(default=4000, description=\"Token\")\n    conversation_similarity_top_k: int = Field(default=6, description=\"Top-K\")\n    # LLM \n    llm_model: str = Field(default=\"gpt-4o-mini\", description=\"LLM\")",
        "detail": "app.services.rag.rag_settings",
        "documentation": {}
    },
    {
        "label": "RAGConfigManager",
        "kind": 6,
        "importPath": "app.services.rag.rag_settings",
        "description": "app.services.rag.rag_settings",
        "peekOfCode": "class RAGConfigManager:\n    \"\"\"RAG - \"\"\"\n    _instance: Optional['RAGConfigManager'] = None\n    _initialized: bool = False\n    def __new__(cls) -> 'RAGConfigManager':\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n        return cls._instance\n    def __init__(self):\n        if not self._initialized:",
        "detail": "app.services.rag.rag_settings",
        "documentation": {}
    },
    {
        "label": "get_rag_config_manager",
        "kind": 2,
        "importPath": "app.services.rag.rag_settings",
        "description": "app.services.rag.rag_settings",
        "peekOfCode": "def get_rag_config_manager() -> RAGConfigManager:\n    \"\"\"RAG\"\"\"\n    return rag_config_manager\ndef initialize_rag_config(app_settings: AppSettings) -> None:\n    \"\"\"RAG\"\"\"\n    rag_config_manager.initialize(app_settings)",
        "detail": "app.services.rag.rag_settings",
        "documentation": {}
    },
    {
        "label": "initialize_rag_config",
        "kind": 2,
        "importPath": "app.services.rag.rag_settings",
        "description": "app.services.rag.rag_settings",
        "peekOfCode": "def initialize_rag_config(app_settings: AppSettings) -> None:\n    \"\"\"RAG\"\"\"\n    rag_config_manager.initialize(app_settings)",
        "detail": "app.services.rag.rag_settings",
        "documentation": {}
    },
    {
        "label": "rag_config_manager",
        "kind": 5,
        "importPath": "app.services.rag.rag_settings",
        "description": "app.services.rag.rag_settings",
        "peekOfCode": "rag_config_manager = RAGConfigManager()\ndef get_rag_config_manager() -> RAGConfigManager:\n    \"\"\"RAG\"\"\"\n    return rag_config_manager\ndef initialize_rag_config(app_settings: AppSettings) -> None:\n    \"\"\"RAG\"\"\"\n    rag_config_manager.initialize(app_settings)",
        "detail": "app.services.rag.rag_settings",
        "documentation": {}
    },
    {
        "label": "ChatService",
        "kind": 6,
        "importPath": "app.services.chat_service",
        "description": "app.services.chat_service",
        "peekOfCode": "class ChatService:\n    \"\"\"\"\"\"\n    def __init__(self, settings: AppSettings):\n        \"\"\"\"\"\"\n        self.settings = settings\n        self._setup_llama_index()\n        self._setup_vector_index()\n        self._load_prompts()\n    def _setup_llama_index(self):\n        \"\"\"LlamaIndex\"\"\"",
        "detail": "app.services.chat_service",
        "documentation": {}
    },
    {
        "label": "CleanupService",
        "kind": 6,
        "importPath": "app.services.cleanup_service",
        "description": "app.services.cleanup_service",
        "peekOfCode": "class CleanupService:\n    \"\"\"\"\"\"\n    def __init__(self):\n        self.settings = get_settings()\n    async def cleanup_course_material(\n        self,\n        request: CleanupRequest\n    ) -> CleanupResponse:\n        \"\"\"\n        ",
        "detail": "app.services.cleanup_service",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app.services.cleanup_service",
        "description": "app.services.cleanup_service",
        "peekOfCode": "logger = get_logger(\"cleanup_service\")\nclass CleanupService:\n    \"\"\"\"\"\"\n    def __init__(self):\n        self.settings = get_settings()\n    async def cleanup_course_material(\n        self,\n        request: CleanupRequest\n    ) -> CleanupResponse:\n        \"\"\"",
        "detail": "app.services.cleanup_service",
        "documentation": {}
    },
    {
        "label": "cleanup_service",
        "kind": 5,
        "importPath": "app.services.cleanup_service",
        "description": "app.services.cleanup_service",
        "peekOfCode": "cleanup_service = CleanupService()",
        "detail": "app.services.cleanup_service",
        "documentation": {}
    },
    {
        "label": "CourseProcessService",
        "kind": 6,
        "importPath": "app.services.course_process_service",
        "description": "app.services.course_process_service",
        "peekOfCode": "class CourseProcessService:\n    \"\"\"\"\"\"\n    def __init__(self):\n        self.settings = get_settings()\n        # \n        self.task_storage: Dict[str, CourseProcessResponse] = {}\n    async def process_course_material(\n        self,\n        file: UploadFile,\n        request: CourseProcessRequest",
        "detail": "app.services.course_process_service",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app.services.course_process_service",
        "description": "app.services.course_process_service",
        "peekOfCode": "logger = get_logger(\"course_process_service\")\nclass CourseProcessService:\n    \"\"\"\"\"\"\n    def __init__(self):\n        self.settings = get_settings()\n        # \n        self.task_storage: Dict[str, CourseProcessResponse] = {}\n    async def process_course_material(\n        self,\n        file: UploadFile,",
        "detail": "app.services.course_process_service",
        "documentation": {}
    },
    {
        "label": "course_process_service",
        "kind": 5,
        "importPath": "app.services.course_process_service",
        "description": "app.services.course_process_service",
        "peekOfCode": "course_process_service = CourseProcessService()",
        "detail": "app.services.course_process_service",
        "documentation": {}
    },
    {
        "label": "OutlineService",
        "kind": 6,
        "importPath": "app.services.outline_service",
        "description": "app.services.outline_service",
        "peekOfCode": "class OutlineService:\n    \"\"\"\"\"\"\n    def __init__(self):\n        self.settings = get_settings()\n        self.client = AsyncOpenAI(\n            api_key=self.settings.api_key,\n            base_url=self.settings.base_url\n        )\n        # \n        self._outline_prompt_template = None",
        "detail": "app.services.outline_service",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app.services.outline_service",
        "description": "app.services.outline_service",
        "peekOfCode": "logger = get_logger(\"outline_service\")\nclass OutlineService:\n    \"\"\"\"\"\"\n    def __init__(self):\n        self.settings = get_settings()\n        self.client = AsyncOpenAI(\n            api_key=self.settings.api_key,\n            base_url=self.settings.base_url\n        )\n        # ",
        "detail": "app.services.outline_service",
        "documentation": {}
    },
    {
        "label": "outline_service",
        "kind": 5,
        "importPath": "app.services.outline_service",
        "description": "app.services.outline_service",
        "peekOfCode": "outline_service = OutlineService()",
        "detail": "app.services.outline_service",
        "documentation": {}
    },
    {
        "label": "RAGService",
        "kind": 6,
        "importPath": "app.services.rag_service",
        "description": "app.services.rag_service",
        "peekOfCode": "class RAGService:\n    \"\"\"RAG\"\"\"\n    def __init__(self, settings: AppSettings):\n        \"\"\"RAG\"\"\"\n        self.settings = settings\n        self.qdrant_repo = QdrantRepository(settings)\n        self._setup_llama_index()\n        self._load_prompts()\n    def _setup_llama_index(self):\n        \"\"\"LlamaIndex\"\"\"",
        "detail": "app.services.rag_service",
        "documentation": {}
    },
    {
        "label": "rag_service",
        "kind": 5,
        "importPath": "app.services.rag_service",
        "description": "app.services.rag_service",
        "peekOfCode": "rag_service = RAGService(get_settings())",
        "detail": "app.services.rag_service",
        "documentation": {}
    },
    {
        "label": "FileIOUtils",
        "kind": 6,
        "importPath": "app.utils.fileio",
        "description": "app.utils.fileio",
        "peekOfCode": "class FileIOUtils:\n    \"\"\"IO\"\"\"\n    @staticmethod\n    def is_safe_path(file_path: Path, base_path: Path) -> bool:\n        \"\"\"\n        \n        Args:\n            file_path: \n            base_path: \n        Returns:",
        "detail": "app.utils.fileio",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app.utils.fileio",
        "description": "app.utils.fileio",
        "peekOfCode": "logger = get_logger(\"fileio\")\nclass FileIOUtils:\n    \"\"\"IO\"\"\"\n    @staticmethod\n    def is_safe_path(file_path: Path, base_path: Path) -> bool:\n        \"\"\"\n        \n        Args:\n            file_path: \n            base_path: ",
        "detail": "app.utils.fileio",
        "documentation": {}
    },
    {
        "label": "file_utils",
        "kind": 5,
        "importPath": "app.utils.fileio",
        "description": "app.utils.fileio",
        "peekOfCode": "file_utils = FileIOUtils()",
        "detail": "app.utils.fileio",
        "documentation": {}
    },
    {
        "label": "IDGenerator",
        "kind": 6,
        "importPath": "app.utils.idgen",
        "description": "app.utils.idgen",
        "peekOfCode": "class IDGenerator:\n    \"\"\"ID\"\"\"\n    @staticmethod\n    def generate_uuid() -> str:\n        \"\"\"UUID\"\"\"\n        return str(uuid.uuid4())\n    @staticmethod\n    def generate_short_id(length: int = 8) -> str:\n        \"\"\"\n        ID",
        "detail": "app.utils.idgen",
        "documentation": {}
    },
    {
        "label": "FilenameGenerator",
        "kind": 6,
        "importPath": "app.utils.idgen",
        "description": "app.utils.idgen",
        "peekOfCode": "class FilenameGenerator:\n    \"\"\"\"\"\"\n    @staticmethod\n    def generate_timestamp_filename(\n        original_filename: str,\n        task_id: Optional[str] = None,\n        prefix: str = \"\",\n        suffix: str = \"\"\n    ) -> str:\n        \"\"\"",
        "detail": "app.utils.idgen",
        "documentation": {}
    },
    {
        "label": "PathGenerator",
        "kind": 6,
        "importPath": "app.utils.idgen",
        "description": "app.utils.idgen",
        "peekOfCode": "class PathGenerator:\n    \"\"\"\"\"\"\n    @staticmethod\n    def generate_upload_path(\n        base_dir: Path,\n        filename: str,\n        create_subdirs: bool = True\n    ) -> Path:\n        \"\"\"\n        ",
        "detail": "app.utils.idgen",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app.utils.idgen",
        "description": "app.utils.idgen",
        "peekOfCode": "logger = get_logger(\"idgen\")\nclass IDGenerator:\n    \"\"\"ID\"\"\"\n    @staticmethod\n    def generate_uuid() -> str:\n        \"\"\"UUID\"\"\"\n        return str(uuid.uuid4())\n    @staticmethod\n    def generate_short_id(length: int = 8) -> str:\n        \"\"\"",
        "detail": "app.utils.idgen",
        "documentation": {}
    },
    {
        "label": "id_generator",
        "kind": 5,
        "importPath": "app.utils.idgen",
        "description": "app.utils.idgen",
        "peekOfCode": "id_generator = IDGenerator()\nfilename_generator = FilenameGenerator()\npath_generator = PathGenerator()",
        "detail": "app.utils.idgen",
        "documentation": {}
    },
    {
        "label": "filename_generator",
        "kind": 5,
        "importPath": "app.utils.idgen",
        "description": "app.utils.idgen",
        "peekOfCode": "filename_generator = FilenameGenerator()\npath_generator = PathGenerator()",
        "detail": "app.utils.idgen",
        "documentation": {}
    },
    {
        "label": "path_generator",
        "kind": 5,
        "importPath": "app.utils.idgen",
        "description": "app.utils.idgen",
        "peekOfCode": "path_generator = PathGenerator()",
        "detail": "app.utils.idgen",
        "documentation": {}
    },
    {
        "label": "Timer",
        "kind": 6,
        "importPath": "app.utils.timers",
        "description": "app.utils.timers",
        "peekOfCode": "class Timer:\n    \"\"\"\"\"\"\n    def __init__(self, name: str = \"Timer\"):\n        self.name = name\n        self.start_time: Optional[float] = None\n        self.end_time: Optional[float] = None\n        self.elapsed_time: Optional[float] = None\n    def start(self) -> 'Timer':\n        \"\"\"\"\"\"\n        self.start_time = time.time()",
        "detail": "app.utils.timers",
        "documentation": {}
    },
    {
        "label": "AsyncTimer",
        "kind": 6,
        "importPath": "app.utils.timers",
        "description": "app.utils.timers",
        "peekOfCode": "class AsyncTimer:\n    \"\"\"\"\"\"\n    def __init__(self, name: str = \"AsyncTimer\"):\n        self.name = name\n        self.start_time: Optional[float] = None\n        self.end_time: Optional[float] = None\n        self.elapsed_time: Optional[float] = None\n    async def start(self) -> 'AsyncTimer':\n        \"\"\"\"\"\"\n        self.start_time = time.time()",
        "detail": "app.utils.timers",
        "documentation": {}
    },
    {
        "label": "PerformanceMonitor",
        "kind": 6,
        "importPath": "app.utils.timers",
        "description": "app.utils.timers",
        "peekOfCode": "class PerformanceMonitor:\n    \"\"\"\"\"\"\n    def __init__(self):\n        self.metrics: Dict[str, Dict[str, Any]] = {}\n    def record_timing(self, operation: str, elapsed_time: float, **metadata):\n        \"\"\"\"\"\"\n        if operation not in self.metrics:\n            self.metrics[operation] = {\n                \"count\": 0,\n                \"total_time\": 0.0,",
        "detail": "app.utils.timers",
        "documentation": {}
    },
    {
        "label": "TimestampUtils",
        "kind": 6,
        "importPath": "app.utils.timers",
        "description": "app.utils.timers",
        "peekOfCode": "class TimestampUtils:\n    \"\"\"\"\"\"\n    @staticmethod\n    def get_current_timestamp() -> float:\n        \"\"\"\"\"\"\n        return time.time()\n    @staticmethod\n    def get_current_datetime() -> datetime:\n        \"\"\"\"\"\"\n        return datetime.now(timezone.utc)",
        "detail": "app.utils.timers",
        "documentation": {}
    },
    {
        "label": "timer_decorator",
        "kind": 2,
        "importPath": "app.utils.timers",
        "description": "app.utils.timers",
        "peekOfCode": "def timer_decorator(name: Optional[str] = None):\n    \"\"\"\"\"\"\n    def decorator(func):\n        operation_name = name or f\"{func.__module__}.{func.__name__}\"\n        if asyncio.iscoroutinefunction(func):\n            async def async_wrapper(*args, **kwargs):\n                async with async_timer(operation_name) as timer:\n                    result = await func(*args, **kwargs)\n                    performance_monitor.record_timing(\n                        operation_name,",
        "detail": "app.utils.timers",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app.utils.timers",
        "description": "app.utils.timers",
        "peekOfCode": "logger = get_logger(\"timers\")\nclass Timer:\n    \"\"\"\"\"\"\n    def __init__(self, name: str = \"Timer\"):\n        self.name = name\n        self.start_time: Optional[float] = None\n        self.end_time: Optional[float] = None\n        self.elapsed_time: Optional[float] = None\n    def start(self) -> 'Timer':\n        \"\"\"\"\"\"",
        "detail": "app.utils.timers",
        "documentation": {}
    },
    {
        "label": "performance_monitor",
        "kind": 5,
        "importPath": "app.utils.timers",
        "description": "app.utils.timers",
        "peekOfCode": "performance_monitor = PerformanceMonitor()\ntimestamp_utils = TimestampUtils()",
        "detail": "app.utils.timers",
        "documentation": {}
    },
    {
        "label": "timestamp_utils",
        "kind": 5,
        "importPath": "app.utils.timers",
        "description": "app.utils.timers",
        "peekOfCode": "timestamp_utils = TimestampUtils()",
        "detail": "app.utils.timers",
        "documentation": {}
    },
    {
        "label": "CourseValidation",
        "kind": 6,
        "importPath": "app.utils.validation",
        "description": "app.utils.validation",
        "peekOfCode": "class CourseValidation:\n    \"\"\"\"\"\"\n    @staticmethod\n    def validate_course_material_id_unique(\n        course_id: str,\n        course_material_id: str,\n        uploads_base_dir: Path = UPLOADS_DIR\n    ) -> bool:\n        \"\"\"\n        course_idcourse_material_id",
        "detail": "app.utils.validation",
        "documentation": {}
    },
    {
        "label": "FileValidation",
        "kind": 6,
        "importPath": "app.utils.validation",
        "description": "app.utils.validation",
        "peekOfCode": "class FileValidation:\n    \"\"\"\"\"\"\n    @staticmethod\n    def validate_file_extension(filename: str, allowed_extensions: List[str]) -> bool:\n        \"\"\"\n        \n        Args:\n            filename: \n            allowed_extensions: \n        Returns:",
        "detail": "app.utils.validation",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app.utils.validation",
        "description": "app.utils.validation",
        "peekOfCode": "logger = get_logger(\"validation\")\nclass CourseValidation:\n    \"\"\"\"\"\"\n    @staticmethod\n    def validate_course_material_id_unique(\n        course_id: str,\n        course_material_id: str,\n        uploads_base_dir: Path = UPLOADS_DIR\n    ) -> bool:\n        \"\"\"",
        "detail": "app.utils.validation",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "app.main",
        "description": "app.main",
        "peekOfCode": "logger = get_logger(\"main\")\n# \napp_start_time = time.time()\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"\"\"\"\n    # \n    logger.info(\" AI Backend ...\")\n    # \n    settings = get_settings()",
        "detail": "app.main",
        "documentation": {}
    },
    {
        "label": "app_start_time",
        "kind": 5,
        "importPath": "app.main",
        "description": "app.main",
        "peekOfCode": "app_start_time = time.time()\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"\"\"\"\n    # \n    logger.info(\" AI Backend ...\")\n    # \n    settings = get_settings()\n    # \n    try:",
        "detail": "app.main",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "app.main",
        "description": "app.main",
        "peekOfCode": "app = FastAPI(\n    title=\"AI Backend API\",\n    description=__description__,\n    version=__version__,\n    docs_url=\"/docs\",\n    redoc_url=\"/redoc\",\n    openapi_url=\"/openapi.json\",\n    lifespan=lifespan\n)\n# ",
        "detail": "app.main",
        "documentation": {}
    },
    {
        "label": "settings",
        "kind": 5,
        "importPath": "app.main",
        "description": "app.main",
        "peekOfCode": "settings = get_settings()\n#  CORS \napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # \n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n# ",
        "detail": "app.main",
        "documentation": {}
    },
    {
        "label": "BaseAgentChatFormatter",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.react.formatter",
        "description": "reference_code.llama-index-core.llama_index.core.agent.react.formatter",
        "peekOfCode": "class BaseAgentChatFormatter(BaseModel):\n    \"\"\"Base chat formatter.\"\"\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    @abstractmethod\n    def format(\n        self,\n        tools: Sequence[BaseTool],\n        chat_history: List[ChatMessage],\n        current_reasoning: Optional[List[BaseReasoningStep]] = None,\n    ) -> List[ChatMessage]:",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.react.formatter",
        "documentation": {}
    },
    {
        "label": "ReActChatFormatter",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.react.formatter",
        "description": "reference_code.llama-index-core.llama_index.core.agent.react.formatter",
        "peekOfCode": "class ReActChatFormatter(BaseAgentChatFormatter):\n    \"\"\"ReAct chat formatter.\"\"\"\n    system_header: str = REACT_CHAT_SYSTEM_HEADER  # default\n    context: str = \"\"  # not needed w/ default\n    observation_role: MessageRole = Field(\n        default=MessageRole.USER,\n        description=(\n            \"Message role of tool outputs. If the LLM you use supports function/tool \"\n            \"calling, you may set it to `MessageRole.TOOL` to avoid the tool outputs \"\n            \"being misinterpreted as new user messages.\"",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.react.formatter",
        "documentation": {}
    },
    {
        "label": "get_react_tool_descriptions",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.react.formatter",
        "description": "reference_code.llama-index-core.llama_index.core.agent.react.formatter",
        "peekOfCode": "def get_react_tool_descriptions(tools: Sequence[BaseTool]) -> List[str]:\n    \"\"\"Tool.\"\"\"\n    tool_descs = []\n    for tool in tools:\n        tool_desc = (\n            f\"> Tool Name: {tool.metadata.name}\\n\"\n            f\"Tool Description: {tool.metadata.description}\\n\"\n            f\"Tool Args: {tool.metadata.fn_schema_str}\\n\"\n        )\n        tool_descs.append(tool_desc)",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.react.formatter",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.react.formatter",
        "description": "reference_code.llama-index-core.llama_index.core.agent.react.formatter",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef get_react_tool_descriptions(tools: Sequence[BaseTool]) -> List[str]:\n    \"\"\"Tool.\"\"\"\n    tool_descs = []\n    for tool in tools:\n        tool_desc = (\n            f\"> Tool Name: {tool.metadata.name}\\n\"\n            f\"Tool Description: {tool.metadata.description}\\n\"\n            f\"Tool Args: {tool.metadata.fn_schema_str}\\n\"\n        )",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.react.formatter",
        "documentation": {}
    },
    {
        "label": "ReActOutputParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.react.output_parser",
        "description": "reference_code.llama-index-core.llama_index.core.agent.react.output_parser",
        "peekOfCode": "class ReActOutputParser(BaseOutputParser):\n    \"\"\"ReAct Output parser.\"\"\"\n    def parse(self, output: str, is_streaming: bool = False) -> BaseReasoningStep:\n        \"\"\"\n        Parse output from ReAct agent.\n        We expect the output to be in one of the following formats:\n        1. If the agent need to use a tool to answer the question:\n            ```\n            Thought: <thought>\n            Action: <action>",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.react.output_parser",
        "documentation": {}
    },
    {
        "label": "extract_tool_use",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.react.output_parser",
        "description": "reference_code.llama-index-core.llama_index.core.agent.react.output_parser",
        "peekOfCode": "def extract_tool_use(input_text: str) -> Tuple[str, str, str]:\n    pattern = r\"(?:\\s*Thought: (.*?)|(.+))\\n+Action: ([^\\n\\(\\) ]+).*?\\n+Action Input: .*?(\\{.*\\})\"\n    match = re.search(pattern, input_text, re.DOTALL)\n    if not match:\n        raise ValueError(f\"Could not extract tool use from input text: {input_text}\")\n    thought = (match.group(1) or match.group(2)).strip()\n    action = match.group(3).strip()\n    action_input = match.group(4).strip()\n    return thought, action, action_input\ndef action_input_parser(json_str: str) -> dict:",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.react.output_parser",
        "documentation": {}
    },
    {
        "label": "action_input_parser",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.react.output_parser",
        "description": "reference_code.llama-index-core.llama_index.core.agent.react.output_parser",
        "peekOfCode": "def action_input_parser(json_str: str) -> dict:\n    processed_string = re.sub(r\"(?<!\\w)\\'|\\'(?!\\w)\", '\"', json_str)\n    pattern = r'\"(\\w+)\":\\s*\"([^\"]*)\"'\n    matches = re.findall(pattern, processed_string)\n    return dict(matches)\ndef extract_final_response(input_text: str) -> Tuple[str, str]:\n    pattern = r\"\\s*Thought:(.*?)Answer:(.*?)(?:$)\"\n    match = re.search(pattern, input_text, re.DOTALL)\n    if not match:\n        raise ValueError(",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.react.output_parser",
        "documentation": {}
    },
    {
        "label": "extract_final_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.react.output_parser",
        "description": "reference_code.llama-index-core.llama_index.core.agent.react.output_parser",
        "peekOfCode": "def extract_final_response(input_text: str) -> Tuple[str, str]:\n    pattern = r\"\\s*Thought:(.*?)Answer:(.*?)(?:$)\"\n    match = re.search(pattern, input_text, re.DOTALL)\n    if not match:\n        raise ValueError(\n            f\"Could not extract final answer from input text: {input_text}\"\n        )\n    thought = match.group(1).strip()\n    answer = match.group(2).strip()\n    return thought, answer",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.react.output_parser",
        "documentation": {}
    },
    {
        "label": "parse_action_reasoning_step",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.react.output_parser",
        "description": "reference_code.llama-index-core.llama_index.core.agent.react.output_parser",
        "peekOfCode": "def parse_action_reasoning_step(output: str) -> ActionReasoningStep:\n    \"\"\"\n    Parse an action reasoning step from the LLM output.\n    \"\"\"\n    # Weaker LLMs may generate ReActAgent steps whose Action Input are horrible JSON strings.\n    # `dirtyjson` is more lenient than `json` in parsing JSON strings.\n    import dirtyjson as json\n    thought, action, action_input = extract_tool_use(output)\n    json_str = extract_json_str(action_input)\n    # First we try json, if this fails we use ast",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.react.output_parser",
        "documentation": {}
    },
    {
        "label": "REACT_CHAT_SYSTEM_HEADER",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.react.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.agent.react.prompts",
        "peekOfCode": "REACT_CHAT_SYSTEM_HEADER = __BASE_REACT_CHAT_SYSTEM_HEADER.replace(\n    \"{context_prompt}\", \"\", 1\n)\nCONTEXT_REACT_CHAT_SYSTEM_HEADER = __BASE_REACT_CHAT_SYSTEM_HEADER.replace(\n    \"{context_prompt}\",\n    \"\"\"\nHere is some context to help you answer the question and plan:\n{context}\n\"\"\",\n    1,",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.react.prompts",
        "documentation": {}
    },
    {
        "label": "CONTEXT_REACT_CHAT_SYSTEM_HEADER",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.react.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.agent.react.prompts",
        "peekOfCode": "CONTEXT_REACT_CHAT_SYSTEM_HEADER = __BASE_REACT_CHAT_SYSTEM_HEADER.replace(\n    \"{context_prompt}\",\n    \"\"\"\nHere is some context to help you answer the question and plan:\n{context}\n\"\"\",\n    1,\n)",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.react.prompts",
        "documentation": {}
    },
    {
        "label": "BaseReasoningStep",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.react.types",
        "description": "reference_code.llama-index-core.llama_index.core.agent.react.types",
        "peekOfCode": "class BaseReasoningStep(BaseModel):\n    \"\"\"Reasoning step.\"\"\"\n    @abstractmethod\n    def get_content(self) -> str:\n        \"\"\"Get content.\"\"\"\n    @property\n    @abstractmethod\n    def is_done(self) -> bool:\n        \"\"\"Is the reasoning step the last one.\"\"\"\nclass ActionReasoningStep(BaseReasoningStep):",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.react.types",
        "documentation": {}
    },
    {
        "label": "ActionReasoningStep",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.react.types",
        "description": "reference_code.llama-index-core.llama_index.core.agent.react.types",
        "peekOfCode": "class ActionReasoningStep(BaseReasoningStep):\n    \"\"\"Action Reasoning step.\"\"\"\n    thought: str\n    action: str\n    action_input: Dict\n    def get_content(self) -> str:\n        \"\"\"Get content.\"\"\"\n        return (\n            f\"Thought: {self.thought}\\nAction: {self.action}\\n\"\n            f\"Action Input: {self.action_input}\"",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.react.types",
        "documentation": {}
    },
    {
        "label": "ObservationReasoningStep",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.react.types",
        "description": "reference_code.llama-index-core.llama_index.core.agent.react.types",
        "peekOfCode": "class ObservationReasoningStep(BaseReasoningStep):\n    \"\"\"Observation reasoning step.\"\"\"\n    observation: str\n    return_direct: bool = False\n    def get_content(self) -> str:\n        \"\"\"Get content.\"\"\"\n        return f\"Observation: {self.observation}\"\n    @property\n    def is_done(self) -> bool:\n        \"\"\"Is the reasoning step the last one.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.react.types",
        "documentation": {}
    },
    {
        "label": "ResponseReasoningStep",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.react.types",
        "description": "reference_code.llama-index-core.llama_index.core.agent.react.types",
        "peekOfCode": "class ResponseReasoningStep(BaseReasoningStep):\n    \"\"\"Response reasoning step.\"\"\"\n    thought: str\n    response: str\n    is_streaming: bool = False\n    def get_content(self) -> str:\n        \"\"\"Get content.\"\"\"\n        if self.is_streaming:\n            return f\"Thought: {self.thought}\\nAnswer (Starts With): {self.response} ...\"\n        else:",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.react.types",
        "documentation": {}
    },
    {
        "label": "BaseWorkflowAgentMeta",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.base_agent",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.base_agent",
        "peekOfCode": "class BaseWorkflowAgentMeta(WorkflowMeta, ModelMetaclass):\n    \"\"\"Metaclass for BaseWorkflowAgent that properly combines WorkflowMeta, BaseModel's metaclass, and ABCMeta.\"\"\"\nclass BaseWorkflowAgent(\n    Workflow, BaseModel, PromptMixin, metaclass=BaseWorkflowAgentMeta\n):\n    \"\"\"Base class for all agents, combining config and logic.\"\"\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    name: str = Field(default=DEFAULT_AGENT_NAME, description=\"The name of the agent\")\n    description: str = Field(\n        default=DEFAULT_AGENT_DESCRIPTION,",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.base_agent",
        "documentation": {}
    },
    {
        "label": "BaseWorkflowAgent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.base_agent",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.base_agent",
        "peekOfCode": "class BaseWorkflowAgent(\n    Workflow, BaseModel, PromptMixin, metaclass=BaseWorkflowAgentMeta\n):\n    \"\"\"Base class for all agents, combining config and logic.\"\"\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    name: str = Field(default=DEFAULT_AGENT_NAME, description=\"The name of the agent\")\n    description: str = Field(\n        default=DEFAULT_AGENT_DESCRIPTION,\n        description=\"The description of what the agent does and is responsible for\",\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.base_agent",
        "documentation": {}
    },
    {
        "label": "get_default_llm",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.base_agent",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.base_agent",
        "peekOfCode": "def get_default_llm() -> LLM:\n    return Settings.llm\nclass BaseWorkflowAgentMeta(WorkflowMeta, ModelMetaclass):\n    \"\"\"Metaclass for BaseWorkflowAgent that properly combines WorkflowMeta, BaseModel's metaclass, and ABCMeta.\"\"\"\nclass BaseWorkflowAgent(\n    Workflow, BaseModel, PromptMixin, metaclass=BaseWorkflowAgentMeta\n):\n    \"\"\"Base class for all agents, combining config and logic.\"\"\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    name: str = Field(default=DEFAULT_AGENT_NAME, description=\"The name of the agent\")",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.base_agent",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MAX_ITERATIONS",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.base_agent",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.base_agent",
        "peekOfCode": "DEFAULT_MAX_ITERATIONS = 20\nDEFAULT_AGENT_NAME = \"Agent\"\nDEFAULT_AGENT_DESCRIPTION = \"An agent that can perform a task\"\nWORKFLOW_KWARGS = (\n    \"timeout\",\n    \"verbose\",\n    \"service_manager\",\n    \"resource_manager\",\n    \"num_concurrent_runs\",\n)",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.base_agent",
        "documentation": {}
    },
    {
        "label": "DEFAULT_AGENT_NAME",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.base_agent",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.base_agent",
        "peekOfCode": "DEFAULT_AGENT_NAME = \"Agent\"\nDEFAULT_AGENT_DESCRIPTION = \"An agent that can perform a task\"\nWORKFLOW_KWARGS = (\n    \"timeout\",\n    \"verbose\",\n    \"service_manager\",\n    \"resource_manager\",\n    \"num_concurrent_runs\",\n)\ndef get_default_llm() -> LLM:",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.base_agent",
        "documentation": {}
    },
    {
        "label": "DEFAULT_AGENT_DESCRIPTION",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.base_agent",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.base_agent",
        "peekOfCode": "DEFAULT_AGENT_DESCRIPTION = \"An agent that can perform a task\"\nWORKFLOW_KWARGS = (\n    \"timeout\",\n    \"verbose\",\n    \"service_manager\",\n    \"resource_manager\",\n    \"num_concurrent_runs\",\n)\ndef get_default_llm() -> LLM:\n    return Settings.llm",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.base_agent",
        "documentation": {}
    },
    {
        "label": "WORKFLOW_KWARGS",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.base_agent",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.base_agent",
        "peekOfCode": "WORKFLOW_KWARGS = (\n    \"timeout\",\n    \"verbose\",\n    \"service_manager\",\n    \"resource_manager\",\n    \"num_concurrent_runs\",\n)\ndef get_default_llm() -> LLM:\n    return Settings.llm\nclass BaseWorkflowAgentMeta(WorkflowMeta, ModelMetaclass):",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.base_agent",
        "documentation": {}
    },
    {
        "label": "CodeActAgent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.codeact_agent",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.codeact_agent",
        "peekOfCode": "class CodeActAgent(BaseWorkflowAgent):\n    \"\"\"\n    A workflow agent that can execute code.\n    \"\"\"\n    scratchpad_key: str = \"scratchpad\"\n    code_execute_fn: Union[Callable, Awaitable] = Field(\n        description=(\n            \"The function to execute code. Required in order to execute code generated by the agent.\\n\"\n            \"The function protocol is as follows: async def code_execute_fn(code: str) -> Dict[str, Any]\"\n        ),",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.codeact_agent",
        "documentation": {}
    },
    {
        "label": "calculate_area",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.codeact_agent",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.codeact_agent",
        "peekOfCode": "def calculate_area(radius):\n    return math.pi * radius**2\n# Calculate the area for radius = 5\narea = calculate_area(5)\nprint(f\"The area of the circle is {area:.2f} square units\")\n</execute>\nIn addition to the Python Standard Library and any functions you have already written, you can use the following functions:\n{tool_descriptions}\nVariables defined at the top level of previous code snippets can be also be referenced in your code.\n## Final Answer Guidelines:",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.codeact_agent",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CODE_ACT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.codeact_agent",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.codeact_agent",
        "peekOfCode": "DEFAULT_CODE_ACT_PROMPT = \"\"\"You are a helpful AI assistant that can write and execute Python code to solve problems.\nYou will be given a task to perform. You should output:\n- Python code wrapped in <execute>...</execute> tags that provides the solution to the task, or a step towards the solution. Any output you want to extract from the code should be printed to the console.\n- Text to be shown directly to the user, if you want to ask for more information or provide the final answer.\n- If the previous code execution can be used to respond to the user, then respond directly (typically you want to avoid mentioning anything related to the code execution in your response).\n## Response Format:\nExample of proper code format:\n<execute>\nimport math\ndef calculate_area(radius):",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.codeact_agent",
        "documentation": {}
    },
    {
        "label": "area",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.codeact_agent",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.codeact_agent",
        "peekOfCode": "area = calculate_area(5)\nprint(f\"The area of the circle is {area:.2f} square units\")\n</execute>\nIn addition to the Python Standard Library and any functions you have already written, you can use the following functions:\n{tool_descriptions}\nVariables defined at the top level of previous code snippets can be also be referenced in your code.\n## Final Answer Guidelines:\n- When providing a final answer, focus on directly answering the user's question\n- Avoid referencing the code you generated unless specifically asked\n- Present the results clearly and concisely as if you computed them directly",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.codeact_agent",
        "documentation": {}
    },
    {
        "label": "EXECUTE_TOOL_NAME",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.codeact_agent",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.codeact_agent",
        "peekOfCode": "EXECUTE_TOOL_NAME = \"execute\"\nclass CodeActAgent(BaseWorkflowAgent):\n    \"\"\"\n    A workflow agent that can execute code.\n    \"\"\"\n    scratchpad_key: str = \"scratchpad\"\n    code_execute_fn: Union[Callable, Awaitable] = Field(\n        description=(\n            \"The function to execute code. Required in order to execute code generated by the agent.\\n\"\n            \"The function protocol is as follows: async def code_execute_fn(code: str) -> Dict[str, Any]\"",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.codeact_agent",
        "documentation": {}
    },
    {
        "label": "FunctionAgent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.function_agent",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.function_agent",
        "peekOfCode": "class FunctionAgent(BaseWorkflowAgent):\n    \"\"\"Function calling agent implementation.\"\"\"\n    scratchpad_key: str = \"scratchpad\"\n    allow_parallel_tool_calls: bool = Field(\n        default=True,\n        description=\"If True, the agent will call multiple tools in parallel. If False, the agent will call tools sequentially.\",\n    )\n    async def _get_response(\n        self, current_llm_input: List[ChatMessage], tools: Sequence[AsyncBaseTool]\n    ) -> ChatResponse:",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.function_agent",
        "documentation": {}
    },
    {
        "label": "AgentWorkflowMeta",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.multi_agent_workflow",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.multi_agent_workflow",
        "peekOfCode": "class AgentWorkflowMeta(WorkflowMeta, ABCMeta):\n    \"\"\"Metaclass for AgentWorkflow that inherits from WorkflowMeta.\"\"\"\nclass AgentWorkflow(Workflow, PromptMixin, metaclass=AgentWorkflowMeta):\n    \"\"\"A workflow for managing multiple agents with handoffs.\"\"\"\n    def __init__(\n        self,\n        agents: List[BaseWorkflowAgent],\n        initial_state: Optional[Dict] = None,\n        root_agent: Optional[str] = None,\n        handoff_prompt: Optional[Union[str, BasePromptTemplate]] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.multi_agent_workflow",
        "documentation": {}
    },
    {
        "label": "AgentWorkflow",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.multi_agent_workflow",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.multi_agent_workflow",
        "peekOfCode": "class AgentWorkflow(Workflow, PromptMixin, metaclass=AgentWorkflowMeta):\n    \"\"\"A workflow for managing multiple agents with handoffs.\"\"\"\n    def __init__(\n        self,\n        agents: List[BaseWorkflowAgent],\n        initial_state: Optional[Dict] = None,\n        root_agent: Optional[str] = None,\n        handoff_prompt: Optional[Union[str, BasePromptTemplate]] = None,\n        handoff_output_prompt: Optional[Union[str, BasePromptTemplate]] = None,\n        state_prompt: Optional[Union[str, BasePromptTemplate]] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.multi_agent_workflow",
        "documentation": {}
    },
    {
        "label": "DEFAULT_HANDOFF_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.prompts",
        "peekOfCode": "DEFAULT_HANDOFF_PROMPT = \"\"\"Useful for handing off to another agent.\nIf you are currently not equipped to handle the user's request, or another agent is better suited to handle the request, please hand off to the appropriate agent.\nCurrently available agents:\n{agent_info}\n\"\"\"\nDEFAULT_STATE_PROMPT = \"\"\"Current state:\n{state}\nCurrent message:\n{msg}\n\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_STATE_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.prompts",
        "peekOfCode": "DEFAULT_STATE_PROMPT = \"\"\"Current state:\n{state}\nCurrent message:\n{msg}\n\"\"\"\nDEFAULT_HANDOFF_OUTPUT_PROMPT = \"Agent {to_agent} is now handling the request due to the following reason: {reason}.\\nPlease continue with the current request.\"",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_HANDOFF_OUTPUT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.prompts",
        "peekOfCode": "DEFAULT_HANDOFF_OUTPUT_PROMPT = \"Agent {to_agent} is now handling the request due to the following reason: {reason}.\\nPlease continue with the current request.\"",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.prompts",
        "documentation": {}
    },
    {
        "label": "ReActAgent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.react_agent",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.react_agent",
        "peekOfCode": "class ReActAgent(BaseWorkflowAgent):\n    \"\"\"React agent implementation.\"\"\"\n    reasoning_key: str = \"current_reasoning\"\n    output_parser: ReActOutputParser = Field(\n        default_factory=ReActOutputParser, description=\"The react output parser\"\n    )\n    formatter: ReActChatFormatter = Field(\n        default_factory=default_formatter,\n        description=\"The react chat formatter to format the reasoning steps and chat history into an llm input.\",\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.react_agent",
        "documentation": {}
    },
    {
        "label": "default_formatter",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.react_agent",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.react_agent",
        "peekOfCode": "def default_formatter(fields: Optional[dict] = None) -> ReActChatFormatter:\n    \"\"\"Sets up a default formatter so that the proper react header is set.\"\"\"\n    fields = fields or {}\n    return ReActChatFormatter.from_defaults(context=fields.get(\"system_prompt\", None))\nclass ReActAgent(BaseWorkflowAgent):\n    \"\"\"React agent implementation.\"\"\"\n    reasoning_key: str = \"current_reasoning\"\n    output_parser: ReActOutputParser = Field(\n        default_factory=ReActOutputParser, description=\"The react output parser\"\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.react_agent",
        "documentation": {}
    },
    {
        "label": "PydanticConversionWarning",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "peekOfCode": "class PydanticConversionWarning(Warning):\n    \"\"\"Warning raised when the conversion from a dictionary to a Pydantic model fails\"\"\"\nclass AgentInput(Event):\n    \"\"\"LLM input.\"\"\"\n    input: list[ChatMessage]\n    current_agent_name: str\nclass AgentSetup(Event):\n    \"\"\"Agent setup.\"\"\"\n    input: list[ChatMessage]\n    current_agent_name: str",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentInput",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "peekOfCode": "class AgentInput(Event):\n    \"\"\"LLM input.\"\"\"\n    input: list[ChatMessage]\n    current_agent_name: str\nclass AgentSetup(Event):\n    \"\"\"Agent setup.\"\"\"\n    input: list[ChatMessage]\n    current_agent_name: str\nclass AgentStream(Event):\n    \"\"\"Agent stream.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentSetup",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "peekOfCode": "class AgentSetup(Event):\n    \"\"\"Agent setup.\"\"\"\n    input: list[ChatMessage]\n    current_agent_name: str\nclass AgentStream(Event):\n    \"\"\"Agent stream.\"\"\"\n    delta: str\n    response: str\n    current_agent_name: str\n    tool_calls: list[ToolSelection] = Field(default_factory=list)",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentStream",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "peekOfCode": "class AgentStream(Event):\n    \"\"\"Agent stream.\"\"\"\n    delta: str\n    response: str\n    current_agent_name: str\n    tool_calls: list[ToolSelection] = Field(default_factory=list)\n    raw: Optional[Any] = Field(default=None, exclude=True)\nclass AgentStreamStructuredOutput(Event):\n    \"\"\"Stream the structured output\"\"\"\n    output: Dict[str, Any]",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentStreamStructuredOutput",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "peekOfCode": "class AgentStreamStructuredOutput(Event):\n    \"\"\"Stream the structured output\"\"\"\n    output: Dict[str, Any]\n    def get_pydantic_model(self, model: Type[BaseModel]) -> Optional[BaseModel]:\n        if self.output is None:\n            return self.output\n        try:\n            return model.model_validate(self.output)\n        except ValidationError as e:\n            warnings.warn(",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentOutput",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "peekOfCode": "class AgentOutput(Event):\n    \"\"\"LLM output.\"\"\"\n    response: ChatMessage\n    structured_response: Optional[Dict[str, Any]] = Field(default=None)\n    current_agent_name: str\n    raw: Optional[Any] = Field(default=None, exclude=True)\n    tool_calls: list[ToolSelection] = Field(default_factory=list)\n    retry_messages: list[ChatMessage] = Field(default_factory=list)\n    def get_pydantic_model(self, model: Type[BaseModel]) -> Optional[BaseModel]:\n        if self.structured_response is None:",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "ToolCall",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "peekOfCode": "class ToolCall(Event):\n    \"\"\"All tool calls are surfaced.\"\"\"\n    tool_name: str\n    tool_kwargs: dict\n    tool_id: str\nclass ToolCallResult(Event):\n    \"\"\"Tool call result.\"\"\"\n    tool_name: str\n    tool_kwargs: dict\n    tool_id: str",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "ToolCallResult",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "peekOfCode": "class ToolCallResult(Event):\n    \"\"\"Tool call result.\"\"\"\n    tool_name: str\n    tool_kwargs: dict\n    tool_id: str\n    tool_output: ToolOutput\n    return_direct: bool\nclass AgentWorkflowStartEvent(StartEvent):\n    def __init__(self, **data: Any) -> None:\n        \"\"\"Convert chat_history items to ChatMessage objects if they aren't already\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "AgentWorkflowStartEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "peekOfCode": "class AgentWorkflowStartEvent(StartEvent):\n    def __init__(self, **data: Any) -> None:\n        \"\"\"Convert chat_history items to ChatMessage objects if they aren't already\"\"\"\n        if \"chat_history\" in data and data[\"chat_history\"]:\n            converted_history = []\n            for i, msg in enumerate(data[\"chat_history\"]):\n                if isinstance(msg, ChatMessage):\n                    converted_history.append(msg)\n                else:\n                    # Convert dict or other formats to ChatMessage with validation",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "description": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass PydanticConversionWarning(Warning):\n    \"\"\"Warning raised when the conversion from a dictionary to a Pydantic model fails\"\"\"\nclass AgentInput(Event):\n    \"\"\"LLM input.\"\"\"\n    input: list[ChatMessage]\n    current_agent_name: str\nclass AgentSetup(Event):\n    \"\"\"Agent setup.\"\"\"\n    input: list[ChatMessage]",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.workflow.workflow_events",
        "documentation": {}
    },
    {
        "label": "messages_to_xml_format",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.agent.utils",
        "description": "reference_code.llama-index-core.llama_index.core.agent.utils",
        "peekOfCode": "def messages_to_xml_format(messages: List[ChatMessage]) -> List[ChatMessage]:\n    blocks = [TextBlock(text=\"<current_conversation>\\n\")]\n    system_msg: Optional[ChatMessage] = None\n    for message in messages:\n        if message.role.value == \"system\":\n            system_msg = message\n        blocks.append(TextBlock(text=f\"\\t<{message.role.value}>\\n\"))\n        for block in message.blocks:\n            if isinstance(block, TextBlock):\n                blocks.append(TextBlock(text=f\"\\t\\t<message>{block.text}</message>\\n\"))",
        "detail": "reference_code.llama-index-core.llama_index.core.agent.utils",
        "documentation": {}
    },
    {
        "label": "SimilarityMode",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.embeddings.base",
        "description": "reference_code.llama-index-core.llama_index.core.base.embeddings.base",
        "peekOfCode": "class SimilarityMode(str, Enum):\n    \"\"\"Modes for similarity/distance.\"\"\"\n    DEFAULT = \"cosine\"\n    DOT_PRODUCT = \"dot_product\"\n    EUCLIDEAN = \"euclidean\"\ndef mean_agg(embeddings: List[Embedding]) -> Embedding:\n    \"\"\"Mean aggregation for embeddings.\"\"\"\n    return np.array(embeddings).mean(axis=0).tolist()\ndef similarity(\n    embedding1: Embedding,",
        "detail": "reference_code.llama-index-core.llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseEmbedding",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.embeddings.base",
        "description": "reference_code.llama-index-core.llama_index.core.base.embeddings.base",
        "peekOfCode": "class BaseEmbedding(TransformComponent, DispatcherSpanMixin):\n    \"\"\"Base class for embeddings.\"\"\"\n    model_config = ConfigDict(\n        protected_namespaces=(\"pydantic_model_\",), arbitrary_types_allowed=True\n    )\n    model_name: str = Field(\n        default=\"unknown\", description=\"The name of the embedding model.\"\n    )\n    embed_batch_size: int = Field(\n        default=DEFAULT_EMBED_BATCH_SIZE,",
        "detail": "reference_code.llama-index-core.llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "mean_agg",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.embeddings.base",
        "description": "reference_code.llama-index-core.llama_index.core.base.embeddings.base",
        "peekOfCode": "def mean_agg(embeddings: List[Embedding]) -> Embedding:\n    \"\"\"Mean aggregation for embeddings.\"\"\"\n    return np.array(embeddings).mean(axis=0).tolist()\ndef similarity(\n    embedding1: Embedding,\n    embedding2: Embedding,\n    mode: SimilarityMode = SimilarityMode.DEFAULT,\n) -> float:\n    \"\"\"Get embedding similarity.\"\"\"\n    if mode == SimilarityMode.EUCLIDEAN:",
        "detail": "reference_code.llama-index-core.llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "similarity",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.embeddings.base",
        "description": "reference_code.llama-index-core.llama_index.core.base.embeddings.base",
        "peekOfCode": "def similarity(\n    embedding1: Embedding,\n    embedding2: Embedding,\n    mode: SimilarityMode = SimilarityMode.DEFAULT,\n) -> float:\n    \"\"\"Get embedding similarity.\"\"\"\n    if mode == SimilarityMode.EUCLIDEAN:\n        # Using -euclidean distance as similarity to achieve same ranking order\n        return -float(np.linalg.norm(np.array(embedding1) - np.array(embedding2)))\n    elif mode == SimilarityMode.DOT_PRODUCT:",
        "detail": "reference_code.llama-index-core.llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "Embedding",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.embeddings.base",
        "description": "reference_code.llama-index-core.llama_index.core.base.embeddings.base",
        "peekOfCode": "Embedding = List[float]\nfrom llama_index.core.instrumentation.events.embedding import (\n    EmbeddingEndEvent,\n    EmbeddingStartEvent,\n)\nimport llama_index.core.instrumentation as instrument\ndispatcher = instrument.get_dispatcher(__name__)\nclass SimilarityMode(str, Enum):\n    \"\"\"Modes for similarity/distance.\"\"\"\n    DEFAULT = \"cosine\"",
        "detail": "reference_code.llama-index-core.llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "dispatcher",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.embeddings.base",
        "description": "reference_code.llama-index-core.llama_index.core.base.embeddings.base",
        "peekOfCode": "dispatcher = instrument.get_dispatcher(__name__)\nclass SimilarityMode(str, Enum):\n    \"\"\"Modes for similarity/distance.\"\"\"\n    DEFAULT = \"cosine\"\n    DOT_PRODUCT = \"dot_product\"\n    EUCLIDEAN = \"euclidean\"\ndef mean_agg(embeddings: List[Embedding]) -> Embedding:\n    \"\"\"Mean aggregation for embeddings.\"\"\"\n    return np.array(embeddings).mean(axis=0).tolist()\ndef similarity(",
        "detail": "reference_code.llama-index-core.llama_index.core.base.embeddings.base",
        "documentation": {}
    },
    {
        "label": "BaseSparseEmbedding",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.embeddings.base_sparse",
        "description": "reference_code.llama-index-core.llama_index.core.base.embeddings.base_sparse",
        "peekOfCode": "class BaseSparseEmbedding(BaseModel, DispatcherSpanMixin):\n    \"\"\"Base class for embeddings.\"\"\"\n    model_config = ConfigDict(\n        protected_namespaces=(\"pydantic_model_\",), arbitrary_types_allowed=True\n    )\n    model_name: str = Field(\n        default=\"unknown\", description=\"The name of the embedding model.\"\n    )\n    embed_batch_size: int = Field(\n        default=DEFAULT_EMBED_BATCH_SIZE,",
        "detail": "reference_code.llama-index-core.llama_index.core.base.embeddings.base_sparse",
        "documentation": {}
    },
    {
        "label": "sparse_similarity",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.embeddings.base_sparse",
        "description": "reference_code.llama-index-core.llama_index.core.base.embeddings.base_sparse",
        "peekOfCode": "def sparse_similarity(\n    embedding1: SparseEmbedding,\n    embedding2: SparseEmbedding,\n) -> float:\n    \"\"\"Get sparse embedding similarity.\"\"\"\n    if not embedding1 or not embedding2:\n        return 0.0\n    # Use the smaller embedding as the primary iteration set\n    if len(embedding1) > len(embedding2):\n        embedding1, embedding2 = embedding2, embedding1",
        "detail": "reference_code.llama-index-core.llama_index.core.base.embeddings.base_sparse",
        "documentation": {}
    },
    {
        "label": "mean_agg",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.embeddings.base_sparse",
        "description": "reference_code.llama-index-core.llama_index.core.base.embeddings.base_sparse",
        "peekOfCode": "def mean_agg(embeddings: List[SparseEmbedding]) -> SparseEmbedding:\n    \"\"\"Get mean aggregation of embeddings.\"\"\"\n    if not embeddings:\n        return {}\n    sum_dict: Dict[int, float] = defaultdict(float)\n    for embedding in embeddings:\n        for idx, value in embedding.items():\n            sum_dict[idx] += value\n    return {idx: value / len(embeddings) for idx, value in sum_dict.items()}\nclass BaseSparseEmbedding(BaseModel, DispatcherSpanMixin):",
        "detail": "reference_code.llama-index-core.llama_index.core.base.embeddings.base_sparse",
        "documentation": {}
    },
    {
        "label": "dispatcher",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.embeddings.base_sparse",
        "description": "reference_code.llama-index-core.llama_index.core.base.embeddings.base_sparse",
        "peekOfCode": "dispatcher = instrument.get_dispatcher(__name__)\nSparseEmbedding = Dict[int, float]\ndef sparse_similarity(\n    embedding1: SparseEmbedding,\n    embedding2: SparseEmbedding,\n) -> float:\n    \"\"\"Get sparse embedding similarity.\"\"\"\n    if not embedding1 or not embedding2:\n        return 0.0\n    # Use the smaller embedding as the primary iteration set",
        "detail": "reference_code.llama-index-core.llama_index.core.base.embeddings.base_sparse",
        "documentation": {}
    },
    {
        "label": "SparseEmbedding",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.embeddings.base_sparse",
        "description": "reference_code.llama-index-core.llama_index.core.base.embeddings.base_sparse",
        "peekOfCode": "SparseEmbedding = Dict[int, float]\ndef sparse_similarity(\n    embedding1: SparseEmbedding,\n    embedding2: SparseEmbedding,\n) -> float:\n    \"\"\"Get sparse embedding similarity.\"\"\"\n    if not embedding1 or not embedding2:\n        return 0.0\n    # Use the smaller embedding as the primary iteration set\n    if len(embedding1) > len(embedding2):",
        "detail": "reference_code.llama-index-core.llama_index.core.base.embeddings.base_sparse",
        "documentation": {}
    },
    {
        "label": "BaseLLM",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.base",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.base",
        "peekOfCode": "class BaseLLM(BaseComponent, DispatcherSpanMixin):\n    \"\"\"BaseLLM interface.\"\"\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    callback_manager: CallbackManager = Field(\n        default_factory=lambda: CallbackManager([]), exclude=True\n    )\n    @model_validator(mode=\"after\")\n    def check_callback_manager(self) -> \"BaseLLM\":\n        if self.callback_manager is None:\n            self.callback_manager = CallbackManager([])",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.base",
        "documentation": {}
    },
    {
        "label": "messages_to_history_str",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "peekOfCode": "def messages_to_history_str(messages: Sequence[ChatMessage]) -> str:\n    \"\"\"Convert messages to a history string.\"\"\"\n    string_messages = []\n    for message in messages:\n        role = message.role\n        content = message.content\n        string_message = f\"{role.value}: {content}\"\n        additional_kwargs = message.additional_kwargs\n        if additional_kwargs:\n            string_message += f\"\\n{additional_kwargs}\"",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "messages_to_prompt",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "peekOfCode": "def messages_to_prompt(messages: Sequence[ChatMessage]) -> str:\n    \"\"\"Convert messages to a prompt string.\"\"\"\n    string_messages = []\n    for message in messages:\n        role = message.role\n        content = message.content\n        string_message = f\"{role.value}: {content}\"\n        additional_kwargs = message.additional_kwargs\n        if additional_kwargs:\n            string_message += f\"\\n{additional_kwargs}\"",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "prompt_to_messages",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "peekOfCode": "def prompt_to_messages(prompt: str) -> List[ChatMessage]:\n    \"\"\"Convert a string prompt to a sequence of messages.\"\"\"\n    return [ChatMessage(role=MessageRole.USER, content=prompt)]\ndef completion_response_to_chat_response(\n    completion_response: CompletionResponse,\n) -> ChatResponse:\n    \"\"\"Convert a completion response to a chat response.\"\"\"\n    return ChatResponse(\n        message=ChatMessage(\n            role=MessageRole.ASSISTANT,",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "completion_response_to_chat_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "peekOfCode": "def completion_response_to_chat_response(\n    completion_response: CompletionResponse,\n) -> ChatResponse:\n    \"\"\"Convert a completion response to a chat response.\"\"\"\n    return ChatResponse(\n        message=ChatMessage(\n            role=MessageRole.ASSISTANT,\n            content=completion_response.text,\n            additional_kwargs=completion_response.additional_kwargs,\n        ),",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "stream_completion_response_to_chat_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "peekOfCode": "def stream_completion_response_to_chat_response(\n    completion_response_gen: CompletionResponseGen,\n) -> ChatResponseGen:\n    \"\"\"Convert a stream completion response to a stream chat response.\"\"\"\n    def gen() -> ChatResponseGen:\n        for response in completion_response_gen:\n            yield ChatResponse(\n                message=ChatMessage(\n                    role=MessageRole.ASSISTANT,\n                    content=response.text,",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "astream_completion_response_to_chat_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "peekOfCode": "def astream_completion_response_to_chat_response(\n    completion_response_gen: CompletionResponseAsyncGen,\n) -> ChatResponseAsyncGen:\n    \"\"\"Convert an async stream completion to an async stream chat response.\"\"\"\n    async def gen() -> ChatResponseAsyncGen:\n        async for response in completion_response_gen:\n            yield ChatResponse(\n                message=ChatMessage(\n                    role=MessageRole.ASSISTANT,\n                    content=response.text,",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "chat_response_to_completion_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "peekOfCode": "def chat_response_to_completion_response(\n    chat_response: ChatResponse,\n) -> CompletionResponse:\n    \"\"\"Convert a chat response to a completion response.\"\"\"\n    additional_kwargs = chat_response.message.additional_kwargs\n    additional_kwargs.update(chat_response.additional_kwargs)\n    return CompletionResponse(\n        text=chat_response.message.content or \"\",\n        additional_kwargs=additional_kwargs,\n        raw=chat_response.raw,",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "stream_chat_response_to_completion_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "peekOfCode": "def stream_chat_response_to_completion_response(\n    chat_response_gen: ChatResponseGen,\n) -> CompletionResponseGen:\n    \"\"\"Convert a stream chat response to a completion response.\"\"\"\n    def gen() -> CompletionResponseGen:\n        for response in chat_response_gen:\n            additional_kwargs = response.message.additional_kwargs\n            additional_kwargs.update(response.additional_kwargs)\n            yield CompletionResponse(\n                text=response.message.content or \"\",",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "completion_to_chat_decorator",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "peekOfCode": "def completion_to_chat_decorator(\n    func: Callable[..., CompletionResponse],\n) -> Callable[..., ChatResponse]:\n    \"\"\"Convert a completion function to a chat function.\"\"\"\n    def wrapper(messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        # normalize input\n        prompt = messages_to_prompt(messages)\n        completion_response = func(prompt, **kwargs)\n        # normalize output\n        return completion_response_to_chat_response(completion_response)",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "stream_completion_to_chat_decorator",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "peekOfCode": "def stream_completion_to_chat_decorator(\n    func: Callable[..., CompletionResponseGen],\n) -> Callable[..., ChatResponseGen]:\n    \"\"\"Convert a completion function to a chat function.\"\"\"\n    def wrapper(messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponseGen:\n        # normalize input\n        prompt = messages_to_prompt(messages)\n        completion_response = func(prompt, **kwargs)\n        # normalize output\n        return stream_completion_response_to_chat_response(completion_response)",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "chat_to_completion_decorator",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "peekOfCode": "def chat_to_completion_decorator(\n    func: Callable[..., ChatResponse],\n) -> Callable[..., CompletionResponse]:\n    \"\"\"Convert a chat function to a completion function.\"\"\"\n    def wrapper(prompt: str, **kwargs: Any) -> CompletionResponse:\n        # normalize input\n        messages = prompt_to_messages(prompt)\n        chat_response = func(messages, **kwargs)\n        # normalize output\n        return chat_response_to_completion_response(chat_response)",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "stream_chat_to_completion_decorator",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "peekOfCode": "def stream_chat_to_completion_decorator(\n    func: Callable[..., ChatResponseGen],\n) -> Callable[..., CompletionResponseGen]:\n    \"\"\"Convert a chat function to a completion function.\"\"\"\n    def wrapper(prompt: str, **kwargs: Any) -> CompletionResponseGen:\n        # normalize input\n        messages = prompt_to_messages(prompt)\n        chat_response = func(messages, **kwargs)\n        # normalize output\n        return stream_chat_response_to_completion_response(chat_response)",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "acompletion_to_chat_decorator",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "peekOfCode": "def acompletion_to_chat_decorator(\n    func: Callable[..., Awaitable[CompletionResponse]],\n) -> Callable[..., Awaitable[ChatResponse]]:\n    \"\"\"Convert a completion function to a chat function.\"\"\"\n    async def wrapper(messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        # normalize input\n        prompt = messages_to_prompt(messages)\n        completion_response = await func(prompt, **kwargs)\n        # normalize output\n        return completion_response_to_chat_response(completion_response)",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "achat_to_completion_decorator",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "peekOfCode": "def achat_to_completion_decorator(\n    func: Callable[..., Awaitable[ChatResponse]],\n) -> Callable[..., Awaitable[CompletionResponse]]:\n    \"\"\"Convert a chat function to a completion function.\"\"\"\n    async def wrapper(prompt: str, **kwargs: Any) -> CompletionResponse:\n        # normalize input\n        messages = prompt_to_messages(prompt)\n        chat_response = await func(messages, **kwargs)\n        # normalize output\n        return chat_response_to_completion_response(chat_response)",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "astream_completion_to_chat_decorator",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "peekOfCode": "def astream_completion_to_chat_decorator(\n    func: Callable[..., Awaitable[CompletionResponseAsyncGen]],\n) -> Callable[..., Awaitable[ChatResponseAsyncGen]]:\n    \"\"\"Convert a completion function to a chat function.\"\"\"\n    async def wrapper(\n        messages: Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseAsyncGen:\n        # normalize input\n        prompt = messages_to_prompt(messages)\n        completion_response = await func(prompt, **kwargs)",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "astream_chat_to_completion_decorator",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "peekOfCode": "def astream_chat_to_completion_decorator(\n    func: Callable[..., Awaitable[ChatResponseAsyncGen]],\n) -> Callable[..., Awaitable[CompletionResponseAsyncGen]]:\n    \"\"\"Convert a chat function to a completion function.\"\"\"\n    async def wrapper(prompt: str, **kwargs: Any) -> CompletionResponseAsyncGen:\n        # normalize input\n        messages = prompt_to_messages(prompt)\n        chat_response = await func(messages, **kwargs)\n        # normalize output\n        return astream_chat_response_to_completion_response(chat_response)",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "async_stream_completion_response_to_chat_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "peekOfCode": "def async_stream_completion_response_to_chat_response(\n    completion_response_gen: CompletionResponseAsyncGen,\n) -> ChatResponseAsyncGen:\n    \"\"\"Convert a stream completion response to a stream chat response.\"\"\"\n    async def gen() -> ChatResponseAsyncGen:\n        async for response in completion_response_gen:\n            yield ChatResponse(\n                message=ChatMessage(\n                    role=MessageRole.ASSISTANT,\n                    content=response.text,",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "astream_chat_response_to_completion_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "peekOfCode": "def astream_chat_response_to_completion_response(\n    chat_response_gen: ChatResponseAsyncGen,\n) -> CompletionResponseAsyncGen:\n    \"\"\"Convert a stream chat response to a completion response.\"\"\"\n    async def gen() -> CompletionResponseAsyncGen:\n        async for response in chat_response_gen:\n            additional_kwargs = response.message.additional_kwargs\n            additional_kwargs.update(response.additional_kwargs)\n            yield CompletionResponse(\n                text=response.message.content or \"\",",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "get_from_param_or_env",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "peekOfCode": "def get_from_param_or_env(\n    key: str,\n    param: Optional[str] = None,\n    env_key: Optional[str] = None,\n    default: Optional[str] = None,\n) -> str:\n    \"\"\"Get a value from a param or an environment variable.\"\"\"\n    if param is not None:\n        return param\n    elif env_key and env_key in os.environ and os.environ[env_key]:",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "image_node_to_image_block",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "peekOfCode": "def image_node_to_image_block(image_node: ImageNode) -> ImageBlock:\n    \"\"\"\n    Get an ImageBlock from an ImageNode.\n    Args:\n        image_node (ImageNode): ImageNode to convert.\n    Returns:\n        ImageBlock: block representation of the node.\n    Raises:\n        ValueError: when the image provided within the ImageNode is not correctly base64-encoded.\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "MessageRole",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "peekOfCode": "class MessageRole(str, Enum):\n    \"\"\"Message role.\"\"\"\n    SYSTEM = \"system\"\n    DEVELOPER = \"developer\"\n    USER = \"user\"\n    ASSISTANT = \"assistant\"\n    FUNCTION = \"function\"\n    TOOL = \"tool\"\n    CHATBOT = \"chatbot\"\n    MODEL = \"model\"",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "TextBlock",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "peekOfCode": "class TextBlock(BaseModel):\n    \"\"\"A representation of text data to directly pass to/from the LLM.\"\"\"\n    block_type: Literal[\"text\"] = \"text\"\n    text: str\nclass ImageBlock(BaseModel):\n    \"\"\"A representation of image data to directly pass to/from the LLM.\"\"\"\n    block_type: Literal[\"image\"] = \"image\"\n    image: bytes | None = None\n    path: FilePath | None = None\n    url: AnyUrl | str | None = None",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ImageBlock",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "peekOfCode": "class ImageBlock(BaseModel):\n    \"\"\"A representation of image data to directly pass to/from the LLM.\"\"\"\n    block_type: Literal[\"image\"] = \"image\"\n    image: bytes | None = None\n    path: FilePath | None = None\n    url: AnyUrl | str | None = None\n    image_mimetype: str | None = None\n    detail: str | None = None\n    @field_validator(\"url\", mode=\"after\")\n    @classmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "AudioBlock",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "peekOfCode": "class AudioBlock(BaseModel):\n    \"\"\"A representation of audio data to directly pass to/from the LLM.\"\"\"\n    block_type: Literal[\"audio\"] = \"audio\"\n    audio: bytes | None = None\n    path: FilePath | None = None\n    url: AnyUrl | str | None = None\n    format: str | None = None\n    @field_validator(\"url\", mode=\"after\")\n    @classmethod\n    def urlstr_to_anyurl(cls, url: str | AnyUrl) -> AnyUrl:",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "DocumentBlock",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "peekOfCode": "class DocumentBlock(BaseModel):\n    \"\"\"A representation of a document to directly pass to the LLM.\"\"\"\n    block_type: Literal[\"document\"] = \"document\"\n    data: Optional[bytes] = None\n    path: Optional[Union[FilePath | str]] = None\n    url: Optional[str] = None\n    title: Optional[str] = None\n    document_mimetype: Optional[str] = None\n    @model_validator(mode=\"after\")\n    def document_validation(self) -> Self:",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CacheControl",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "peekOfCode": "class CacheControl(BaseModel):\n    type: str\n    ttl: str = Field(default=\"5m\")\nclass CachePoint(BaseModel):\n    \"\"\"Used to set the point to cache up to, if the LLM supports caching.\"\"\"\n    block_type: Literal[\"cache\"] = \"cache\"\n    cache_control: CacheControl\nclass CitableBlock(BaseModel):\n    \"\"\"Supports providing citable content to LLMs that have built-in citation support.\"\"\"\n    block_type: Literal[\"citable\"] = \"citable\"",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CachePoint",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "peekOfCode": "class CachePoint(BaseModel):\n    \"\"\"Used to set the point to cache up to, if the LLM supports caching.\"\"\"\n    block_type: Literal[\"cache\"] = \"cache\"\n    cache_control: CacheControl\nclass CitableBlock(BaseModel):\n    \"\"\"Supports providing citable content to LLMs that have built-in citation support.\"\"\"\n    block_type: Literal[\"citable\"] = \"citable\"\n    title: str\n    source: str\n    # TODO: We could maybe expand the types here,",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CitableBlock",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "peekOfCode": "class CitableBlock(BaseModel):\n    \"\"\"Supports providing citable content to LLMs that have built-in citation support.\"\"\"\n    block_type: Literal[\"citable\"] = \"citable\"\n    title: str\n    source: str\n    # TODO: We could maybe expand the types here,\n    # limiting for now to known use cases\n    content: List[\n        Annotated[\n            Union[TextBlock, ImageBlock, DocumentBlock],",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CitationBlock",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "peekOfCode": "class CitationBlock(BaseModel):\n    \"\"\"A representation of cited content from past messages.\"\"\"\n    block_type: Literal[\"citation\"] = \"citation\"\n    cited_content: Annotated[\n        Union[TextBlock, ImageBlock], Field(discriminator=\"block_type\")\n    ]\n    source: str\n    title: str\n    additional_location_info: Dict[str, int]\n    @field_validator(\"cited_content\", mode=\"before\")",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "peekOfCode": "class ChatMessage(BaseModel):\n    \"\"\"Chat message.\"\"\"\n    role: MessageRole = MessageRole.USER\n    additional_kwargs: dict[str, Any] = Field(default_factory=dict)\n    blocks: list[ContentBlock] = Field(default_factory=list)\n    def __init__(self, /, content: Any | None = None, **data: Any) -> None:\n        \"\"\"\n        Keeps backward compatibility with the old `content` field.\n        If content was passed and contained text, store a single TextBlock.\n        If content was passed and it was a list, assume it's a list of content blocks and store it.",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "LogProb",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "peekOfCode": "class LogProb(BaseModel):\n    \"\"\"LogProb of a token.\"\"\"\n    token: str = Field(default_factory=str)\n    logprob: float = Field(default_factory=float)\n    bytes: List[int] = Field(default_factory=list)\n# ===== Generic Model Output - Chat =====\nclass ChatResponse(BaseModel):\n    \"\"\"Chat response.\"\"\"\n    message: ChatMessage\n    raw: Optional[Any] = None",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponse",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "peekOfCode": "class ChatResponse(BaseModel):\n    \"\"\"Chat response.\"\"\"\n    message: ChatMessage\n    raw: Optional[Any] = None\n    delta: Optional[str] = None\n    logprobs: Optional[List[List[LogProb]]] = None\n    additional_kwargs: dict = Field(default_factory=dict)\n    def __str__(self) -> str:\n        return str(self.message)\nChatResponseGen = Generator[ChatResponse, None, None]",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponse",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "peekOfCode": "class CompletionResponse(BaseModel):\n    \"\"\"\n    Completion response.\n    Fields:\n        text: Text content of the response if not streaming, or if streaming,\n            the current extent of streamed text.\n        additional_kwargs: Additional information on the response(i.e. token\n            counts, function calling information).\n        raw: Optional raw JSON that was parsed to populate text, if relevant.\n        delta: New text that just streamed in (only relevant when streaming).",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "LLMMetadata",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "peekOfCode": "class LLMMetadata(BaseModel):\n    model_config = ConfigDict(\n        protected_namespaces=(\"pydantic_model_\",), arbitrary_types_allowed=True\n    )\n    context_window: int = Field(\n        default=DEFAULT_CONTEXT_WINDOW,\n        description=(\n            \"Total number of tokens the model can be input and output for one response.\"\n        ),\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ContentBlock",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "peekOfCode": "ContentBlock = Annotated[\n    Union[\n        TextBlock,\n        ImageBlock,\n        AudioBlock,\n        DocumentBlock,\n        CachePoint,\n        CitableBlock,\n        CitationBlock,\n    ],",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseGen",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "peekOfCode": "ChatResponseGen = Generator[ChatResponse, None, None]\nChatResponseAsyncGen = AsyncGenerator[ChatResponse, None]\n# ===== Generic Model Output - Completion =====\nclass CompletionResponse(BaseModel):\n    \"\"\"\n    Completion response.\n    Fields:\n        text: Text content of the response if not streaming, or if streaming,\n            the current extent of streamed text.\n        additional_kwargs: Additional information on the response(i.e. token",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "ChatResponseAsyncGen",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "peekOfCode": "ChatResponseAsyncGen = AsyncGenerator[ChatResponse, None]\n# ===== Generic Model Output - Completion =====\nclass CompletionResponse(BaseModel):\n    \"\"\"\n    Completion response.\n    Fields:\n        text: Text content of the response if not streaming, or if streaming,\n            the current extent of streamed text.\n        additional_kwargs: Additional information on the response(i.e. token\n            counts, function calling information).",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponseGen",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "peekOfCode": "CompletionResponseGen = Generator[CompletionResponse, None, None]\nCompletionResponseAsyncGen = AsyncGenerator[CompletionResponse, None]\nclass LLMMetadata(BaseModel):\n    model_config = ConfigDict(\n        protected_namespaces=(\"pydantic_model_\",), arbitrary_types_allowed=True\n    )\n    context_window: int = Field(\n        default=DEFAULT_CONTEXT_WINDOW,\n        description=(\n            \"Total number of tokens the model can be input and output for one response.\"",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "CompletionResponseAsyncGen",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "description": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "peekOfCode": "CompletionResponseAsyncGen = AsyncGenerator[CompletionResponse, None]\nclass LLMMetadata(BaseModel):\n    model_config = ConfigDict(\n        protected_namespaces=(\"pydantic_model_\",), arbitrary_types_allowed=True\n    )\n    context_window: int = Field(\n        default=DEFAULT_CONTEXT_WINDOW,\n        description=(\n            \"Total number of tokens the model can be input and output for one response.\"\n        ),",
        "detail": "reference_code.llama-index-core.llama_index.core.base.llms.types",
        "documentation": {}
    },
    {
        "label": "Response",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.response.schema",
        "description": "reference_code.llama-index-core.llama_index.core.base.response.schema",
        "peekOfCode": "class Response:\n    \"\"\"\n    Response object.\n    Returned if streaming=False.\n    Attributes:\n        response: The response text.\n    \"\"\"\n    response: Optional[str]\n    source_nodes: List[NodeWithScore] = field(default_factory=list)\n    metadata: Optional[Dict[str, Any]] = None",
        "detail": "reference_code.llama-index-core.llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "PydanticResponse",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.response.schema",
        "description": "reference_code.llama-index-core.llama_index.core.base.response.schema",
        "peekOfCode": "class PydanticResponse:\n    \"\"\"\n    PydanticResponse object.\n    Returned if streaming=False.\n    Attributes:\n        response: The response text.\n    \"\"\"\n    response: Optional[BaseModel]\n    source_nodes: List[NodeWithScore] = field(default_factory=list)\n    metadata: Optional[Dict[str, Any]] = None",
        "detail": "reference_code.llama-index-core.llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "StreamingResponse",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.response.schema",
        "description": "reference_code.llama-index-core.llama_index.core.base.response.schema",
        "peekOfCode": "class StreamingResponse:\n    \"\"\"\n    StreamingResponse object.\n    Returned if streaming=True.\n    Attributes:\n        response_gen: The response generator.\n    \"\"\"\n    response_gen: TokenGen\n    source_nodes: List[NodeWithScore] = field(default_factory=list)\n    metadata: Optional[Dict[str, Any]] = None",
        "detail": "reference_code.llama-index-core.llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "AsyncStreamingResponse",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.response.schema",
        "description": "reference_code.llama-index-core.llama_index.core.base.response.schema",
        "peekOfCode": "class AsyncStreamingResponse:\n    \"\"\"\n    AsyncStreamingResponse object.\n    Returned if streaming=True while using async.\n    Attributes:\n        _async_response_gen: The response async generator.\n    \"\"\"\n    response_gen: TokenAsyncGen\n    source_nodes: List[NodeWithScore] = field(default_factory=list)\n    metadata: Optional[Dict[str, Any]] = None",
        "detail": "reference_code.llama-index-core.llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TYPE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.response.schema",
        "description": "reference_code.llama-index-core.llama_index.core.base.response.schema",
        "peekOfCode": "RESPONSE_TYPE = Union[\n    Response, StreamingResponse, AsyncStreamingResponse, PydanticResponse\n]",
        "detail": "reference_code.llama-index-core.llama_index.core.base.response.schema",
        "documentation": {}
    },
    {
        "label": "BaseAutoRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.base_auto_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.base.base_auto_retriever",
        "peekOfCode": "class BaseAutoRetriever(BaseRetriever):\n    \"\"\"Base auto retriever.\"\"\"\n    @abstractmethod\n    def generate_retrieval_spec(\n        self, query_bundle: QueryBundle, **kwargs: Any\n    ) -> BaseModel:\n        \"\"\"Generate retrieval spec synchronously.\"\"\"\n        ...\n    @abstractmethod\n    async def agenerate_retrieval_spec(",
        "detail": "reference_code.llama-index-core.llama_index.core.base.base_auto_retriever",
        "documentation": {}
    },
    {
        "label": "MultiModalRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.base_multi_modal_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.base.base_multi_modal_retriever",
        "peekOfCode": "class MultiModalRetriever(BaseRetriever, BaseImageRetriever):\n    \"\"\"Multi Modal base retriever.\"\"\"\n    @abstractmethod\n    def text_retrieve(self, str_or_query_bundle: QueryType) -> List[NodeWithScore]:\n        \"\"\"\n        Retrieve text nodes given text query.\n        Implemented by the user.\n        \"\"\"\n    @abstractmethod\n    def text_to_image_retrieve(",
        "detail": "reference_code.llama-index-core.llama_index.core.base.base_multi_modal_retriever",
        "documentation": {}
    },
    {
        "label": "BaseQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.base_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.base.base_query_engine",
        "peekOfCode": "class BaseQueryEngine(PromptMixin, DispatcherSpanMixin):\n    \"\"\"Base query engine.\"\"\"\n    def __init__(\n        self,\n        callback_manager: Optional[CallbackManager],\n    ) -> None:\n        self.callback_manager = callback_manager or CallbackManager([])\n    def _get_prompts(self) -> Dict[str, Any]:\n        \"\"\"Get prompts.\"\"\"\n        return {}",
        "detail": "reference_code.llama-index-core.llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "dispatcher",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.base_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.base.base_query_engine",
        "peekOfCode": "dispatcher = instrument.get_dispatcher(__name__)\nlogger = logging.getLogger(__name__)\nclass BaseQueryEngine(PromptMixin, DispatcherSpanMixin):\n    \"\"\"Base query engine.\"\"\"\n    def __init__(\n        self,\n        callback_manager: Optional[CallbackManager],\n    ) -> None:\n        self.callback_manager = callback_manager or CallbackManager([])\n    def _get_prompts(self) -> Dict[str, Any]:",
        "detail": "reference_code.llama-index-core.llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.base_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.base.base_query_engine",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass BaseQueryEngine(PromptMixin, DispatcherSpanMixin):\n    \"\"\"Base query engine.\"\"\"\n    def __init__(\n        self,\n        callback_manager: Optional[CallbackManager],\n    ) -> None:\n        self.callback_manager = callback_manager or CallbackManager([])\n    def _get_prompts(self) -> Dict[str, Any]:\n        \"\"\"Get prompts.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.base.base_query_engine",
        "documentation": {}
    },
    {
        "label": "BaseRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.base_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.base.base_retriever",
        "peekOfCode": "class BaseRetriever(PromptMixin, DispatcherSpanMixin):\n    \"\"\"Base retriever.\"\"\"\n    def __init__(\n        self,\n        callback_manager: Optional[CallbackManager] = None,\n        object_map: Optional[Dict] = None,\n        objects: Optional[List[IndexNode]] = None,\n        verbose: bool = False,\n    ) -> None:\n        self.callback_manager = callback_manager or CallbackManager()",
        "detail": "reference_code.llama-index-core.llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "dispatcher",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.base_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.base.base_retriever",
        "peekOfCode": "dispatcher = instrument.get_dispatcher(__name__)\nclass BaseRetriever(PromptMixin, DispatcherSpanMixin):\n    \"\"\"Base retriever.\"\"\"\n    def __init__(\n        self,\n        callback_manager: Optional[CallbackManager] = None,\n        object_map: Optional[Dict] = None,\n        objects: Optional[List[IndexNode]] = None,\n        verbose: bool = False,\n    ) -> None:",
        "detail": "reference_code.llama-index-core.llama_index.core.base.base_retriever",
        "documentation": {}
    },
    {
        "label": "SingleSelection",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.base_selector",
        "description": "reference_code.llama-index-core.llama_index.core.base.base_selector",
        "peekOfCode": "class SingleSelection(BaseModel):\n    \"\"\"A single selection of a choice.\"\"\"\n    index: int\n    reason: str\nclass MultiSelection(BaseModel):\n    \"\"\"A multi-selection of choices.\"\"\"\n    selections: List[SingleSelection]\n    @property\n    def ind(self) -> int:\n        if len(self.selections) != 1:",
        "detail": "reference_code.llama-index-core.llama_index.core.base.base_selector",
        "documentation": {}
    },
    {
        "label": "MultiSelection",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.base_selector",
        "description": "reference_code.llama-index-core.llama_index.core.base.base_selector",
        "peekOfCode": "class MultiSelection(BaseModel):\n    \"\"\"A multi-selection of choices.\"\"\"\n    selections: List[SingleSelection]\n    @property\n    def ind(self) -> int:\n        if len(self.selections) != 1:\n            raise ValueError(\n                f\"There are {len(self.selections)} selections, please use .inds.\"\n            )\n        return self.selections[0].index",
        "detail": "reference_code.llama-index-core.llama_index.core.base.base_selector",
        "documentation": {}
    },
    {
        "label": "BaseSelector",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.base_selector",
        "description": "reference_code.llama-index-core.llama_index.core.base.base_selector",
        "peekOfCode": "class BaseSelector(PromptMixin, DispatcherSpanMixin):\n    \"\"\"Base selector.\"\"\"\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\"\"\"\n        return {}\n    def select(\n        self, choices: Sequence[MetadataType], query: QueryType\n    ) -> SelectorResult:\n        metadatas = [_wrap_choice(choice) for choice in choices]\n        query_bundle = _wrap_query(query)",
        "detail": "reference_code.llama-index-core.llama_index.core.base.base_selector",
        "documentation": {}
    },
    {
        "label": "MetadataType",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.base_selector",
        "description": "reference_code.llama-index-core.llama_index.core.base.base_selector",
        "peekOfCode": "MetadataType = Union[str, ToolMetadata]\nclass SingleSelection(BaseModel):\n    \"\"\"A single selection of a choice.\"\"\"\n    index: int\n    reason: str\nclass MultiSelection(BaseModel):\n    \"\"\"A multi-selection of choices.\"\"\"\n    selections: List[SingleSelection]\n    @property\n    def ind(self) -> int:",
        "detail": "reference_code.llama-index-core.llama_index.core.base.base_selector",
        "documentation": {}
    },
    {
        "label": "SelectorResult",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.base.base_selector",
        "description": "reference_code.llama-index-core.llama_index.core.base.base_selector",
        "peekOfCode": "SelectorResult = MultiSelection\ndef _wrap_choice(choice: MetadataType) -> ToolMetadata:\n    if isinstance(choice, ToolMetadata):\n        return choice\n    elif isinstance(choice, str):\n        return ToolMetadata(description=choice)\n    else:\n        raise ValueError(f\"Unexpected type: {type(choice)}\")\ndef _wrap_query(query: QueryType) -> QueryBundle:\n    if isinstance(query, QueryBundle):",
        "detail": "reference_code.llama-index-core.llama_index.core.base.base_selector",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.bridge.langchain",
        "description": "reference_code.llama-index-core.llama_index.core.bridge.langchain",
        "peekOfCode": "__all__ = [\n    \"langchain\",\n    \"BaseLLM\",\n    \"FakeListLLM\",\n    \"OpenAI\",\n    \"AI21\",\n    \"Cohere\",\n    \"BaseChatModel\",\n    \"ChatAnyscale\",\n    \"ChatOpenAI\",",
        "detail": "reference_code.llama-index-core.llama_index.core.bridge.langchain",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.bridge.pydantic",
        "description": "reference_code.llama-index-core.llama_index.core.bridge.pydantic",
        "peekOfCode": "__all__ = [\n    \"pydantic\",\n    \"BaseModel\",\n    \"ConfigDict\",\n    \"GetJsonSchemaHandler\",\n    \"GetCoreSchemaHandler\",\n    \"Field\",\n    \"PlainSerializer\",\n    \"PrivateAttr\",\n    \"model_validator\",",
        "detail": "reference_code.llama-index-core.llama_index.core.bridge.pydantic",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.bridge.pydantic_core",
        "description": "reference_code.llama-index-core.llama_index.core.bridge.pydantic_core",
        "peekOfCode": "__all__ = [\"pydantic_core\", \"CoreSchema\", \"core_schema\"]",
        "detail": "reference_code.llama-index-core.llama_index.core.bridge.pydantic_core",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.bridge.pydantic_settings",
        "description": "reference_code.llama-index-core.llama_index.core.bridge.pydantic_settings",
        "peekOfCode": "__all__ = [\"pydantic_settings\", \"BaseSettings\", \"SettingsConfigDict\"]",
        "detail": "reference_code.llama-index-core.llama_index.core.bridge.pydantic_settings",
        "documentation": {}
    },
    {
        "label": "CallbackManager",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.base",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.base",
        "peekOfCode": "class CallbackManager(BaseCallbackHandler, ABC):\n    \"\"\"\n    Callback manager that handles callbacks for events within LlamaIndex.\n    The callback manager provides a way to call handlers on event starts/ends.\n    Additionally, the callback manager traces the current stack of events.\n    It does this by using a few key attributes.\n    - trace_stack - The current stack of events that have not ended yet.\n                    When an event ends, it's removed from the stack.\n                    Since this is a contextvar, it is unique to each\n                    thread/task.",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "EventContext",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.base",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.base",
        "peekOfCode": "class EventContext:\n    \"\"\"\n    Simple wrapper to call callbacks on event starts and ends\n    with an event type and id.\n    \"\"\"\n    def __init__(\n        self,\n        callback_manager: CallbackManager,\n        event_type: CBEventType,\n        event_id: Optional[str] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.base",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.base",
        "peekOfCode": "logger = logging.getLogger(__name__)\nglobal_stack_trace = ContextVar(\"trace\", default=[BASE_TRACE_EVENT])\nempty_trace_ids: List[str] = []\nglobal_stack_trace_ids = ContextVar(\"trace_ids\", default=empty_trace_ids)\nclass CallbackManager(BaseCallbackHandler, ABC):\n    \"\"\"\n    Callback manager that handles callbacks for events within LlamaIndex.\n    The callback manager provides a way to call handlers on event starts/ends.\n    Additionally, the callback manager traces the current stack of events.\n    It does this by using a few key attributes.",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "global_stack_trace",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.base",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.base",
        "peekOfCode": "global_stack_trace = ContextVar(\"trace\", default=[BASE_TRACE_EVENT])\nempty_trace_ids: List[str] = []\nglobal_stack_trace_ids = ContextVar(\"trace_ids\", default=empty_trace_ids)\nclass CallbackManager(BaseCallbackHandler, ABC):\n    \"\"\"\n    Callback manager that handles callbacks for events within LlamaIndex.\n    The callback manager provides a way to call handlers on event starts/ends.\n    Additionally, the callback manager traces the current stack of events.\n    It does this by using a few key attributes.\n    - trace_stack - The current stack of events that have not ended yet.",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "global_stack_trace_ids",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.base",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.base",
        "peekOfCode": "global_stack_trace_ids = ContextVar(\"trace_ids\", default=empty_trace_ids)\nclass CallbackManager(BaseCallbackHandler, ABC):\n    \"\"\"\n    Callback manager that handles callbacks for events within LlamaIndex.\n    The callback manager provides a way to call handlers on event starts/ends.\n    Additionally, the callback manager traces the current stack of events.\n    It does this by using a few key attributes.\n    - trace_stack - The current stack of events that have not ended yet.\n                    When an event ends, it's removed from the stack.\n                    Since this is a contextvar, it is unique to each",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.base",
        "documentation": {}
    },
    {
        "label": "BaseCallbackHandler",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.base_handler",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.base_handler",
        "peekOfCode": "class BaseCallbackHandler(ABC):\n    \"\"\"Base callback handler that can be used to track event starts and ends.\"\"\"\n    def __init__(\n        self,\n        event_starts_to_ignore: List[CBEventType],\n        event_ends_to_ignore: List[CBEventType],\n    ) -> None:\n        \"\"\"Initialize the base callback handler.\"\"\"\n        self.event_starts_to_ignore = tuple(event_starts_to_ignore)\n        self.event_ends_to_ignore = tuple(event_ends_to_ignore)",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.base_handler",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.base_handler",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.base_handler",
        "peekOfCode": "logger = logging.getLogger(__name__)\nglobal_stack_trace = ContextVar(\"trace\", default=[BASE_TRACE_EVENT])\nclass BaseCallbackHandler(ABC):\n    \"\"\"Base callback handler that can be used to track event starts and ends.\"\"\"\n    def __init__(\n        self,\n        event_starts_to_ignore: List[CBEventType],\n        event_ends_to_ignore: List[CBEventType],\n    ) -> None:\n        \"\"\"Initialize the base callback handler.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.base_handler",
        "documentation": {}
    },
    {
        "label": "global_stack_trace",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.base_handler",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.base_handler",
        "peekOfCode": "global_stack_trace = ContextVar(\"trace\", default=[BASE_TRACE_EVENT])\nclass BaseCallbackHandler(ABC):\n    \"\"\"Base callback handler that can be used to track event starts and ends.\"\"\"\n    def __init__(\n        self,\n        event_starts_to_ignore: List[CBEventType],\n        event_ends_to_ignore: List[CBEventType],\n    ) -> None:\n        \"\"\"Initialize the base callback handler.\"\"\"\n        self.event_starts_to_ignore = tuple(event_starts_to_ignore)",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.base_handler",
        "documentation": {}
    },
    {
        "label": "set_global_handler",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.global_handlers",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.global_handlers",
        "peekOfCode": "def set_global_handler(eval_mode: str, **eval_params: Any) -> None:\n    \"\"\"Set global eval handlers.\"\"\"\n    import llama_index.core\n    handler = create_global_handler(eval_mode, **eval_params)\n    if handler:\n        llama_index.core.global_handler = handler\ndef create_global_handler(\n    eval_mode: str, **eval_params: Any\n) -> Optional[BaseCallbackHandler]:\n    \"\"\"Get global eval handler.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.global_handlers",
        "documentation": {}
    },
    {
        "label": "create_global_handler",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.global_handlers",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.global_handlers",
        "peekOfCode": "def create_global_handler(\n    eval_mode: str, **eval_params: Any\n) -> Optional[BaseCallbackHandler]:\n    \"\"\"Get global eval handler.\"\"\"\n    handler: Optional[BaseCallbackHandler] = None\n    if eval_mode == \"wandb\":\n        try:\n            from llama_index.callbacks.wandb import (\n                WandbCallbackHandler,\n            )  # pants: no-infer-dep",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.global_handlers",
        "documentation": {}
    },
    {
        "label": "LlamaDebugHandler",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.llama_debug",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.llama_debug",
        "peekOfCode": "class LlamaDebugHandler(PythonicallyPrintingBaseHandler):\n    \"\"\"\n    Callback handler that keeps track of debug info.\n    NOTE: this is a beta feature. The usage within our codebase, and the interface\n    may change.\n    This handler simply keeps track of event starts/ends, separated by event types.\n    You can use this callback handler to keep track of and debug events.\n    Args:\n        event_starts_to_ignore (Optional[List[CBEventType]]): list of event types to\n            ignore when tracking event starts.",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.llama_debug",
        "documentation": {}
    },
    {
        "label": "PythonicallyPrintingBaseHandler",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.pythonically_printing_base_handler",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.pythonically_printing_base_handler",
        "peekOfCode": "class PythonicallyPrintingBaseHandler(BaseCallbackHandler):\n    \"\"\"\n    Callback handler that prints logs in a Pythonic way. That is, not using `print` at all; use the logger instead.\n    See https://stackoverflow.com/a/6918596/1147061 for why you should prefer using a logger over `print`.\n    This class is meant to be subclassed, not used directly.\n    Using this class, your LlamaIndex Callback Handlers can now make use of vanilla Python logging handlers now.\n    One popular choice is https://rich.readthedocs.io/en/stable/logging.html#logging-handler.\n    \"\"\"\n    def __init__(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.pythonically_printing_base_handler",
        "documentation": {}
    },
    {
        "label": "CBEventType",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.schema",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.schema",
        "peekOfCode": "class CBEventType(str, Enum):\n    \"\"\"\n    Callback manager event types.\n    Attributes:\n        CHUNKING: Logs for the before and after of text splitting.\n        NODE_PARSING: Logs for the documents and the nodes that they are parsed into.\n        EMBEDDING: Logs for the number of texts embedded.\n        LLM: Logs for the template and response of LLM calls.\n        QUERY: Keeps track of the start and end of each query.\n        RETRIEVE: Logs for the nodes retrieved for a query.",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventPayload",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.schema",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.schema",
        "peekOfCode": "class EventPayload(str, Enum):\n    DOCUMENTS = \"documents\"  # list of documents before parsing\n    CHUNKS = \"chunks\"  # list of text chunks\n    NODES = \"nodes\"  # list of nodes\n    PROMPT = \"formatted_prompt\"  # formatted prompt sent to LLM\n    MESSAGES = \"messages\"  # list of messages sent to LLM\n    COMPLETION = \"completion\"  # completion from LLM\n    RESPONSE = \"response\"  # message response from LLM\n    QUERY_STR = \"query_str\"  # query used for query engine\n    SUB_QUESTION = \"sub_question\"  # a sub question & answer + sources",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "CBEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.schema",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.schema",
        "peekOfCode": "class CBEvent:\n    \"\"\"Generic class to store event information.\"\"\"\n    event_type: CBEventType\n    payload: Optional[Dict[str, Any]] = None\n    time: str = \"\"\n    id_: str = \"\"\n    def __post_init__(self) -> None:\n        \"\"\"Init time and id if needed.\"\"\"\n        if not self.time:\n            self.time = datetime.now().strftime(TIMESTAMP_FORMAT)",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "EventStats",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.schema",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.schema",
        "peekOfCode": "class EventStats:\n    \"\"\"Time-based Statistics for events.\"\"\"\n    total_secs: float\n    average_secs: float\n    total_count: int",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "TIMESTAMP_FORMAT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.schema",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.schema",
        "peekOfCode": "TIMESTAMP_FORMAT = \"%m/%d/%Y, %H:%M:%S.%f\"\n# base trace_id for the tracemap in callback_manager\nBASE_TRACE_EVENT = \"root\"\nclass CBEventType(str, Enum):\n    \"\"\"\n    Callback manager event types.\n    Attributes:\n        CHUNKING: Logs for the before and after of text splitting.\n        NODE_PARSING: Logs for the documents and the nodes that they are parsed into.\n        EMBEDDING: Logs for the number of texts embedded.",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "BASE_TRACE_EVENT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.schema",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.schema",
        "peekOfCode": "BASE_TRACE_EVENT = \"root\"\nclass CBEventType(str, Enum):\n    \"\"\"\n    Callback manager event types.\n    Attributes:\n        CHUNKING: Logs for the before and after of text splitting.\n        NODE_PARSING: Logs for the documents and the nodes that they are parsed into.\n        EMBEDDING: Logs for the number of texts embedded.\n        LLM: Logs for the template and response of LLM calls.\n        QUERY: Keeps track of the start and end of each query.",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "LEAF_EVENTS",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.schema",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.schema",
        "peekOfCode": "LEAF_EVENTS = (CBEventType.CHUNKING, CBEventType.LLM, CBEventType.EMBEDDING)\n@dataclass\nclass CBEvent:\n    \"\"\"Generic class to store event information.\"\"\"\n    event_type: CBEventType\n    payload: Optional[Dict[str, Any]] = None\n    time: str = \"\"\n    id_: str = \"\"\n    def __post_init__(self) -> None:\n        \"\"\"Init time and id if needed.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.schema",
        "documentation": {}
    },
    {
        "label": "SimpleLLMHandler",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.simple_llm_handler",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.simple_llm_handler",
        "peekOfCode": "class SimpleLLMHandler(PythonicallyPrintingBaseHandler):\n    \"\"\"Callback handler for printing llms inputs/outputs.\"\"\"\n    def __init__(self, logger: Optional[logging.Logger] = None) -> None:\n        super().__init__(\n            event_starts_to_ignore=[], event_ends_to_ignore=[], logger=logger\n        )\n    def start_trace(self, trace_id: Optional[str] = None) -> None:\n        return\n    def end_trace(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.simple_llm_handler",
        "documentation": {}
    },
    {
        "label": "TokenCountingEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.token_counting",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.token_counting",
        "peekOfCode": "class TokenCountingEvent:\n    prompt: str\n    completion: str\n    completion_token_count: int\n    prompt_token_count: int\n    total_token_count: int = 0\n    event_id: str = \"\"\n    def __post_init__(self) -> None:\n        self.total_token_count = self.prompt_token_count + self.completion_token_count\ndef get_tokens_from_response(",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.token_counting",
        "documentation": {}
    },
    {
        "label": "TokenCountingHandler",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.token_counting",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.token_counting",
        "peekOfCode": "class TokenCountingHandler(PythonicallyPrintingBaseHandler):\n    \"\"\"\n    Callback handler for counting tokens in LLM and Embedding events.\n    Args:\n        tokenizer:\n            Tokenizer to use. Defaults to the global tokenizer\n            (see llama_index.core.utils.globals_helper).\n        event_starts_to_ignore: List of event types to ignore at the start of a trace.\n        event_ends_to_ignore: List of event types to ignore at the end of a trace.\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.token_counting",
        "documentation": {}
    },
    {
        "label": "get_tokens_from_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.token_counting",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.token_counting",
        "peekOfCode": "def get_tokens_from_response(\n    response: Union[\"CompletionResponse\", \"ChatResponse\"],\n) -> Tuple[int, int]:\n    \"\"\"Get the token counts from a raw response.\"\"\"\n    raw_response = response.raw\n    if not isinstance(raw_response, dict):\n        raw_response = dict(raw_response or {})\n    usage = raw_response.get(\"usage\", raw_response.get(\"usage_metadata\", {}))\n    if usage is None:\n        usage = response.additional_kwargs",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.token_counting",
        "documentation": {}
    },
    {
        "label": "get_llm_token_counts",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.token_counting",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.token_counting",
        "peekOfCode": "def get_llm_token_counts(\n    token_counter: TokenCounter, payload: Dict[str, Any], event_id: str = \"\"\n) -> TokenCountingEvent:\n    from llama_index.core.llms import ChatMessage\n    if EventPayload.PROMPT in payload:\n        prompt = payload.get(EventPayload.PROMPT)\n        completion = payload.get(EventPayload.COMPLETION)\n        if completion:\n            # get from raw or additional_kwargs\n            prompt_tokens, completion_tokens = get_tokens_from_response(completion)",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.token_counting",
        "documentation": {}
    },
    {
        "label": "trace_method",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.utils",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.utils",
        "peekOfCode": "def trace_method(\n    trace_id: str, callback_manager_attr: str = \"callback_manager\"\n) -> Callable[[Callable], Callable]:\n    \"\"\"\n    Decorator to trace a method.\n    Example:\n        @trace_method(\"my_trace_id\")\n        def my_method(self):\n            pass\n    Assumes that the self instance has a CallbackManager instance in an attribute",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.utils",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.callbacks.utils",
        "description": "reference_code.llama-index-core.llama_index.core.callbacks.utils",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef trace_method(\n    trace_id: str, callback_manager_attr: str = \"callback_manager\"\n) -> Callable[[Callable], Callable]:\n    \"\"\"\n    Decorator to trace a method.\n    Example:\n        @trace_method(\"my_trace_id\")\n        def my_method(self):\n            pass",
        "detail": "reference_code.llama-index-core.llama_index.core.callbacks.utils",
        "documentation": {}
    },
    {
        "label": "CondensePlusContextChatEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_plus_context",
        "description": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_plus_context",
        "peekOfCode": "class CondensePlusContextChatEngine(BaseChatEngine):\n    \"\"\"\n    Condensed Conversation & Context Chat Engine.\n    First condense a conversation and latest user message to a standalone question\n    Then build a context for the standalone question from a retriever,\n    Then pass the context along with prompt and user message to LLM to generate a response.\n    \"\"\"\n    def __init__(\n        self,\n        retriever: BaseRetriever,",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_plus_context",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_plus_context",
        "description": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_plus_context",
        "peekOfCode": "logger = logging.getLogger(__name__)\nDEFAULT_CONTEXT_PROMPT_TEMPLATE = \"\"\"\n  The following is a friendly conversation between a user and an AI assistant.\n  The assistant is talkative and provides lots of specific details from its context.\n  If the assistant does not know the answer to a question, it truthfully says it\n  does not know.\n  Here are the relevant documents for the context:\n  {context_str}\n  Instruction: Based on the above documents, provide a detailed answer for the user question below.\n  Answer \"don't know\" if not present in the document.",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_plus_context",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CONTEXT_PROMPT_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_plus_context",
        "description": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_plus_context",
        "peekOfCode": "DEFAULT_CONTEXT_PROMPT_TEMPLATE = \"\"\"\n  The following is a friendly conversation between a user and an AI assistant.\n  The assistant is talkative and provides lots of specific details from its context.\n  If the assistant does not know the answer to a question, it truthfully says it\n  does not know.\n  Here are the relevant documents for the context:\n  {context_str}\n  Instruction: Based on the above documents, provide a detailed answer for the user question below.\n  Answer \"don't know\" if not present in the document.\n  \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_plus_context",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CONTEXT_REFINE_PROMPT_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_plus_context",
        "description": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_plus_context",
        "peekOfCode": "DEFAULT_CONTEXT_REFINE_PROMPT_TEMPLATE = \"\"\"\n  The following is a friendly conversation between a user and an AI assistant.\n  The assistant is talkative and provides lots of specific details from its context.\n  If the assistant does not know the answer to a question, it truthfully says it\n  does not know.\n  Here are the relevant documents for the context:\n  {context_msg}\n  Existing Answer:\n  {existing_answer}\n  Instruction: Refine the existing answer using the provided context to assist the user.",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_plus_context",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CONDENSE_PROMPT_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_plus_context",
        "description": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_plus_context",
        "peekOfCode": "DEFAULT_CONDENSE_PROMPT_TEMPLATE = \"\"\"\n  Given the following conversation between a user and an AI assistant and a follow up question from user,\n  rephrase the follow up question to be a standalone question.\n  Chat History:\n  {chat_history}\n  Follow Up Input: {question}\n  Standalone question:\"\"\"\nclass CondensePlusContextChatEngine(BaseChatEngine):\n    \"\"\"\n    Condensed Conversation & Context Chat Engine.",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_plus_context",
        "documentation": {}
    },
    {
        "label": "CondenseQuestionChatEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_question",
        "description": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_question",
        "peekOfCode": "class CondenseQuestionChatEngine(BaseChatEngine):\n    \"\"\"\n    Condense Question Chat Engine.\n    First generate a standalone question from conversation context and last message,\n    then query the query engine for a response.\n    \"\"\"\n    def __init__(\n        self,\n        query_engine: BaseQueryEngine,\n        condense_question_prompt: BasePromptTemplate,",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_question",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_question",
        "description": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_question",
        "peekOfCode": "logger = logging.getLogger(__name__)\nDEFAULT_TEMPLATE = \"\"\"\\\nGiven a conversation (between Human and Assistant) and a follow up message from Human, \\\nrewrite the message to be a standalone question that captures all relevant context \\\nfrom the conversation.\n<Chat History>\n{chat_history}\n<Follow Up Message>\n{question}\n<Standalone question>",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_question",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_question",
        "description": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_question",
        "peekOfCode": "DEFAULT_TEMPLATE = \"\"\"\\\nGiven a conversation (between Human and Assistant) and a follow up message from Human, \\\nrewrite the message to be a standalone question that captures all relevant context \\\nfrom the conversation.\n<Chat History>\n{chat_history}\n<Follow Up Message>\n{question}\n<Standalone question>\n\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_question",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_question",
        "description": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_question",
        "peekOfCode": "DEFAULT_PROMPT = PromptTemplate(DEFAULT_TEMPLATE)\nclass CondenseQuestionChatEngine(BaseChatEngine):\n    \"\"\"\n    Condense Question Chat Engine.\n    First generate a standalone question from conversation context and last message,\n    then query the query engine for a response.\n    \"\"\"\n    def __init__(\n        self,\n        query_engine: BaseQueryEngine,",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_engine.condense_question",
        "documentation": {}
    },
    {
        "label": "ContextChatEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_engine.context",
        "description": "reference_code.llama-index-core.llama_index.core.chat_engine.context",
        "peekOfCode": "class ContextChatEngine(BaseChatEngine):\n    \"\"\"\n    Context Chat Engine.\n    Uses a retriever to retrieve a context, set the context in the system prompt,\n    and then uses an LLM to generate a response, for a fluid chat experience.\n    \"\"\"\n    def __init__(\n        self,\n        retriever: BaseRetriever,\n        llm: LLM,",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_engine.context",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CONTEXT_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_engine.context",
        "description": "reference_code.llama-index-core.llama_index.core.chat_engine.context",
        "peekOfCode": "DEFAULT_CONTEXT_TEMPLATE = (\n    \"Use the context information below to assist the user.\"\n    \"\\n--------------------\\n\"\n    \"{context_str}\"\n    \"\\n--------------------\\n\"\n)\nDEFAULT_REFINE_TEMPLATE = (\n    \"Using the context below, refine the following existing answer using the provided context to assist the user.\\n\"\n    \"If the context isn't helpful, just repeat the existing answer and nothing more.\\n\"\n    \"\\n--------------------\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_engine.context",
        "documentation": {}
    },
    {
        "label": "DEFAULT_REFINE_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_engine.context",
        "description": "reference_code.llama-index-core.llama_index.core.chat_engine.context",
        "peekOfCode": "DEFAULT_REFINE_TEMPLATE = (\n    \"Using the context below, refine the following existing answer using the provided context to assist the user.\\n\"\n    \"If the context isn't helpful, just repeat the existing answer and nothing more.\\n\"\n    \"\\n--------------------\\n\"\n    \"{context_msg}\"\n    \"\\n--------------------\\n\"\n    \"Existing Answer:\\n\"\n    \"{existing_answer}\"\n    \"\\n--------------------\\n\"\n)",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_engine.context",
        "documentation": {}
    },
    {
        "label": "SimpleChatEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_engine.simple",
        "description": "reference_code.llama-index-core.llama_index.core.chat_engine.simple",
        "peekOfCode": "class SimpleChatEngine(BaseChatEngine):\n    \"\"\"\n    Simple Chat Engine.\n    Have a conversation with the LLM.\n    This does not make use of a knowledge base.\n    \"\"\"\n    def __init__(\n        self,\n        llm: LLM,\n        memory: BaseMemory,",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_engine.simple",
        "documentation": {}
    },
    {
        "label": "ChatResponseMode",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "description": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "peekOfCode": "class ChatResponseMode(str, Enum):\n    \"\"\"Flag toggling waiting/streaming in `Agent._chat`.\"\"\"\n    WAIT = \"wait\"\n    STREAM = \"stream\"\n@dataclass\nclass AgentChatResponse:\n    \"\"\"Agent chat response.\"\"\"\n    response: str = \"\"\n    sources: List[ToolOutput] = field(default_factory=list)\n    source_nodes: List[NodeWithScore] = field(default_factory=list)",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "AgentChatResponse",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "description": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "peekOfCode": "class AgentChatResponse:\n    \"\"\"Agent chat response.\"\"\"\n    response: str = \"\"\n    sources: List[ToolOutput] = field(default_factory=list)\n    source_nodes: List[NodeWithScore] = field(default_factory=list)\n    is_dummy_stream: bool = False\n    metadata: Optional[Dict[str, Any]] = None\n    def set_source_nodes(self) -> None:\n        if self.sources and not self.source_nodes:\n            for tool_output in self.sources:",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "StreamingAgentChatResponse",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "description": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "peekOfCode": "class StreamingAgentChatResponse:\n    \"\"\"Streaming chat response to user and writing to chat history.\"\"\"\n    response: str = \"\"\n    sources: List[ToolOutput] = field(default_factory=list)\n    chat_stream: Optional[ChatResponseGen] = None\n    achat_stream: Optional[ChatResponseAsyncGen] = None\n    source_nodes: List[NodeWithScore] = field(default_factory=list)\n    unformatted_response: str = \"\"\n    queue: Queue = field(default_factory=Queue)\n    aqueue: Optional[asyncio.Queue] = None",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "BaseChatEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "description": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "peekOfCode": "class BaseChatEngine(DispatcherSpanMixin, ABC):\n    \"\"\"Base Chat Engine.\"\"\"\n    @abstractmethod\n    def reset(self) -> None:\n        \"\"\"Reset conversation state.\"\"\"\n    @abstractmethod\n    def chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> AGENT_CHAT_RESPONSE_TYPE:\n        \"\"\"Main chat interface.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "ChatMode",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "description": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "peekOfCode": "class ChatMode(str, Enum):\n    \"\"\"Chat Engine Modes.\"\"\"\n    SIMPLE = \"simple\"\n    \"\"\"Corresponds to `SimpleChatEngine`.\n    Chat with LLM, without making use of a knowledge base.\n    \"\"\"\n    CONDENSE_QUESTION = \"condense_question\"\n    \"\"\"Corresponds to `CondenseQuestionChatEngine`.\n    First generate a standalone question from conversation context and last message,\n    then query the query engine for a response.",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "is_function",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "description": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "peekOfCode": "def is_function(message: ChatMessage) -> bool:\n    \"\"\"Utility for ChatMessage responses from OpenAI models.\"\"\"\n    return (\n        \"tool_calls\" in message.additional_kwargs\n        and len(message.additional_kwargs[\"tool_calls\"]) > 0\n    )\nclass ChatResponseMode(str, Enum):\n    \"\"\"Flag toggling waiting/streaming in `Agent._chat`.\"\"\"\n    WAIT = \"wait\"\n    STREAM = \"stream\"",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "dispatcher",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "description": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "peekOfCode": "dispatcher = instrument.get_dispatcher(__name__)\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.WARNING)\ndef is_function(message: ChatMessage) -> bool:\n    \"\"\"Utility for ChatMessage responses from OpenAI models.\"\"\"\n    return (\n        \"tool_calls\" in message.additional_kwargs\n        and len(message.additional_kwargs[\"tool_calls\"]) > 0\n    )\nclass ChatResponseMode(str, Enum):",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "description": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "peekOfCode": "logger = logging.getLogger(__name__)\nlogger.setLevel(logging.WARNING)\ndef is_function(message: ChatMessage) -> bool:\n    \"\"\"Utility for ChatMessage responses from OpenAI models.\"\"\"\n    return (\n        \"tool_calls\" in message.additional_kwargs\n        and len(message.additional_kwargs[\"tool_calls\"]) > 0\n    )\nclass ChatResponseMode(str, Enum):\n    \"\"\"Flag toggling waiting/streaming in `Agent._chat`.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "AGENT_CHAT_RESPONSE_TYPE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "description": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "peekOfCode": "AGENT_CHAT_RESPONSE_TYPE = Union[AgentChatResponse, StreamingAgentChatResponse]\nclass BaseChatEngine(DispatcherSpanMixin, ABC):\n    \"\"\"Base Chat Engine.\"\"\"\n    @abstractmethod\n    def reset(self) -> None:\n        \"\"\"Reset conversation state.\"\"\"\n    @abstractmethod\n    def chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> AGENT_CHAT_RESPONSE_TYPE:",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_engine.types",
        "documentation": {}
    },
    {
        "label": "get_prefix_messages_with_context",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_engine.utils",
        "description": "reference_code.llama-index-core.llama_index.core.chat_engine.utils",
        "peekOfCode": "def get_prefix_messages_with_context(\n    context_template: PromptTemplate,\n    system_prompt: str,\n    prefix_messages: List[ChatMessage],\n    chat_history: List[ChatMessage],\n    llm_metadata_system_role: MessageRole,\n) -> List[ChatMessage]:\n    context_str_w_sys_prompt = context_template.template + system_prompt.strip()\n    return [\n        ChatMessage(content=context_str_w_sys_prompt, role=llm_metadata_system_role),",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_engine.utils",
        "documentation": {}
    },
    {
        "label": "get_response_synthesizer",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_engine.utils",
        "description": "reference_code.llama-index-core.llama_index.core.chat_engine.utils",
        "peekOfCode": "def get_response_synthesizer(\n    llm: LLM,\n    callback_manager: CallbackManager,\n    qa_messages: List[ChatMessage],\n    refine_messages: List[ChatMessage],\n    streaming: bool = False,\n    qa_function_mappings: Optional[Dict[str, Callable]] = None,\n    refine_function_mappings: Optional[Dict[str, Callable]] = None,\n) -> CompactAndRefine:\n    return CompactAndRefine(",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_engine.utils",
        "documentation": {}
    },
    {
        "label": "response_gen_from_query_engine",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_engine.utils",
        "description": "reference_code.llama-index-core.llama_index.core.chat_engine.utils",
        "peekOfCode": "def response_gen_from_query_engine(response_gen: TokenGen) -> ChatResponseGen:\n    response_str = \"\"\n    for token in response_gen:\n        response_str += token\n        yield ChatResponse(\n            message=ChatMessage(role=MessageRole.ASSISTANT, content=response_str),\n            delta=token,\n        )\nasync def aresponse_gen_from_query_engine(\n    response_gen: TokenAsyncGen,",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_engine.utils",
        "documentation": {}
    },
    {
        "label": "ArtifactType",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_ui.models.artifact",
        "description": "reference_code.llama-index-core.llama_index.core.chat_ui.models.artifact",
        "peekOfCode": "class ArtifactType(str, Enum):\n    CODE = \"code\"\n    DOCUMENT = \"document\"\nclass CodeArtifactData(BaseModel):\n    file_name: str\n    code: str\n    language: str\nclass DocumentArtifactSource(BaseModel):\n    id: str\nclass DocumentArtifactData(BaseModel):",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_ui.models.artifact",
        "documentation": {}
    },
    {
        "label": "CodeArtifactData",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_ui.models.artifact",
        "description": "reference_code.llama-index-core.llama_index.core.chat_ui.models.artifact",
        "peekOfCode": "class CodeArtifactData(BaseModel):\n    file_name: str\n    code: str\n    language: str\nclass DocumentArtifactSource(BaseModel):\n    id: str\nclass DocumentArtifactData(BaseModel):\n    title: str\n    content: str\n    type: Literal[\"markdown\", \"html\"]",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_ui.models.artifact",
        "documentation": {}
    },
    {
        "label": "DocumentArtifactSource",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_ui.models.artifact",
        "description": "reference_code.llama-index-core.llama_index.core.chat_ui.models.artifact",
        "peekOfCode": "class DocumentArtifactSource(BaseModel):\n    id: str\nclass DocumentArtifactData(BaseModel):\n    title: str\n    content: str\n    type: Literal[\"markdown\", \"html\"]\n    sources: Optional[List[DocumentArtifactSource]] = None\nclass Artifact(BaseModel):\n    created_at: Optional[int] = None\n    type: ArtifactType",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_ui.models.artifact",
        "documentation": {}
    },
    {
        "label": "DocumentArtifactData",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_ui.models.artifact",
        "description": "reference_code.llama-index-core.llama_index.core.chat_ui.models.artifact",
        "peekOfCode": "class DocumentArtifactData(BaseModel):\n    title: str\n    content: str\n    type: Literal[\"markdown\", \"html\"]\n    sources: Optional[List[DocumentArtifactSource]] = None\nclass Artifact(BaseModel):\n    created_at: Optional[int] = None\n    type: ArtifactType\n    data: Union[CodeArtifactData, DocumentArtifactData]",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_ui.models.artifact",
        "documentation": {}
    },
    {
        "label": "Artifact",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_ui.models.artifact",
        "description": "reference_code.llama-index-core.llama_index.core.chat_ui.models.artifact",
        "peekOfCode": "class Artifact(BaseModel):\n    created_at: Optional[int] = None\n    type: ArtifactType\n    data: Union[CodeArtifactData, DocumentArtifactData]",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_ui.models.artifact",
        "documentation": {}
    },
    {
        "label": "UIEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_ui.events",
        "description": "reference_code.llama-index-core.llama_index.core.chat_ui.events",
        "peekOfCode": "class UIEvent(Event):\n    type: str\n    data: Any\nclass SourceNodesEvent(Event):\n    nodes: List[NodeWithScore]\nclass ArtifactEvent(Event):\n    type: str = \"artifact\"\n    data: Artifact",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_ui.events",
        "documentation": {}
    },
    {
        "label": "SourceNodesEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_ui.events",
        "description": "reference_code.llama-index-core.llama_index.core.chat_ui.events",
        "peekOfCode": "class SourceNodesEvent(Event):\n    nodes: List[NodeWithScore]\nclass ArtifactEvent(Event):\n    type: str = \"artifact\"\n    data: Artifact",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_ui.events",
        "documentation": {}
    },
    {
        "label": "ArtifactEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.chat_ui.events",
        "description": "reference_code.llama-index-core.llama_index.core.chat_ui.events",
        "peekOfCode": "class ArtifactEvent(Event):\n    type: str = \"artifact\"\n    data: Artifact",
        "detail": "reference_code.llama-index-core.llama_index.core.chat_ui.events",
        "documentation": {}
    },
    {
        "label": "parse_lines",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.command_line.upgrade",
        "description": "reference_code.llama-index-core.llama_index.core.command_line.upgrade",
        "peekOfCode": "def parse_lines(\n    lines: List[str], installed_modules: List[str], verbose: bool = False\n) -> Tuple[List[str], List[str]]:\n    with open(mappings_path) as f:\n        mappings = json.load(f)\n    new_installs = []\n    new_lines = []\n    just_found_imports = False\n    skipped_lines = 0\n    for idx, line in enumerate(lines):",
        "detail": "reference_code.llama-index-core.llama_index.core.command_line.upgrade",
        "documentation": {}
    },
    {
        "label": "upgrade_nb_file",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.command_line.upgrade",
        "description": "reference_code.llama-index-core.llama_index.core.command_line.upgrade",
        "peekOfCode": "def upgrade_nb_file(file_path: str) -> None:\n    print(f\"\\n=====================\\n{file_path}\\n\", flush=True)\n    with open(file_path) as f:\n        notebook = json.load(f)\n    verbose = False\n    if file_path == \"../docs/examples/managed/manage_retrieval_benchmark.ipynb\":\n        verbose = True\n    installed_modules = [\"llama-index-core\"]  # default installs\n    cur_cells = []\n    new_installs = []",
        "detail": "reference_code.llama-index-core.llama_index.core.command_line.upgrade",
        "documentation": {}
    },
    {
        "label": "upgrade_py_md_file",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.command_line.upgrade",
        "description": "reference_code.llama-index-core.llama_index.core.command_line.upgrade",
        "peekOfCode": "def upgrade_py_md_file(file_path: str) -> None:\n    with open(file_path) as f:\n        lines = f.readlines()\n    installed_modules = [\"llama-index-core\"]  # default installs\n    new_lines, new_installs = parse_lines(lines, installed_modules)\n    with open(file_path, \"w\") as f:\n        f.write(\"\".join(new_lines))\n    if len(new_installs) > 0:\n        print(\"New installs:\")\n    for install in new_installs:",
        "detail": "reference_code.llama-index-core.llama_index.core.command_line.upgrade",
        "documentation": {}
    },
    {
        "label": "upgrade_file",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.command_line.upgrade",
        "description": "reference_code.llama-index-core.llama_index.core.command_line.upgrade",
        "peekOfCode": "def upgrade_file(file_path: str) -> None:\n    if file_path.endswith(\".ipynb\"):\n        upgrade_nb_file(file_path)\n    elif file_path.endswith((\".py\", \".md\")):\n        upgrade_py_md_file(file_path)\n    else:\n        raise Exception(f\"File type not supported: {file_path}\")\ndef _is_hidden(path: Path) -> bool:\n    return any(part.startswith(\".\") and part not in [\".\", \"..\"] for part in path.parts)\ndef upgrade_dir(input_dir: str) -> None:",
        "detail": "reference_code.llama-index-core.llama_index.core.command_line.upgrade",
        "documentation": {}
    },
    {
        "label": "upgrade_dir",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.command_line.upgrade",
        "description": "reference_code.llama-index-core.llama_index.core.command_line.upgrade",
        "peekOfCode": "def upgrade_dir(input_dir: str) -> None:\n    file_refs = list(Path(input_dir).rglob(\"*.py\"))\n    file_refs += list(Path(input_dir).rglob(\"*.ipynb\"))\n    file_refs += list(Path(input_dir).rglob(\"*.md\"))\n    file_refs = [x for x in file_refs if not _is_hidden(x)]\n    for file_ref in file_refs:\n        if file_ref.is_file():\n            upgrade_file(str(file_ref))",
        "detail": "reference_code.llama-index-core.llama_index.core.command_line.upgrade",
        "documentation": {}
    },
    {
        "label": "mappings_path",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.command_line.upgrade",
        "description": "reference_code.llama-index-core.llama_index.core.command_line.upgrade",
        "peekOfCode": "mappings_path = os.path.join(os.path.dirname(__file__), \"mappings.json\")\ndef _parse_from_imports(\n    mappings: Dict[str, str],\n    installed_modules: List[str],\n    line_idx: int,\n    lines: List[str],\n    verbose: bool = False,\n) -> Tuple[List[str], List[str], List[str], int]:\n    new_lines = []\n    new_installs = []",
        "detail": "reference_code.llama-index-core.llama_index.core.command_line.upgrade",
        "documentation": {}
    },
    {
        "label": "QASummaryQueryEngineBuilder",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.composability.joint_qa_summary",
        "description": "reference_code.llama-index-core.llama_index.core.composability.joint_qa_summary",
        "peekOfCode": "class QASummaryQueryEngineBuilder:\n    \"\"\"\n    Joint QA Summary graph builder.\n    Can build a graph that provides a unified query interface\n    for both QA and summarization tasks.\n    NOTE: this is a beta feature. The API may change in the future.\n    Args:\n        docstore (BaseDocumentStore): A BaseDocumentStore to use for storing nodes.\n        summary_text (str): Text to use for the summary index.\n        qa_text (str): Text to use for the QA index.",
        "detail": "reference_code.llama-index-core.llama_index.core.composability.joint_qa_summary",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SUMMARY_TEXT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.composability.joint_qa_summary",
        "description": "reference_code.llama-index-core.llama_index.core.composability.joint_qa_summary",
        "peekOfCode": "DEFAULT_SUMMARY_TEXT = \"Use this index for summarization queries\"\nDEFAULT_QA_TEXT = (\n    \"Use this index for queries that require retrieval of specific \"\n    \"context from documents.\"\n)\nclass QASummaryQueryEngineBuilder:\n    \"\"\"\n    Joint QA Summary graph builder.\n    Can build a graph that provides a unified query interface\n    for both QA and summarization tasks.",
        "detail": "reference_code.llama-index-core.llama_index.core.composability.joint_qa_summary",
        "documentation": {}
    },
    {
        "label": "DEFAULT_QA_TEXT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.composability.joint_qa_summary",
        "description": "reference_code.llama-index-core.llama_index.core.composability.joint_qa_summary",
        "peekOfCode": "DEFAULT_QA_TEXT = (\n    \"Use this index for queries that require retrieval of specific \"\n    \"context from documents.\"\n)\nclass QASummaryQueryEngineBuilder:\n    \"\"\"\n    Joint QA Summary graph builder.\n    Can build a graph that provides a unified query interface\n    for both QA and summarization tasks.\n    NOTE: this is a beta feature. The API may change in the future.",
        "detail": "reference_code.llama-index-core.llama_index.core.composability.joint_qa_summary",
        "documentation": {}
    },
    {
        "label": "IndexStruct",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "description": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "peekOfCode": "class IndexStruct(DataClassJsonMixin):\n    \"\"\"A base data struct for a LlamaIndex.\"\"\"\n    index_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    summary: Optional[str] = None\n    def get_summary(self) -> str:\n        \"\"\"Get text summary.\"\"\"\n        if self.summary is None:\n            raise ValueError(\"summary field of the index_struct not set.\")\n        return self.summary\n    @classmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexGraph",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "description": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "peekOfCode": "class IndexGraph(IndexStruct):\n    \"\"\"A graph representing the tree-structured index.\"\"\"\n    # mapping from index in tree to Node doc id.\n    all_nodes: Dict[int, str] = field(default_factory=dict)\n    root_nodes: Dict[int, str] = field(default_factory=dict)\n    node_id_to_children_ids: Dict[str, List[str]] = field(default_factory=dict)\n    @property\n    def node_id_to_index(self) -> Dict[str, int]:\n        \"\"\"Map from node id to index.\"\"\"\n        return {node_id: index for index, node_id in self.all_nodes.items()}",
        "detail": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "KeywordTable",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "description": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "peekOfCode": "class KeywordTable(IndexStruct):\n    \"\"\"A table of keywords mapping keywords to text chunks.\"\"\"\n    table: Dict[str, Set[str]] = field(default_factory=dict)\n    def add_node(self, keywords: List[str], node: BaseNode) -> None:\n        \"\"\"Add text to table.\"\"\"\n        for keyword in keywords:\n            if keyword not in self.table:\n                self.table[keyword] = set()\n            self.table[keyword].add(node.node_id)\n    @property",
        "detail": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexList",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "description": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "peekOfCode": "class IndexList(IndexStruct):\n    \"\"\"A list of documents.\"\"\"\n    nodes: List[str] = field(default_factory=list)\n    def add_node(self, node: BaseNode) -> None:\n        \"\"\"Add text to table, return current position in list.\"\"\"\n        # don't worry about child indices for now, nodes are all in order\n        self.nodes.append(node.node_id)\n    @classmethod\n    def get_type(cls) -> IndexStructType:\n        \"\"\"Get type.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexLPG",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "description": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "peekOfCode": "class IndexLPG(IndexStruct):\n    \"\"\"An index struct for LPG index (doesn't actually store anything).\"\"\"\n    def add_node(self, node: BaseNode) -> None:\n        pass\n    @classmethod\n    def get_type(cls) -> IndexStructType:\n        return IndexStructType.SIMPLE_LPG\n@dataclass\nclass IndexDict(IndexStruct):\n    \"\"\"A simple dictionary of documents.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexDict",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "description": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "peekOfCode": "class IndexDict(IndexStruct):\n    \"\"\"A simple dictionary of documents.\"\"\"\n    # TODO: slightly deprecated, should likely be a list or set now\n    # mapping from vector store id to node doc_id\n    nodes_dict: Dict[str, str] = field(default_factory=dict)\n    # TODO: deprecated, not used\n    # mapping from node doc_id to vector store id\n    doc_id_dict: Dict[str, List[str]] = field(default_factory=dict)\n    # TODO: deprecated, not used\n    # this should be empty for all other indices",
        "detail": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "MultiModelIndexDict",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "description": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "peekOfCode": "class MultiModelIndexDict(IndexDict):\n    \"\"\"A simple dictionary of documents, but loads a MultiModelVectorStore.\"\"\"\n    @classmethod\n    def get_type(cls) -> IndexStructType:\n        \"\"\"Get type.\"\"\"\n        return IndexStructType.MULTIMODAL_VECTOR_STORE\n@dataclass\nclass KG(IndexStruct):\n    \"\"\"A table of keywords mapping keywords to text chunks.\"\"\"\n    # Unidirectional",
        "detail": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "KG",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "description": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "peekOfCode": "class KG(IndexStruct):\n    \"\"\"A table of keywords mapping keywords to text chunks.\"\"\"\n    # Unidirectional\n    # table of keywords to node ids\n    table: Dict[str, Set[str]] = field(default_factory=dict)\n    # TODO: legacy attribute, remove in future releases\n    rel_map: Dict[str, List[List[str]]] = field(default_factory=dict)\n    # TBD, should support vector store, now we just persist the embedding memory\n    # maybe chainable abstractions for *_stores could be designed\n    embedding_dict: Dict[str, List[float]] = field(default_factory=dict)",
        "detail": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "EmptyIndexStruct",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "description": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "peekOfCode": "class EmptyIndexStruct(IndexStruct):\n    \"\"\"Empty index.\"\"\"\n    @classmethod\n    def get_type(cls) -> IndexStructType:\n        \"\"\"Get type.\"\"\"\n        return IndexStructType.EMPTY",
        "detail": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "Node",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "description": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "peekOfCode": "Node = TextNode\n@dataclass\nclass IndexStruct(DataClassJsonMixin):\n    \"\"\"A base data struct for a LlamaIndex.\"\"\"\n    index_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    summary: Optional[str] = None\n    def get_summary(self) -> str:\n        \"\"\"Get text summary.\"\"\"\n        if self.summary is None:\n            raise ValueError(\"summary field of the index_struct not set.\")",
        "detail": "reference_code.llama-index-core.llama_index.core.data_structs.data_structs",
        "documentation": {}
    },
    {
        "label": "IndexDocumentSummary",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.data_structs.document_summary",
        "description": "reference_code.llama-index-core.llama_index.core.data_structs.document_summary",
        "peekOfCode": "class IndexDocumentSummary(IndexStruct):\n    \"\"\"\n    A simple struct containing a mapping from summary node_id to doc node_ids.\n    Also mapping vice versa.\n    \"\"\"\n    summary_id_to_node_ids: Dict[str, List[str]] = field(default_factory=dict)\n    node_id_to_summary_id: Dict[str, str] = field(default_factory=dict)\n    # track mapping from doc id to node summary id\n    doc_id_to_summary_id: Dict[str, str] = field(default_factory=dict)\n    def add_summary_and_nodes(",
        "detail": "reference_code.llama-index-core.llama_index.core.data_structs.document_summary",
        "documentation": {}
    },
    {
        "label": "IndexStructType",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.data_structs.struct_type",
        "description": "reference_code.llama-index-core.llama_index.core.data_structs.struct_type",
        "peekOfCode": "class IndexStructType(str, Enum):\n    \"\"\"\n    Index struct type. Identifier for a \"type\" of index.\n    Attributes:\n        TREE (\"tree\"): Tree index. See :ref:`Ref-Indices-Tree` for tree indices.\n        LIST (\"list\"): Summary index. See :ref:`Ref-Indices-List` for summary indices.\n        KEYWORD_TABLE (\"keyword_table\"): Keyword table index. See\n            :ref:`Ref-Indices-Table`\n            for keyword table indices.\n        DICT (\"dict\"): Faiss Vector Store Index. See",
        "detail": "reference_code.llama-index-core.llama_index.core.data_structs.struct_type",
        "documentation": {}
    },
    {
        "label": "StructDatapoint",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.data_structs.table",
        "description": "reference_code.llama-index-core.llama_index.core.data_structs.table",
        "peekOfCode": "class StructDatapoint(DataClassJsonMixin):\n    \"\"\"Struct outputs.\"\"\"\n    # map from field name to StructValue\n    fields: Dict[str, Any]\n@dataclass\nclass BaseStructTable(IndexStruct):\n    \"\"\"Struct outputs.\"\"\"\n@dataclass\nclass SQLStructTable(BaseStructTable):\n    \"\"\"SQL struct outputs.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.data_structs.table",
        "documentation": {}
    },
    {
        "label": "BaseStructTable",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.data_structs.table",
        "description": "reference_code.llama-index-core.llama_index.core.data_structs.table",
        "peekOfCode": "class BaseStructTable(IndexStruct):\n    \"\"\"Struct outputs.\"\"\"\n@dataclass\nclass SQLStructTable(BaseStructTable):\n    \"\"\"SQL struct outputs.\"\"\"\n    context_dict: Dict[str, str] = field(default_factory=dict)\n    @classmethod\n    def get_type(cls) -> IndexStructType:\n        \"\"\"Get type.\"\"\"\n        # TODO: consolidate with IndexStructType",
        "detail": "reference_code.llama-index-core.llama_index.core.data_structs.table",
        "documentation": {}
    },
    {
        "label": "SQLStructTable",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.data_structs.table",
        "description": "reference_code.llama-index-core.llama_index.core.data_structs.table",
        "peekOfCode": "class SQLStructTable(BaseStructTable):\n    \"\"\"SQL struct outputs.\"\"\"\n    context_dict: Dict[str, str] = field(default_factory=dict)\n    @classmethod\n    def get_type(cls) -> IndexStructType:\n        \"\"\"Get type.\"\"\"\n        # TODO: consolidate with IndexStructType\n        return IndexStructType.SQL\n@dataclass\nclass PandasStructTable(BaseStructTable):",
        "detail": "reference_code.llama-index-core.llama_index.core.data_structs.table",
        "documentation": {}
    },
    {
        "label": "PandasStructTable",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.data_structs.table",
        "description": "reference_code.llama-index-core.llama_index.core.data_structs.table",
        "peekOfCode": "class PandasStructTable(BaseStructTable):\n    \"\"\"Pandas struct outputs.\"\"\"\n    @classmethod\n    def get_type(cls) -> IndexStructType:\n        \"\"\"Get type.\"\"\"\n        return IndexStructType.PANDAS",
        "detail": "reference_code.llama-index-core.llama_index.core.data_structs.table",
        "documentation": {}
    },
    {
        "label": "get_dataset_info",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "description": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "peekOfCode": "def get_dataset_info(\n    local_dir_path: PATH_TYPE,\n    remote_dir_path: PATH_TYPE,\n    remote_source_dir_path: PATH_TYPE,\n    dataset_class: str,\n    refresh_cache: bool = False,\n    library_path: str = \"library.json\",\n    source_files_path: str = \"source_files\",\n    disable_library_cache: bool = False,\n) -> Dict:",
        "detail": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "documentation": {}
    },
    {
        "label": "download_dataset_and_source_files",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "description": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "peekOfCode": "def download_dataset_and_source_files(\n    local_dir_path: PATH_TYPE,\n    remote_lfs_dir_path: PATH_TYPE,\n    source_files_dir_path: PATH_TYPE,\n    dataset_id: str,\n    dataset_class_name: str,\n    source_files: List[str],\n    refresh_cache: bool = False,\n    base_file_name: str = \"rag_dataset.json\",\n    override_path: bool = False,",
        "detail": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "documentation": {}
    },
    {
        "label": "download_llama_dataset",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "description": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "peekOfCode": "def download_llama_dataset(\n    dataset_class: str,\n    llama_datasets_url: str = LLAMA_DATASETS_URL,\n    llama_datasets_lfs_url: str = LLAMA_DATASETS_LFS_URL,\n    llama_datasets_source_files_tree_url: str = LLAMA_DATASETS_SOURCE_FILES_GITHUB_TREE_URL,\n    refresh_cache: bool = False,\n    custom_dir: Optional[str] = None,\n    custom_path: Optional[str] = None,\n    source_files_dirpath: str = LLAMA_SOURCE_FILES_PATH,\n    library_path: str = \"llama_datasets/library.json\",",
        "detail": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "documentation": {}
    },
    {
        "label": "LLAMA_INDEX_CONTENTS_URL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "description": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "peekOfCode": "LLAMA_INDEX_CONTENTS_URL = (\n    f\"https://raw.githubusercontent.com/run-llama/llama_index/main\"\n)\nLLAMA_DATASETS_PATH = \"/llama-datasets\"\nLLAMA_DATASETS_URL = LLAMA_INDEX_CONTENTS_URL + LLAMA_DATASETS_PATH\nLLAMA_DATASETS_LFS_URL = (\n    f\"https://media.githubusercontent.com/media/run-llama/llama-datasets/main\"\n)\nLLAMA_DATASETS_SOURCE_FILES_GITHUB_TREE_URL = (\n    \"https://github.com/run-llama/llama-datasets/tree/main\"",
        "detail": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "documentation": {}
    },
    {
        "label": "LLAMA_DATASETS_PATH",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "description": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "peekOfCode": "LLAMA_DATASETS_PATH = \"/llama-datasets\"\nLLAMA_DATASETS_URL = LLAMA_INDEX_CONTENTS_URL + LLAMA_DATASETS_PATH\nLLAMA_DATASETS_LFS_URL = (\n    f\"https://media.githubusercontent.com/media/run-llama/llama-datasets/main\"\n)\nLLAMA_DATASETS_SOURCE_FILES_GITHUB_TREE_URL = (\n    \"https://github.com/run-llama/llama-datasets/tree/main\"\n)\nLLAMA_SOURCE_FILES_PATH = \"source_files\"\nDATASET_CLASS_FILENAME_REGISTRY = {",
        "detail": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "documentation": {}
    },
    {
        "label": "LLAMA_DATASETS_URL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "description": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "peekOfCode": "LLAMA_DATASETS_URL = LLAMA_INDEX_CONTENTS_URL + LLAMA_DATASETS_PATH\nLLAMA_DATASETS_LFS_URL = (\n    f\"https://media.githubusercontent.com/media/run-llama/llama-datasets/main\"\n)\nLLAMA_DATASETS_SOURCE_FILES_GITHUB_TREE_URL = (\n    \"https://github.com/run-llama/llama-datasets/tree/main\"\n)\nLLAMA_SOURCE_FILES_PATH = \"source_files\"\nDATASET_CLASS_FILENAME_REGISTRY = {\n    \"LabelledRagDataset\": \"rag_dataset.json\",",
        "detail": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "documentation": {}
    },
    {
        "label": "LLAMA_DATASETS_LFS_URL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "description": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "peekOfCode": "LLAMA_DATASETS_LFS_URL = (\n    f\"https://media.githubusercontent.com/media/run-llama/llama-datasets/main\"\n)\nLLAMA_DATASETS_SOURCE_FILES_GITHUB_TREE_URL = (\n    \"https://github.com/run-llama/llama-datasets/tree/main\"\n)\nLLAMA_SOURCE_FILES_PATH = \"source_files\"\nDATASET_CLASS_FILENAME_REGISTRY = {\n    \"LabelledRagDataset\": \"rag_dataset.json\",\n    \"LabeledRagDataset\": \"rag_dataset.json\",",
        "detail": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "documentation": {}
    },
    {
        "label": "LLAMA_DATASETS_SOURCE_FILES_GITHUB_TREE_URL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "description": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "peekOfCode": "LLAMA_DATASETS_SOURCE_FILES_GITHUB_TREE_URL = (\n    \"https://github.com/run-llama/llama-datasets/tree/main\"\n)\nLLAMA_SOURCE_FILES_PATH = \"source_files\"\nDATASET_CLASS_FILENAME_REGISTRY = {\n    \"LabelledRagDataset\": \"rag_dataset.json\",\n    \"LabeledRagDataset\": \"rag_dataset.json\",\n    \"LabelledPairwiseEvaluatorDataset\": \"pairwise_evaluator_dataset.json\",\n    \"LabeledPairwiseEvaluatorDataset\": \"pairwise_evaluator_dataset.json\",\n    \"LabelledEvaluatorDataset\": \"evaluator_dataset.json\",",
        "detail": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "documentation": {}
    },
    {
        "label": "LLAMA_SOURCE_FILES_PATH",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "description": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "peekOfCode": "LLAMA_SOURCE_FILES_PATH = \"source_files\"\nDATASET_CLASS_FILENAME_REGISTRY = {\n    \"LabelledRagDataset\": \"rag_dataset.json\",\n    \"LabeledRagDataset\": \"rag_dataset.json\",\n    \"LabelledPairwiseEvaluatorDataset\": \"pairwise_evaluator_dataset.json\",\n    \"LabeledPairwiseEvaluatorDataset\": \"pairwise_evaluator_dataset.json\",\n    \"LabelledEvaluatorDataset\": \"evaluator_dataset.json\",\n    \"LabeledEvaluatorDataset\": \"evaluator_dataset.json\",\n}\nPATH_TYPE = Union[str, Path]",
        "detail": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "documentation": {}
    },
    {
        "label": "DATASET_CLASS_FILENAME_REGISTRY",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "description": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "peekOfCode": "DATASET_CLASS_FILENAME_REGISTRY = {\n    \"LabelledRagDataset\": \"rag_dataset.json\",\n    \"LabeledRagDataset\": \"rag_dataset.json\",\n    \"LabelledPairwiseEvaluatorDataset\": \"pairwise_evaluator_dataset.json\",\n    \"LabeledPairwiseEvaluatorDataset\": \"pairwise_evaluator_dataset.json\",\n    \"LabelledEvaluatorDataset\": \"evaluator_dataset.json\",\n    \"LabeledEvaluatorDataset\": \"evaluator_dataset.json\",\n}\nPATH_TYPE = Union[str, Path]\ndef _resolve_dataset_file_name(class_name: str) -> str:",
        "detail": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "documentation": {}
    },
    {
        "label": "PATH_TYPE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "description": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "peekOfCode": "PATH_TYPE = Union[str, Path]\ndef _resolve_dataset_file_name(class_name: str) -> str:\n    \"\"\"Resolve filename based on dataset class.\"\"\"\n    try:\n        return DATASET_CLASS_FILENAME_REGISTRY[class_name]\n    except KeyError as err:\n        raise ValueError(\"Invalid dataset filename.\") from err\ndef get_dataset_info(\n    local_dir_path: PATH_TYPE,\n    remote_dir_path: PATH_TYPE,",
        "detail": "reference_code.llama-index-core.llama_index.core.download.dataset",
        "documentation": {}
    },
    {
        "label": "pip_install",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.integration",
        "description": "reference_code.llama-index-core.llama_index.core.download.integration",
        "peekOfCode": "def pip_install(package: str) -> None:\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\ndef download_integration(module_str: str, module_import_str: str, cls_name: str) -> Any:\n    \"\"\"Returns an integration class by first pip installing its parent module.\"\"\"\n    try:\n        pip_install(module_str)  # this works for any integration not just packs\n    except Exception as e:\n        raise Exception(f\"Failed to pip install `{module_str}`\") from e\n    try:\n        module_spec = importlib.util.find_spec(module_import_str)",
        "detail": "reference_code.llama-index-core.llama_index.core.download.integration",
        "documentation": {}
    },
    {
        "label": "download_integration",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.integration",
        "description": "reference_code.llama-index-core.llama_index.core.download.integration",
        "peekOfCode": "def download_integration(module_str: str, module_import_str: str, cls_name: str) -> Any:\n    \"\"\"Returns an integration class by first pip installing its parent module.\"\"\"\n    try:\n        pip_install(module_str)  # this works for any integration not just packs\n    except Exception as e:\n        raise Exception(f\"Failed to pip install `{module_str}`\") from e\n    try:\n        module_spec = importlib.util.find_spec(module_import_str)\n        module = importlib.util.module_from_spec(module_spec)  # type: ignore\n        module_spec.loader.exec_module(module)  # type: ignore",
        "detail": "reference_code.llama-index-core.llama_index.core.download.integration",
        "documentation": {}
    },
    {
        "label": "MODULE_TYPE",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.module",
        "description": "reference_code.llama-index-core.llama_index.core.download.module",
        "peekOfCode": "class MODULE_TYPE(str, Enum):\n    LOADER = \"loader\"\n    TOOL = \"tool\"\n    LLAMAPACK = \"llamapack\"\n    DATASETS = \"datasets\"\ndef get_module_info(\n    local_dir_path: PATH_TYPE,\n    remote_dir_path: PATH_TYPE,\n    module_class: str,\n    refresh_cache: bool = False,",
        "detail": "reference_code.llama-index-core.llama_index.core.download.module",
        "documentation": {}
    },
    {
        "label": "get_module_info",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.module",
        "description": "reference_code.llama-index-core.llama_index.core.download.module",
        "peekOfCode": "def get_module_info(\n    local_dir_path: PATH_TYPE,\n    remote_dir_path: PATH_TYPE,\n    module_class: str,\n    refresh_cache: bool = False,\n    library_path: str = \"library.json\",\n    disable_library_cache: bool = False,\n) -> Dict:\n    \"\"\"Get module info.\"\"\"\n    if isinstance(local_dir_path, str):",
        "detail": "reference_code.llama-index-core.llama_index.core.download.module",
        "documentation": {}
    },
    {
        "label": "download_module_and_reqs",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.module",
        "description": "reference_code.llama-index-core.llama_index.core.download.module",
        "peekOfCode": "def download_module_and_reqs(\n    local_dir_path: PATH_TYPE,\n    remote_dir_path: PATH_TYPE,\n    module_id: str,\n    extra_files: List[str],\n    refresh_cache: bool = False,\n    use_gpt_index_import: bool = False,\n    base_file_name: str = \"base.py\",\n    override_path: bool = False,\n) -> None:",
        "detail": "reference_code.llama-index-core.llama_index.core.download.module",
        "documentation": {}
    },
    {
        "label": "download_llama_module",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.module",
        "description": "reference_code.llama-index-core.llama_index.core.download.module",
        "peekOfCode": "def download_llama_module(\n    module_class: str,\n    llama_hub_url: str = LLAMA_HUB_URL,\n    refresh_cache: bool = False,\n    custom_dir: Optional[str] = None,\n    custom_path: Optional[str] = None,\n    library_path: str = \"library.json\",\n    base_file_name: str = \"base.py\",\n    use_gpt_index_import: bool = False,\n    disable_library_cache: bool = False,",
        "detail": "reference_code.llama-index-core.llama_index.core.download.module",
        "documentation": {}
    },
    {
        "label": "track_download",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.module",
        "description": "reference_code.llama-index-core.llama_index.core.download.module",
        "peekOfCode": "def track_download(module_class: str, module_type: str) -> None:\n    \"\"\"\n    Tracks number of downloads via Llamahub proxy.\n    Args:\n        module_class: The name of the llama module being downloaded, e.g.,`GmailOpenAIAgentPack`.\n        module_type: Can be \"loader\", \"tool\", \"llamapack\", or \"datasets\"\n    \"\"\"\n    try:\n        requests.post(\n            LLAMAHUB_ANALYTICS_PROXY_SERVER,",
        "detail": "reference_code.llama-index-core.llama_index.core.download.module",
        "documentation": {}
    },
    {
        "label": "LLAMA_HUB_CONTENTS_URL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.module",
        "description": "reference_code.llama-index-core.llama_index.core.download.module",
        "peekOfCode": "LLAMA_HUB_CONTENTS_URL = f\"https://raw.githubusercontent.com/run-llama/llama-hub/main\"\nLLAMA_HUB_PATH = \"/llama_hub\"\nLLAMA_HUB_URL = LLAMA_HUB_CONTENTS_URL + LLAMA_HUB_PATH\nPATH_TYPE = Union[str, Path]\nlogger = logging.getLogger(__name__)\nLLAMAHUB_ANALYTICS_PROXY_SERVER = \"https://llamahub.ai/api/analytics/downloads\"\nclass MODULE_TYPE(str, Enum):\n    LOADER = \"loader\"\n    TOOL = \"tool\"\n    LLAMAPACK = \"llamapack\"",
        "detail": "reference_code.llama-index-core.llama_index.core.download.module",
        "documentation": {}
    },
    {
        "label": "LLAMA_HUB_PATH",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.module",
        "description": "reference_code.llama-index-core.llama_index.core.download.module",
        "peekOfCode": "LLAMA_HUB_PATH = \"/llama_hub\"\nLLAMA_HUB_URL = LLAMA_HUB_CONTENTS_URL + LLAMA_HUB_PATH\nPATH_TYPE = Union[str, Path]\nlogger = logging.getLogger(__name__)\nLLAMAHUB_ANALYTICS_PROXY_SERVER = \"https://llamahub.ai/api/analytics/downloads\"\nclass MODULE_TYPE(str, Enum):\n    LOADER = \"loader\"\n    TOOL = \"tool\"\n    LLAMAPACK = \"llamapack\"\n    DATASETS = \"datasets\"",
        "detail": "reference_code.llama-index-core.llama_index.core.download.module",
        "documentation": {}
    },
    {
        "label": "LLAMA_HUB_URL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.module",
        "description": "reference_code.llama-index-core.llama_index.core.download.module",
        "peekOfCode": "LLAMA_HUB_URL = LLAMA_HUB_CONTENTS_URL + LLAMA_HUB_PATH\nPATH_TYPE = Union[str, Path]\nlogger = logging.getLogger(__name__)\nLLAMAHUB_ANALYTICS_PROXY_SERVER = \"https://llamahub.ai/api/analytics/downloads\"\nclass MODULE_TYPE(str, Enum):\n    LOADER = \"loader\"\n    TOOL = \"tool\"\n    LLAMAPACK = \"llamapack\"\n    DATASETS = \"datasets\"\ndef get_module_info(",
        "detail": "reference_code.llama-index-core.llama_index.core.download.module",
        "documentation": {}
    },
    {
        "label": "PATH_TYPE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.module",
        "description": "reference_code.llama-index-core.llama_index.core.download.module",
        "peekOfCode": "PATH_TYPE = Union[str, Path]\nlogger = logging.getLogger(__name__)\nLLAMAHUB_ANALYTICS_PROXY_SERVER = \"https://llamahub.ai/api/analytics/downloads\"\nclass MODULE_TYPE(str, Enum):\n    LOADER = \"loader\"\n    TOOL = \"tool\"\n    LLAMAPACK = \"llamapack\"\n    DATASETS = \"datasets\"\ndef get_module_info(\n    local_dir_path: PATH_TYPE,",
        "detail": "reference_code.llama-index-core.llama_index.core.download.module",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.module",
        "description": "reference_code.llama-index-core.llama_index.core.download.module",
        "peekOfCode": "logger = logging.getLogger(__name__)\nLLAMAHUB_ANALYTICS_PROXY_SERVER = \"https://llamahub.ai/api/analytics/downloads\"\nclass MODULE_TYPE(str, Enum):\n    LOADER = \"loader\"\n    TOOL = \"tool\"\n    LLAMAPACK = \"llamapack\"\n    DATASETS = \"datasets\"\ndef get_module_info(\n    local_dir_path: PATH_TYPE,\n    remote_dir_path: PATH_TYPE,",
        "detail": "reference_code.llama-index-core.llama_index.core.download.module",
        "documentation": {}
    },
    {
        "label": "LLAMAHUB_ANALYTICS_PROXY_SERVER",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.module",
        "description": "reference_code.llama-index-core.llama_index.core.download.module",
        "peekOfCode": "LLAMAHUB_ANALYTICS_PROXY_SERVER = \"https://llamahub.ai/api/analytics/downloads\"\nclass MODULE_TYPE(str, Enum):\n    LOADER = \"loader\"\n    TOOL = \"tool\"\n    LLAMAPACK = \"llamapack\"\n    DATASETS = \"datasets\"\ndef get_module_info(\n    local_dir_path: PATH_TYPE,\n    remote_dir_path: PATH_TYPE,\n    module_class: str,",
        "detail": "reference_code.llama-index-core.llama_index.core.download.module",
        "documentation": {}
    },
    {
        "label": "download_module_and_reqs",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.pack",
        "description": "reference_code.llama-index-core.llama_index.core.download.pack",
        "peekOfCode": "def download_module_and_reqs(\n    local_dir_path: PATH_TYPE,\n    remote_dir_path: PATH_TYPE,\n    remote_source_dir_path: PATH_TYPE,\n    package: str,\n    sub_module: str,\n    refresh_cache: bool = False,\n) -> None:\n    \"\"\"Load module.\"\"\"\n    if isinstance(local_dir_path, str):",
        "detail": "reference_code.llama-index-core.llama_index.core.download.pack",
        "documentation": {}
    },
    {
        "label": "download_llama_pack_template",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.pack",
        "description": "reference_code.llama-index-core.llama_index.core.download.pack",
        "peekOfCode": "def download_llama_pack_template(\n    new_install_parent: str,\n    llama_pack_class: str,\n    llama_pack_url: str = LLAMA_PACKS_CONTENTS_URL,\n    llama_pack_source_files_dir_path: str = LLAMA_PACKS_SOURCE_FILES_GITHUB_TREE_URL,\n    refresh_cache: bool = False,\n    custom_dir: Optional[str] = None,\n    custom_path: Optional[str] = None,\n    base_file_name: str = \"__init__.py\",\n) -> Any:",
        "detail": "reference_code.llama-index-core.llama_index.core.download.pack",
        "documentation": {}
    },
    {
        "label": "track_download",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.pack",
        "description": "reference_code.llama-index-core.llama_index.core.download.pack",
        "peekOfCode": "def track_download(module_class: str, module_type: str) -> None:\n    \"\"\"\n    Tracks number of downloads via Llamahub proxy.\n    Args:\n        module_class: The name of the llama module being downloaded, e.g.,`GmailOpenAIAgentPack`.\n        module_type: Can be \"loader\", \"tool\", \"llamapack\", or \"datasets\"\n    \"\"\"\n    try:\n        requests.post(\n            LLAMAHUB_ANALYTICS_PROXY_SERVER,",
        "detail": "reference_code.llama-index-core.llama_index.core.download.pack",
        "documentation": {}
    },
    {
        "label": "LLAMA_PACKS_CONTENTS_URL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.pack",
        "description": "reference_code.llama-index-core.llama_index.core.download.pack",
        "peekOfCode": "LLAMA_PACKS_CONTENTS_URL = (\n    \"https://raw.githubusercontent.com/run-llama/llama_index/main/llama-index-packs\"\n)\nLLAMA_PACKS_SOURCE_FILES_GITHUB_TREE_URL = (\n    \"https://github.com/run-llama/llama_index/tree/main\"\n)\nPY_NAMESPACE = \"llama_index/packs\"\nPATH_TYPE = Union[str, Path]\nLLAMAHUB_ANALYTICS_PROXY_SERVER = \"https://llamahub.ai/api/analytics/downloads\"\nlogger = logging.getLogger(__name__)",
        "detail": "reference_code.llama-index-core.llama_index.core.download.pack",
        "documentation": {}
    },
    {
        "label": "LLAMA_PACKS_SOURCE_FILES_GITHUB_TREE_URL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.pack",
        "description": "reference_code.llama-index-core.llama_index.core.download.pack",
        "peekOfCode": "LLAMA_PACKS_SOURCE_FILES_GITHUB_TREE_URL = (\n    \"https://github.com/run-llama/llama_index/tree/main\"\n)\nPY_NAMESPACE = \"llama_index/packs\"\nPATH_TYPE = Union[str, Path]\nLLAMAHUB_ANALYTICS_PROXY_SERVER = \"https://llamahub.ai/api/analytics/downloads\"\nlogger = logging.getLogger(__name__)\ndef download_module_and_reqs(\n    local_dir_path: PATH_TYPE,\n    remote_dir_path: PATH_TYPE,",
        "detail": "reference_code.llama-index-core.llama_index.core.download.pack",
        "documentation": {}
    },
    {
        "label": "PY_NAMESPACE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.pack",
        "description": "reference_code.llama-index-core.llama_index.core.download.pack",
        "peekOfCode": "PY_NAMESPACE = \"llama_index/packs\"\nPATH_TYPE = Union[str, Path]\nLLAMAHUB_ANALYTICS_PROXY_SERVER = \"https://llamahub.ai/api/analytics/downloads\"\nlogger = logging.getLogger(__name__)\ndef download_module_and_reqs(\n    local_dir_path: PATH_TYPE,\n    remote_dir_path: PATH_TYPE,\n    remote_source_dir_path: PATH_TYPE,\n    package: str,\n    sub_module: str,",
        "detail": "reference_code.llama-index-core.llama_index.core.download.pack",
        "documentation": {}
    },
    {
        "label": "PATH_TYPE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.pack",
        "description": "reference_code.llama-index-core.llama_index.core.download.pack",
        "peekOfCode": "PATH_TYPE = Union[str, Path]\nLLAMAHUB_ANALYTICS_PROXY_SERVER = \"https://llamahub.ai/api/analytics/downloads\"\nlogger = logging.getLogger(__name__)\ndef download_module_and_reqs(\n    local_dir_path: PATH_TYPE,\n    remote_dir_path: PATH_TYPE,\n    remote_source_dir_path: PATH_TYPE,\n    package: str,\n    sub_module: str,\n    refresh_cache: bool = False,",
        "detail": "reference_code.llama-index-core.llama_index.core.download.pack",
        "documentation": {}
    },
    {
        "label": "LLAMAHUB_ANALYTICS_PROXY_SERVER",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.pack",
        "description": "reference_code.llama-index-core.llama_index.core.download.pack",
        "peekOfCode": "LLAMAHUB_ANALYTICS_PROXY_SERVER = \"https://llamahub.ai/api/analytics/downloads\"\nlogger = logging.getLogger(__name__)\ndef download_module_and_reqs(\n    local_dir_path: PATH_TYPE,\n    remote_dir_path: PATH_TYPE,\n    remote_source_dir_path: PATH_TYPE,\n    package: str,\n    sub_module: str,\n    refresh_cache: bool = False,\n) -> None:",
        "detail": "reference_code.llama-index-core.llama_index.core.download.pack",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.pack",
        "description": "reference_code.llama-index-core.llama_index.core.download.pack",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef download_module_and_reqs(\n    local_dir_path: PATH_TYPE,\n    remote_dir_path: PATH_TYPE,\n    remote_source_dir_path: PATH_TYPE,\n    package: str,\n    sub_module: str,\n    refresh_cache: bool = False,\n) -> None:\n    \"\"\"Load module.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.download.pack",
        "documentation": {}
    },
    {
        "label": "ChangeDirectory",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.utils",
        "description": "reference_code.llama-index-core.llama_index.core.download.utils",
        "peekOfCode": "class ChangeDirectory:\n    \"\"\"Context manager for changing the current working directory.\"\"\"\n    def __init__(self, new_path: str):\n        self.new_path = os.path.expanduser(new_path)\n    def __enter__(self) -> None:\n        self.saved_path = os.getcwd()\n        os.chdir(self.new_path)\n    def __exit__(self, etype: object, value: object, traceback: object) -> None:\n        os.chdir(self.saved_path)",
        "detail": "reference_code.llama-index-core.llama_index.core.download.utils",
        "documentation": {}
    },
    {
        "label": "get_file_content",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.utils",
        "description": "reference_code.llama-index-core.llama_index.core.download.utils",
        "peekOfCode": "def get_file_content(url: str, path: str) -> Tuple[str, int]:\n    \"\"\"Get the content of a file from the GitHub REST API.\"\"\"\n    resp = requests.get(url + path)\n    return resp.text, resp.status_code\ndef get_file_content_bytes(url: str, path: str) -> Tuple[bytes, int]:\n    \"\"\"Get the content of a file from the GitHub REST API.\"\"\"\n    resp = requests.get(url + path)\n    return resp.content, resp.status_code\ndef get_exports(raw_content: str) -> List:\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.download.utils",
        "documentation": {}
    },
    {
        "label": "get_file_content_bytes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.utils",
        "description": "reference_code.llama-index-core.llama_index.core.download.utils",
        "peekOfCode": "def get_file_content_bytes(url: str, path: str) -> Tuple[bytes, int]:\n    \"\"\"Get the content of a file from the GitHub REST API.\"\"\"\n    resp = requests.get(url + path)\n    return resp.content, resp.status_code\ndef get_exports(raw_content: str) -> List:\n    \"\"\"\n    Read content of a Python file and returns a list of exported class names.\n    For example:\n    ```python\n    from .a import A",
        "detail": "reference_code.llama-index-core.llama_index.core.download.utils",
        "documentation": {}
    },
    {
        "label": "get_exports",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.utils",
        "description": "reference_code.llama-index-core.llama_index.core.download.utils",
        "peekOfCode": "def get_exports(raw_content: str) -> List:\n    \"\"\"\n    Read content of a Python file and returns a list of exported class names.\n    For example:\n    ```python\n    from .a import A\n    from .b import B\n    __all__ = [\"A\", \"B\"]\n    ```\n    will return `[\"A\", \"B\"]`.",
        "detail": "reference_code.llama-index-core.llama_index.core.download.utils",
        "documentation": {}
    },
    {
        "label": "rewrite_exports",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.utils",
        "description": "reference_code.llama-index-core.llama_index.core.download.utils",
        "peekOfCode": "def rewrite_exports(exports: List[str], dirpath: str) -> None:\n    \"\"\"\n    Write the `__all__` variable to the `__init__.py` file in the modules dir.\n    Removes the line that contains `__all__` and appends a new line with the updated\n    `__all__` variable.\n    Args:\n        - exports: A list of exported class names.\n    \"\"\"\n    init_path = f\"{dirpath}/__init__.py\"\n    with open(init_path) as f:",
        "detail": "reference_code.llama-index-core.llama_index.core.download.utils",
        "documentation": {}
    },
    {
        "label": "initialize_directory",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.utils",
        "description": "reference_code.llama-index-core.llama_index.core.download.utils",
        "peekOfCode": "def initialize_directory(\n    custom_path: Optional[str] = None, custom_dir: Optional[str] = None\n) -> Path:\n    \"\"\"Initialize directory.\"\"\"\n    if custom_path is not None and custom_dir is not None:\n        raise ValueError(\n            \"You cannot specify both `custom_path` and `custom_dir` at the same time.\"\n        )\n    custom_dir = custom_dir or \"llamadatasets\"\n    if custom_path is not None:",
        "detail": "reference_code.llama-index-core.llama_index.core.download.utils",
        "documentation": {}
    },
    {
        "label": "get_source_files_list",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.utils",
        "description": "reference_code.llama-index-core.llama_index.core.download.utils",
        "peekOfCode": "def get_source_files_list(source_tree_url: str, path: str) -> List[str]:\n    \"\"\"Get the list of source files to download.\"\"\"\n    resp = requests.get(\n        source_tree_url + path + \"?recursive=1\", headers={\"Accept\": \"application/json\"}\n    )\n    payload = resp.json()[\"payload\"]\n    return [item[\"name\"] for item in payload[\"tree\"][\"items\"]]\ndef recursive_tree_traverse(\n    tree_urls: List[str], acc: List[str], source_tree_url: str\n) -> List[str]:",
        "detail": "reference_code.llama-index-core.llama_index.core.download.utils",
        "documentation": {}
    },
    {
        "label": "recursive_tree_traverse",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.utils",
        "description": "reference_code.llama-index-core.llama_index.core.download.utils",
        "peekOfCode": "def recursive_tree_traverse(\n    tree_urls: List[str], acc: List[str], source_tree_url: str\n) -> List[str]:\n    \"\"\"Recursively traversge Github trees to get all file paths in a folder.\"\"\"\n    if not tree_urls:\n        return acc\n    else:\n        url = tree_urls[0]\n        try:\n            res = requests.get(url, headers={\"Accept\": \"application/json\"})",
        "detail": "reference_code.llama-index-core.llama_index.core.download.utils",
        "documentation": {}
    },
    {
        "label": "get_source_files_recursive",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.download.utils",
        "description": "reference_code.llama-index-core.llama_index.core.download.utils",
        "peekOfCode": "def get_source_files_recursive(source_tree_url: str, path: str) -> List[str]:\n    \"\"\"Get source files of a Github folder recursively.\"\"\"\n    initial_url = source_tree_url + path + \"?recursive=1\"\n    initial_tree_urls = [initial_url]\n    return recursive_tree_traverse(initial_tree_urls, [], source_tree_url)\nclass ChangeDirectory:\n    \"\"\"Context manager for changing the current working directory.\"\"\"\n    def __init__(self, new_path: str):\n        self.new_path = os.path.expanduser(new_path)\n    def __enter__(self) -> None:",
        "detail": "reference_code.llama-index-core.llama_index.core.download.utils",
        "documentation": {}
    },
    {
        "label": "load_embed_model",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.embeddings.loading",
        "description": "reference_code.llama-index-core.llama_index.core.embeddings.loading",
        "peekOfCode": "def load_embed_model(data: dict) -> BaseEmbedding:\n    \"\"\"Load Embedding by name.\"\"\"\n    if isinstance(data, BaseEmbedding):\n        return data\n    name = data.get(\"class_name\")\n    if name is None:\n        raise ValueError(\"Embedding loading requires a class_name\")\n    if name not in RECOGNIZED_EMBEDDINGS:\n        raise ValueError(f\"Invalid Embedding name: {name}\")\n    return RECOGNIZED_EMBEDDINGS[name].from_dict(data)",
        "detail": "reference_code.llama-index-core.llama_index.core.embeddings.loading",
        "documentation": {}
    },
    {
        "label": "MockEmbedding",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.embeddings.mock_embed_model",
        "description": "reference_code.llama-index-core.llama_index.core.embeddings.mock_embed_model",
        "peekOfCode": "class MockEmbedding(BaseEmbedding):\n    \"\"\"\n    Mock embedding.\n    Used for token prediction.\n    Args:\n        embed_dim (int): embedding dimension\n    \"\"\"\n    embed_dim: int\n    def __init__(self, embed_dim: int, **kwargs: Any) -> None:\n        \"\"\"Init params.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.embeddings.mock_embed_model",
        "documentation": {}
    },
    {
        "label": "MultiModalEmbedding",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.embeddings.multi_modal_base",
        "description": "reference_code.llama-index-core.llama_index.core.embeddings.multi_modal_base",
        "peekOfCode": "class MultiModalEmbedding(BaseEmbedding):\n    \"\"\"Base class for Multi Modal embeddings.\"\"\"\n    @abstractmethod\n    def _get_image_embedding(self, img_file_path: ImageType) -> Embedding:\n        \"\"\"\n        Embed the input image synchronously.\n        Subclasses should implement this method. Reference get_image_embedding's\n        docstring for more information.\n        \"\"\"\n    @abstractmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.embeddings.multi_modal_base",
        "documentation": {}
    },
    {
        "label": "Pooling",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.embeddings.pooling",
        "description": "reference_code.llama-index-core.llama_index.core.embeddings.pooling",
        "peekOfCode": "class Pooling(str, Enum):\n    \"\"\"Enum of possible pooling choices with pooling behaviors.\"\"\"\n    CLS = \"cls\"\n    MEAN = \"mean\"\n    def __call__(self, array: np.ndarray) -> np.ndarray:\n        if self == self.CLS:\n            return Pooling.cls_pooling(array)\n        return Pooling.mean_pooling(array)\n    @classmethod\n    @overload",
        "detail": "reference_code.llama-index-core.llama_index.core.embeddings.pooling",
        "documentation": {}
    },
    {
        "label": "save_embedding",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.embeddings.utils",
        "description": "reference_code.llama-index-core.llama_index.core.embeddings.utils",
        "peekOfCode": "def save_embedding(embedding: List[float], file_path: str) -> None:\n    \"\"\"Save embedding to file.\"\"\"\n    with open(file_path, \"w\") as f:\n        f.write(\",\".join([str(x) for x in embedding]))\ndef load_embedding(file_path: str) -> List[float]:\n    \"\"\"Load embedding from file. Will only return first embedding in file.\"\"\"\n    with open(file_path) as f:\n        for line in f:\n            embedding = [float(x) for x in line.strip().split(\",\")]\n            break",
        "detail": "reference_code.llama-index-core.llama_index.core.embeddings.utils",
        "documentation": {}
    },
    {
        "label": "load_embedding",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.embeddings.utils",
        "description": "reference_code.llama-index-core.llama_index.core.embeddings.utils",
        "peekOfCode": "def load_embedding(file_path: str) -> List[float]:\n    \"\"\"Load embedding from file. Will only return first embedding in file.\"\"\"\n    with open(file_path) as f:\n        for line in f:\n            embedding = [float(x) for x in line.strip().split(\",\")]\n            break\n        return embedding\ndef resolve_embed_model(\n    embed_model: Optional[EmbedType] = None,\n    callback_manager: Optional[CallbackManager] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.embeddings.utils",
        "documentation": {}
    },
    {
        "label": "resolve_embed_model",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.embeddings.utils",
        "description": "reference_code.llama-index-core.llama_index.core.embeddings.utils",
        "peekOfCode": "def resolve_embed_model(\n    embed_model: Optional[EmbedType] = None,\n    callback_manager: Optional[CallbackManager] = None,\n) -> BaseEmbedding:\n    \"\"\"Resolve embed model.\"\"\"\n    from llama_index.core.settings import Settings\n    try:\n        from llama_index.core.bridge.langchain import Embeddings as LCEmbeddings\n    except ImportError:\n        LCEmbeddings = None  # type: ignore",
        "detail": "reference_code.llama-index-core.llama_index.core.embeddings.utils",
        "documentation": {}
    },
    {
        "label": "EmbedType",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.embeddings.utils",
        "description": "reference_code.llama-index-core.llama_index.core.embeddings.utils",
        "peekOfCode": "EmbedType = Union[BaseEmbedding, \"LCEmbeddings\", str]\ndef save_embedding(embedding: List[float], file_path: str) -> None:\n    \"\"\"Save embedding to file.\"\"\"\n    with open(file_path, \"w\") as f:\n        f.write(\",\".join([str(x) for x in embedding]))\ndef load_embedding(file_path: str) -> List[float]:\n    \"\"\"Load embedding from file. Will only return first embedding in file.\"\"\"\n    with open(file_path) as f:\n        for line in f:\n            embedding = [float(x) for x in line.strip().split(\",\")]",
        "detail": "reference_code.llama-index-core.llama_index.core.embeddings.utils",
        "documentation": {}
    },
    {
        "label": "BeirEvaluator",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.benchmarks.beir",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.benchmarks.beir",
        "peekOfCode": "class BeirEvaluator:\n    \"\"\"\n    Refer to: https://github.com/beir-cellar/beir for a full list of supported datasets\n    and a full description of BEIR.\n    \"\"\"\n    def __init__(self) -> None:\n        try:\n            pass\n        except ImportError:\n            raise ImportError(",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.benchmarks.beir",
        "documentation": {}
    },
    {
        "label": "HotpotQAEvaluator",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.benchmarks.hotpotqa",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.benchmarks.hotpotqa",
        "peekOfCode": "class HotpotQAEvaluator:\n    \"\"\"\n    Refer to https://hotpotqa.github.io/ for more details on the dataset.\n    \"\"\"\n    def _download_datasets(self) -> Dict[str, str]:\n        cache_dir = get_cache_dir()\n        dataset_paths = {}\n        dataset = \"hotpot_dev_distractor\"\n        dataset_full_path = os.path.join(cache_dir, \"datasets\", \"HotpotQA\")\n        if not os.path.exists(dataset_full_path):",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.benchmarks.hotpotqa",
        "documentation": {}
    },
    {
        "label": "HotpotQARetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.benchmarks.hotpotqa",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.benchmarks.hotpotqa",
        "peekOfCode": "class HotpotQARetriever(BaseRetriever):\n    \"\"\"\n    This is a mocked retriever for HotpotQA dataset. It is only meant to be used\n    with the hotpotqa dev dataset in the distractor setting. This is the setting that\n    does not require retrieval but requires identifying the supporting facts from\n    a list of 10 sources.\n    \"\"\"\n    def __init__(self, query_objects: Any) -> None:\n        assert isinstance(\n            query_objects,",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.benchmarks.hotpotqa",
        "documentation": {}
    },
    {
        "label": "normalize_answer",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.benchmarks.hotpotqa",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.benchmarks.hotpotqa",
        "peekOfCode": "def normalize_answer(s: str) -> str:\n    def remove_articles(text: str) -> str:\n        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n    def white_space_fix(text: str) -> str:\n        return \" \".join(text.split())\n    def remove_punc(text: str) -> str:\n        exclude = set(string.punctuation)\n        return \"\".join(ch for ch in text if ch not in exclude)\n    def lower(text: str) -> str:\n        return text.lower()",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.benchmarks.hotpotqa",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.benchmarks.hotpotqa",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.benchmarks.hotpotqa",
        "peekOfCode": "def f1_score(prediction: str, ground_truth: str) -> Tuple[float, float, float]:\n    normalized_prediction = normalize_answer(prediction)\n    normalized_ground_truth = normalize_answer(ground_truth)\n    ZERO_METRIC = (0, 0, 0)\n    if (\n        normalized_prediction in [\"yes\", \"no\", \"noanswer\"]\n        and normalized_prediction != normalized_ground_truth\n    ):\n        return ZERO_METRIC\n    if (",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.benchmarks.hotpotqa",
        "documentation": {}
    },
    {
        "label": "exact_match_score",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.benchmarks.hotpotqa",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.benchmarks.hotpotqa",
        "peekOfCode": "def exact_match_score(prediction: str, ground_truth: str) -> bool:\n    return normalize_answer(prediction) == normalize_answer(ground_truth)",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.benchmarks.hotpotqa",
        "documentation": {}
    },
    {
        "label": "DEV_DISTRACTOR_URL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.benchmarks.hotpotqa",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.benchmarks.hotpotqa",
        "peekOfCode": "DEV_DISTRACTOR_URL = \"\"\"https://web.archive.org/web/20250512032701id_/http://curtis.ml.cmu.edu/datasets/\\\nhotpot/hotpot_dev_distractor_v1.json\"\"\"\nclass HotpotQAEvaluator:\n    \"\"\"\n    Refer to https://hotpotqa.github.io/ for more details on the dataset.\n    \"\"\"\n    def _download_datasets(self) -> Dict[str, str]:\n        cache_dir = get_cache_dir()\n        dataset_paths = {}\n        dataset = \"hotpot_dev_distractor\"",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.benchmarks.hotpotqa",
        "documentation": {}
    },
    {
        "label": "MultiModalFaithfulnessEvaluator",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.multi_modal.faithfulness",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.multi_modal.faithfulness",
        "peekOfCode": "class MultiModalFaithfulnessEvaluator(BaseEvaluator):\n    \"\"\"\n    Multi-Modal Faithfulness evaluator.\n    Evaluates whether a response is faithful to the contexts\n    (i.e. whether the response is supported by the contexts or hallucinated.)\n    This evaluator only considers the response string and the list of context strings.\n    Args:\n        multi_modal_llm(Optional[LLM]):\n            The LLM Judge to use for evaluations.\n        raise_error(bool): Whether to raise an error when the response is invalid.",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.multi_modal.faithfulness",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EVAL_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.multi_modal.faithfulness",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.multi_modal.faithfulness",
        "peekOfCode": "DEFAULT_EVAL_TEMPLATE = PromptTemplate(\n    \"Please tell if a given piece of information \"\n    \"is supported by the visual as well as textual context information.\\n\"\n    \"You need to answer with either YES or NO.\\n\"\n    \"Answer YES if any of the image(s) and textual context supports the information, even \"\n    \"if most of the context is unrelated. \"\n    \"Some examples are provided below with only text context, but please do use\\n\"\n    \"any images for context if they are provided.\\n\\n\"\n    \"Information: Apple pie is generally double-crusted.\\n\"\n    \"Context: An apple pie is a fruit pie in which the principal filling \"",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.multi_modal.faithfulness",
        "documentation": {}
    },
    {
        "label": "DEFAULT_REFINE_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.multi_modal.faithfulness",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.multi_modal.faithfulness",
        "peekOfCode": "DEFAULT_REFINE_TEMPLATE = PromptTemplate(\n    \"We want to understand if the following information is present \"\n    \"in the context information: {query_str}\\n\"\n    \"We have provided an existing YES/NO answer: {existing_answer}\\n\"\n    \"We have the opportunity to refine the existing answer \"\n    \"(only if needed) with some more context below.\\n\"\n    \"------------\\n\"\n    \"{context_msg}\\n\"\n    \"------------\\n\"\n    \"If the existing answer was already YES, still answer YES. \"",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.multi_modal.faithfulness",
        "documentation": {}
    },
    {
        "label": "MultiModalRelevancyEvaluator",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.multi_modal.relevancy",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.multi_modal.relevancy",
        "peekOfCode": "class MultiModalRelevancyEvaluator(BaseEvaluator):\n    \"\"\"\n    Relevancy evaluator.\n    Evaluates the relevancy of retrieved image and textual contexts and response to a query.\n    This evaluator considers the query string, retrieved contexts, and response string.\n    Args:\n        multi_modal_llm(Optional[LLM]):\n            The Multi-Modal LLM Judge to use for evaluations.\n        raise_error(Optional[bool]):\n            Whether to raise an error if the response is invalid.",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.multi_modal.relevancy",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EVAL_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.multi_modal.relevancy",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.multi_modal.relevancy",
        "peekOfCode": "DEFAULT_EVAL_TEMPLATE = PromptTemplate(\n    \"Your task is to evaluate if the response for the query \\\n    is in line with the images and textual context information provided.\\n\"\n    \"You have two options to answer. Either YES/ NO.\\n\"\n    \"Answer - YES, if the response for the query \\\n    is in line with context information otherwise NO.\\n\"\n    \"Query and Response: \\n {query_str}\\n\"\n    \"Context: \\n {context_str}\\n\"\n    \"Answer: \"\n)",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.multi_modal.relevancy",
        "documentation": {}
    },
    {
        "label": "DEFAULT_REFINE_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.multi_modal.relevancy",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.multi_modal.relevancy",
        "peekOfCode": "DEFAULT_REFINE_TEMPLATE = PromptTemplate(\n    \"We want to understand if the following query and response is\"\n    \"in line with the textual and visual context information: \\n {query_str}\\n\"\n    \"We have provided an existing YES/NO answer: \\n {existing_answer}\\n\"\n    \"We have the opportunity to refine the existing answer \"\n    \"(only if needed) with some more context below.\\n\"\n    \"------------\\n\"\n    \"{context_msg}\\n\"\n    \"------------\\n\"\n    \"If the existing answer was already YES, still answer YES. \"",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.multi_modal.relevancy",
        "documentation": {}
    },
    {
        "label": "RetrievalEvalMode",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.base",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.base",
        "peekOfCode": "class RetrievalEvalMode(str, Enum):\n    \"\"\"Evaluation of retrieval modality.\"\"\"\n    TEXT = \"text\"\n    IMAGE = \"image\"\n    @classmethod\n    def from_str(cls, label: str) -> \"RetrievalEvalMode\":\n        if label == \"text\":\n            return RetrievalEvalMode.TEXT\n        elif label == \"image\":\n            return RetrievalEvalMode.IMAGE",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.base",
        "documentation": {}
    },
    {
        "label": "RetrievalEvalResult",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.base",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.base",
        "peekOfCode": "class RetrievalEvalResult(BaseModel):\n    \"\"\"\n    Retrieval eval result.\n    NOTE: this abstraction might change in the future.\n    Attributes:\n        query (str): Query string\n        expected_ids (List[str]): Expected ids\n        retrieved_ids (List[str]): Retrieved ids\n        metric_dict (Dict[str, BaseRetrievalMetric]): \\\n            Metric dictionary for the evaluation",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.base",
        "documentation": {}
    },
    {
        "label": "BaseRetrievalEvaluator",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.base",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.base",
        "peekOfCode": "class BaseRetrievalEvaluator(BaseModel):\n    \"\"\"Base Retrieval Evaluator class.\"\"\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    metrics: List[BaseRetrievalMetric] = Field(\n        ..., description=\"List of metrics to evaluate\"\n    )\n    @classmethod\n    def from_metric_names(\n        cls, metric_names: List[str], **kwargs: Any\n    ) -> \"BaseRetrievalEvaluator\":",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.base",
        "documentation": {}
    },
    {
        "label": "RetrieverEvaluator",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.evaluator",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.evaluator",
        "peekOfCode": "class RetrieverEvaluator(BaseRetrievalEvaluator):\n    \"\"\"\n    Retriever evaluator.\n    This module will evaluate a retriever using a set of metrics.\n    Args:\n        metrics (List[BaseRetrievalMetric]): Sequence of metrics to evaluate\n        retriever: Retriever to evaluate.\n        node_postprocessors (Optional[List[BaseNodePostprocessor]]): Post-processor to apply after retrieval.\n    \"\"\"\n    retriever: BaseRetriever = Field(..., description=\"Retriever to evaluate\")",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.evaluator",
        "documentation": {}
    },
    {
        "label": "MultiModalRetrieverEvaluator",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.evaluator",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.evaluator",
        "peekOfCode": "class MultiModalRetrieverEvaluator(BaseRetrievalEvaluator):\n    \"\"\"\n    Retriever evaluator.\n    This module will evaluate a retriever using a set of metrics.\n    Args:\n        metrics (List[BaseRetrievalMetric]): Sequence of metrics to evaluate\n        retriever: Retriever to evaluate.\n        node_postprocessors (Optional[List[BaseNodePostprocessor]]): Post-processor to apply after retrieval.\n    \"\"\"\n    retriever: BaseRetriever = Field(..., description=\"Retriever to evaluate\")",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.evaluator",
        "documentation": {}
    },
    {
        "label": "HitRate",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "peekOfCode": "class HitRate(BaseRetrievalMetric):\n    \"\"\"\n    Hit rate metric: Compute hit rate with two calculation options.\n    - The default method checks for a single match between any of the retrieved docs and expected docs.\n    - The more granular method checks for all potential matches between retrieved docs and expected docs.\n    Attributes:\n        metric_name (str): The name of the metric.\n        use_granular_hit_rate (bool): Determines whether to use the granular method for calculation.\n    \"\"\"\n    metric_name: ClassVar[str] = \"hit_rate\"",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "documentation": {}
    },
    {
        "label": "MRR",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "peekOfCode": "class MRR(BaseRetrievalMetric):\n    \"\"\"\n    MRR (Mean Reciprocal Rank) metric with two calculation options.\n    - The default method calculates the reciprocal rank of the first relevant retrieved document.\n    - The more granular method sums the reciprocal ranks of all relevant retrieved documents and divides by the count of relevant documents.\n    Attributes:\n        metric_name (str): The name of the metric.\n        use_granular_mrr (bool): Determines whether to use the granular method for calculation.\n    \"\"\"\n    metric_name: ClassVar[str] = \"mrr\"",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "documentation": {}
    },
    {
        "label": "Precision",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "peekOfCode": "class Precision(BaseRetrievalMetric):\n    \"\"\"\n    Precision metric.\n    The `K`-value in `Precision@K` usually corresponds to `top_k` of the retriever.\n    Attributes:\n        metric_name (str): The name of the metric.\n    \"\"\"\n    metric_name: ClassVar[str] = \"precision\"\n    def compute(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "documentation": {}
    },
    {
        "label": "Recall",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "peekOfCode": "class Recall(BaseRetrievalMetric):\n    \"\"\"\n    Recall metric.\n    Attributes:\n        metric_name (str): The name of the metric.\n    \"\"\"\n    metric_name: ClassVar[str] = \"recall\"\n    def compute(\n        self,\n        query: Optional[str] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "documentation": {}
    },
    {
        "label": "AveragePrecision",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "peekOfCode": "class AveragePrecision(BaseRetrievalMetric):\n    \"\"\"\n    Average Precision (AP) metric.\n    Attributes:\n        metric_name (str): The name of the metric.\n    \"\"\"\n    metric_name: ClassVar[str] = \"ap\"\n    def compute(\n        self,\n        query: Optional[str] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "documentation": {}
    },
    {
        "label": "NDCG",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "peekOfCode": "class NDCG(BaseRetrievalMetric):\n    \"\"\"\n    NDCG (Normalized Discounted Cumulative Gain) metric.\n    The position `p` is taken as the size of the query results (which is usually\n    `top_k` of the retriever).\n    Currently only supports binary relevance\n    (``rel=1`` if document is in ``expected_ids``, otherwise ``rel=0``)\n    since we assume that ``expected_ids`` is unordered.\n    Attributes:\n        metric_name (str): The name of the metric.",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "documentation": {}
    },
    {
        "label": "CohereRerankRelevancyMetric",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "peekOfCode": "class CohereRerankRelevancyMetric(BaseRetrievalMetric):\n    \"\"\"Cohere rerank relevancy metric.\"\"\"\n    metric_name: ClassVar[str] = \"cohere_rerank_relevancy\"\n    model: str = Field(description=\"Cohere model name.\")\n    _client: Any = PrivateAttr()\n    def __init__(\n        self,\n        model: str = \"rerank-english-v2.0\",\n        api_key: Optional[str] = None,\n    ):",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "documentation": {}
    },
    {
        "label": "discounted_gain",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "peekOfCode": "def discounted_gain(*, rel: float, i: int, mode: DiscountedGainMode) -> float:\n    # Avoid unnecessary calculations. Note that `False == 0` and `True == 1`.\n    if rel == 0:\n        return 0\n    if rel == 1:\n        return 1 / math.log2(i + 1)\n    if mode == \"linear\":\n        return rel / math.log2(i + 1)\n    elif mode == \"exponential\":\n        return (2**rel - 1) / math.log2(i + 1)",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "documentation": {}
    },
    {
        "label": "resolve_metrics",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "peekOfCode": "def resolve_metrics(metrics: List[str]) -> List[Type[BaseRetrievalMetric]]:\n    \"\"\"Resolve metrics from list of metric names.\"\"\"\n    for metric in metrics:\n        if metric not in METRIC_REGISTRY:\n            raise ValueError(f\"Invalid metric name: {metric}\")\n    return [METRIC_REGISTRY[metric] for metric in metrics]",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "documentation": {}
    },
    {
        "label": "DiscountedGainMode",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "peekOfCode": "DiscountedGainMode = Literal[\"linear\", \"exponential\"]\ndef discounted_gain(*, rel: float, i: int, mode: DiscountedGainMode) -> float:\n    # Avoid unnecessary calculations. Note that `False == 0` and `True == 1`.\n    if rel == 0:\n        return 0\n    if rel == 1:\n        return 1 / math.log2(i + 1)\n    if mode == \"linear\":\n        return rel / math.log2(i + 1)\n    elif mode == \"exponential\":",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics",
        "documentation": {}
    },
    {
        "label": "RetrievalMetricResult",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics_base",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics_base",
        "peekOfCode": "class RetrievalMetricResult(BaseModel):\n    \"\"\"\n    Metric result.\n    Attributes:\n        score (float): Score for the metric\n        metadata (Dict[str, Any]): Metadata for the metric result\n    \"\"\"\n    score: float = Field(..., description=\"Score for the metric\")\n    metadata: Dict[str, Any] = Field(\n        default_factory=dict, description=\"Metadata for the metric result\"",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics_base",
        "documentation": {}
    },
    {
        "label": "BaseRetrievalMetric",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics_base",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics_base",
        "peekOfCode": "class BaseRetrievalMetric(BaseModel, ABC):\n    \"\"\"Base class for retrieval metrics.\"\"\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    metric_name: ClassVar[str]\n    @abstractmethod\n    def compute(\n        self,\n        query: Optional[str] = None,\n        expected_ids: Optional[List[str]] = None,\n        retrieved_ids: Optional[List[str]] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.retrieval.metrics_base",
        "documentation": {}
    },
    {
        "label": "AnswerRelevancyEvaluator",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.answer_relevancy",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.answer_relevancy",
        "peekOfCode": "class AnswerRelevancyEvaluator(BaseEvaluator):\n    \"\"\"\n    Answer relevancy evaluator.\n    Evaluates the relevancy of response to a query.\n    This evaluator considers the query string and response string.\n    Args:\n        raise_error(Optional[bool]):\n            Whether to raise an error if the response is invalid.\n            Defaults to False.\n        eval_template(Optional[Union[str, BasePromptTemplate]]):",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.answer_relevancy",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EVAL_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.answer_relevancy",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.answer_relevancy",
        "peekOfCode": "DEFAULT_EVAL_TEMPLATE = PromptTemplate(\n    \"Your task is to evaluate if the response is relevant to the query.\\n\"\n    \"The evaluation should be performed in a step-by-step manner by answering the following questions:\\n\"\n    \"1. Does the provided response match the subject matter of the user's query?\\n\"\n    \"2. Does the provided response attempt to address the focus or perspective \"\n    \"on the subject matter taken on by the user's query?\\n\"\n    \"Each question above is worth 1 point. Provide detailed feedback on response according to the criteria questions above  \"\n    \"After your feedback provide a final result by strictly following this format: '[RESULT] followed by the integer number representing the total score assigned to the response'\\n\\n\"\n    \"Query: \\n {query}\\n\"\n    \"Response: \\n {response}\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.answer_relevancy",
        "documentation": {}
    },
    {
        "label": "_DEFAULT_SCORE_THRESHOLD",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.answer_relevancy",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.answer_relevancy",
        "peekOfCode": "_DEFAULT_SCORE_THRESHOLD = 2.0\ndef _default_parser_function(output_str: str) -> Tuple[Optional[float], Optional[str]]:\n    # Pattern to match the feedback and response\n    # This pattern looks for any text ending with '[RESULT]' followed by a number\n    pattern = r\"([\\s\\S]+)(?:\\[RESULT\\]\\s*)(\\d)\"\n    # Using regex to find all matches\n    result = re.search(pattern, output_str)\n    # Check if any match is found\n    if result:\n        # Assuming there's only one match in the text, extract feedback and response",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.answer_relevancy",
        "documentation": {}
    },
    {
        "label": "EvaluationResult",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.base",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.base",
        "peekOfCode": "class EvaluationResult(BaseModel):\n    \"\"\"\n    Evaluation result.\n    Output of an BaseEvaluator.\n    \"\"\"\n    query: Optional[str] = Field(default=None, description=\"Query string\")\n    contexts: Optional[Sequence[str]] = Field(\n        default=None, description=\"Context strings\"\n    )\n    response: Optional[str] = Field(default=None, description=\"Response string\")",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "BaseEvaluator",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.base",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.base",
        "peekOfCode": "class BaseEvaluator(PromptMixin):\n    \"\"\"Base Evaluator class.\"\"\"\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt modules.\"\"\"\n        return {}\n    def evaluate(\n        self,\n        query: Optional[str] = None,\n        response: Optional[str] = None,\n        contexts: Optional[Sequence[str]] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "Evaluation",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.base",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.base",
        "peekOfCode": "Evaluation = EvaluationResult",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.base",
        "documentation": {}
    },
    {
        "label": "BatchEvalRunner",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.batch_runner",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.batch_runner",
        "peekOfCode": "class BatchEvalRunner:\n    \"\"\"\n    Batch evaluation runner.\n    Args:\n        evaluators (Dict[str, BaseEvaluator]): Dictionary of evaluators.\n        workers (int): Number of workers to use for parallelization.\n            Defaults to 2.\n        show_progress (bool): Whether to show progress bars. Defaults to False.\n    \"\"\"\n    def __init__(",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.batch_runner",
        "documentation": {}
    },
    {
        "label": "ContextRelevancyEvaluator",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.context_relevancy",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.context_relevancy",
        "peekOfCode": "class ContextRelevancyEvaluator(BaseEvaluator):\n    \"\"\"\n    Context relevancy evaluator.\n    Evaluates the relevancy of retrieved contexts to a query.\n    This evaluator considers the query string and retrieved contexts.\n    Args:\n        raise_error(Optional[bool]):\n            Whether to raise an error if the response is invalid.\n            Defaults to False.\n        eval_template(Optional[Union[str, BasePromptTemplate]]):",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.context_relevancy",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EVAL_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.context_relevancy",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.context_relevancy",
        "peekOfCode": "DEFAULT_EVAL_TEMPLATE = PromptTemplate(\n    \"Your task is to evaluate if the retrieved context from the document sources are relevant to the query.\\n\"\n    \"The evaluation should be performed in a step-by-step manner by answering the following questions:\\n\"\n    \"1. Does the retrieved context match the subject matter of the user's query?\\n\"\n    \"2. Can the retrieved context be used exclusively to provide a full answer to the user's query?\\n\"\n    \"Each question above is worth 2 points, where partial marks are allowed and encouraged. Provide detailed feedback on the response \"\n    \"according to the criteria questions previously mentioned. \"\n    \"After your feedback provide a final result by strictly following this format: \"\n    \"'[RESULT] followed by the float number representing the total score assigned to the response'\\n\\n\"\n    \"Query: \\n {query_str}\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.context_relevancy",
        "documentation": {}
    },
    {
        "label": "_DEFAULT_SCORE_THRESHOLD",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.context_relevancy",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.context_relevancy",
        "peekOfCode": "_DEFAULT_SCORE_THRESHOLD = 4.0\nDEFAULT_REFINE_TEMPLATE = PromptTemplate(\n    \"We want to understand if the following query and response is\"\n    \"in line with the context information: \\n {query_str}\\n\"\n    \"We have provided an existing evaluation score: \\n {existing_answer}\\n\"\n    \"We have the opportunity to refine the existing evaluation \"\n    \"(only if needed) with some more context below.\\n\"\n    \"------------\\n\"\n    \"{context_msg}\\n\"\n    \"------------\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.context_relevancy",
        "documentation": {}
    },
    {
        "label": "DEFAULT_REFINE_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.context_relevancy",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.context_relevancy",
        "peekOfCode": "DEFAULT_REFINE_TEMPLATE = PromptTemplate(\n    \"We want to understand if the following query and response is\"\n    \"in line with the context information: \\n {query_str}\\n\"\n    \"We have provided an existing evaluation score: \\n {existing_answer}\\n\"\n    \"We have the opportunity to refine the existing evaluation \"\n    \"(only if needed) with some more context below.\\n\"\n    \"------------\\n\"\n    \"{context_msg}\\n\"\n    \"------------\\n\"\n    f\"If the existing evaluation was already {_DEFAULT_SCORE_THRESHOLD}, still answer {_DEFAULT_SCORE_THRESHOLD}. \"",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.context_relevancy",
        "documentation": {}
    },
    {
        "label": "CorrectnessEvaluator",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.correctness",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.correctness",
        "peekOfCode": "class CorrectnessEvaluator(BaseEvaluator):\n    \"\"\"\n    Correctness evaluator.\n    Evaluates the correctness of a question answering system.\n    This evaluator depends on `reference` answer to be provided, in addition to the\n    query string and response string.\n    It outputs a score between 1 and 5, where 1 is the worst and 5 is the best,\n    along with a reasoning for the score.\n    Passing is defined as a score greater than or equal to the given threshold.\n    Args:",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.correctness",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SYSTEM_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.correctness",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.correctness",
        "peekOfCode": "DEFAULT_SYSTEM_TEMPLATE = \"\"\"\nYou are an expert evaluation system for a question answering chatbot.\nYou are given the following information:\n- a user query, and\n- a generated answer\nYou may also be given a reference answer to use for reference in your evaluation.\nYour job is to judge the relevance and correctness of the generated answer.\nOutput a single score that represents a holistic evaluation.\nYou must return your response in a line with only the score.\nDo not return answers in any other format.",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.correctness",
        "documentation": {}
    },
    {
        "label": "DEFAULT_USER_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.correctness",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.correctness",
        "peekOfCode": "DEFAULT_USER_TEMPLATE = \"\"\"\n## User Query\n{query}\n## Reference Answer\n{reference_answer}\n## Generated Answer\n{generated_answer}\n\"\"\"\nDEFAULT_EVAL_TEMPLATE = ChatPromptTemplate(\n    message_templates=[",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.correctness",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EVAL_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.correctness",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.correctness",
        "peekOfCode": "DEFAULT_EVAL_TEMPLATE = ChatPromptTemplate(\n    message_templates=[\n        ChatMessage(role=MessageRole.SYSTEM, content=DEFAULT_SYSTEM_TEMPLATE),\n        ChatMessage(role=MessageRole.USER, content=DEFAULT_USER_TEMPLATE),\n    ]\n)\nclass CorrectnessEvaluator(BaseEvaluator):\n    \"\"\"\n    Correctness evaluator.\n    Evaluates the correctness of a question answering system.",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.correctness",
        "documentation": {}
    },
    {
        "label": "QueryResponseDataset",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.dataset_generation",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.dataset_generation",
        "peekOfCode": "class QueryResponseDataset(BaseModel):\n    \"\"\"\n    Query Response Dataset.\n    The response can be empty if the dataset is generated from documents.\n    Args:\n        queries (Dict[str, str]): Query id -> query.\n        responses (Dict[str, str]): Query id -> response.\n    \"\"\"\n    queries: Dict[str, str] = Field(\n        default_factory=dict, description=\"Query id -> query\"",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.dataset_generation",
        "documentation": {}
    },
    {
        "label": "DatasetGenerator",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.dataset_generation",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.dataset_generation",
        "peekOfCode": "class DatasetGenerator(PromptMixin):\n    \"\"\"Generate dataset (question/ question-answer pairs) \\\n    based on the given documents.\n    NOTE: this is a beta feature, subject to change!\n    Args:\n        nodes (List[Node]): List of nodes. (Optional)\n        llm (LLM): Language model.\n        callback_manager (CallbackManager): Callback manager.\n        num_questions_per_chunk: number of question to be \\\n        generated per chunk. Each document is chunked of size 512 words.",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.dataset_generation",
        "documentation": {}
    },
    {
        "label": "DEFAULT_QUESTION_GENERATION_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.dataset_generation",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.dataset_generation",
        "peekOfCode": "DEFAULT_QUESTION_GENERATION_PROMPT = \"\"\"\\\nContext information is below.\n---------------------\n{context_str}\n---------------------\nGiven the context information and not prior knowledge.\ngenerate only questions based on the below query.\n{query_str}\n\"\"\"\n@deprecated(",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.dataset_generation",
        "documentation": {}
    },
    {
        "label": "get_responses",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.eval_utils",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.eval_utils",
        "peekOfCode": "def get_responses(\n    *args: Any,\n    **kwargs: Any,\n) -> List[str]:\n    \"\"\"\n    Get responses.\n    Sync version of aget_responses.\n    \"\"\"\n    return asyncio_run(aget_responses(*args, **kwargs))\ndef get_results_df(",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.eval_utils",
        "documentation": {}
    },
    {
        "label": "get_results_df",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.eval_utils",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.eval_utils",
        "peekOfCode": "def get_results_df(\n    eval_results_list: List[Dict[str, List[EvaluationResult]]],\n    names: List[str],\n    metric_keys: List[str],\n) -> Any:\n    \"\"\"\n    Get results df.\n    Args:\n        eval_results_list (List[Dict[str, List[EvaluationResult]]]):\n            List of evaluation results.",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.eval_utils",
        "documentation": {}
    },
    {
        "label": "upload_eval_dataset",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.eval_utils",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.eval_utils",
        "peekOfCode": "def upload_eval_dataset(\n    dataset_name: str,\n    questions: Optional[List[str]] = None,\n    llama_dataset_id: Optional[str] = None,\n    project_name: str = DEFAULT_PROJECT_NAME,\n    base_url: Optional[str] = None,\n    api_key: Optional[str] = None,\n    overwrite: bool = False,\n    append: bool = False,\n) -> str:",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.eval_utils",
        "documentation": {}
    },
    {
        "label": "upload_eval_results",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.eval_utils",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.eval_utils",
        "peekOfCode": "def upload_eval_results(\n    project_name: str, app_name: str, results: Dict[str, List[EvaluationResult]]\n) -> None:\n    \"\"\"\n    Upload the evaluation results to LlamaCloud.\n    Args:\n        project_name (str): The name of the project.\n        app_name (str): The name of the app.\n        results (Dict[str, List[EvaluationResult]]):\n            The evaluation results, a mapping of metric name to a list of EvaluationResult objects.",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.eval_utils",
        "documentation": {}
    },
    {
        "label": "default_parser",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.eval_utils",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.eval_utils",
        "peekOfCode": "def default_parser(eval_response: str) -> Tuple[Optional[float], Optional[str]]:\n    \"\"\"\n    Default parser function for evaluation response.\n    Args:\n        eval_response (str): The response string from the evaluation.\n    Returns:\n        Tuple[float, str]: A tuple containing the score as a float and the reasoning as a string.\n    \"\"\"\n    if not eval_response.strip():\n        # Return None or default values if the response is empty",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.eval_utils",
        "documentation": {}
    },
    {
        "label": "FaithfulnessEvaluator",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.faithfulness",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.faithfulness",
        "peekOfCode": "class FaithfulnessEvaluator(BaseEvaluator):\n    \"\"\"\n    Faithfulness evaluator.\n    Evaluates whether a response is faithful to the contexts\n    (i.e. whether the response is supported by the contexts or hallucinated.)\n    This evaluator only considers the response string and the list of context strings.\n    Args:\n        raise_error(bool): Whether to raise an error when the response is invalid.\n            Defaults to False.\n        eval_template(Optional[Union[str, BasePromptTemplate]]):",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.faithfulness",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EVAL_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.faithfulness",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.faithfulness",
        "peekOfCode": "DEFAULT_EVAL_TEMPLATE = PromptTemplate(\n    \"Please tell if a given piece of information \"\n    \"is supported by the context.\\n\"\n    \"You need to answer with either YES or NO.\\n\"\n    \"Answer YES if any of the context supports the information, even \"\n    \"if most of the context is unrelated. \"\n    \"Some examples are provided below. \\n\\n\"\n    \"Information: Apple pie is generally double-crusted.\\n\"\n    \"Context: An apple pie is a fruit pie in which the principal filling \"\n    \"ingredient is apples. \\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.faithfulness",
        "documentation": {}
    },
    {
        "label": "DEFAULT_REFINE_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.faithfulness",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.faithfulness",
        "peekOfCode": "DEFAULT_REFINE_TEMPLATE = PromptTemplate(\n    \"We want to understand if the following information is present \"\n    \"in the context information: {query_str}\\n\"\n    \"We have provided an existing YES/NO answer: {existing_answer}\\n\"\n    \"We have the opportunity to refine the existing answer \"\n    \"(only if needed) with some more context below.\\n\"\n    \"------------\\n\"\n    \"{context_msg}\\n\"\n    \"------------\\n\"\n    \"If the existing answer was already YES, still answer YES. \"",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.faithfulness",
        "documentation": {}
    },
    {
        "label": "LLAMA3_8B_EVAL_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.faithfulness",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.faithfulness",
        "peekOfCode": "LLAMA3_8B_EVAL_TEMPLATE = PromptTemplate(\n    \"\"\"Please tell if a given piece of information is supported by the context.\nYou need to answer with either YES or NO.\nAnswer YES if **any part** of the context supports the information, even if most of the context is unrelated.\nAnswer NO if the context does not support the information at all.\nBe sure to read all provided context segments carefully before making your decision.\nSome examples are provided below:\nExample 1:\nInformation: The Eiffel Tower is located in Paris.\nContext: The Eiffel Tower, a symbol of French culture, stands prominently in the city of Paris.",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.faithfulness",
        "documentation": {}
    },
    {
        "label": "TEMPLATES_CATALOG",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.faithfulness",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.faithfulness",
        "peekOfCode": "TEMPLATES_CATALOG = {\"llama3:8b\": LLAMA3_8B_EVAL_TEMPLATE}\nclass FaithfulnessEvaluator(BaseEvaluator):\n    \"\"\"\n    Faithfulness evaluator.\n    Evaluates whether a response is faithful to the contexts\n    (i.e. whether the response is supported by the contexts or hallucinated.)\n    This evaluator only considers the response string and the list of context strings.\n    Args:\n        raise_error(bool): Whether to raise an error when the response is invalid.\n            Defaults to False.",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.faithfulness",
        "documentation": {}
    },
    {
        "label": "ResponseEvaluator",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.faithfulness",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.faithfulness",
        "peekOfCode": "ResponseEvaluator = FaithfulnessEvaluator",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.faithfulness",
        "documentation": {}
    },
    {
        "label": "EvaluationData",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.guideline",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.guideline",
        "peekOfCode": "class EvaluationData(BaseModel):\n    passing: bool = Field(description=\"Whether the response passes the guidelines.\")\n    feedback: str = Field(\n        description=\"The feedback for the response based on the guidelines.\"\n    )\nclass GuidelineEvaluator(BaseEvaluator):\n    \"\"\"\n    Guideline evaluator.\n    Evaluates whether a query and response pair passes the given guidelines.\n    This evaluator only considers the query string and the response string.",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.guideline",
        "documentation": {}
    },
    {
        "label": "GuidelineEvaluator",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.guideline",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.guideline",
        "peekOfCode": "class GuidelineEvaluator(BaseEvaluator):\n    \"\"\"\n    Guideline evaluator.\n    Evaluates whether a query and response pair passes the given guidelines.\n    This evaluator only considers the query string and the response string.\n    Args:\n        guidelines(Optional[str]): User-added guidelines to use for evaluation.\n            Defaults to None, which uses the default guidelines.\n        eval_template(Optional[Union[str, BasePromptTemplate]] ):\n            The template to use for evaluation.",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.guideline",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.guideline",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.guideline",
        "peekOfCode": "logger = logging.getLogger(__name__)\nDEFAULT_GUIDELINES = (\n    \"The response should fully answer the query.\\n\"\n    \"The response should avoid being vague or ambiguous.\\n\"\n    \"The response should be specific and use statistics or numbers when possible.\\n\"\n)\nDEFAULT_EVAL_TEMPLATE = PromptTemplate(\n    \"Here is the original query:\\n\"\n    \"Query: {query}\\n\"\n    \"Critique the following response based on the guidelines below:\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.guideline",
        "documentation": {}
    },
    {
        "label": "DEFAULT_GUIDELINES",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.guideline",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.guideline",
        "peekOfCode": "DEFAULT_GUIDELINES = (\n    \"The response should fully answer the query.\\n\"\n    \"The response should avoid being vague or ambiguous.\\n\"\n    \"The response should be specific and use statistics or numbers when possible.\\n\"\n)\nDEFAULT_EVAL_TEMPLATE = PromptTemplate(\n    \"Here is the original query:\\n\"\n    \"Query: {query}\\n\"\n    \"Critique the following response based on the guidelines below:\\n\"\n    \"Response: {response}\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.guideline",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EVAL_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.guideline",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.guideline",
        "peekOfCode": "DEFAULT_EVAL_TEMPLATE = PromptTemplate(\n    \"Here is the original query:\\n\"\n    \"Query: {query}\\n\"\n    \"Critique the following response based on the guidelines below:\\n\"\n    \"Response: {response}\\n\"\n    \"Guidelines: {guidelines}\\n\"\n    \"Now please provide constructive criticism.\\n\"\n)\nclass EvaluationData(BaseModel):\n    passing: bool = Field(description=\"Whether the response passes the guidelines.\")",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.guideline",
        "documentation": {}
    },
    {
        "label": "get_retrieval_results_df",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.notebook_utils",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.notebook_utils",
        "peekOfCode": "def get_retrieval_results_df(\n    names: List[str],\n    results_arr: List[List[RetrievalEvalResult]],\n    metric_keys: Optional[List[str]] = None,\n) -> Any:\n    \"\"\"Display retrieval results.\"\"\"\n    try:\n        import pandas as pd\n    except ImportError:\n        raise ImportError(",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.notebook_utils",
        "documentation": {}
    },
    {
        "label": "get_eval_results_df",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.notebook_utils",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.notebook_utils",
        "peekOfCode": "def get_eval_results_df(\n    names: List[str], results_arr: List[EvaluationResult], metric: Optional[str] = None\n) -> Tuple[Any, Any]:\n    \"\"\"\n    Organizes EvaluationResults into a deep dataframe and computes the mean\n    score.\n    result:\n        result_df: pd.DataFrame representing all the evaluation results\n        mean_df: pd.DataFrame of average scores groupby names\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.notebook_utils",
        "documentation": {}
    },
    {
        "label": "DEFAULT_METRIC_KEYS",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.notebook_utils",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.notebook_utils",
        "peekOfCode": "DEFAULT_METRIC_KEYS = [\"hit_rate\", \"mrr\"]\ndef get_retrieval_results_df(\n    names: List[str],\n    results_arr: List[List[RetrievalEvalResult]],\n    metric_keys: Optional[List[str]] = None,\n) -> Any:\n    \"\"\"Display retrieval results.\"\"\"\n    try:\n        import pandas as pd\n    except ImportError:",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.notebook_utils",
        "documentation": {}
    },
    {
        "label": "EvaluationSource",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.pairwise",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.pairwise",
        "peekOfCode": "class EvaluationSource(str, Enum):\n    \"\"\"To distinguish between flipped or original.\"\"\"\n    ORIGINAL = \"original\"\n    FLIPPED = \"flipped\"\n    NEITHER = \"neither\"\nclass PairwiseComparisonEvaluator(BaseEvaluator):\n    \"\"\"\n    Pairwise comparison evaluator.\n    Evaluates the quality of a response vs. a \"reference\" response given a question by\n    having an LLM judge which response is better.",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.pairwise",
        "documentation": {}
    },
    {
        "label": "PairwiseComparisonEvaluator",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.pairwise",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.pairwise",
        "peekOfCode": "class PairwiseComparisonEvaluator(BaseEvaluator):\n    \"\"\"\n    Pairwise comparison evaluator.\n    Evaluates the quality of a response vs. a \"reference\" response given a question by\n    having an LLM judge which response is better.\n    Outputs whether the `response` given is better than the `reference` response.\n    Args:\n        eval_template (Optional[Union[str, BasePromptTemplate]]):\n            The template to use for evaluation.\n        enforce_consensus (bool): Whether to enforce consensus (consistency if we",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.pairwise",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SYSTEM_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.pairwise",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.pairwise",
        "peekOfCode": "DEFAULT_SYSTEM_TEMPLATE = (\n    \"Please act as an impartial judge and evaluate the quality of the responses provided by two \"\n    \"AI question-answering assistants to the user question perhaps with added reference which \"\n    \"are displayed below. You should choose the assistant that \"\n    \"follows the users instructions and answers the users question better using the provided \"\n    \"context. Your evaluation \"\n    \"should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, \"\n    \"and level of detail of their responses. Begin your evaluation by comparing the two \"\n    \"responses and provide a short explanation. Avoid any position biases and ensure that the \"\n    \"order in which the responses were presented does not influence your decision. Do not allow \"",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.pairwise",
        "documentation": {}
    },
    {
        "label": "DEFAULT_USER_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.pairwise",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.pairwise",
        "peekOfCode": "DEFAULT_USER_TEMPLATE = (\n    \"[User Question]\\n\"\n    \"{query}\"\n    \"\\n\\n\"\n    \"[The Start of Reference]\\n\"\n    \"{reference}\\n\"\n    \"[The End of Reference]\"\n    \"\\n\\n\"\n    \"[The Start of Assistant As Answer]\\n\"\n    \"{answer_1}\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.pairwise",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EVAL_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.pairwise",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.pairwise",
        "peekOfCode": "DEFAULT_EVAL_TEMPLATE = ChatPromptTemplate(\n    message_templates=[\n        ChatMessage(role=MessageRole.SYSTEM, content=DEFAULT_SYSTEM_TEMPLATE),\n        ChatMessage(role=MessageRole.USER, content=DEFAULT_USER_TEMPLATE),\n    ]\n)\ndef _default_parser_function(\n    eval_response: str,\n) -> Tuple[Optional[bool], Optional[float], Optional[str]]:\n    # Extract from response",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.pairwise",
        "documentation": {}
    },
    {
        "label": "RelevancyEvaluator",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.relevancy",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.relevancy",
        "peekOfCode": "class RelevancyEvaluator(BaseEvaluator):\n    \"\"\"\n    Relenvancy evaluator.\n    Evaluates the relevancy of retrieved contexts and response to a query.\n    This evaluator considers the query string, retrieved contexts, and response string.\n    Args:\n        raise_error(Optional[bool]):\n            Whether to raise an error if the response is invalid.\n            Defaults to False.\n        eval_template(Optional[Union[str, BasePromptTemplate]]):",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.relevancy",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EVAL_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.relevancy",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.relevancy",
        "peekOfCode": "DEFAULT_EVAL_TEMPLATE = PromptTemplate(\n    \"Your task is to evaluate if the response for the query \\\n    is in line with the context information provided.\\n\"\n    \"You have two options to answer. Either YES/ NO.\\n\"\n    \"Answer - YES, if the response for the query \\\n    is in line with context information otherwise NO.\\n\"\n    \"Query and Response: \\n {query_str}\\n\"\n    \"Context: \\n {context_str}\\n\"\n    \"Answer: \"\n)",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.relevancy",
        "documentation": {}
    },
    {
        "label": "DEFAULT_REFINE_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.relevancy",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.relevancy",
        "peekOfCode": "DEFAULT_REFINE_TEMPLATE = PromptTemplate(\n    \"We want to understand if the following query and response is\"\n    \"in line with the context information: \\n {query_str}\\n\"\n    \"We have provided an existing YES/NO answer: \\n {existing_answer}\\n\"\n    \"We have the opportunity to refine the existing answer \"\n    \"(only if needed) with some more context below.\\n\"\n    \"------------\\n\"\n    \"{context_msg}\\n\"\n    \"------------\\n\"\n    \"If the existing answer was already YES, still answer YES. \"",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.relevancy",
        "documentation": {}
    },
    {
        "label": "QueryResponseEvaluator",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.relevancy",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.relevancy",
        "peekOfCode": "QueryResponseEvaluator = RelevancyEvaluator",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.relevancy",
        "documentation": {}
    },
    {
        "label": "SemanticSimilarityEvaluator",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.evaluation.semantic_similarity",
        "description": "reference_code.llama-index-core.llama_index.core.evaluation.semantic_similarity",
        "peekOfCode": "class SemanticSimilarityEvaluator(BaseEvaluator):\n    \"\"\"\n    Embedding similarity evaluator.\n    Evaluate the quality of a question answering system by\n    comparing the similarity between embeddings of the generated answer\n    and the reference answer.\n    Inspired by this paper:\n    - Semantic Answer Similarity for Evaluating Question Answering Models\n        https://arxiv.org/pdf/2108.06130.pdf\n    Args:",
        "detail": "reference_code.llama-index-core.llama_index.core.evaluation.semantic_similarity",
        "documentation": {}
    },
    {
        "label": "DocumentContextExtractor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.extractors.document_context",
        "description": "reference_code.llama-index-core.llama_index.core.extractors.document_context",
        "peekOfCode": "class DocumentContextExtractor(BaseExtractor):\n    \"\"\"\n    An LLM-based context extractor for enhancing RAG accuracy through document analysis.\n    ! Nodes that already have the 'key' in node.metadata will NOT be processed - will be skipped !\n    This extractor processes documents and their nodes to generate contextual metadata,\n    implementing the approach described in the Anthropic \"Contextual Retrieval\" blog post.\n    It handles rate limits, document size constraints, and parallel processing of nodes.\n    Attributes:\n        llm (LLM): Language model instance for generating context\n        docstore (BaseDocumentStore): Storage for parent documents",
        "detail": "reference_code.llama-index-core.llama_index.core.extractors.document_context",
        "documentation": {}
    },
    {
        "label": "is_text_node",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.extractors.document_context",
        "description": "reference_code.llama-index-core.llama_index.core.extractors.document_context",
        "peekOfCode": "def is_text_node(node: BaseNode) -> TypeGuard[Union[Node, TextNode]]:\n    return isinstance(node, (Node, TextNode))\nOversizeStrategy = Literal[\"warn\", \"error\", \"ignore\"]\n# original context prompt from the Anthropic cookbook/blogpost, works well\nORIGINAL_CONTEXT_PROMPT: str = \"\"\"\nPlease give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.\nAnswer only with the succinct context and nothing else.\n\"\"\"\n# miniaturized context prompt, generates better results, produces more keyword-laden results for better matches\nSUCCINCT_CONTEXT_PROMPT: str = \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.extractors.document_context",
        "documentation": {}
    },
    {
        "label": "OversizeStrategy",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.extractors.document_context",
        "description": "reference_code.llama-index-core.llama_index.core.extractors.document_context",
        "peekOfCode": "OversizeStrategy = Literal[\"warn\", \"error\", \"ignore\"]\n# original context prompt from the Anthropic cookbook/blogpost, works well\nORIGINAL_CONTEXT_PROMPT: str = \"\"\"\nPlease give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.\nAnswer only with the succinct context and nothing else.\n\"\"\"\n# miniaturized context prompt, generates better results, produces more keyword-laden results for better matches\nSUCCINCT_CONTEXT_PROMPT: str = \"\"\"\nGenerate keywords and brief phrases describing the main topics, entities, and actions in this text. Replace pronouns with their specific referents. Disambiguate pronouns and ambiguous terms in the chunk. Format as comma-separated phrases. Exclude meta-commentary about the text itself.\n\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.extractors.document_context",
        "documentation": {}
    },
    {
        "label": "BaseExtractor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.extractors.interface",
        "description": "reference_code.llama-index-core.llama_index.core.extractors.interface",
        "peekOfCode": "class BaseExtractor(TransformComponent):\n    \"\"\"Metadata extractor.\"\"\"\n    is_text_node_only: bool = True\n    show_progress: bool = Field(default=True, description=\"Whether to show progress.\")\n    metadata_mode: MetadataMode = Field(\n        default=MetadataMode.ALL, description=\"Metadata mode to use when reading nodes.\"\n    )\n    node_text_template: str = Field(\n        default=DEFAULT_NODE_TEXT_TEMPLATE,\n        description=\"Template to represent how node text is mixed with metadata text.\",",
        "detail": "reference_code.llama-index-core.llama_index.core.extractors.interface",
        "documentation": {}
    },
    {
        "label": "DEFAULT_NODE_TEXT_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.extractors.interface",
        "description": "reference_code.llama-index-core.llama_index.core.extractors.interface",
        "peekOfCode": "DEFAULT_NODE_TEXT_TEMPLATE = \"\"\"\\\n[Excerpt from document]\\n{metadata_str}\\n\\\nExcerpt:\\n-----\\n{content}\\n-----\\n\"\"\"\nclass BaseExtractor(TransformComponent):\n    \"\"\"Metadata extractor.\"\"\"\n    is_text_node_only: bool = True\n    show_progress: bool = Field(default=True, description=\"Whether to show progress.\")\n    metadata_mode: MetadataMode = Field(\n        default=MetadataMode.ALL, description=\"Metadata mode to use when reading nodes.\"\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.extractors.interface",
        "documentation": {}
    },
    {
        "label": "load_extractor",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.extractors.loading",
        "description": "reference_code.llama-index-core.llama_index.core.extractors.loading",
        "peekOfCode": "def load_extractor(\n    data: dict,\n) -> BaseExtractor:\n    if isinstance(data, BaseExtractor):\n        return data\n    extractor_name = data.get(\"class_name\")\n    if extractor_name is None:\n        raise ValueError(\"Extractor loading requires a class_name\")\n    if extractor_name == SummaryExtractor.class_name():\n        return SummaryExtractor.from_dict(data)",
        "detail": "reference_code.llama-index-core.llama_index.core.extractors.loading",
        "documentation": {}
    },
    {
        "label": "TitleExtractor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "description": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "peekOfCode": "class TitleExtractor(BaseExtractor):\n    \"\"\"\n    Title extractor. Useful for long documents. Extracts `document_title`\n    metadata field.\n    Args:\n        llm (Optional[LLM]): LLM\n        nodes (int): number of nodes from front to use for title extraction\n        node_template (str): template for node-level title clues extraction\n        combine_template (str): template for combining node-level clues into\n            a document-level title",
        "detail": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "documentation": {}
    },
    {
        "label": "KeywordExtractor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "description": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "peekOfCode": "class KeywordExtractor(BaseExtractor):\n    \"\"\"\n    Keyword extractor. Node-level extractor. Extracts\n    `excerpt_keywords` metadata field.\n    Args:\n        llm (Optional[LLM]): LLM\n        keywords (int): number of keywords to extract\n        prompt_template (str): template for keyword extraction\n    \"\"\"\n    llm: SerializeAsAny[LLM] = Field(description=\"The LLM to use for generation.\")",
        "detail": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "documentation": {}
    },
    {
        "label": "QuestionsAnsweredExtractor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "description": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "peekOfCode": "class QuestionsAnsweredExtractor(BaseExtractor):\n    \"\"\"\n    Questions answered extractor. Node-level extractor.\n    Extracts `questions_this_excerpt_can_answer` metadata field.\n    Args:\n        llm (Optional[LLM]): LLM\n        questions (int): number of questions to extract\n        prompt_template (str): template for question extraction,\n        embedding_only (bool): whether to use embedding only\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "documentation": {}
    },
    {
        "label": "SummaryExtractor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "description": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "peekOfCode": "class SummaryExtractor(BaseExtractor):\n    \"\"\"\n    Summary extractor. Node-level extractor with adjacent sharing.\n    Extracts `section_summary`, `prev_section_summary`, `next_section_summary`\n    metadata fields.\n    Args:\n        llm (Optional[LLM]): LLM\n        summaries (List[str]): list of summaries to extract: 'self', 'prev', 'next'\n        prompt_template (str): template for summary extraction\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "documentation": {}
    },
    {
        "label": "PydanticProgramExtractor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "description": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "peekOfCode": "class PydanticProgramExtractor(BaseExtractor, Generic[Model]):\n    \"\"\"\n    Pydantic program extractor.\n    Uses an LLM to extract out a Pydantic object. Return attributes of that object\n    in a dictionary.\n    \"\"\"\n    program: SerializeAsAny[BasePydanticProgram[Model]] = Field(\n        ..., description=\"Pydantic program to extract.\"\n    )\n    input_key: str = Field(",
        "detail": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "documentation": {}
    },
    {
        "label": "add_class_name",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "description": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "peekOfCode": "def add_class_name(value: Any, handler: Callable, info: Any) -> Dict[str, Any]:\n    partial_result = handler(value, info)\n    if hasattr(value, \"class_name\"):\n        partial_result.update({\"class_name\": value.class_name()})\n    return partial_result\nclass TitleExtractor(BaseExtractor):\n    \"\"\"\n    Title extractor. Useful for long documents. Extracts `document_title`\n    metadata field.\n    Args:",
        "detail": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TITLE_NODE_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "description": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "peekOfCode": "DEFAULT_TITLE_NODE_TEMPLATE = \"\"\"\\\nContext: {context_str}. Give a title that summarizes all of \\\nthe unique entities, titles or themes found in the context. Title: \"\"\"\nDEFAULT_TITLE_COMBINE_TEMPLATE = \"\"\"\\\n{context_str}. Based on the above candidate titles and content, \\\nwhat is the comprehensive title for this document? Title: \"\"\"\ndef add_class_name(value: Any, handler: Callable, info: Any) -> Dict[str, Any]:\n    partial_result = handler(value, info)\n    if hasattr(value, \"class_name\"):\n        partial_result.update({\"class_name\": value.class_name()})",
        "detail": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TITLE_COMBINE_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "description": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "peekOfCode": "DEFAULT_TITLE_COMBINE_TEMPLATE = \"\"\"\\\n{context_str}. Based on the above candidate titles and content, \\\nwhat is the comprehensive title for this document? Title: \"\"\"\ndef add_class_name(value: Any, handler: Callable, info: Any) -> Dict[str, Any]:\n    partial_result = handler(value, info)\n    if hasattr(value, \"class_name\"):\n        partial_result.update({\"class_name\": value.class_name()})\n    return partial_result\nclass TitleExtractor(BaseExtractor):\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "documentation": {}
    },
    {
        "label": "DEFAULT_KEYWORD_EXTRACT_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "description": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "peekOfCode": "DEFAULT_KEYWORD_EXTRACT_TEMPLATE = \"\"\"\\\n{context_str}. Give {keywords} unique keywords for this \\\ndocument. Format as comma separated. Keywords: \"\"\"\nclass KeywordExtractor(BaseExtractor):\n    \"\"\"\n    Keyword extractor. Node-level extractor. Extracts\n    `excerpt_keywords` metadata field.\n    Args:\n        llm (Optional[LLM]): LLM\n        keywords (int): number of keywords to extract",
        "detail": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "documentation": {}
    },
    {
        "label": "DEFAULT_QUESTION_GEN_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "description": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "peekOfCode": "DEFAULT_QUESTION_GEN_TMPL = \"\"\"\\\nHere is the context:\n{context_str}\nGiven the contextual information, \\\ngenerate {num_questions} questions this context can provide \\\nspecific answers to which are unlikely to be found elsewhere.\nHigher-level summaries of surrounding context may be provided \\\nas well. Try using these summaries to generate better questions \\\nthat this context can answer.\n\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SUMMARY_EXTRACT_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "description": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "peekOfCode": "DEFAULT_SUMMARY_EXTRACT_TEMPLATE = \"\"\"\\\nHere is the content of the section:\n{context_str}\nSummarize the key topics and entities of the section. \\\nSummary: \"\"\"\nclass SummaryExtractor(BaseExtractor):\n    \"\"\"\n    Summary extractor. Node-level extractor with adjacent sharing.\n    Extracts `section_summary`, `prev_section_summary`, `next_section_summary`\n    metadata fields.",
        "detail": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "documentation": {}
    },
    {
        "label": "DEFAULT_ENTITY_MAP",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "description": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "peekOfCode": "DEFAULT_ENTITY_MAP = {\n    \"PER\": \"persons\",\n    \"ORG\": \"organizations\",\n    \"LOC\": \"locations\",\n    \"ANIM\": \"animals\",\n    \"BIO\": \"biological\",\n    \"CEL\": \"celestial\",\n    \"DIS\": \"diseases\",\n    \"EVE\": \"events\",\n    \"FOOD\": \"foods\",",
        "detail": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "documentation": {}
    },
    {
        "label": "DEFAULT_ENTITY_MODEL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "description": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "peekOfCode": "DEFAULT_ENTITY_MODEL = \"tomaarsen/span-marker-mbert-base-multinerd\"\nDEFAULT_EXTRACT_TEMPLATE_STR = \"\"\"\\\nHere is the content of the section:\n----------------\n{context_str}\n----------------\nGiven the contextual information, extract out a {class_name} object.\\\n\"\"\"\nclass PydanticProgramExtractor(BaseExtractor, Generic[Model]):\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EXTRACT_TEMPLATE_STR",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "description": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "peekOfCode": "DEFAULT_EXTRACT_TEMPLATE_STR = \"\"\"\\\nHere is the content of the section:\n----------------\n{context_str}\n----------------\nGiven the contextual information, extract out a {class_name} object.\\\n\"\"\"\nclass PydanticProgramExtractor(BaseExtractor, Generic[Model]):\n    \"\"\"\n    Pydantic program extractor.",
        "detail": "reference_code.llama-index-core.llama_index.core.extractors.metadata_extractors",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CYPHER_TEMPALTE_STR",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.graph_stores.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.graph_stores.prompts",
        "peekOfCode": "DEFAULT_CYPHER_TEMPALTE_STR = \"\"\"Task:Generate Cypher statement to query a graph database.\nInstructions:\nUse only the provided relationship types and properties in the schema.\nDo not use any other relationship types or properties that are not provided.\nSchema:\n{schema}\nNote: Do not include any explanations or apologies in your responses.\nDo not respond to any questions that might ask anything else than for you to construct a Cypher statement.\nDo not include any text except the generated Cypher statement.\nThe question is:",
        "detail": "reference_code.llama-index-core.llama_index.core.graph_stores.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CYPHER_TEMPALTE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.graph_stores.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.graph_stores.prompts",
        "peekOfCode": "DEFAULT_CYPHER_TEMPALTE = PromptTemplate(DEFAULT_CYPHER_TEMPALTE_STR)",
        "detail": "reference_code.llama-index-core.llama_index.core.graph_stores.prompts",
        "documentation": {}
    },
    {
        "label": "SimpleGraphStoreData",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.graph_stores.simple",
        "description": "reference_code.llama-index-core.llama_index.core.graph_stores.simple",
        "peekOfCode": "class SimpleGraphStoreData(DataClassJsonMixin):\n    \"\"\"\n    Simple Graph Store Data container.\n    Args:\n        graph_dict (Optional[dict]): dict mapping subject to\n    \"\"\"\n    graph_dict: Dict[str, List[List[str]]] = field(default_factory=dict)\n    def get_rel_map(\n        self, subjs: Optional[List[str]] = None, depth: int = 2, limit: int = 30\n    ) -> Dict[str, List[List[str]]]:",
        "detail": "reference_code.llama-index-core.llama_index.core.graph_stores.simple",
        "documentation": {}
    },
    {
        "label": "SimpleGraphStore",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.graph_stores.simple",
        "description": "reference_code.llama-index-core.llama_index.core.graph_stores.simple",
        "peekOfCode": "class SimpleGraphStore(GraphStore):\n    \"\"\"\n    Simple Graph Store.\n    In this graph store, triplets are stored within a simple, in-memory dictionary.\n    Args:\n        simple_graph_store_data_dict (Optional[dict]): data dict\n            containing the triplets. See SimpleGraphStoreData\n            for more details.\n    \"\"\"\n    def __init__(",
        "detail": "reference_code.llama-index-core.llama_index.core.graph_stores.simple",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.graph_stores.simple",
        "description": "reference_code.llama-index-core.llama_index.core.graph_stores.simple",
        "peekOfCode": "logger = logging.getLogger(__name__)\n@dataclass\nclass SimpleGraphStoreData(DataClassJsonMixin):\n    \"\"\"\n    Simple Graph Store Data container.\n    Args:\n        graph_dict (Optional[dict]): dict mapping subject to\n    \"\"\"\n    graph_dict: Dict[str, List[List[str]]] = field(default_factory=dict)\n    def get_rel_map(",
        "detail": "reference_code.llama-index-core.llama_index.core.graph_stores.simple",
        "documentation": {}
    },
    {
        "label": "SimplePropertyGraphStore",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.graph_stores.simple_labelled",
        "description": "reference_code.llama-index-core.llama_index.core.graph_stores.simple_labelled",
        "peekOfCode": "class SimplePropertyGraphStore(PropertyGraphStore):\n    \"\"\"\n    Simple Labelled Property Graph Store.\n    This class implements a simple in-memory labelled property graph store.\n    Args:\n        graph (Optional[LabelledPropertyGraph]): Labelled property graph to initialize the store.\n    \"\"\"\n    supports_structured_queries: bool = False\n    supports_vector_queries: bool = False\n    def __init__(",
        "detail": "reference_code.llama-index-core.llama_index.core.graph_stores.simple_labelled",
        "documentation": {}
    },
    {
        "label": "LabelledNode",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "peekOfCode": "class LabelledNode(BaseModel):\n    \"\"\"An entity in a graph.\"\"\"\n    label: str = Field(default=\"node\", description=\"The label of the node.\")\n    embedding: Optional[List[float]] = Field(\n        default=None, description=\"The embeddings of the node.\"\n    )\n    properties: Dict[str, Any] = Field(default_factory=dict)\n    @abstractmethod\n    def __str__(self) -> str:\n        \"\"\"Return the string representation of the node.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "EntityNode",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "peekOfCode": "class EntityNode(LabelledNode):\n    \"\"\"An entity in a graph.\"\"\"\n    name: str = Field(description=\"The name of the entity.\")\n    label: str = Field(default=\"entity\", description=\"The label of the node.\")\n    properties: Dict[str, Any] = Field(default_factory=dict)\n    def __str__(self) -> str:\n        \"\"\"Return the string representation of the node.\"\"\"\n        if self.properties:\n            return f\"{self.name} ({self.properties})\"\n        return self.name",
        "detail": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "ChunkNode",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "peekOfCode": "class ChunkNode(LabelledNode):\n    \"\"\"A text chunk in a graph.\"\"\"\n    text: str = Field(description=\"The text content of the chunk.\")\n    id_: Optional[str] = Field(\n        default=None, description=\"The id of the node. Defaults to a hash of the text.\"\n    )\n    label: str = Field(default=\"text_chunk\", description=\"The label of the node.\")\n    properties: Dict[str, Any] = Field(default_factory=dict)\n    def __str__(self) -> str:\n        \"\"\"Return the string representation of the node.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "Relation",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "peekOfCode": "class Relation(BaseModel):\n    \"\"\"A relation connecting two entities in a graph.\"\"\"\n    label: str\n    source_id: str\n    target_id: str\n    properties: Dict[str, Any] = Field(default_factory=dict)\n    def __str__(self) -> str:\n        \"\"\"Return the string representation of the relation.\"\"\"\n        if self.properties:\n            return f\"{self.label} ({self.properties})\"",
        "detail": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "LabelledPropertyGraph",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "peekOfCode": "class LabelledPropertyGraph(BaseModel):\n    \"\"\"In memory labelled property graph containing entities and relations.\"\"\"\n    nodes: SerializeAsAny[Dict[str, LabelledNode]] = Field(default_factory=dict)\n    relations: SerializeAsAny[Dict[str, Relation]] = Field(default_factory=dict)\n    triplets: Set[Tuple[str, str, str]] = Field(\n        default_factory=set, description=\"List of triplets (subject, relation, object).\"\n    )\n    def _get_relation_key(\n        self,\n        relation: Optional[Relation] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "GraphStore",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "peekOfCode": "class GraphStore(Protocol):\n    \"\"\"\n    Abstract graph store protocol.\n    This protocol defines the interface for a graph store, which is responsible\n    for storing and retrieving knowledge graph data.\n    Attributes:\n        client: Any: The client used to connect to the graph store.\n        get: Callable[[str], List[List[str]]]: Get triplets for a given subject.\n        get_rel_map: Callable[[Optional[List[str]], int], Dict[str, List[List[str]]]]:\n            Get subjects' rel map in max depth.",
        "detail": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "PropertyGraphStore",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "peekOfCode": "class PropertyGraphStore(ABC):\n    \"\"\"\n    Abstract labelled graph store protocol.\n    This protocol defines the interface for a graph store, which is responsible\n    for storing and retrieving knowledge graph data.\n    Attributes:\n        client: Any: The client used to connect to the graph store.\n        get: Callable[[str], List[List[str]]]: Get triplets for a given subject.\n        get_rel_map: Callable[[Optional[List[str]], int], Dict[str, List[List[str]]]]:\n            Get subjects' rel map in max depth.",
        "detail": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_DIR",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "peekOfCode": "DEFAULT_PERSIST_DIR = \"./storage\"\nDEFAULT_PERSIST_FNAME = \"graph_store.json\"\nDEFAULT_PG_PERSIST_FNAME = \"property_graph_store.json\"\nTRIPLET_SOURCE_KEY = \"triplet_source_id\"\nVECTOR_SOURCE_KEY = \"vector_source_id\"\nKG_NODES_KEY = \"nodes\"\nKG_RELATIONS_KEY = \"relations\"\nKG_SOURCE_REL = \"SOURCE\"\nclass LabelledNode(BaseModel):\n    \"\"\"An entity in a graph.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_FNAME",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "peekOfCode": "DEFAULT_PERSIST_FNAME = \"graph_store.json\"\nDEFAULT_PG_PERSIST_FNAME = \"property_graph_store.json\"\nTRIPLET_SOURCE_KEY = \"triplet_source_id\"\nVECTOR_SOURCE_KEY = \"vector_source_id\"\nKG_NODES_KEY = \"nodes\"\nKG_RELATIONS_KEY = \"relations\"\nKG_SOURCE_REL = \"SOURCE\"\nclass LabelledNode(BaseModel):\n    \"\"\"An entity in a graph.\"\"\"\n    label: str = Field(default=\"node\", description=\"The label of the node.\")",
        "detail": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PG_PERSIST_FNAME",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "peekOfCode": "DEFAULT_PG_PERSIST_FNAME = \"property_graph_store.json\"\nTRIPLET_SOURCE_KEY = \"triplet_source_id\"\nVECTOR_SOURCE_KEY = \"vector_source_id\"\nKG_NODES_KEY = \"nodes\"\nKG_RELATIONS_KEY = \"relations\"\nKG_SOURCE_REL = \"SOURCE\"\nclass LabelledNode(BaseModel):\n    \"\"\"An entity in a graph.\"\"\"\n    label: str = Field(default=\"node\", description=\"The label of the node.\")\n    embedding: Optional[List[float]] = Field(",
        "detail": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "TRIPLET_SOURCE_KEY",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "peekOfCode": "TRIPLET_SOURCE_KEY = \"triplet_source_id\"\nVECTOR_SOURCE_KEY = \"vector_source_id\"\nKG_NODES_KEY = \"nodes\"\nKG_RELATIONS_KEY = \"relations\"\nKG_SOURCE_REL = \"SOURCE\"\nclass LabelledNode(BaseModel):\n    \"\"\"An entity in a graph.\"\"\"\n    label: str = Field(default=\"node\", description=\"The label of the node.\")\n    embedding: Optional[List[float]] = Field(\n        default=None, description=\"The embeddings of the node.\"",
        "detail": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "VECTOR_SOURCE_KEY",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "peekOfCode": "VECTOR_SOURCE_KEY = \"vector_source_id\"\nKG_NODES_KEY = \"nodes\"\nKG_RELATIONS_KEY = \"relations\"\nKG_SOURCE_REL = \"SOURCE\"\nclass LabelledNode(BaseModel):\n    \"\"\"An entity in a graph.\"\"\"\n    label: str = Field(default=\"node\", description=\"The label of the node.\")\n    embedding: Optional[List[float]] = Field(\n        default=None, description=\"The embeddings of the node.\"\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "KG_NODES_KEY",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "peekOfCode": "KG_NODES_KEY = \"nodes\"\nKG_RELATIONS_KEY = \"relations\"\nKG_SOURCE_REL = \"SOURCE\"\nclass LabelledNode(BaseModel):\n    \"\"\"An entity in a graph.\"\"\"\n    label: str = Field(default=\"node\", description=\"The label of the node.\")\n    embedding: Optional[List[float]] = Field(\n        default=None, description=\"The embeddings of the node.\"\n    )\n    properties: Dict[str, Any] = Field(default_factory=dict)",
        "detail": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "KG_RELATIONS_KEY",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "peekOfCode": "KG_RELATIONS_KEY = \"relations\"\nKG_SOURCE_REL = \"SOURCE\"\nclass LabelledNode(BaseModel):\n    \"\"\"An entity in a graph.\"\"\"\n    label: str = Field(default=\"node\", description=\"The label of the node.\")\n    embedding: Optional[List[float]] = Field(\n        default=None, description=\"The embeddings of the node.\"\n    )\n    properties: Dict[str, Any] = Field(default_factory=dict)\n    @abstractmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "KG_SOURCE_REL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "peekOfCode": "KG_SOURCE_REL = \"SOURCE\"\nclass LabelledNode(BaseModel):\n    \"\"\"An entity in a graph.\"\"\"\n    label: str = Field(default=\"node\", description=\"The label of the node.\")\n    embedding: Optional[List[float]] = Field(\n        default=None, description=\"The embeddings of the node.\"\n    )\n    properties: Dict[str, Any] = Field(default_factory=dict)\n    @abstractmethod\n    def __str__(self) -> str:",
        "detail": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "Triplet",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "peekOfCode": "Triplet = Tuple[LabelledNode, Relation, LabelledNode]\nclass LabelledPropertyGraph(BaseModel):\n    \"\"\"In memory labelled property graph containing entities and relations.\"\"\"\n    nodes: SerializeAsAny[Dict[str, LabelledNode]] = Field(default_factory=dict)\n    relations: SerializeAsAny[Dict[str, Relation]] = Field(default_factory=dict)\n    triplets: Set[Tuple[str, str, str]] = Field(\n        default_factory=set, description=\"List of triplets (subject, relation, object).\"\n    )\n    def _get_relation_key(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.graph_stores.types",
        "documentation": {}
    },
    {
        "label": "clean_string_values",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.graph_stores.utils",
        "description": "reference_code.llama-index-core.llama_index.core.graph_stores.utils",
        "peekOfCode": "def clean_string_values(text: str) -> str:\n    return text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\ndef value_sanitize(d: Any) -> Any:\n    \"\"\"\n    Sanitize the input dictionary or list.\n    Sanitizes the input by removing embedding-like values,\n    lists with more than 128 elements, that are mostly irrelevant for\n    generating answers in a LLM context. These properties, if left in\n    results, can occupy significant context space and detract from\n    the LLM's performance by introducing unnecessary noise and cost.",
        "detail": "reference_code.llama-index-core.llama_index.core.graph_stores.utils",
        "documentation": {}
    },
    {
        "label": "value_sanitize",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.graph_stores.utils",
        "description": "reference_code.llama-index-core.llama_index.core.graph_stores.utils",
        "peekOfCode": "def value_sanitize(d: Any) -> Any:\n    \"\"\"\n    Sanitize the input dictionary or list.\n    Sanitizes the input by removing embedding-like values,\n    lists with more than 128 elements, that are mostly irrelevant for\n    generating answers in a LLM context. These properties, if left in\n    results, can occupy significant context space and detract from\n    the LLM's performance by introducing unnecessary noise and cost.\n    \"\"\"\n    if isinstance(d, dict):",
        "detail": "reference_code.llama-index-core.llama_index.core.graph_stores.utils",
        "documentation": {}
    },
    {
        "label": "LIST_LIMIT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.graph_stores.utils",
        "description": "reference_code.llama-index-core.llama_index.core.graph_stores.utils",
        "peekOfCode": "LIST_LIMIT = 128\ndef clean_string_values(text: str) -> str:\n    return text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\ndef value_sanitize(d: Any) -> Any:\n    \"\"\"\n    Sanitize the input dictionary or list.\n    Sanitizes the input by removing embedding-like values,\n    lists with more than 128 elements, that are mostly irrelevant for\n    generating answers in a LLM context. These properties, if left in\n    results, can occupy significant context space and detract from",
        "detail": "reference_code.llama-index-core.llama_index.core.graph_stores.utils",
        "documentation": {}
    },
    {
        "label": "SQLDocumentContextBuilder",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.common.struct_store.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.common.struct_store.base",
        "peekOfCode": "class SQLDocumentContextBuilder:\n    \"\"\"\n    Builder that builds context for a given set of SQL tables.\n    Args:\n        sql_database (Optional[SQLDatabase]): SQL database to use,\n        text_splitter (Optional[TextSplitter]): Text Splitter to use.\n        table_context_prompt (Optional[BasePromptTemplate]): A\n            Table Context Prompt (see :ref:`Prompt-Templates`).\n        refine_table_context_prompt (Optional[BasePromptTemplate]):\n            A Refine Table Context Prompt (see :ref:`Prompt-Templates`).",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.common.struct_store.base",
        "documentation": {}
    },
    {
        "label": "BaseStructDatapointExtractor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.common.struct_store.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.common.struct_store.base",
        "peekOfCode": "class BaseStructDatapointExtractor:\n    \"\"\"Extracts datapoints from a structured document.\"\"\"\n    def __init__(\n        self,\n        llm: LLM,\n        schema_extract_prompt: BasePromptTemplate,\n        output_parser: OUTPUT_PARSER_TYPE,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._llm = llm",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.common.struct_store.base",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.common.struct_store.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.common.struct_store.base",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass SQLDocumentContextBuilder:\n    \"\"\"\n    Builder that builds context for a given set of SQL tables.\n    Args:\n        sql_database (Optional[SQLDatabase]): SQL database to use,\n        text_splitter (Optional[TextSplitter]): Text Splitter to use.\n        table_context_prompt (Optional[BasePromptTemplate]): A\n            Table Context Prompt (see :ref:`Prompt-Templates`).\n        refine_table_context_prompt (Optional[BasePromptTemplate]):",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.common.struct_store.base",
        "documentation": {}
    },
    {
        "label": "OUTPUT_PARSER_TYPE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.common.struct_store.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.common.struct_store.base",
        "peekOfCode": "OUTPUT_PARSER_TYPE = Callable[[str], Optional[Dict[str, Any]]]\nclass BaseStructDatapointExtractor:\n    \"\"\"Extracts datapoints from a structured document.\"\"\"\n    def __init__(\n        self,\n        llm: LLM,\n        schema_extract_prompt: BasePromptTemplate,\n        output_parser: OUTPUT_PARSER_TYPE,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.common.struct_store.base",
        "documentation": {}
    },
    {
        "label": "SQLContextContainer",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.common.struct_store.schema",
        "description": "reference_code.llama-index-core.llama_index.core.indices.common.struct_store.schema",
        "peekOfCode": "class SQLContextContainer(DataClassJsonMixin):\n    \"\"\"\n    SQLContextContainer.\n    A container interface to store context for a given table.\n    Context can be built from unstructured documents (e.g. using SQLContextBuilder).\n    Context can also be dumped to an underlying LlamaIndex data structure.\n    Contains both the raw context_dict as well as any index_structure.\n    Should be not be used directly - build one from SQLContextContainerBuilder instead.\n    \"\"\"\n    context_dict: Optional[Dict[str, str]] = None",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.common.struct_store.schema",
        "documentation": {}
    },
    {
        "label": "SQLStructDatapointExtractor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.common.struct_store.sql",
        "description": "reference_code.llama-index-core.llama_index.core.indices.common.struct_store.sql",
        "peekOfCode": "class SQLStructDatapointExtractor(BaseStructDatapointExtractor):\n    \"\"\"Extracts datapoints from a structured document for a SQL db.\"\"\"\n    def __init__(\n        self,\n        llm: LLM,\n        schema_extract_prompt: BasePromptTemplate,\n        output_parser: OUTPUT_PARSER_TYPE,\n        sql_database: SQLDatabase,\n        table_name: Optional[str] = None,\n        table: Optional[Table] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.common.struct_store.sql",
        "documentation": {}
    },
    {
        "label": "GPTTreeIndexBuilder",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.common_tree.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.common_tree.base",
        "peekOfCode": "class GPTTreeIndexBuilder:\n    \"\"\"\n    GPT tree index builder.\n    Helper class to build the tree-structured index,\n    or to synthesize an answer.\n    \"\"\"\n    def __init__(\n        self,\n        num_children: int,\n        summary_prompt: BasePromptTemplate,",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.common_tree.base",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.common_tree.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.common_tree.base",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass GPTTreeIndexBuilder:\n    \"\"\"\n    GPT tree index builder.\n    Helper class to build the tree-structured index,\n    or to synthesize an answer.\n    \"\"\"\n    def __init__(\n        self,\n        num_children: int,",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.common_tree.base",
        "documentation": {}
    },
    {
        "label": "ComposableGraph",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.composability.graph",
        "description": "reference_code.llama-index-core.llama_index.core.indices.composability.graph",
        "peekOfCode": "class ComposableGraph:\n    \"\"\"Composable graph.\"\"\"\n    def __init__(\n        self,\n        all_indices: Dict[str, BaseIndex],\n        root_id: str,\n        storage_context: Optional[StorageContext] = None,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self._all_indices = all_indices",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.composability.graph",
        "documentation": {}
    },
    {
        "label": "DocumentSummaryRetrieverMode",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.document_summary.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.document_summary.base",
        "peekOfCode": "class DocumentSummaryRetrieverMode(str, Enum):\n    EMBEDDING = \"embedding\"\n    LLM = \"llm\"\n_RetrieverMode = DocumentSummaryRetrieverMode\nclass DocumentSummaryIndex(BaseIndex[IndexDocumentSummary]):\n    \"\"\"\n    Document Summary Index.\n    Args:\n        response_synthesizer (BaseSynthesizer): A response synthesizer for generating\n            summaries.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.document_summary.base",
        "documentation": {}
    },
    {
        "label": "DocumentSummaryIndex",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.document_summary.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.document_summary.base",
        "peekOfCode": "class DocumentSummaryIndex(BaseIndex[IndexDocumentSummary]):\n    \"\"\"\n    Document Summary Index.\n    Args:\n        response_synthesizer (BaseSynthesizer): A response synthesizer for generating\n            summaries.\n        summary_query (str): The query to use to generate the summary for each document.\n        show_progress (bool): Whether to show tqdm progress bars.\n            Defaults to False.\n        embed_summaries (bool): Whether to embed the summaries.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.document_summary.base",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.document_summary.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.document_summary.base",
        "peekOfCode": "logger = logging.getLogger(__name__)\nDEFAULT_SUMMARY_QUERY = (\n    \"Describe what the provided text is about. \"\n    \"Also describe some of the questions that this text can answer. \"\n)\nclass DocumentSummaryRetrieverMode(str, Enum):\n    EMBEDDING = \"embedding\"\n    LLM = \"llm\"\n_RetrieverMode = DocumentSummaryRetrieverMode\nclass DocumentSummaryIndex(BaseIndex[IndexDocumentSummary]):",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.document_summary.base",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SUMMARY_QUERY",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.document_summary.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.document_summary.base",
        "peekOfCode": "DEFAULT_SUMMARY_QUERY = (\n    \"Describe what the provided text is about. \"\n    \"Also describe some of the questions that this text can answer. \"\n)\nclass DocumentSummaryRetrieverMode(str, Enum):\n    EMBEDDING = \"embedding\"\n    LLM = \"llm\"\n_RetrieverMode = DocumentSummaryRetrieverMode\nclass DocumentSummaryIndex(BaseIndex[IndexDocumentSummary]):\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.document_summary.base",
        "documentation": {}
    },
    {
        "label": "_RetrieverMode",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.document_summary.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.document_summary.base",
        "peekOfCode": "_RetrieverMode = DocumentSummaryRetrieverMode\nclass DocumentSummaryIndex(BaseIndex[IndexDocumentSummary]):\n    \"\"\"\n    Document Summary Index.\n    Args:\n        response_synthesizer (BaseSynthesizer): A response synthesizer for generating\n            summaries.\n        summary_query (str): The query to use to generate the summary for each document.\n        show_progress (bool): Whether to show tqdm progress bars.\n            Defaults to False.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.document_summary.base",
        "documentation": {}
    },
    {
        "label": "GPTDocumentSummaryIndex",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.document_summary.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.document_summary.base",
        "peekOfCode": "GPTDocumentSummaryIndex = DocumentSummaryIndex",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.document_summary.base",
        "documentation": {}
    },
    {
        "label": "DocumentSummaryIndexLLMRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.document_summary.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.document_summary.retrievers",
        "peekOfCode": "class DocumentSummaryIndexLLMRetriever(BaseRetriever):\n    \"\"\"\n    Document Summary Index LLM Retriever.\n    By default, select relevant summaries from index using LLM calls.\n    Args:\n        index (DocumentSummaryIndex): The index to retrieve from.\n        choice_select_prompt (Optional[BasePromptTemplate]): The prompt to use for selecting relevant summaries.\n        choice_batch_size (int): The number of summary nodes to send to LLM at a time.\n        choice_top_k (int): The number of summary nodes to retrieve.\n        format_node_batch_fn (Callable): Function to format a batch of nodes for LLM.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.document_summary.retrievers",
        "documentation": {}
    },
    {
        "label": "DocumentSummaryIndexEmbeddingRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.document_summary.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.document_summary.retrievers",
        "peekOfCode": "class DocumentSummaryIndexEmbeddingRetriever(BaseRetriever):\n    \"\"\"\n    Document Summary Index Embedding Retriever.\n    Args:\n        index (DocumentSummaryIndex): The index to retrieve from.\n        similarity_top_k (int): The number of summary nodes to retrieve.\n    \"\"\"\n    def __init__(\n        self,\n        index: DocumentSummaryIndex,",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.document_summary.retrievers",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.document_summary.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.document_summary.retrievers",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass DocumentSummaryIndexLLMRetriever(BaseRetriever):\n    \"\"\"\n    Document Summary Index LLM Retriever.\n    By default, select relevant summaries from index using LLM calls.\n    Args:\n        index (DocumentSummaryIndex): The index to retrieve from.\n        choice_select_prompt (Optional[BasePromptTemplate]): The prompt to use for selecting relevant summaries.\n        choice_batch_size (int): The number of summary nodes to send to LLM at a time.\n        choice_top_k (int): The number of summary nodes to retrieve.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.document_summary.retrievers",
        "documentation": {}
    },
    {
        "label": "DocumentSummaryIndexRetriever",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.document_summary.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.document_summary.retrievers",
        "peekOfCode": "DocumentSummaryIndexRetriever = DocumentSummaryIndexLLMRetriever",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.document_summary.retrievers",
        "documentation": {}
    },
    {
        "label": "EmptyIndex",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.empty.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.empty.base",
        "peekOfCode": "class EmptyIndex(BaseIndex[EmptyIndexStruct]):\n    \"\"\"\n    Empty Index.\n    An index that doesn't contain any documents. Used for\n    pure LLM calls.\n    NOTE: this exists because an empty index it allows certain properties,\n    such as the ability to be composed with other indices + token\n    counting + others.\n    \"\"\"\n    index_struct_cls = EmptyIndexStruct",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.empty.base",
        "documentation": {}
    },
    {
        "label": "GPTEmptyIndex",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.empty.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.empty.base",
        "peekOfCode": "GPTEmptyIndex = EmptyIndex",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.empty.base",
        "documentation": {}
    },
    {
        "label": "EmptyIndexRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.empty.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.empty.retrievers",
        "peekOfCode": "class EmptyIndexRetriever(BaseRetriever):\n    \"\"\"\n    EmptyIndex query.\n    Passes the raw LLM call to the underlying LLM model.\n    Args:\n        input_prompt (Optional[BasePromptTemplate]): A Simple Input Prompt\n            (see :ref:`Prompt-Templates`).\n    \"\"\"\n    def __init__(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.empty.retrievers",
        "documentation": {}
    },
    {
        "label": "KeywordTableRetrieverMode",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.base",
        "peekOfCode": "class KeywordTableRetrieverMode(str, Enum):\n    DEFAULT = \"default\"\n    SIMPLE = \"simple\"\n    RAKE = \"rake\"\nclass BaseKeywordTableIndex(BaseIndex[KeywordTable]):\n    \"\"\"\n    Base Keyword Table Index.\n    This index extracts keywords from the text, and maps each\n    keyword to the node(s) that it corresponds to. In this sense it mimics a\n    \"hash table\". During index construction, the keyword table is constructed",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.base",
        "documentation": {}
    },
    {
        "label": "BaseKeywordTableIndex",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.base",
        "peekOfCode": "class BaseKeywordTableIndex(BaseIndex[KeywordTable]):\n    \"\"\"\n    Base Keyword Table Index.\n    This index extracts keywords from the text, and maps each\n    keyword to the node(s) that it corresponds to. In this sense it mimics a\n    \"hash table\". During index construction, the keyword table is constructed\n    by extracting keywords from each node and creating an internal mapping.\n    During query time, the keywords are extracted from the query text, and these\n    keywords are used to index into the keyword table. The retrieved nodes\n    are then used to answer the query.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.base",
        "documentation": {}
    },
    {
        "label": "KeywordTableIndex",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.base",
        "peekOfCode": "class KeywordTableIndex(BaseKeywordTableIndex):\n    \"\"\"\n    Keyword Table Index.\n    This index uses a GPT model to extract keywords from the text.\n    \"\"\"\n    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n        response = self._llm.predict(\n            self.keyword_extract_template,\n            text=text,",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.base",
        "documentation": {}
    },
    {
        "label": "DQKET",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.base",
        "peekOfCode": "DQKET = DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE\nclass KeywordTableRetrieverMode(str, Enum):\n    DEFAULT = \"default\"\n    SIMPLE = \"simple\"\n    RAKE = \"rake\"\nclass BaseKeywordTableIndex(BaseIndex[KeywordTable]):\n    \"\"\"\n    Base Keyword Table Index.\n    This index extracts keywords from the text, and maps each\n    keyword to the node(s) that it corresponds to. In this sense it mimics a",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.base",
        "documentation": {}
    },
    {
        "label": "GPTKeywordTableIndex",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.base",
        "peekOfCode": "GPTKeywordTableIndex = KeywordTableIndex",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.base",
        "documentation": {}
    },
    {
        "label": "RAKEKeywordTableIndex",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.rake_base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.rake_base",
        "peekOfCode": "class RAKEKeywordTableIndex(BaseKeywordTableIndex):\n    \"\"\"\n    RAKE Keyword Table Index.\n    This index uses a RAKE keyword extractor to extract keywords from the text.\n    \"\"\"\n    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n        return rake_extract_keywords(text, max_keywords=self.max_keywords_per_chunk)\n    def as_retriever(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.rake_base",
        "documentation": {}
    },
    {
        "label": "GPTRAKEKeywordTableIndex",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.rake_base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.rake_base",
        "peekOfCode": "GPTRAKEKeywordTableIndex = RAKEKeywordTableIndex",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.rake_base",
        "documentation": {}
    },
    {
        "label": "BaseKeywordTableRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.retrievers",
        "peekOfCode": "class BaseKeywordTableRetriever(BaseRetriever):\n    \"\"\"\n    Base Keyword Table Retriever.\n    Arguments are shared among subclasses.\n    Args:\n        keyword_extract_template (Optional[BasePromptTemplate]): A Keyword\n            Extraction Prompt\n            (see :ref:`Prompt-Templates`).\n        query_keyword_extract_template (Optional[BasePromptTemplate]): A Query\n            Keyword Extraction",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.retrievers",
        "documentation": {}
    },
    {
        "label": "KeywordTableGPTRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.retrievers",
        "peekOfCode": "class KeywordTableGPTRetriever(BaseKeywordTableRetriever):\n    \"\"\"\n    Keyword Table Index GPT Retriever.\n    Extracts keywords using GPT. Set when using `retriever_mode=\"default\"`.\n    See BaseGPTKeywordTableQuery for arguments.\n    \"\"\"\n    def __init__(\n        self,\n        index: BaseKeywordTableIndex,\n        keyword_extract_template: Optional[BasePromptTemplate] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.retrievers",
        "documentation": {}
    },
    {
        "label": "KeywordTableSimpleRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.retrievers",
        "peekOfCode": "class KeywordTableSimpleRetriever(BaseKeywordTableRetriever):\n    \"\"\"\n    Keyword Table Index Simple Retriever.\n    Extracts keywords using simple regex-based keyword extractor.\n    Set when `retriever_mode=\"simple\"`.\n    See BaseGPTKeywordTableQuery for arguments.\n    \"\"\"\n    def _get_keywords(self, query_str: str) -> List[str]:\n        \"\"\"Extract keywords.\"\"\"\n        return list(",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.retrievers",
        "documentation": {}
    },
    {
        "label": "KeywordTableRAKERetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.retrievers",
        "peekOfCode": "class KeywordTableRAKERetriever(BaseKeywordTableRetriever):\n    \"\"\"\n    Keyword Table Index RAKE Retriever.\n    Extracts keywords using RAKE keyword extractor.\n    Set when `retriever_mode=\"rake\"`.\n    See BaseGPTKeywordTableQuery for arguments.\n    \"\"\"\n    def _get_keywords(self, query_str: str) -> List[str]:\n        \"\"\"Extract keywords.\"\"\"\n        return list(",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.retrievers",
        "documentation": {}
    },
    {
        "label": "DQKET",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.retrievers",
        "peekOfCode": "DQKET = DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE\nlogger = logging.getLogger(__name__)\nclass BaseKeywordTableRetriever(BaseRetriever):\n    \"\"\"\n    Base Keyword Table Retriever.\n    Arguments are shared among subclasses.\n    Args:\n        keyword_extract_template (Optional[BasePromptTemplate]): A Keyword\n            Extraction Prompt\n            (see :ref:`Prompt-Templates`).",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.retrievers",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.retrievers",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass BaseKeywordTableRetriever(BaseRetriever):\n    \"\"\"\n    Base Keyword Table Retriever.\n    Arguments are shared among subclasses.\n    Args:\n        keyword_extract_template (Optional[BasePromptTemplate]): A Keyword\n            Extraction Prompt\n            (see :ref:`Prompt-Templates`).\n        query_keyword_extract_template (Optional[BasePromptTemplate]): A Query",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.retrievers",
        "documentation": {}
    },
    {
        "label": "SimpleKeywordTableIndex",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.simple_base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.simple_base",
        "peekOfCode": "class SimpleKeywordTableIndex(BaseKeywordTableIndex):\n    \"\"\"\n    Simple Keyword Table Index.\n    This index uses a simple regex extractor to extract keywords from the text.\n    \"\"\"\n    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n        return simple_extract_keywords(text, self.max_keywords_per_chunk)\n    def as_retriever(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.simple_base",
        "documentation": {}
    },
    {
        "label": "DQKET",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.simple_base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.simple_base",
        "peekOfCode": "DQKET = DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE\nclass SimpleKeywordTableIndex(BaseKeywordTableIndex):\n    \"\"\"\n    Simple Keyword Table Index.\n    This index uses a simple regex extractor to extract keywords from the text.\n    \"\"\"\n    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n        return simple_extract_keywords(text, self.max_keywords_per_chunk)\n    def as_retriever(",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.simple_base",
        "documentation": {}
    },
    {
        "label": "GPTSimpleKeywordTableIndex",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.simple_base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.simple_base",
        "peekOfCode": "GPTSimpleKeywordTableIndex = SimpleKeywordTableIndex",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.simple_base",
        "documentation": {}
    },
    {
        "label": "simple_extract_keywords",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.utils",
        "description": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.utils",
        "peekOfCode": "def simple_extract_keywords(\n    text_chunk: str, max_keywords: Optional[int] = None, filter_stopwords: bool = True\n) -> Set[str]:\n    \"\"\"Extract keywords with simple algorithm.\"\"\"\n    tokens = [t.strip().lower() for t in re.findall(r\"\\w+\", text_chunk)]\n    if filter_stopwords:\n        tokens = [t for t in tokens if t not in globals_helper.stopwords]\n    token_counts = Counter(tokens)\n    keywords = [keyword for keyword, count in token_counts.most_common(max_keywords)]\n    return set(keywords)",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.utils",
        "documentation": {}
    },
    {
        "label": "rake_extract_keywords",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.utils",
        "description": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.utils",
        "peekOfCode": "def rake_extract_keywords(\n    text_chunk: str,\n    max_keywords: Optional[int] = None,\n    expand_with_subtokens: bool = True,\n) -> Set[str]:\n    \"\"\"Extract keywords with RAKE.\"\"\"\n    try:\n        import nltk\n    except ImportError:\n        raise ImportError(\"Please install nltk: `pip install nltk`\")",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.utils",
        "documentation": {}
    },
    {
        "label": "extract_keywords_given_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.utils",
        "description": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.utils",
        "peekOfCode": "def extract_keywords_given_response(\n    response: str, lowercase: bool = True, start_token: str = \"\"\n) -> Set[str]:\n    \"\"\"\n    Extract keywords given the GPT-generated response.\n    Used by keyword table indices.\n    Parses <start_token>: <word1>, <word2>, ... into [word1, word2, ...]\n    Raises exception if response doesn't start with <start_token>\n    \"\"\"\n    results = []",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.keyword_table.utils",
        "documentation": {}
    },
    {
        "label": "KnowledgeGraphIndex",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.base",
        "peekOfCode": "class KnowledgeGraphIndex(BaseIndex[KG]):\n    \"\"\"\n    Knowledge Graph Index.\n    Build a KG by extracting triplets, and leveraging the KG during query-time.\n    Args:\n        kg_triplet_extract_template (BasePromptTemplate): The prompt to use for\n            extracting triplets.\n        max_triplets_per_chunk (int): The maximum number of triplets to extract.\n        storage_context (Optional[StorageContext]): The storage context to use.\n        graph_store (Optional[GraphStore]): The graph store to use.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.base",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.base",
        "peekOfCode": "logger = logging.getLogger(__name__)\n@deprecated.deprecated(\n    version=\"0.10.53\",\n    reason=(\n        \"The KnowledgeGraphIndex class has been deprecated. \"\n        \"Please use the new PropertyGraphIndex class instead. \"\n        \"If a certain graph store integration is missing in the new class, \"\n        \"please open an issue on the GitHub repository or contribute it!\"\n    ),\n)",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.base",
        "documentation": {}
    },
    {
        "label": "KGRetrieverMode",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "peekOfCode": "class KGRetrieverMode(str, Enum):\n    \"\"\"\n    Query mode enum for Knowledge Graphs.\n    Can be passed as the enum struct, or as the underlying string.\n    Attributes:\n        KEYWORD (\"keyword\"): Default query mode, using keywords to find triplets.\n        EMBEDDING (\"embedding\"): Embedding mode, using embeddings to find\n            similar triplets.\n        HYBRID (\"hybrid\"): Hybrid mode, combining both keywords and embeddings\n            to find relevant triplets.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "documentation": {}
    },
    {
        "label": "KGTableRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "peekOfCode": "class KGTableRetriever(BaseRetriever):\n    \"\"\"\n    KG Table Retriever.\n    Arguments are shared among subclasses.\n    Args:\n        query_keyword_extract_template (Optional[QueryKGExtractPrompt]): A Query\n            KG Extraction\n            Prompt (see :ref:`Prompt-Templates`).\n        refine_template (Optional[BasePromptTemplate]): A Refinement Prompt\n            (see :ref:`Prompt-Templates`).",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "documentation": {}
    },
    {
        "label": "KnowledgeGraphRAGRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "peekOfCode": "class KnowledgeGraphRAGRetriever(BaseRetriever):\n    \"\"\"\n    Knowledge Graph RAG retriever.\n    Retriever that perform SubGraph RAG towards knowledge graph.\n    Args:\n        storage_context (Optional[StorageContext]): A storage context to use.\n        entity_extract_fn (Optional[Callable]): A function to extract entities.\n        entity_extract_template Optional[BasePromptTemplate]): A Query Key Entity\n            Extraction Prompt (see :ref:`Prompt-Templates`).\n        entity_extract_policy (Optional[str]): The entity extraction policy to use.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "documentation": {}
    },
    {
        "label": "DQKET",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "peekOfCode": "DQKET = DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE\nDEFAULT_NODE_SCORE = 1000.0\nGLOBAL_EXPLORE_NODE_LIMIT = 3\nREL_TEXT_LIMIT = 30\nlogger = logging.getLogger(__name__)\nclass KGRetrieverMode(str, Enum):\n    \"\"\"\n    Query mode enum for Knowledge Graphs.\n    Can be passed as the enum struct, or as the underlying string.\n    Attributes:",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "documentation": {}
    },
    {
        "label": "DEFAULT_NODE_SCORE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "peekOfCode": "DEFAULT_NODE_SCORE = 1000.0\nGLOBAL_EXPLORE_NODE_LIMIT = 3\nREL_TEXT_LIMIT = 30\nlogger = logging.getLogger(__name__)\nclass KGRetrieverMode(str, Enum):\n    \"\"\"\n    Query mode enum for Knowledge Graphs.\n    Can be passed as the enum struct, or as the underlying string.\n    Attributes:\n        KEYWORD (\"keyword\"): Default query mode, using keywords to find triplets.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "documentation": {}
    },
    {
        "label": "GLOBAL_EXPLORE_NODE_LIMIT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "peekOfCode": "GLOBAL_EXPLORE_NODE_LIMIT = 3\nREL_TEXT_LIMIT = 30\nlogger = logging.getLogger(__name__)\nclass KGRetrieverMode(str, Enum):\n    \"\"\"\n    Query mode enum for Knowledge Graphs.\n    Can be passed as the enum struct, or as the underlying string.\n    Attributes:\n        KEYWORD (\"keyword\"): Default query mode, using keywords to find triplets.\n        EMBEDDING (\"embedding\"): Embedding mode, using embeddings to find",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "documentation": {}
    },
    {
        "label": "REL_TEXT_LIMIT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "peekOfCode": "REL_TEXT_LIMIT = 30\nlogger = logging.getLogger(__name__)\nclass KGRetrieverMode(str, Enum):\n    \"\"\"\n    Query mode enum for Knowledge Graphs.\n    Can be passed as the enum struct, or as the underlying string.\n    Attributes:\n        KEYWORD (\"keyword\"): Default query mode, using keywords to find triplets.\n        EMBEDDING (\"embedding\"): Embedding mode, using embeddings to find\n            similar triplets.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass KGRetrieverMode(str, Enum):\n    \"\"\"\n    Query mode enum for Knowledge Graphs.\n    Can be passed as the enum struct, or as the underlying string.\n    Attributes:\n        KEYWORD (\"keyword\"): Default query mode, using keywords to find triplets.\n        EMBEDDING (\"embedding\"): Embedding mode, using embeddings to find\n            similar triplets.\n        HYBRID (\"hybrid\"): Hybrid mode, combining both keywords and embeddings",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SYNONYM_EXPAND_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "peekOfCode": "DEFAULT_SYNONYM_EXPAND_TEMPLATE = \"\"\"\nGenerate synonyms or possible form of keywords up to {max_keywords} in total,\nconsidering possible cases of capitalization, pluralization, common expressions, etc.\nProvide all synonyms of keywords in comma-separated format: 'SYNONYMS: <keywords>'\nNote, result should be in one-line with only one 'SYNONYMS: ' prefix\n----\nKEYWORDS: {question}\n----\n\"\"\"\nDEFAULT_SYNONYM_EXPAND_PROMPT = PromptTemplate(",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SYNONYM_EXPAND_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "peekOfCode": "DEFAULT_SYNONYM_EXPAND_PROMPT = PromptTemplate(\n    DEFAULT_SYNONYM_EXPAND_TEMPLATE,\n    prompt_type=PromptType.QUERY_KEYWORD_EXTRACT,\n)\n@deprecated.deprecated(\n    version=\"0.10.53\",\n    reason=(\n        \"KnowledgeGraphRAGRetriever is deprecated, it is recommended to use \"\n        \"PropertyGraphIndex and associated retrievers instead.\"\n    ),",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.knowledge_graph.retrievers",
        "documentation": {}
    },
    {
        "label": "ListRetrieverMode",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.list.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.list.base",
        "peekOfCode": "class ListRetrieverMode(str, Enum):\n    DEFAULT = \"default\"\n    EMBEDDING = \"embedding\"\n    LLM = \"llm\"\nclass SummaryIndex(BaseIndex[IndexList]):\n    \"\"\"\n    Summary Index.\n    The summary index is a simple data structure where nodes are stored in\n    a sequence. During index construction, the document texts are\n    chunked up, converted to nodes, and stored in a list.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.list.base",
        "documentation": {}
    },
    {
        "label": "SummaryIndex",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.list.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.list.base",
        "peekOfCode": "class SummaryIndex(BaseIndex[IndexList]):\n    \"\"\"\n    Summary Index.\n    The summary index is a simple data structure where nodes are stored in\n    a sequence. During index construction, the document texts are\n    chunked up, converted to nodes, and stored in a list.\n    During query time, the summary index iterates through the nodes\n    with some optional filter parameters, and synthesizes an\n    answer from all the nodes.\n    Args:",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.list.base",
        "documentation": {}
    },
    {
        "label": "GPTListIndex",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.list.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.list.base",
        "peekOfCode": "GPTListIndex = SummaryIndex\n# New name\nListIndex = SummaryIndex",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.list.base",
        "documentation": {}
    },
    {
        "label": "ListIndex",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.list.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.list.base",
        "peekOfCode": "ListIndex = SummaryIndex",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.list.base",
        "documentation": {}
    },
    {
        "label": "SummaryIndexRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.list.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.list.retrievers",
        "peekOfCode": "class SummaryIndexRetriever(BaseRetriever):\n    \"\"\"\n    Simple retriever for SummaryIndex that returns all nodes.\n    Args:\n        index (SummaryIndex): The index to retrieve from.\n    \"\"\"\n    def __init__(\n        self,\n        index: SummaryIndex,\n        callback_manager: Optional[CallbackManager] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.list.retrievers",
        "documentation": {}
    },
    {
        "label": "SummaryIndexEmbeddingRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.list.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.list.retrievers",
        "peekOfCode": "class SummaryIndexEmbeddingRetriever(BaseRetriever):\n    \"\"\"\n    Embedding based retriever for SummaryIndex.\n    Generates embeddings in a lazy fashion for all\n    nodes that are traversed.\n    Args:\n        index (SummaryIndex): The index to retrieve from.\n        similarity_top_k (Optional[int]): The number of top nodes to return.\n    \"\"\"\n    def __init__(",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.list.retrievers",
        "documentation": {}
    },
    {
        "label": "SummaryIndexLLMRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.list.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.list.retrievers",
        "peekOfCode": "class SummaryIndexLLMRetriever(BaseRetriever):\n    \"\"\"\n    LLM retriever for SummaryIndex.\n    Args:\n        index (SummaryIndex): The index to retrieve from.\n        choice_select_prompt (Optional[PromptTemplate]): A Choice-Select Prompt\n           (see :ref:`Prompt-Templates`).)\n        choice_batch_size (int): The number of nodes to query at a time.\n        format_node_batch_fn (Optional[Callable]): A function that formats a\n            batch of nodes.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.list.retrievers",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.list.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.list.retrievers",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass SummaryIndexRetriever(BaseRetriever):\n    \"\"\"\n    Simple retriever for SummaryIndex that returns all nodes.\n    Args:\n        index (SummaryIndex): The index to retrieve from.\n    \"\"\"\n    def __init__(\n        self,\n        index: SummaryIndex,",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.list.retrievers",
        "documentation": {}
    },
    {
        "label": "ListIndexEmbeddingRetriever",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.list.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.list.retrievers",
        "peekOfCode": "ListIndexEmbeddingRetriever = SummaryIndexEmbeddingRetriever\nListIndexLLMRetriever = SummaryIndexLLMRetriever\nListIndexRetriever = SummaryIndexRetriever",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.list.retrievers",
        "documentation": {}
    },
    {
        "label": "ListIndexLLMRetriever",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.list.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.list.retrievers",
        "peekOfCode": "ListIndexLLMRetriever = SummaryIndexLLMRetriever\nListIndexRetriever = SummaryIndexRetriever",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.list.retrievers",
        "documentation": {}
    },
    {
        "label": "ListIndexRetriever",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.list.retrievers",
        "description": "reference_code.llama-index-core.llama_index.core.indices.list.retrievers",
        "peekOfCode": "ListIndexRetriever = SummaryIndexRetriever",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.list.retrievers",
        "documentation": {}
    },
    {
        "label": "BaseManagedIndex",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.managed.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.managed.base",
        "peekOfCode": "class BaseManagedIndex(BaseIndex[IndexDict], ABC):\n    \"\"\"\n    Managed Index.\n    The managed service can index documents into a managed service.\n    How documents are structured into nodes is a detail for the managed service,\n    and not exposed in this interface (although could be controlled by\n    configuration parameters).\n    Args:\n        show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.managed.base",
        "documentation": {}
    },
    {
        "label": "ManagedIndexQueryMode",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.managed.types",
        "description": "reference_code.llama-index-core.llama_index.core.indices.managed.types",
        "peekOfCode": "class ManagedIndexQueryMode(str, Enum):\n    \"\"\"Managed Index query mode.\"\"\"\n    DEFAULT = \"default\"\n    MMR = \"mmr\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.managed.types",
        "documentation": {}
    },
    {
        "label": "MultiModalVectorStoreIndex",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.multi_modal.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.multi_modal.base",
        "peekOfCode": "class MultiModalVectorStoreIndex(VectorStoreIndex):\n    \"\"\"\n    Multi-Modal Vector Store Index.\n    Args:\n        use_async (bool): Whether to use asynchronous calls. Defaults to False.\n        show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n        store_nodes_override (bool): set to True to always store Node objects in index\n            store and document store even if vector store keeps text. Defaults to False\n    \"\"\"\n    image_namespace = \"image\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.multi_modal.base",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.multi_modal.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.multi_modal.base",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass MultiModalVectorStoreIndex(VectorStoreIndex):\n    \"\"\"\n    Multi-Modal Vector Store Index.\n    Args:\n        use_async (bool): Whether to use asynchronous calls. Defaults to False.\n        show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n        store_nodes_override (bool): set to True to always store Node objects in index\n            store and document store even if vector store keeps text. Defaults to False\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.multi_modal.base",
        "documentation": {}
    },
    {
        "label": "MultiModalVectorIndexRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.multi_modal.retriever",
        "description": "reference_code.llama-index-core.llama_index.core.indices.multi_modal.retriever",
        "peekOfCode": "class MultiModalVectorIndexRetriever(MultiModalRetriever):\n    \"\"\"\n    Multi Modal Vector index retriever.\n    Args:\n        index (MultiModalVectorStoreIndex): Multi Modal vector store index for images and texts.\n        similarity_top_k (int): number of top k results to return.\n        vector_store_query_mode (str): vector store query mode\n            See reference for VectorStoreQueryMode for full list of supported modes.\n        filters (Optional[MetadataFilters]): metadata filters, defaults to None\n        alpha (float): weight for sparse/dense retrieval, only used for",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.multi_modal.retriever",
        "documentation": {}
    },
    {
        "label": "BasePGRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.base",
        "peekOfCode": "class BasePGRetriever(BaseRetriever):\n    \"\"\"\n    The base class for property graph retrievers.\n    By default, will retrieve nodes from the graph store and add source text to the nodes if needed.\n    Args:\n        graph_store (PropertyGraphStore):\n            The graph store to retrieve data from.\n        include_text (bool, optional):\n            Whether to include source text in the retrieved nodes. Defaults to True.\n        include_text_preamble (Optional[str], optional):",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.base",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PREAMBLE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.base",
        "peekOfCode": "DEFAULT_PREAMBLE = \"Here are some facts extracted from the provided text:\\n\\n\"\nclass BasePGRetriever(BaseRetriever):\n    \"\"\"\n    The base class for property graph retrievers.\n    By default, will retrieve nodes from the graph store and add source text to the nodes if needed.\n    Args:\n        graph_store (PropertyGraphStore):\n            The graph store to retrieve data from.\n        include_text (bool, optional):\n            Whether to include source text in the retrieved nodes. Defaults to True.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.base",
        "documentation": {}
    },
    {
        "label": "CustomPGRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.custom",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.custom",
        "peekOfCode": "class CustomPGRetriever(BasePGRetriever):\n    \"\"\"\n    A retriever meant to be easily subclassed to implement custom retrieval logic.\n    The user only has to implement:\n    - `init` to initialize the retriever and assign any necessary attributes.\n    - `custom_retrieve` to implement the custom retrieval logic.\n    - `aretrieve_custom` (optional) to implement asynchronous retrieval logic.\n    Args:\n        graph_store (PropertyGraphStore):\n            The graph store to retrieve data from.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.custom",
        "documentation": {}
    },
    {
        "label": "CUSTOM_RETRIEVE_TYPE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.custom",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.custom",
        "peekOfCode": "CUSTOM_RETRIEVE_TYPE = Union[\n    str, List[str], TextNode, List[TextNode], NodeWithScore, List[NodeWithScore]\n]\nclass CustomPGRetriever(BasePGRetriever):\n    \"\"\"\n    A retriever meant to be easily subclassed to implement custom retrieval logic.\n    The user only has to implement:\n    - `init` to initialize the retriever and assign any necessary attributes.\n    - `custom_retrieve` to implement the custom retrieval logic.\n    - `aretrieve_custom` (optional) to implement asynchronous retrieval logic.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.custom",
        "documentation": {}
    },
    {
        "label": "CypherTemplateRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.cypher_template",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.cypher_template",
        "peekOfCode": "class CypherTemplateRetriever(BasePGRetriever):\n    \"\"\"\n    A Cypher retriever that fills in params for a cypher query using an LLM.\n    Args:\n        graph_store (PropertyGraphStore):\n            The graph store to retrieve data from.\n        output_cls (Type[BaseModel]):\n            The output class to use for the LLM.\n            Should contain the params needed for the cypher query.\n        cypher_query (str):",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.cypher_template",
        "documentation": {}
    },
    {
        "label": "LLMSynonymRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.llm_synonym",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.llm_synonym",
        "peekOfCode": "class LLMSynonymRetriever(BasePGRetriever):\n    \"\"\"\n    A retriever that uses a language model to expand a query with synonyms.\n    The synonyms are then used to retrieve nodes from a property graph.\n    Args:\n        graph_store (PropertyGraphStore):\n            The graph store to retrieve data from.\n        include_text (bool, optional):\n            Whether to include source text in the retrieved nodes. Defaults to True.\n        synonym_prompt (Union[BasePromptTemplate, str], optional):",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.llm_synonym",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SYNONYM_EXPAND_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.llm_synonym",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.llm_synonym",
        "peekOfCode": "DEFAULT_SYNONYM_EXPAND_TEMPLATE = (\n    \"Given some initial query, generate synonyms or related keywords up to {max_keywords} in total, \"\n    \"considering possible cases of capitalization, pluralization, common expressions, etc.\\n\"\n    \"Provide all synonyms/keywords separated by '^' symbols: 'keyword1^keyword2^...'\\n\"\n    \"Note, result should be in one-line, separated by '^' symbols.\"\n    \"----\\n\"\n    \"QUERY: {query_str}\\n\"\n    \"----\\n\"\n    \"KEYWORDS: \"\n)",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.llm_synonym",
        "documentation": {}
    },
    {
        "label": "TextToCypherRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.text_to_cypher",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.text_to_cypher",
        "peekOfCode": "class TextToCypherRetriever(BasePGRetriever):\n    \"\"\"\n    A Text-to-Cypher retriever that uses a language model to generate Cypher queries.\n    NOTE: Executing arbitrary cypher has its risks. Ensure you take the needed measures\n    (read-only roles, sandboxed env, etc.) to ensure safe usage in a production environment.\n    Args:\n        graph_store (PropertyGraphStore):\n            The graph store to retrieve data from.\n        llm (Optional[LLM], optional):\n            The language model to use. Defaults to Settings.llm.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.text_to_cypher",
        "documentation": {}
    },
    {
        "label": "DEFAULT_RESPONSE_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.text_to_cypher",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.text_to_cypher",
        "peekOfCode": "DEFAULT_RESPONSE_TEMPLATE = (\n    \"Generated Cypher query:\\n{query}\\n\\nCypher Response:\\n{response}\"\n)\nDEFAULT_SUMMARY_TEMPLATE = PromptTemplate(\n    \"\"\"You are an assistant that helps to form nice and human understandable answers.\n        The information part contains the provided information you must use to construct an answer.\n        The provided information is authoritative, never doubt it or try to use your internal knowledge to correct it.\n        If the provided information is empty, say that you don't know the answer.\n        Make the answer sound as a response to the question. Do not mention that you based the result on the given information.\n        Here is an example:",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.text_to_cypher",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SUMMARY_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.text_to_cypher",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.text_to_cypher",
        "peekOfCode": "DEFAULT_SUMMARY_TEMPLATE = PromptTemplate(\n    \"\"\"You are an assistant that helps to form nice and human understandable answers.\n        The information part contains the provided information you must use to construct an answer.\n        The provided information is authoritative, never doubt it or try to use your internal knowledge to correct it.\n        If the provided information is empty, say that you don't know the answer.\n        Make the answer sound as a response to the question. Do not mention that you based the result on the given information.\n        Here is an example:\n        Question: How many miles is the flight between the ANC and SEA airports?\n        Information:\n        [{\"r.dist\": 1440}]",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.text_to_cypher",
        "documentation": {}
    },
    {
        "label": "VectorContextRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.vector",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.vector",
        "peekOfCode": "class VectorContextRetriever(BasePGRetriever):\n    \"\"\"\n    A retriever that uses a vector store to retrieve nodes based on a query.\n    Args:\n        graph_store (PropertyGraphStore):\n            The graph store to retrieve data from.\n        include_text (bool, optional):\n            Whether to include source text in the retrieved nodes. Defaults to True.\n        embed_model (Optional[BaseEmbedding], optional):\n            The embedding model to use. Defaults to Settings.embed_model.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.sub_retrievers.vector",
        "documentation": {}
    },
    {
        "label": "DynamicLLMPathExtractor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.dynamic_llm",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.dynamic_llm",
        "peekOfCode": "class DynamicLLMPathExtractor(TransformComponent):\n    \"\"\"\n    DynamicLLMPathExtractor is a component for extracting structured information from text\n    to build a knowledge graph. It uses an LLM to identify entities and their relationships,\n    with the ability to infer entity types and expand upon an initial ontology.\n    This extractor improves upon SimpleLLMPathExtractor by:\n    1. Detecting entity types instead of labeling them generically as \"entity\" and \"chunk\".\n    2. Accepting an initial ontology as input, specifying desired nodes and relationships.\n    3. Encouraging ontology expansion through its prompt design.\n    This extractor differs from SchemaLLMPathExtractor because:",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.dynamic_llm",
        "documentation": {}
    },
    {
        "label": "default_parse_dynamic_triplets",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.dynamic_llm",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.dynamic_llm",
        "peekOfCode": "def default_parse_dynamic_triplets(\n    llm_output: str,\n) -> List[Tuple[EntityNode, Relation, EntityNode]]:\n    \"\"\"\n    Parse the LLM output and convert it into a list of entity-relation-entity triplets.\n    This function is flexible and can handle various output formats.\n    Args:\n        llm_output (str): The output from the LLM, which may be JSON-like or plain text.\n    Returns:\n        List[Tuple[EntityNode, Relation, EntityNode]]: A list of triplets.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.dynamic_llm",
        "documentation": {}
    },
    {
        "label": "default_parse_dynamic_triplets_with_props",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.dynamic_llm",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.dynamic_llm",
        "peekOfCode": "def default_parse_dynamic_triplets_with_props(\n    llm_output: str,\n) -> List[Tuple[EntityNode, Relation, EntityNode]]:\n    \"\"\"\n    Parse the LLM output and convert it into a list of entity-relation-entity triplets.\n    This function is flexible and can handle various output formats.\n    Args:\n        llm_output (str): The output from the LLM, which may be JSON-like or plain text.\n    Returns:\n        List[Tuple[EntityNode, Relation, EntityNode]]: A list of triplets.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.dynamic_llm",
        "documentation": {}
    },
    {
        "label": "ImplicitPathExtractor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.implicit",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.implicit",
        "peekOfCode": "class ImplicitPathExtractor(TransformComponent):\n    \"\"\"\n    Extract edges from node relationships.\n    Uses `node.relationships` to extract relations between nodes.\n    \"\"\"\n    @classmethod\n    def class_name(cls) -> str:\n        return \"ImplicitPathExtractor\"\n    def __call__(\n        self, nodes: Sequence[BaseNode], show_progress: bool = False, **kwargs: Any",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.implicit",
        "documentation": {}
    },
    {
        "label": "get_node_rel_string",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.implicit",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.implicit",
        "peekOfCode": "def get_node_rel_string(relationship: NodeRelationship) -> str:\n    return str(relationship).split(\".\")[-1]\nclass ImplicitPathExtractor(TransformComponent):\n    \"\"\"\n    Extract edges from node relationships.\n    Uses `node.relationships` to extract relations between nodes.\n    \"\"\"\n    @classmethod\n    def class_name(cls) -> str:\n        return \"ImplicitPathExtractor\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.implicit",
        "documentation": {}
    },
    {
        "label": "SchemaLLMPathExtractor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.schema_llm",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.schema_llm",
        "peekOfCode": "class SchemaLLMPathExtractor(TransformComponent):\n    \"\"\"\n    Extract paths from a graph using a schema.\n    Args:\n        llm (LLM):\n            The language model to use.\n        extract_prompt (Union[PromptTemplate, str], optional):\n            The template to use for the extraction query. Defaults to None.\n        possible_entities (Optional[Type[Any]], optional):\n            The possible entities to extract. Defaults to None.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.schema_llm",
        "documentation": {}
    },
    {
        "label": "DEFAULT_ENTITIES",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.schema_llm",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.schema_llm",
        "peekOfCode": "DEFAULT_ENTITIES = Literal[\n    \"PRODUCT\",\n    \"MARKET\",\n    \"TECHNOLOGY\",\n    \"EVENT\",\n    \"CONCEPT\",\n    \"ORGANIZATION\",\n    \"PERSON\",\n    \"LOCATION\",\n    \"TIME\",",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.schema_llm",
        "documentation": {}
    },
    {
        "label": "DEFAULT_RELATIONS",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.schema_llm",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.schema_llm",
        "peekOfCode": "DEFAULT_RELATIONS = Literal[\n    \"USED_BY\",\n    \"USED_FOR\",\n    \"LOCATED_IN\",\n    \"PART_OF\",\n    \"WORKED_ON\",\n    \"HAS\",\n    \"IS_A\",\n    \"BORN_IN\",\n    \"DIED_IN\",",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.schema_llm",
        "documentation": {}
    },
    {
        "label": "Triple",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.schema_llm",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.schema_llm",
        "peekOfCode": "Triple = Tuple[str, str, str]\nDEFAULT_VALIDATION_SCHEMA: List[Triple] = [\n    (\"PRODUCT\", \"USED_BY\", \"PRODUCT\"),\n    (\"PRODUCT\", \"USED_FOR\", \"MARKET\"),\n    (\"PRODUCT\", \"HAS\", \"TECHNOLOGY\"),\n    (\"MARKET\", \"LOCATED_IN\", \"LOCATION\"),\n    (\"MARKET\", \"HAS\", \"TECHNOLOGY\"),\n    (\"TECHNOLOGY\", \"USED_BY\", \"PRODUCT\"),\n    (\"TECHNOLOGY\", \"USED_FOR\", \"MARKET\"),\n    (\"TECHNOLOGY\", \"LOCATED_IN\", \"LOCATION\"),",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.schema_llm",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SCHEMA_PATH_EXTRACT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.schema_llm",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.schema_llm",
        "peekOfCode": "DEFAULT_SCHEMA_PATH_EXTRACT_PROMPT = PromptTemplate(\n    \"Give the following text, extract the knowledge graph according to the provided schema. \"\n    \"Try to limit to the output {max_triplets_per_chunk} extracted paths.s\\n\"\n    \"-------\\n\"\n    \"{text}\\n\"\n    \"-------\\n\"\n)\nclass SchemaLLMPathExtractor(TransformComponent):\n    \"\"\"\n    Extract paths from a graph using a schema.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.schema_llm",
        "documentation": {}
    },
    {
        "label": "SimpleLLMPathExtractor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.simple_llm",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.simple_llm",
        "peekOfCode": "class SimpleLLMPathExtractor(TransformComponent):\n    \"\"\"\n    Extract triples from a graph.\n    Uses an LLM and a simple prompt + output parsing to extract paths (i.e. triples) from text.\n    Args:\n        llm (LLM):\n            The language model to use.\n        extract_prompt (Union[str, PromptTemplate]):\n            The prompt to use for extracting triples.\n        parse_fn (callable):",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.simple_llm",
        "documentation": {}
    },
    {
        "label": "get_entity_class",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.utils",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.utils",
        "peekOfCode": "def get_entity_class(\n    possible_entities: TypeAlias,\n    possible_entity_props: Optional[List[str]],\n    strict: bool,\n) -> Any:\n    \"\"\"Get entity class.\"\"\"\n    if not possible_entity_props:\n        return create_model(\n            \"Entity\",\n            type=(",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.utils",
        "documentation": {}
    },
    {
        "label": "get_relation_class",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.utils",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.utils",
        "peekOfCode": "def get_relation_class(\n    possible_relations: TypeAlias,\n    possible_relation_props: Optional[List[str]],\n    strict: bool,\n) -> Any:\n    \"\"\"Get relation class.\"\"\"\n    if not possible_relation_props:\n        return create_model(\n            \"Relation\",\n            type=(",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.transformations.utils",
        "documentation": {}
    },
    {
        "label": "PropertyGraphIndex",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.base",
        "peekOfCode": "class PropertyGraphIndex(BaseIndex[IndexLPG]):\n    \"\"\"\n    An index for a property graph.\n    Args:\n        nodes (Optional[Sequence[BaseNode]]):\n            A list of nodes to insert into the index.\n        llm (Optional[LLM]):\n            The language model to use for extracting triplets. Defaults to `Settings.llm`.\n        kg_extractors (Optional[List[TransformComponent]]):\n            A list of transformations to apply to the nodes to extract triplets.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.base",
        "documentation": {}
    },
    {
        "label": "PGRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.retriever",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.retriever",
        "peekOfCode": "class PGRetriever(BaseRetriever):\n    \"\"\"\n    A retriever that uses multiple sub-retrievers to retrieve nodes from a property graph.\n    Args:\n        sub_retrievers (List[BasePGRetriever]):\n            The sub-retrievers to use.\n        num_workers (int, optional):\n            The number of workers to use for async retrieval. Defaults to 4.\n        use_async (bool, optional):\n            Whether to use async retrieval. Defaults to True.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.retriever",
        "documentation": {}
    },
    {
        "label": "default_parse_triplets_fn",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.property_graph.utils",
        "description": "reference_code.llama-index-core.llama_index.core.indices.property_graph.utils",
        "peekOfCode": "def default_parse_triplets_fn(\n    response: str, max_length: int = 128\n) -> List[Tuple[str, str, str]]:\n    knowledge_strs = response.strip().split(\"\\n\")\n    results = []\n    for text in knowledge_strs:\n        if \"(\" not in text or \")\" not in text or text.index(\")\") < text.index(\"(\"):\n            # skip empty lines and non-triplets\n            continue\n        triplet_part = text[text.index(\"(\") + 1 : text.index(\")\")]",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.property_graph.utils",
        "documentation": {}
    },
    {
        "label": "BaseQueryTransform",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.base",
        "peekOfCode": "class BaseQueryTransform(PromptMixin, DispatcherSpanMixin):\n    \"\"\"\n    Base class for query transform.\n    A query transform augments a raw query string with associated transformations\n    to improve index querying.\n    The query transformation is performed before the query is sent to the index.\n    \"\"\"\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt modules.\"\"\"\n        # TODO: keep this for now since response synthesizers don't generally have sub-modules",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.base",
        "documentation": {}
    },
    {
        "label": "IdentityQueryTransform",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.base",
        "peekOfCode": "class IdentityQueryTransform(BaseQueryTransform):\n    \"\"\"\n    Identity query transform.\n    Do nothing to the query.\n    \"\"\"\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {}\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.base",
        "documentation": {}
    },
    {
        "label": "HyDEQueryTransform",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.base",
        "peekOfCode": "class HyDEQueryTransform(BaseQueryTransform):\n    \"\"\"\n    Hypothetical Document Embeddings (HyDE) query transform.\n    It uses an LLM to generate hypothetical answer(s) to a given query,\n    and use the resulting documents as embedding strings.\n    As described in `[Precise Zero-Shot Dense Retrieval without Relevance Labels]\n    (https://arxiv.org/abs/2212.10496)`\n    \"\"\"\n    def __init__(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.base",
        "documentation": {}
    },
    {
        "label": "DecomposeQueryTransform",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.base",
        "peekOfCode": "class DecomposeQueryTransform(BaseQueryTransform):\n    \"\"\"\n    Decompose query transform.\n    Decomposes query into a subquery given the current index struct.\n    Performs a single step transformation.\n    Args:\n        llm_predictor (Optional[LLM]): LLM for generating\n            hypothetical documents\n    \"\"\"\n    def __init__(",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.base",
        "documentation": {}
    },
    {
        "label": "ImageOutputQueryTransform",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.base",
        "peekOfCode": "class ImageOutputQueryTransform(BaseQueryTransform):\n    \"\"\"\n    Image output query transform.\n    Adds instructions for formatting image output.\n    By default, this prompts the LLM to format image output as an HTML <img> tag,\n    which can be displayed nicely in jupyter notebook.\n    \"\"\"\n    def __init__(\n        self,\n        width: int = 400,",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.base",
        "documentation": {}
    },
    {
        "label": "StepDecomposeQueryTransform",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.base",
        "peekOfCode": "class StepDecomposeQueryTransform(BaseQueryTransform):\n    \"\"\"\n    Step decompose query transform.\n    Decomposes query into a subquery given the current index struct\n    and previous reasoning.\n    NOTE: doesn't work yet.\n    Args:\n        llm_predictor (Optional[LLM]): LLM for generating\n            hypothetical documents\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.base",
        "documentation": {}
    },
    {
        "label": "FeedbackQueryTransformation",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.feedback_transform",
        "description": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.feedback_transform",
        "peekOfCode": "class FeedbackQueryTransformation(BaseQueryTransform):\n    \"\"\"\n    Transform the query given the evaluation feedback.\n    Args:\n        eval(Evaluation): An evaluation object.\n        llm(LLM): An LLM.\n        resynthesize_query(bool): Whether to resynthesize the query.\n        resynthesis_prompt(BasePromptTemplate): A prompt for resynthesizing the query.\n    \"\"\"\n    def __init__(",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.feedback_transform",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.feedback_transform",
        "description": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.feedback_transform",
        "peekOfCode": "logger = logging.getLogger(__name__)\nDEFAULT_RESYNTHESIS_PROMPT_TMPL = (\n    \"Here is the original query:\\n\"\n    \"{query_str}\\n\"\n    \"Here is the response given:\\n\"\n    \"{response}\\n\"\n    \"Here is some feedback from evaluator about the response given.\\n\"\n    \"{feedback}\\n\"\n    \"If you want to resynthesize the query, please return the modified query below.\\n\"\n    \"Otherwise, please return the original query.\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.feedback_transform",
        "documentation": {}
    },
    {
        "label": "DEFAULT_RESYNTHESIS_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.feedback_transform",
        "description": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.feedback_transform",
        "peekOfCode": "DEFAULT_RESYNTHESIS_PROMPT_TMPL = (\n    \"Here is the original query:\\n\"\n    \"{query_str}\\n\"\n    \"Here is the response given:\\n\"\n    \"{response}\\n\"\n    \"Here is some feedback from evaluator about the response given.\\n\"\n    \"{feedback}\\n\"\n    \"If you want to resynthesize the query, please return the modified query below.\\n\"\n    \"Otherwise, please return the original query.\\n\"\n)",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.feedback_transform",
        "documentation": {}
    },
    {
        "label": "DEFAULT_RESYNTHESIS_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.feedback_transform",
        "description": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.feedback_transform",
        "peekOfCode": "DEFAULT_RESYNTHESIS_PROMPT = PromptTemplate(DEFAULT_RESYNTHESIS_PROMPT_TMPL)\nclass FeedbackQueryTransformation(BaseQueryTransform):\n    \"\"\"\n    Transform the query given the evaluation feedback.\n    Args:\n        eval(Evaluation): An evaluation object.\n        llm(LLM): An LLM.\n        resynthesize_query(bool): Whether to resynthesize the query.\n        resynthesis_prompt(BasePromptTemplate): A prompt for resynthesizing the query.\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.feedback_transform",
        "documentation": {}
    },
    {
        "label": "DecomposeQueryTransformPrompt",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "peekOfCode": "DecomposeQueryTransformPrompt = PromptTemplate\n\"\"\"Step Decompose prompt for query transformation.\nPromptTemplate to \"decompose\" a query into another query\ngiven the existing context + previous reasoning (the previous steps).\nRequired template variables: `context_str`, `query_str`, `prev_reasoning`\n\"\"\"\nStepDecomposeQueryTransformPrompt = PromptTemplate\n\"\"\"Image output prompt for query transformation.\nPromptTemplate to add instructions for formatting image output.\nRequired template variables: `query_str`, `image_width`",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "documentation": {}
    },
    {
        "label": "StepDecomposeQueryTransformPrompt",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "peekOfCode": "StepDecomposeQueryTransformPrompt = PromptTemplate\n\"\"\"Image output prompt for query transformation.\nPromptTemplate to add instructions for formatting image output.\nRequired template variables: `query_str`, `image_width`\n\"\"\"\nImageOutputQueryTransformPrompt = PromptTemplate\nDEFAULT_DECOMPOSE_QUERY_TRANSFORM_TMPL = (\n    \"The original question is as follows: {query_str}\\n\"\n    \"We have an opportunity to answer some, or all of the question from a \"\n    \"knowledge source. \"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "documentation": {}
    },
    {
        "label": "ImageOutputQueryTransformPrompt",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "peekOfCode": "ImageOutputQueryTransformPrompt = PromptTemplate\nDEFAULT_DECOMPOSE_QUERY_TRANSFORM_TMPL = (\n    \"The original question is as follows: {query_str}\\n\"\n    \"We have an opportunity to answer some, or all of the question from a \"\n    \"knowledge source. \"\n    \"Context information for the knowledge source is provided below. \\n\"\n    \"Given the context, return a new question that can be answered from \"\n    \"the context. The question can be the same as the original question, \"\n    \"or a new question that represents a subcomponent of the overall question.\\n\"\n    \"As an example: \"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_DECOMPOSE_QUERY_TRANSFORM_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "peekOfCode": "DEFAULT_DECOMPOSE_QUERY_TRANSFORM_TMPL = (\n    \"The original question is as follows: {query_str}\\n\"\n    \"We have an opportunity to answer some, or all of the question from a \"\n    \"knowledge source. \"\n    \"Context information for the knowledge source is provided below. \\n\"\n    \"Given the context, return a new question that can be answered from \"\n    \"the context. The question can be the same as the original question, \"\n    \"or a new question that represents a subcomponent of the overall question.\\n\"\n    \"As an example: \"\n    \"\\n\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_DECOMPOSE_QUERY_TRANSFORM_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "peekOfCode": "DEFAULT_DECOMPOSE_QUERY_TRANSFORM_PROMPT = PromptTemplate(\n    DEFAULT_DECOMPOSE_QUERY_TRANSFORM_TMPL, prompt_type=PromptType.DECOMPOSE\n)\nDEFAULT_IMAGE_OUTPUT_TMPL = (\n    \"{query_str}\"\n    \"Show any image with a HTML <img/> tag with {image_width}.\"\n    'e.g., <image src=\"data/img.jpg\" width=\"{image_width}\" />.'\n)\nDEFAULT_IMAGE_OUTPUT_PROMPT = PromptTemplate(DEFAULT_IMAGE_OUTPUT_TMPL)\nDEFAULT_STEP_DECOMPOSE_QUERY_TRANSFORM_TMPL = (",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_IMAGE_OUTPUT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "peekOfCode": "DEFAULT_IMAGE_OUTPUT_TMPL = (\n    \"{query_str}\"\n    \"Show any image with a HTML <img/> tag with {image_width}.\"\n    'e.g., <image src=\"data/img.jpg\" width=\"{image_width}\" />.'\n)\nDEFAULT_IMAGE_OUTPUT_PROMPT = PromptTemplate(DEFAULT_IMAGE_OUTPUT_TMPL)\nDEFAULT_STEP_DECOMPOSE_QUERY_TRANSFORM_TMPL = (\n    \"The original question is as follows: {query_str}\\n\"\n    \"We have an opportunity to answer some, or all of the question from a \"\n    \"knowledge source. \"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_IMAGE_OUTPUT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "peekOfCode": "DEFAULT_IMAGE_OUTPUT_PROMPT = PromptTemplate(DEFAULT_IMAGE_OUTPUT_TMPL)\nDEFAULT_STEP_DECOMPOSE_QUERY_TRANSFORM_TMPL = (\n    \"The original question is as follows: {query_str}\\n\"\n    \"We have an opportunity to answer some, or all of the question from a \"\n    \"knowledge source. \"\n    \"Context information for the knowledge source is provided below, as \"\n    \"well as previous reasoning steps.\\n\"\n    \"Given the context and previous reasoning, return a question that can \"\n    \"be answered from \"\n    \"the context. This question can be the same as the original question, \"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_STEP_DECOMPOSE_QUERY_TRANSFORM_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "peekOfCode": "DEFAULT_STEP_DECOMPOSE_QUERY_TRANSFORM_TMPL = (\n    \"The original question is as follows: {query_str}\\n\"\n    \"We have an opportunity to answer some, or all of the question from a \"\n    \"knowledge source. \"\n    \"Context information for the knowledge source is provided below, as \"\n    \"well as previous reasoning steps.\\n\"\n    \"Given the context and previous reasoning, return a question that can \"\n    \"be answered from \"\n    \"the context. This question can be the same as the original question, \"\n    \"or this question can represent a subcomponent of the overall question.\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_STEP_DECOMPOSE_QUERY_TRANSFORM_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "peekOfCode": "DEFAULT_STEP_DECOMPOSE_QUERY_TRANSFORM_PROMPT = PromptTemplate(\n    DEFAULT_STEP_DECOMPOSE_QUERY_TRANSFORM_TMPL\n)",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.query.query_transform.prompts",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.query.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.query.base",
        "peekOfCode": "__all__ = [\n    \"BaseQueryEngine\",\n]",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.query.base",
        "documentation": {}
    },
    {
        "label": "get_top_k_embeddings",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.query.embedding_utils",
        "description": "reference_code.llama-index-core.llama_index.core.indices.query.embedding_utils",
        "peekOfCode": "def get_top_k_embeddings(\n    query_embedding: List[float],\n    embeddings: List[List[float]],\n    similarity_fn: Optional[Callable[..., float]] = None,\n    similarity_top_k: Optional[int] = None,\n    embedding_ids: Optional[List] = None,\n    similarity_cutoff: Optional[float] = None,\n) -> Tuple[List[float], List]:\n    \"\"\"Get top nodes by similarity to the query.\"\"\"\n    if embedding_ids is None:",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.query.embedding_utils",
        "documentation": {}
    },
    {
        "label": "get_top_k_embeddings_learner",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.query.embedding_utils",
        "description": "reference_code.llama-index-core.llama_index.core.indices.query.embedding_utils",
        "peekOfCode": "def get_top_k_embeddings_learner(\n    query_embedding: List[float],\n    embeddings: List[List[float]],\n    similarity_top_k: Optional[int] = None,\n    embedding_ids: Optional[List] = None,\n    query_mode: VectorStoreQueryMode = VectorStoreQueryMode.SVM,\n) -> Tuple[List[float], List]:\n    \"\"\"\n    Get top embeddings by fitting a learner against query.\n    Inspired by Karpathy's SVM demo:",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.query.embedding_utils",
        "documentation": {}
    },
    {
        "label": "get_top_k_mmr_embeddings",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.query.embedding_utils",
        "description": "reference_code.llama-index-core.llama_index.core.indices.query.embedding_utils",
        "peekOfCode": "def get_top_k_mmr_embeddings(\n    query_embedding: List[float],\n    embeddings: List[List[float]],\n    similarity_fn: Optional[Callable[..., float]] = None,\n    similarity_top_k: Optional[int] = None,\n    embedding_ids: Optional[List] = None,\n    similarity_cutoff: Optional[float] = None,\n    mmr_threshold: Optional[float] = None,\n) -> Tuple[List[float], List]:\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.query.embedding_utils",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.query.schema",
        "description": "reference_code.llama-index-core.llama_index.core.indices.query.schema",
        "peekOfCode": "__all__ = [\"QueryBundle\", \"QueryType\"]",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.query.schema",
        "documentation": {}
    },
    {
        "label": "BaseStructStoreIndex",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.base",
        "peekOfCode": "class BaseStructStoreIndex(BaseIndex[BST], Generic[BST]):\n    \"\"\"Base Struct Store Index.\"\"\"\n    def __init__(\n        self,\n        nodes: Optional[Sequence[BaseNode]] = None,\n        index_struct: Optional[BST] = None,\n        schema_extract_prompt: Optional[BasePromptTemplate] = None,\n        output_parser: Optional[OUTPUT_PARSER_TYPE] = None,\n        **kwargs: Any,\n    ) -> None:",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.base",
        "documentation": {}
    },
    {
        "label": "default_output_parser",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.base",
        "peekOfCode": "def default_output_parser(output: str) -> Optional[Dict[str, Any]]:\n    \"\"\"\n    Parse output of schema extraction.\n    Attempt to parse the following format from the default prompt:\n    field1: <value>, field2: <value>, ...\n    \"\"\"\n    tups = output.split(\"\\n\")\n    fields = {}\n    for tup in tups:\n        if \":\" in tup:",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.base",
        "documentation": {}
    },
    {
        "label": "BST",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.base",
        "peekOfCode": "BST = TypeVar(\"BST\", bound=BaseStructTable)\ndef default_output_parser(output: str) -> Optional[Dict[str, Any]]:\n    \"\"\"\n    Parse output of schema extraction.\n    Attempt to parse the following format from the default prompt:\n    field1: <value>, field2: <value>, ...\n    \"\"\"\n    tups = output.split(\"\\n\")\n    fields = {}\n    for tup in tups:",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.base",
        "documentation": {}
    },
    {
        "label": "OUTPUT_PARSER_TYPE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.base",
        "peekOfCode": "OUTPUT_PARSER_TYPE = Callable[[str], Optional[Dict[str, Any]]]\nclass BaseStructStoreIndex(BaseIndex[BST], Generic[BST]):\n    \"\"\"Base Struct Store Index.\"\"\"\n    def __init__(\n        self,\n        nodes: Optional[Sequence[BaseNode]] = None,\n        index_struct: Optional[BST] = None,\n        schema_extract_prompt: Optional[BasePromptTemplate] = None,\n        output_parser: Optional[OUTPUT_PARSER_TYPE] = None,\n        **kwargs: Any,",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.base",
        "documentation": {}
    },
    {
        "label": "SQLContextContainerBuilder",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.container_builder",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.container_builder",
        "peekOfCode": "class SQLContextContainerBuilder:\n    \"\"\"\n    SQLContextContainerBuilder.\n    Build a SQLContextContainer that can be passed to the SQL index\n    during index construction or during query-time.\n    NOTE: if context_str is specified, that will be used as context\n    instead of context_dict\n    Args:\n        sql_database (SQLDatabase): SQL database\n        context_dict (Optional[Dict[str, str]]): context dict",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.container_builder",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CONTEXT_QUERY_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.container_builder",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.container_builder",
        "peekOfCode": "DEFAULT_CONTEXT_QUERY_TMPL = (\n    \"Please return the relevant tables (including the full schema) \"\n    \"for the following query: {orig_query_str}\"\n)\nclass SQLContextContainerBuilder:\n    \"\"\"\n    SQLContextContainerBuilder.\n    Build a SQLContextContainer that can be passed to the SQL index\n    during index construction or during query-time.\n    NOTE: if context_str is specified, that will be used as context",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.container_builder",
        "documentation": {}
    },
    {
        "label": "JSONQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.json_query",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.json_query",
        "peekOfCode": "class JSONQueryEngine(BaseQueryEngine):\n    \"\"\"\n    GPT JSON Query Engine.\n    Converts natural language to JSON Path queries.\n    Args:\n        json_value (JSONType): JSON value\n        json_schema (JSONType): JSON schema\n        json_path_prompt (BasePromptTemplate): The JSON Path prompt to use.\n        output_processor (Callable): The output processor that executes the\n            JSON Path query.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.json_query",
        "documentation": {}
    },
    {
        "label": "default_output_response_parser",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.json_query",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.json_query",
        "peekOfCode": "def default_output_response_parser(llm_output: str) -> str:\n    \"\"\"Attempts to parse the JSON path prompt output. Only applicable if the default prompt is used.\"\"\"\n    try:\n        llm_output_parsed = re.search(  # type: ignore\n            pattern=r\"JSONPath:\\s+(.*)\", string=llm_output\n        ).groups()[0]\n    except Exception:\n        logger.warning(\n            f\"JSON Path could not be parsed in the LLM response after the 'JSONPath' identifier. \"\n            \"Try passing a custom JSON path prompt and processor. \"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.json_query",
        "documentation": {}
    },
    {
        "label": "default_output_processor",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.json_query",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.json_query",
        "peekOfCode": "def default_output_processor(llm_output: str, json_value: JSONType) -> Dict[str, str]:\n    \"\"\"Default output processor that extracts values based on JSON Path expressions.\"\"\"\n    # Post-process the LLM output to remove the JSONPath: prefix\n    llm_output = llm_output.replace(\"JSONPath: \", \"\").replace(\"JSON Path: \", \"\").strip()\n    # Split the given string into separate JSON Path expressions\n    expressions = [expr.strip() for expr in llm_output.split(\",\")]\n    try:\n        from jsonpath_ng.ext import parse  # pants: no-infer-dep\n        from jsonpath_ng.jsonpath import DatumInContext  # pants: no-infer-dep\n    except ImportError as exc:",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.json_query",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.json_query",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.json_query",
        "peekOfCode": "logger = logging.getLogger(__name__)\nIMPORT_ERROR_MSG = (\n    \"`jsonpath_ng` package not found, please run `pip install jsonpath-ng`\"\n)\nJSONType = Union[Dict[str, \"JSONType\"], List[\"JSONType\"], str, int, float, bool, None]\nDEFAULT_RESPONSE_SYNTHESIS_PROMPT_TMPL = (\n    \"Given a query, synthesize a response \"\n    \"to satisfy the query using the JSON results. \"\n    \"Only include details that are relevant to the query. \"\n    \"If you don't know the answer, then say that.\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.json_query",
        "documentation": {}
    },
    {
        "label": "IMPORT_ERROR_MSG",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.json_query",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.json_query",
        "peekOfCode": "IMPORT_ERROR_MSG = (\n    \"`jsonpath_ng` package not found, please run `pip install jsonpath-ng`\"\n)\nJSONType = Union[Dict[str, \"JSONType\"], List[\"JSONType\"], str, int, float, bool, None]\nDEFAULT_RESPONSE_SYNTHESIS_PROMPT_TMPL = (\n    \"Given a query, synthesize a response \"\n    \"to satisfy the query using the JSON results. \"\n    \"Only include details that are relevant to the query. \"\n    \"If you don't know the answer, then say that.\\n\"\n    \"JSON Schema: {json_schema}\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.json_query",
        "documentation": {}
    },
    {
        "label": "JSONType",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.json_query",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.json_query",
        "peekOfCode": "JSONType = Union[Dict[str, \"JSONType\"], List[\"JSONType\"], str, int, float, bool, None]\nDEFAULT_RESPONSE_SYNTHESIS_PROMPT_TMPL = (\n    \"Given a query, synthesize a response \"\n    \"to satisfy the query using the JSON results. \"\n    \"Only include details that are relevant to the query. \"\n    \"If you don't know the answer, then say that.\\n\"\n    \"JSON Schema: {json_schema}\\n\"\n    \"JSON Path: {json_path}\\n\"\n    \"Value at path: {json_path_value}\\n\"\n    \"Query: {query_str}\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.json_query",
        "documentation": {}
    },
    {
        "label": "DEFAULT_RESPONSE_SYNTHESIS_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.json_query",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.json_query",
        "peekOfCode": "DEFAULT_RESPONSE_SYNTHESIS_PROMPT_TMPL = (\n    \"Given a query, synthesize a response \"\n    \"to satisfy the query using the JSON results. \"\n    \"Only include details that are relevant to the query. \"\n    \"If you don't know the answer, then say that.\\n\"\n    \"JSON Schema: {json_schema}\\n\"\n    \"JSON Path: {json_path}\\n\"\n    \"Value at path: {json_path_value}\\n\"\n    \"Query: {query_str}\\n\"\n    \"Response: \"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.json_query",
        "documentation": {}
    },
    {
        "label": "DEFAULT_RESPONSE_SYNTHESIS_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.json_query",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.json_query",
        "peekOfCode": "DEFAULT_RESPONSE_SYNTHESIS_PROMPT = PromptTemplate(\n    DEFAULT_RESPONSE_SYNTHESIS_PROMPT_TMPL,\n    prompt_type=PromptType.SQL_RESPONSE_SYNTHESIS,\n)\ndef default_output_response_parser(llm_output: str) -> str:\n    \"\"\"Attempts to parse the JSON path prompt output. Only applicable if the default prompt is used.\"\"\"\n    try:\n        llm_output_parsed = re.search(  # type: ignore\n            pattern=r\"JSONPath:\\s+(.*)\", string=llm_output\n        ).groups()[0]",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.json_query",
        "documentation": {}
    },
    {
        "label": "PandasIndex",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.pandas",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.pandas",
        "peekOfCode": "class PandasIndex:\n    def __init__(\n        self,\n        *args: Any,\n        **kwargs: Any,\n    ) -> None:\n        raise DeprecationWarning(\n            \"PandasQueryEngine has been moved to `llama-index-experimental`.\\n\"\n            \"`pip install llama-index-experimental`\\n\"\n            \"`from llama_index.experimental.query_engine import PandasQueryEngine`\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.pandas",
        "documentation": {}
    },
    {
        "label": "GPTPandasIndex",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.pandas",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.pandas",
        "peekOfCode": "GPTPandasIndex = PandasIndex",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.pandas",
        "documentation": {}
    },
    {
        "label": "SQLQueryMode",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql",
        "peekOfCode": "class SQLQueryMode(str, Enum):\n    SQL = \"sql\"\n    NL = \"nl\"\nclass SQLStructStoreIndex(BaseStructStoreIndex[SQLStructTable]):\n    \"\"\"\n    SQL Struct Store Index.\n    The SQLStructStoreIndex is an index that uses a SQL database\n    under the hood. During index construction, the data can be inferred\n    from unstructured documents given a schema extract prompt,\n    or it can be pre-loaded in the database.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql",
        "documentation": {}
    },
    {
        "label": "SQLStructStoreIndex",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql",
        "peekOfCode": "class SQLStructStoreIndex(BaseStructStoreIndex[SQLStructTable]):\n    \"\"\"\n    SQL Struct Store Index.\n    The SQLStructStoreIndex is an index that uses a SQL database\n    under the hood. During index construction, the data can be inferred\n    from unstructured documents given a schema extract prompt,\n    or it can be pre-loaded in the database.\n    During query time, the user can either specify a raw SQL query\n    or a natural language query to retrieve their data.\n    NOTE: this is deprecated.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql",
        "documentation": {}
    },
    {
        "label": "GPTSQLStructStoreIndex",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql",
        "peekOfCode": "GPTSQLStructStoreIndex = SQLStructStoreIndex",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql",
        "documentation": {}
    },
    {
        "label": "SQLStructStoreQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "peekOfCode": "class SQLStructStoreQueryEngine(BaseQueryEngine):\n    \"\"\"\n    GPT SQL query engine over a structured database.\n    NOTE: deprecated in favor of SQLTableRetriever, kept for backward compatibility.\n    Runs raw SQL over a SQLStructStoreIndex. No LLM calls are made here.\n    NOTE: this query cannot work with composed indices - if the index\n    contains subindices, those subindices will not be queried.\n    NOTE: Any Text-to-SQL application should be aware that executing\n    arbitrary SQL queries can be a security risk. It is recommended to\n    take precautions as needed, such as using restricted roles, read-only",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "documentation": {}
    },
    {
        "label": "NLStructStoreQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "peekOfCode": "class NLStructStoreQueryEngine(BaseQueryEngine):\n    \"\"\"\n    GPT natural language query engine over a structured database.\n    NOTE: deprecated in favor of SQLTableRetriever, kept for backward compatibility.\n    Given a natural language query, we will extract the query to SQL.\n    Runs raw SQL over a SQLStructStoreIndex. No LLM calls are made during\n    the SQL execution.\n    NOTE: this query cannot work with composed indices - if the index\n    contains subindices, those subindices will not be queried.\n    NOTE: Any Text-to-SQL application should be aware that executing",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "documentation": {}
    },
    {
        "label": "BaseSQLTableQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "peekOfCode": "class BaseSQLTableQueryEngine(BaseQueryEngine):\n    \"\"\"\n    Base SQL Table query engine.\n    NOTE: Any Text-to-SQL application should be aware that executing\n    arbitrary SQL queries can be a security risk. It is recommended to\n    take precautions as needed, such as using restricted roles, read-only\n    databases, sandboxing, etc.\n    \"\"\"\n    def __init__(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "documentation": {}
    },
    {
        "label": "NLSQLTableQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "peekOfCode": "class NLSQLTableQueryEngine(BaseSQLTableQueryEngine):\n    \"\"\"\n    Natural language SQL Table query engine.\n    Read NLStructStoreQueryEngine's docstring for more info on NL SQL.\n    NOTE: Any Text-to-SQL application should be aware that executing\n    arbitrary SQL queries can be a security risk. It is recommended to\n    take precautions as needed, such as using restricted roles, read-only\n    databases, sandboxing, etc.\n    \"\"\"\n    def __init__(",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "documentation": {}
    },
    {
        "label": "PGVectorSQLQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "peekOfCode": "class PGVectorSQLQueryEngine(BaseSQLTableQueryEngine):\n    \"\"\"\n    PGvector SQL query engine.\n    A modified version of the normal text-to-SQL query engine because\n    we can infer embedding vectors in the sql query.\n    NOTE: this is a beta feature\n    NOTE: Any Text-to-SQL application should be aware that executing\n    arbitrary SQL queries can be a security risk. It is recommended to\n    take precautions as needed, such as using restricted roles, read-only\n    databases, sandboxing, etc.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "documentation": {}
    },
    {
        "label": "SQLTableRetrieverQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "peekOfCode": "class SQLTableRetrieverQueryEngine(BaseSQLTableQueryEngine):\n    \"\"\"SQL Table retriever query engine.\"\"\"\n    def __init__(\n        self,\n        sql_database: SQLDatabase,\n        table_retriever: ObjectRetriever[SQLTableSchema],\n        rows_retrievers: Optional[dict[str, BaseRetriever]] = None,\n        cols_retrievers: Optional[dict[str, dict[str, BaseRetriever]]] = None,\n        llm: Optional[LLM] = None,\n        text_to_sql_prompt: Optional[BasePromptTemplate] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "peekOfCode": "logger = logging.getLogger(__name__)\n# **NOTE**: deprecated (for older versions of sql query engine)\nDEFAULT_RESPONSE_SYNTHESIS_PROMPT_TMPL = (\n    \"Given an input question, synthesize a response from the query results.\\n\"\n    \"Query: {query_str}\\n\"\n    \"SQL: {sql_query}\\n\"\n    \"SQL Response: {sql_response_str}\\n\"\n    \"Response: \"\n)\nDEFAULT_RESPONSE_SYNTHESIS_PROMPT = PromptTemplate(",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "documentation": {}
    },
    {
        "label": "DEFAULT_RESPONSE_SYNTHESIS_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "peekOfCode": "DEFAULT_RESPONSE_SYNTHESIS_PROMPT_TMPL = (\n    \"Given an input question, synthesize a response from the query results.\\n\"\n    \"Query: {query_str}\\n\"\n    \"SQL: {sql_query}\\n\"\n    \"SQL Response: {sql_response_str}\\n\"\n    \"Response: \"\n)\nDEFAULT_RESPONSE_SYNTHESIS_PROMPT = PromptTemplate(\n    DEFAULT_RESPONSE_SYNTHESIS_PROMPT_TMPL,\n    prompt_type=PromptType.SQL_RESPONSE_SYNTHESIS,",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "documentation": {}
    },
    {
        "label": "DEFAULT_RESPONSE_SYNTHESIS_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "peekOfCode": "DEFAULT_RESPONSE_SYNTHESIS_PROMPT = PromptTemplate(\n    DEFAULT_RESPONSE_SYNTHESIS_PROMPT_TMPL,\n    prompt_type=PromptType.SQL_RESPONSE_SYNTHESIS,\n)\n# **NOTE**: newer version of sql query engine\nDEFAULT_RESPONSE_SYNTHESIS_PROMPT_TMPL_V2 = (\n    \"Given an input question, synthesize a response from the query results.\\n\"\n    \"Query: {query_str}\\n\"\n    \"SQL: {sql_query}\\n\"\n    \"SQL Response: {context_str}\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "documentation": {}
    },
    {
        "label": "DEFAULT_RESPONSE_SYNTHESIS_PROMPT_TMPL_V2",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "peekOfCode": "DEFAULT_RESPONSE_SYNTHESIS_PROMPT_TMPL_V2 = (\n    \"Given an input question, synthesize a response from the query results.\\n\"\n    \"Query: {query_str}\\n\"\n    \"SQL: {sql_query}\\n\"\n    \"SQL Response: {context_str}\\n\"\n    \"Response: \"\n)\nDEFAULT_RESPONSE_SYNTHESIS_PROMPT_V2 = PromptTemplate(\n    DEFAULT_RESPONSE_SYNTHESIS_PROMPT_TMPL_V2,\n    prompt_type=PromptType.SQL_RESPONSE_SYNTHESIS_V2,",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "documentation": {}
    },
    {
        "label": "DEFAULT_RESPONSE_SYNTHESIS_PROMPT_V2",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "peekOfCode": "DEFAULT_RESPONSE_SYNTHESIS_PROMPT_V2 = PromptTemplate(\n    DEFAULT_RESPONSE_SYNTHESIS_PROMPT_TMPL_V2,\n    prompt_type=PromptType.SQL_RESPONSE_SYNTHESIS_V2,\n)\nclass SQLStructStoreQueryEngine(BaseQueryEngine):\n    \"\"\"\n    GPT SQL query engine over a structured database.\n    NOTE: deprecated in favor of SQLTableRetriever, kept for backward compatibility.\n    Runs raw SQL over a SQLStructStoreIndex. No LLM calls are made here.\n    NOTE: this query cannot work with composed indices - if the index",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "documentation": {}
    },
    {
        "label": "GPTNLStructStoreQueryEngine",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "peekOfCode": "GPTNLStructStoreQueryEngine = NLStructStoreQueryEngine\nGPTSQLStructStoreQueryEngine = SQLStructStoreQueryEngine",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "documentation": {}
    },
    {
        "label": "GPTSQLStructStoreQueryEngine",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "peekOfCode": "GPTSQLStructStoreQueryEngine = SQLStructStoreQueryEngine",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_query",
        "documentation": {}
    },
    {
        "label": "SQLRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_retriever",
        "peekOfCode": "class SQLRetriever(BaseRetriever):\n    \"\"\"\n    SQL Retriever.\n    Retrieves via raw SQL statements.\n    Args:\n        sql_database (SQLDatabase): SQL database.\n        return_raw (bool): Whether to return raw results or format results.\n            Defaults to True.\n    \"\"\"\n    def __init__(",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_retriever",
        "documentation": {}
    },
    {
        "label": "SQLParserMode",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_retriever",
        "peekOfCode": "class SQLParserMode(str, Enum):\n    \"\"\"SQL Parser Mode.\"\"\"\n    DEFAULT = \"default\"\n    PGVECTOR = \"pgvector\"\nclass BaseSQLParser(DispatcherSpanMixin, ABC):\n    \"\"\"Base SQL Parser.\"\"\"\n    @abstractmethod\n    def parse_response_to_sql(self, response: str, query_bundle: QueryBundle) -> str:\n        \"\"\"Parse response to SQL.\"\"\"\nclass DefaultSQLParser(BaseSQLParser):",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_retriever",
        "documentation": {}
    },
    {
        "label": "BaseSQLParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_retriever",
        "peekOfCode": "class BaseSQLParser(DispatcherSpanMixin, ABC):\n    \"\"\"Base SQL Parser.\"\"\"\n    @abstractmethod\n    def parse_response_to_sql(self, response: str, query_bundle: QueryBundle) -> str:\n        \"\"\"Parse response to SQL.\"\"\"\nclass DefaultSQLParser(BaseSQLParser):\n    \"\"\"Default SQL Parser.\"\"\"\n    def parse_response_to_sql(self, response: str, query_bundle: QueryBundle) -> str:\n        \"\"\"Parse response to SQL.\"\"\"\n        sql_query_start = response.find(\"SQLQuery:\")",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_retriever",
        "documentation": {}
    },
    {
        "label": "DefaultSQLParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_retriever",
        "peekOfCode": "class DefaultSQLParser(BaseSQLParser):\n    \"\"\"Default SQL Parser.\"\"\"\n    def parse_response_to_sql(self, response: str, query_bundle: QueryBundle) -> str:\n        \"\"\"Parse response to SQL.\"\"\"\n        sql_query_start = response.find(\"SQLQuery:\")\n        if sql_query_start != -1:\n            response = response[sql_query_start:]\n            # TODO: move to removeprefix after Python 3.9+\n            if response.startswith(\"SQLQuery:\"):\n                response = response[len(\"SQLQuery:\") :]",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_retriever",
        "documentation": {}
    },
    {
        "label": "PGVectorSQLParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_retriever",
        "peekOfCode": "class PGVectorSQLParser(BaseSQLParser):\n    \"\"\"PGVector SQL Parser.\"\"\"\n    def __init__(\n        self,\n        embed_model: BaseEmbedding,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._embed_model = embed_model\n    def parse_response_to_sql(self, response: str, query_bundle: QueryBundle) -> str:\n        \"\"\"Parse response to SQL.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_retriever",
        "documentation": {}
    },
    {
        "label": "NLSQLRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_retriever",
        "peekOfCode": "class NLSQLRetriever(BaseRetriever, PromptMixin):\n    \"\"\"\n    Text-to-SQL Retriever.\n    Retrieves via text.\n    Args:\n        sql_database (SQLDatabase): SQL database.\n        text_to_sql_prompt (BasePromptTemplate): Prompt template for text-to-sql.\n            Defaults to DEFAULT_TEXT_TO_SQL_PROMPT.\n        context_query_kwargs (dict): Mapping from table name to context query.\n            Defaults to None.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_retriever",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_retriever",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass SQLRetriever(BaseRetriever):\n    \"\"\"\n    SQL Retriever.\n    Retrieves via raw SQL statements.\n    Args:\n        sql_database (SQLDatabase): SQL database.\n        return_raw (bool): Whether to return raw results or format results.\n            Defaults to True.\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.struct_store.sql_retriever",
        "documentation": {}
    },
    {
        "label": "TreeAllLeafRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.tree.all_leaf_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.indices.tree.all_leaf_retriever",
        "peekOfCode": "class TreeAllLeafRetriever(BaseRetriever):\n    \"\"\"\n    GPT all leaf retriever.\n    This class builds a query-specific tree from leaf nodes to return a response.\n    Using this query mode means that the tree index doesn't need to be built\n    when initialized, since we rebuild the tree for each query.\n    Args:\n        text_qa_template (Optional[BasePromptTemplate]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.tree.all_leaf_retriever",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.tree.all_leaf_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.indices.tree.all_leaf_retriever",
        "peekOfCode": "logger = logging.getLogger(__name__)\nDEFAULT_NUM_CHILDREN = 10\nclass TreeAllLeafRetriever(BaseRetriever):\n    \"\"\"\n    GPT all leaf retriever.\n    This class builds a query-specific tree from leaf nodes to return a response.\n    Using this query mode means that the tree index doesn't need to be built\n    when initialized, since we rebuild the tree for each query.\n    Args:\n        text_qa_template (Optional[BasePromptTemplate]): Question-Answer Prompt",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.tree.all_leaf_retriever",
        "documentation": {}
    },
    {
        "label": "DEFAULT_NUM_CHILDREN",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.tree.all_leaf_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.indices.tree.all_leaf_retriever",
        "peekOfCode": "DEFAULT_NUM_CHILDREN = 10\nclass TreeAllLeafRetriever(BaseRetriever):\n    \"\"\"\n    GPT all leaf retriever.\n    This class builds a query-specific tree from leaf nodes to return a response.\n    Using this query mode means that the tree index doesn't need to be built\n    when initialized, since we rebuild the tree for each query.\n    Args:\n        text_qa_template (Optional[BasePromptTemplate]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.tree.all_leaf_retriever",
        "documentation": {}
    },
    {
        "label": "TreeRetrieverMode",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.tree.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.tree.base",
        "peekOfCode": "class TreeRetrieverMode(str, Enum):\n    SELECT_LEAF = \"select_leaf\"\n    SELECT_LEAF_EMBEDDING = \"select_leaf_embedding\"\n    ALL_LEAF = \"all_leaf\"\n    ROOT = \"root\"\nREQUIRE_TREE_MODES = {\n    TreeRetrieverMode.SELECT_LEAF,\n    TreeRetrieverMode.SELECT_LEAF_EMBEDDING,\n    TreeRetrieverMode.ROOT,\n}",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.tree.base",
        "documentation": {}
    },
    {
        "label": "TreeIndex",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.tree.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.tree.base",
        "peekOfCode": "class TreeIndex(BaseIndex[IndexGraph]):\n    \"\"\"\n    Tree Index.\n    The tree index is a tree-structured index, where each node is a summary of\n    the children nodes. During index construction, the tree is constructed\n    in a bottoms-up fashion until we end up with a set of root_nodes.\n    There are a few different options during query time (see :ref:`Ref-Query`).\n    The main option is to traverse down the tree from the root nodes.\n    A secondary answer is to directly synthesize the answer from the root nodes.\n    Args:",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.tree.base",
        "documentation": {}
    },
    {
        "label": "REQUIRE_TREE_MODES",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.tree.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.tree.base",
        "peekOfCode": "REQUIRE_TREE_MODES = {\n    TreeRetrieverMode.SELECT_LEAF,\n    TreeRetrieverMode.SELECT_LEAF_EMBEDDING,\n    TreeRetrieverMode.ROOT,\n}\nclass TreeIndex(BaseIndex[IndexGraph]):\n    \"\"\"\n    Tree Index.\n    The tree index is a tree-structured index, where each node is a summary of\n    the children nodes. During index construction, the tree is constructed",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.tree.base",
        "documentation": {}
    },
    {
        "label": "GPTTreeIndex",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.tree.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.tree.base",
        "peekOfCode": "GPTTreeIndex = TreeIndex",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.tree.base",
        "documentation": {}
    },
    {
        "label": "TreeIndexInserter",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.tree.inserter",
        "description": "reference_code.llama-index-core.llama_index.core.indices.tree.inserter",
        "peekOfCode": "class TreeIndexInserter:\n    \"\"\"LlamaIndex inserter.\"\"\"\n    def __init__(\n        self,\n        index_graph: IndexGraph,\n        llm: Optional[LLM] = None,\n        num_children: int = 10,\n        insert_prompt: BasePromptTemplate = DEFAULT_INSERT_PROMPT,\n        summary_prompt: BasePromptTemplate = DEFAULT_SUMMARY_PROMPT,\n        docstore: Optional[BaseDocumentStore] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.tree.inserter",
        "documentation": {}
    },
    {
        "label": "TreeSelectLeafEmbeddingRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.tree.select_leaf_embedding_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.indices.tree.select_leaf_embedding_retriever",
        "peekOfCode": "class TreeSelectLeafEmbeddingRetriever(TreeSelectLeafRetriever):\n    \"\"\"\n    Tree select leaf embedding retriever.\n    This class traverses the index graph using the embedding similarity between the\n    query and the node text.\n    Args:\n        query_template (Optional[BasePromptTemplate]): Tree Select Query Prompt\n            (see :ref:`Prompt-Templates`).\n        query_template_multiple (Optional[BasePromptTemplate]): Tree Select\n            Query Prompt (Multiple)",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.tree.select_leaf_embedding_retriever",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.tree.select_leaf_embedding_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.indices.tree.select_leaf_embedding_retriever",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass TreeSelectLeafEmbeddingRetriever(TreeSelectLeafRetriever):\n    \"\"\"\n    Tree select leaf embedding retriever.\n    This class traverses the index graph using the embedding similarity between the\n    query and the node text.\n    Args:\n        query_template (Optional[BasePromptTemplate]): Tree Select Query Prompt\n            (see :ref:`Prompt-Templates`).\n        query_template_multiple (Optional[BasePromptTemplate]): Tree Select",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.tree.select_leaf_embedding_retriever",
        "documentation": {}
    },
    {
        "label": "TreeSelectLeafRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.tree.select_leaf_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.indices.tree.select_leaf_retriever",
        "peekOfCode": "class TreeSelectLeafRetriever(BaseRetriever):\n    \"\"\"\n    Tree select leaf retriever.\n    This class traverses the index graph and searches for a leaf node that can best\n    answer the query.\n    Args:\n        query_template (Optional[BasePromptTemplate]): Tree Select Query Prompt\n            (see :ref:`Prompt-Templates`).\n        query_template_multiple (Optional[BasePromptTemplate]): Tree Select\n            Query Prompt (Multiple)",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.tree.select_leaf_retriever",
        "documentation": {}
    },
    {
        "label": "get_text_from_node",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.tree.select_leaf_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.indices.tree.select_leaf_retriever",
        "peekOfCode": "def get_text_from_node(\n    node: BaseNode,\n    level: Optional[int] = None,\n    verbose: bool = False,\n) -> str:\n    \"\"\"Get text from node.\"\"\"\n    level_str = \"\" if level is None else f\"[Level {level}]\"\n    fmt_text_chunk = truncate_text(node.get_content(metadata_mode=MetadataMode.LLM), 50)\n    logger.debug(f\">{level_str} Searching in chunk: {fmt_text_chunk}\")\n    response_txt = node.get_content(metadata_mode=MetadataMode.LLM)",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.tree.select_leaf_retriever",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.tree.select_leaf_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.indices.tree.select_leaf_retriever",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef get_text_from_node(\n    node: BaseNode,\n    level: Optional[int] = None,\n    verbose: bool = False,\n) -> str:\n    \"\"\"Get text from node.\"\"\"\n    level_str = \"\" if level is None else f\"[Level {level}]\"\n    fmt_text_chunk = truncate_text(node.get_content(metadata_mode=MetadataMode.LLM), 50)\n    logger.debug(f\">{level_str} Searching in chunk: {fmt_text_chunk}\")",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.tree.select_leaf_retriever",
        "documentation": {}
    },
    {
        "label": "TreeRootRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.tree.tree_root_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.indices.tree.tree_root_retriever",
        "peekOfCode": "class TreeRootRetriever(BaseRetriever):\n    \"\"\"\n    Tree root retriever.\n    This class directly retrieves the answer from the root nodes.\n    Unlike GPTTreeIndexLeafQuery, this class assumes the graph already stores\n    the answer (because it was constructed with a query_str), so it does not\n    attempt to parse information down the graph in order to synthesize an answer.\n    \"\"\"\n    def __init__(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.tree.tree_root_retriever",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.tree.tree_root_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.indices.tree.tree_root_retriever",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass TreeRootRetriever(BaseRetriever):\n    \"\"\"\n    Tree root retriever.\n    This class directly retrieves the answer from the root nodes.\n    Unlike GPTTreeIndexLeafQuery, this class assumes the graph already stores\n    the answer (because it was constructed with a query_str), so it does not\n    attempt to parse information down the graph in order to synthesize an answer.\n    \"\"\"\n    def __init__(",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.tree.tree_root_retriever",
        "documentation": {}
    },
    {
        "label": "get_numbered_text_from_nodes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.tree.utils",
        "description": "reference_code.llama-index-core.llama_index.core.indices.tree.utils",
        "peekOfCode": "def get_numbered_text_from_nodes(\n    node_list: List[BaseNode],\n    text_splitter: Optional[TokenTextSplitter] = None,\n) -> str:\n    \"\"\"\n    Get text from nodes in the format of a numbered list.\n    Used by tree-structured indices.\n    \"\"\"\n    results = []\n    number = 1",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.tree.utils",
        "documentation": {}
    },
    {
        "label": "VectorIndexAutoRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.auto_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.auto_retriever",
        "peekOfCode": "class VectorIndexAutoRetriever(BaseAutoRetriever):\n    \"\"\"\n    Vector store auto retriever.\n    A retriever for vector store index that uses an LLM to automatically set\n    vector store query parameters.\n    Args:\n        index (VectorStoreIndex): vector store index\n        vector_store_info (VectorStoreInfo): additional information about\n            vector store content and supported metadata filters. The natural language\n            description is used by an LLM to automatically set vector store query",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.auto_retriever",
        "documentation": {}
    },
    {
        "label": "_logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.auto_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.auto_retriever",
        "peekOfCode": "_logger = logging.getLogger(__name__)\nclass VectorIndexAutoRetriever(BaseAutoRetriever):\n    \"\"\"\n    Vector store auto retriever.\n    A retriever for vector store index that uses an LLM to automatically set\n    vector store query parameters.\n    Args:\n        index (VectorStoreIndex): vector store index\n        vector_store_info (VectorStoreInfo): additional information about\n            vector store content and supported metadata filters. The natural language",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.auto_retriever",
        "documentation": {}
    },
    {
        "label": "VectorStoreQueryOutputParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.output_parser",
        "description": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.output_parser",
        "peekOfCode": "class VectorStoreQueryOutputParser(BaseOutputParser):\n    def parse(self, output: str) -> Any:\n        json_dict = parse_json_markdown(output)\n        query_and_filters = VectorStoreQuerySpec.model_validate(json_dict)\n        return StructuredOutput(raw_output=output, parsed_output=query_and_filters)\n    def format(self, prompt_template: str) -> str:\n        return prompt_template",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.output_parser",
        "documentation": {}
    },
    {
        "label": "PREFIX",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "peekOfCode": "PREFIX = \"\"\"\\\nYour goal is to structure the user's query to match the request schema provided below.\n<< Structured Request Schema >>\nWhen responding use a markdown code snippet with a JSON object formatted in the \\\nfollowing schema:\n{schema_str}\nThe query string should contain only text that is expected to match the contents of \\\ndocuments. Any conditions in the filter should not be mentioned in the query as well.\nMake sure that filters only refer to attributes that exist in the data source.\nMake sure that filters take into account the descriptions of attributes.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "documentation": {}
    },
    {
        "label": "example_info",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "peekOfCode": "example_info = VectorStoreInfo(\n    content_info=\"Lyrics of a song\",\n    metadata_info=[\n        MetadataInfo(name=\"artist\", type=\"str\", description=\"Name of the song artist\"),\n        MetadataInfo(\n            name=\"genre\",\n            type=\"str\",\n            description='The song genre, one of \"pop\", \"rock\" or \"rap\"',\n        ),\n    ],",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "documentation": {}
    },
    {
        "label": "example_query",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "peekOfCode": "example_query = \"What are songs by Taylor Swift or Katy Perry in the dance pop genre\"\nexample_output = VectorStoreQuerySpec(\n    query=\"teenager love\",\n    filters=[\n        MetadataFilter(key=\"artist\", value=\"Taylor Swift\"),\n        MetadataFilter(key=\"artist\", value=\"Katy Perry\"),\n        MetadataFilter(key=\"genre\", value=\"pop\"),\n    ],\n)\nexample_info_2 = VectorStoreInfo(",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "documentation": {}
    },
    {
        "label": "example_output",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "peekOfCode": "example_output = VectorStoreQuerySpec(\n    query=\"teenager love\",\n    filters=[\n        MetadataFilter(key=\"artist\", value=\"Taylor Swift\"),\n        MetadataFilter(key=\"artist\", value=\"Katy Perry\"),\n        MetadataFilter(key=\"genre\", value=\"pop\"),\n    ],\n)\nexample_info_2 = VectorStoreInfo(\n    content_info=\"Classic literature\",",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "documentation": {}
    },
    {
        "label": "example_info_2",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "peekOfCode": "example_info_2 = VectorStoreInfo(\n    content_info=\"Classic literature\",\n    metadata_info=[\n        MetadataInfo(name=\"author\", type=\"str\", description=\"Author name\"),\n        MetadataInfo(\n            name=\"book_title\",\n            type=\"str\",\n            description=\"Book title\",\n        ),\n        MetadataInfo(",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "documentation": {}
    },
    {
        "label": "example_query_2",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "peekOfCode": "example_query_2 = \"What are some books by Jane Austen published after 1813 that explore the theme of marriage for social standing?\"\nexample_output_2 = VectorStoreQuerySpec(\n    query=\"Books related to theme of marriage for social standing\",\n    filters=[\n        MetadataFilter(key=\"year\", value=\"1813\", operator=FilterOperator.GT),\n        MetadataFilter(key=\"author\", value=\"Jane Austen\"),\n    ],\n)\nEXAMPLES = f\"\"\"\\\n<< Example 1. >>",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "documentation": {}
    },
    {
        "label": "example_output_2",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "peekOfCode": "example_output_2 = VectorStoreQuerySpec(\n    query=\"Books related to theme of marriage for social standing\",\n    filters=[\n        MetadataFilter(key=\"year\", value=\"1813\", operator=FilterOperator.GT),\n        MetadataFilter(key=\"author\", value=\"Jane Austen\"),\n    ],\n)\nEXAMPLES = f\"\"\"\\\n<< Example 1. >>\nData Source:",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "documentation": {}
    },
    {
        "label": "EXAMPLES",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "peekOfCode": "EXAMPLES = f\"\"\"\\\n<< Example 1. >>\nData Source:\n```json\n{example_info.model_dump_json(indent=4)}\n```\nUser Query:\n{example_query}\nStructured Request:\n```json",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "documentation": {}
    },
    {
        "label": "SUFFIX",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "peekOfCode": "SUFFIX = \"\"\"\n<< Example 3. >>\nData Source:\n```json\n{info_str}\n```\nUser Query:\n{query_str}\nStructured Request:\n\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_VECTOR_STORE_QUERY_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "peekOfCode": "DEFAULT_VECTOR_STORE_QUERY_PROMPT_TMPL = PREFIX + EXAMPLES + SUFFIX\n# deprecated, kept for backwards compatibility\n\"\"\"Vector store query prompt.\"\"\"\nVectorStoreQueryPrompt = PromptTemplate\nDEFAULT_VECTOR_STORE_QUERY_PROMPT = PromptTemplate(\n    template=DEFAULT_VECTOR_STORE_QUERY_PROMPT_TMPL,\n    prompt_type=PromptType.VECTOR_STORE_QUERY,\n)",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "documentation": {}
    },
    {
        "label": "VectorStoreQueryPrompt",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "peekOfCode": "VectorStoreQueryPrompt = PromptTemplate\nDEFAULT_VECTOR_STORE_QUERY_PROMPT = PromptTemplate(\n    template=DEFAULT_VECTOR_STORE_QUERY_PROMPT_TMPL,\n    prompt_type=PromptType.VECTOR_STORE_QUERY,\n)",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_VECTOR_STORE_QUERY_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "peekOfCode": "DEFAULT_VECTOR_STORE_QUERY_PROMPT = PromptTemplate(\n    template=DEFAULT_VECTOR_STORE_QUERY_PROMPT_TMPL,\n    prompt_type=PromptType.VECTOR_STORE_QUERY,\n)",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts",
        "documentation": {}
    },
    {
        "label": "VectorIndexRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.retriever",
        "description": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.retriever",
        "peekOfCode": "class VectorIndexRetriever(BaseRetriever):\n    \"\"\"\n    Vector index retriever.\n    Args:\n        index (VectorStoreIndex): vector store index.\n        similarity_top_k (int): number of top k results to return.\n        vector_store_query_mode (str): vector store query mode\n            See reference for VectorStoreQueryMode for full list of supported modes.\n        filters (Optional[MetadataFilters]): metadata filters, defaults to None\n        alpha (float): weight for sparse/dense retrieval, only used for",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.retriever",
        "documentation": {}
    },
    {
        "label": "dispatcher",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.retriever",
        "description": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.retriever",
        "peekOfCode": "dispatcher = instrument.get_dispatcher(__name__)\nclass VectorIndexRetriever(BaseRetriever):\n    \"\"\"\n    Vector index retriever.\n    Args:\n        index (VectorStoreIndex): vector store index.\n        similarity_top_k (int): number of top k results to return.\n        vector_store_query_mode (str): vector store query mode\n            See reference for VectorStoreQueryMode for full list of supported modes.\n        filters (Optional[MetadataFilters]): metadata filters, defaults to None",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.vector_store.retrievers.retriever",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.vector_store.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.vector_store.base",
        "peekOfCode": "class VectorStoreIndex(BaseIndex[IndexDict]):\n    \"\"\"\n    Vector Store Index.\n    Args:\n        use_async (bool): Whether to use asynchronous calls. Defaults to False.\n        show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n        store_nodes_override (bool): set to True to always store Node objects in index\n            store and document store even if vector store keeps text. Defaults to False\n    \"\"\"\n    index_struct_cls = IndexDict",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.vector_store.base",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.vector_store.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.vector_store.base",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass VectorStoreIndex(BaseIndex[IndexDict]):\n    \"\"\"\n    Vector Store Index.\n    Args:\n        use_async (bool): Whether to use asynchronous calls. Defaults to False.\n        show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n        store_nodes_override (bool): set to True to always store Node objects in index\n            store and document store even if vector store keeps text. Defaults to False\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.vector_store.base",
        "documentation": {}
    },
    {
        "label": "GPTVectorStoreIndex",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.vector_store.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.vector_store.base",
        "peekOfCode": "GPTVectorStoreIndex = VectorStoreIndex",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.vector_store.base",
        "documentation": {}
    },
    {
        "label": "BaseIndex",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.base",
        "peekOfCode": "class BaseIndex(Generic[IS], ABC):\n    \"\"\"\n    Base LlamaIndex.\n    Args:\n        nodes (List[Node]): List of nodes to index\n        show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n    \"\"\"\n    index_struct_cls: Type[IS]\n    def __init__(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "IS",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.base",
        "peekOfCode": "IS = TypeVar(\"IS\", bound=IndexStruct)\nIndexType = TypeVar(\"IndexType\", bound=\"BaseIndex\")\nlogger = logging.getLogger(__name__)\nclass BaseIndex(Generic[IS], ABC):\n    \"\"\"\n    Base LlamaIndex.\n    Args:\n        nodes (List[Node]): List of nodes to index\n        show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "IndexType",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.base",
        "peekOfCode": "IndexType = TypeVar(\"IndexType\", bound=\"BaseIndex\")\nlogger = logging.getLogger(__name__)\nclass BaseIndex(Generic[IS], ABC):\n    \"\"\"\n    Base LlamaIndex.\n    Args:\n        nodes (List[Node]): List of nodes to index\n        show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n    \"\"\"\n    index_struct_cls: Type[IS]",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.base",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass BaseIndex(Generic[IS], ABC):\n    \"\"\"\n    Base LlamaIndex.\n    Args:\n        nodes (List[Node]): List of nodes to index\n        show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n    \"\"\"\n    index_struct_cls: Type[IS]\n    def __init__(",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "BaseGPTIndex",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.base",
        "description": "reference_code.llama-index-core.llama_index.core.indices.base",
        "peekOfCode": "BaseGPTIndex = BaseIndex",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.base",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.base_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.indices.base_retriever",
        "peekOfCode": "__all__ = [\n    \"BaseRetriever\",\n]",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.base_retriever",
        "documentation": {}
    },
    {
        "label": "load_index_from_storage",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.loading",
        "description": "reference_code.llama-index-core.llama_index.core.indices.loading",
        "peekOfCode": "def load_index_from_storage(\n    storage_context: StorageContext,\n    index_id: Optional[str] = None,\n    **kwargs: Any,\n) -> BaseIndex:\n    \"\"\"\n    Load index from storage context.\n    Args:\n        storage_context (StorageContext): storage context containing\n            docstore, index store and vector store.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.loading",
        "documentation": {}
    },
    {
        "label": "load_indices_from_storage",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.loading",
        "description": "reference_code.llama-index-core.llama_index.core.indices.loading",
        "peekOfCode": "def load_indices_from_storage(\n    storage_context: StorageContext,\n    index_ids: Optional[Sequence[str]] = None,\n    **kwargs: Any,\n) -> List[BaseIndex]:\n    \"\"\"\n    Load multiple indices from storage context.\n    Args:\n        storage_context (StorageContext): storage context containing\n            docstore, index store and vector store.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.loading",
        "documentation": {}
    },
    {
        "label": "load_graph_from_storage",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.loading",
        "description": "reference_code.llama-index-core.llama_index.core.indices.loading",
        "peekOfCode": "def load_graph_from_storage(\n    storage_context: StorageContext,\n    root_id: str,\n    **kwargs: Any,\n) -> ComposableGraph:\n    \"\"\"\n    Load composable graph from storage context.\n    Args:\n        storage_context (StorageContext): storage context containing\n            docstore, index store and vector store.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.loading",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.loading",
        "description": "reference_code.llama-index-core.llama_index.core.indices.loading",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef load_index_from_storage(\n    storage_context: StorageContext,\n    index_id: Optional[str] = None,\n    **kwargs: Any,\n) -> BaseIndex:\n    \"\"\"\n    Load index from storage context.\n    Args:\n        storage_context (StorageContext): storage context containing",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.loading",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.postprocessor",
        "description": "reference_code.llama-index-core.llama_index.core.indices.postprocessor",
        "peekOfCode": "__all__ = [\n    \"SimilarityPostprocessor\",\n    \"KeywordNodePostprocessor\",\n    \"PrevNextNodePostprocessor\",\n    \"AutoPrevNextNodePostprocessor\",\n    \"FixedRecencyPostprocessor\",\n    \"EmbeddingRecencyPostprocessor\",\n    \"TimeWeightedPostprocessor\",\n    \"PIINodePostprocessor\",\n    \"NERPIINodePostprocessor\",",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.postprocessor",
        "documentation": {}
    },
    {
        "label": "PromptHelper",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.prompt_helper",
        "description": "reference_code.llama-index-core.llama_index.core.indices.prompt_helper",
        "peekOfCode": "class PromptHelper(BaseComponent):\n    \"\"\"\n    Prompt helper.\n    General prompt helper that can help deal with LLM context window token limitations.\n    At its core, it calculates available context size by starting with the context\n    window size of an LLM and reserve token space for the prompt template, and the\n    output.\n    It provides utility for \"repacking\" text chunks (retrieved from index) to maximally\n    make use of the available context window (and thereby reducing the number of LLM\n    calls needed), or truncating them so that they fit in a single LLM call.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.prompt_helper",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PADDING",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.prompt_helper",
        "description": "reference_code.llama-index-core.llama_index.core.indices.prompt_helper",
        "peekOfCode": "DEFAULT_PADDING = 5\nDEFAULT_CHUNK_OVERLAP_RATIO = 0.1\nlogger = logging.getLogger(__name__)\nclass PromptHelper(BaseComponent):\n    \"\"\"\n    Prompt helper.\n    General prompt helper that can help deal with LLM context window token limitations.\n    At its core, it calculates available context size by starting with the context\n    window size of an LLM and reserve token space for the prompt template, and the\n    output.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.prompt_helper",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CHUNK_OVERLAP_RATIO",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.prompt_helper",
        "description": "reference_code.llama-index-core.llama_index.core.indices.prompt_helper",
        "peekOfCode": "DEFAULT_CHUNK_OVERLAP_RATIO = 0.1\nlogger = logging.getLogger(__name__)\nclass PromptHelper(BaseComponent):\n    \"\"\"\n    Prompt helper.\n    General prompt helper that can help deal with LLM context window token limitations.\n    At its core, it calculates available context size by starting with the context\n    window size of an LLM and reserve token space for the prompt template, and the\n    output.\n    It provides utility for \"repacking\" text chunks (retrieved from index) to maximally",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.prompt_helper",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.prompt_helper",
        "description": "reference_code.llama-index-core.llama_index.core.indices.prompt_helper",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass PromptHelper(BaseComponent):\n    \"\"\"\n    Prompt helper.\n    General prompt helper that can help deal with LLM context window token limitations.\n    At its core, it calculates available context size by starting with the context\n    window size of an LLM and reserve token space for the prompt template, and the\n    output.\n    It provides utility for \"repacking\" text chunks (retrieved from index) to maximally\n    make use of the available context window (and thereby reducing the number of LLM",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.prompt_helper",
        "documentation": {}
    },
    {
        "label": "get_sorted_node_list",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "description": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "peekOfCode": "def get_sorted_node_list(node_dict: Dict[int, BaseNode]) -> List[BaseNode]:\n    \"\"\"Get sorted node list. Used by tree-strutured indices.\"\"\"\n    sorted_indices = sorted(node_dict.keys())\n    return [node_dict[index] for index in sorted_indices]\ndef extract_numbers_given_response(response: str, n: int = 1) -> Optional[List[int]]:\n    \"\"\"\n    Extract number given the GPT-generated response.\n    Used by tree-structured indices.\n    \"\"\"\n    numbers = re.findall(r\"\\d+\", response)",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "extract_numbers_given_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "description": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "peekOfCode": "def extract_numbers_given_response(response: str, n: int = 1) -> Optional[List[int]]:\n    \"\"\"\n    Extract number given the GPT-generated response.\n    Used by tree-structured indices.\n    \"\"\"\n    numbers = re.findall(r\"\\d+\", response)\n    if len(numbers) == 0:\n        return None\n    else:\n        return numbers[:n]",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "expand_tokens_with_subtokens",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "description": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "peekOfCode": "def expand_tokens_with_subtokens(tokens: Set[str]) -> Set[str]:\n    \"\"\"Get subtokens from a list of tokens., filtering for stopwords.\"\"\"\n    results = set()\n    for token in tokens:\n        results.add(token)\n        sub_tokens = re.findall(r\"\\w+\", token)\n        if len(sub_tokens) > 1:\n            results.update({w for w in sub_tokens if w not in globals_helper.stopwords})\n    return results\ndef log_vector_store_query_result(",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "log_vector_store_query_result",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "description": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "peekOfCode": "def log_vector_store_query_result(\n    result: VectorStoreQueryResult, logger: Optional[logging.Logger] = None\n) -> None:\n    \"\"\"Log vector store query result.\"\"\"\n    logger = logger or _logger\n    assert result.ids is not None\n    assert result.nodes is not None\n    similarities = (\n        result.similarities\n        if result.similarities is not None and len(result.similarities) > 0",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "default_format_node_batch_fn",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "description": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "peekOfCode": "def default_format_node_batch_fn(\n    summary_nodes: List[BaseNode],\n) -> str:\n    \"\"\"\n    Default format node batch function.\n    Assign each summary node a number, and format the batch of nodes.\n    \"\"\"\n    fmt_node_txts = []\n    for idx in range(len(summary_nodes)):\n        number = idx + 1",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "default_parse_choice_select_answer_fn",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "description": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "peekOfCode": "def default_parse_choice_select_answer_fn(\n    answer: str, num_choices: int, raise_error: bool = False\n) -> Tuple[List[int], List[float]]:\n    \"\"\"Default parse choice select answer function.\"\"\"\n    answer_lines = answer.split(\"\\n\")\n    answer_nums = []\n    answer_relevances = []\n    for answer_line in answer_lines:\n        line_tokens = answer_line.split(\",\")\n        if len(line_tokens) != 2:",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "embed_nodes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "description": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "peekOfCode": "def embed_nodes(\n    nodes: Sequence[BaseNode], embed_model: BaseEmbedding, show_progress: bool = False\n) -> Dict[str, List[float]]:\n    \"\"\"\n    Get embeddings of the given nodes, run embedding model if necessary.\n    Args:\n        nodes (Sequence[BaseNode]): The nodes to embed.\n        embed_model (BaseEmbedding): The embedding model to use.\n        show_progress (bool): Whether to show progress bar.\n    Returns:",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "embed_image_nodes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "description": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "peekOfCode": "def embed_image_nodes(\n    nodes: Sequence[ImageNode],\n    embed_model: MultiModalEmbedding,\n    show_progress: bool = False,\n) -> Dict[str, List[float]]:\n    \"\"\"\n    Get image embeddings of the given nodes, run image embedding model if necessary.\n    Args:\n        nodes (Sequence[ImageNode]): The nodes to embed.\n        embed_model (MultiModalEmbedding): The embedding model to use.",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "_logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "description": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "peekOfCode": "_logger = logging.getLogger(__name__)\ndef get_sorted_node_list(node_dict: Dict[int, BaseNode]) -> List[BaseNode]:\n    \"\"\"Get sorted node list. Used by tree-strutured indices.\"\"\"\n    sorted_indices = sorted(node_dict.keys())\n    return [node_dict[index] for index in sorted_indices]\ndef extract_numbers_given_response(response: str, n: int = 1) -> Optional[List[int]]:\n    \"\"\"\n    Extract number given the GPT-generated response.\n    Used by tree-structured indices.\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.indices.utils",
        "documentation": {}
    },
    {
        "label": "get_client",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.api_utils",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.api_utils",
        "peekOfCode": "def get_client(\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    app_url: Optional[str] = None,\n    timeout: int = 60,\n    httpx_client: Optional[httpx.Client] = None,\n) -> \"LlamaCloud\":\n    \"\"\"Get the sync platform API client.\"\"\"\n    from llama_cloud.client import LlamaCloud\n    base_url = base_url or os.environ.get(\"LLAMA_CLOUD_BASE_URL\", DEFAULT_BASE_URL)",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.api_utils",
        "documentation": {}
    },
    {
        "label": "get_aclient",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.api_utils",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.api_utils",
        "peekOfCode": "def get_aclient(\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    app_url: Optional[str] = None,\n    timeout: int = 60,\n    httpx_client: Optional[httpx.AsyncClient] = None,\n) -> \"AsyncLlamaCloud\":\n    \"\"\"Get the async platform API client.\"\"\"\n    from llama_cloud.client import AsyncLlamaCloud\n    base_url = base_url or os.environ.get(\"LLAMA_CLOUD_BASE_URL\", DEFAULT_BASE_URL)",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.api_utils",
        "documentation": {}
    },
    {
        "label": "IngestionCache",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.cache",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.cache",
        "peekOfCode": "class IngestionCache(BaseModel):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    nodes_key: str = \"nodes\"\n    collection: str = Field(\n        default=DEFAULT_CACHE_NAME, description=\"Collection name of the cache.\"\n    )\n    cache: BaseCache = Field(default_factory=SimpleCache, description=\"Cache to use.\")\n    # TODO: add async get/put methods?\n    def put(\n        self, key: str, nodes: Sequence[BaseNode], collection: Optional[str] = None",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.cache",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CACHE_NAME",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.cache",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.cache",
        "peekOfCode": "DEFAULT_CACHE_NAME = \"llama_cache\"\nclass IngestionCache(BaseModel):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    nodes_key: str = \"nodes\"\n    collection: str = Field(\n        default=DEFAULT_CACHE_NAME, description=\"Collection name of the cache.\"\n    )\n    cache: BaseCache = Field(default_factory=SimpleCache, description=\"Cache to use.\")\n    # TODO: add async get/put methods?\n    def put(",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.cache",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.cache",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.cache",
        "peekOfCode": "__all__ = [\"SimpleCache\", \"BaseCache\"]",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.cache",
        "documentation": {}
    },
    {
        "label": "DataSink",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.data_sinks",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.data_sinks",
        "peekOfCode": "class DataSink(BaseModel):\n    \"\"\"\n    A class containing metadata for a type of data sink.\n    \"\"\"\n    name: str = Field(\n        description=\"Unique and human-readable name for the type of data sink\"\n    )\n    component_type: Type[BasePydanticVectorStore] = Field(\n        description=\"Type of component that implements the data sink\"\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.data_sinks",
        "documentation": {}
    },
    {
        "label": "ConfigurableComponent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.data_sinks",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.data_sinks",
        "peekOfCode": "class ConfigurableComponent(Enum):\n    @classmethod\n    def from_component(\n        cls, component: BasePydanticVectorStore\n    ) -> \"ConfigurableComponent\":\n        component_class = type(component)\n        for component_type in cls:\n            if component_type.value.component_type == component_class:\n                return component_type\n        raise ValueError(",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.data_sinks",
        "documentation": {}
    },
    {
        "label": "ConfiguredDataSink",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.data_sinks",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.data_sinks",
        "peekOfCode": "class ConfiguredDataSink(BaseModel, Generic[T]):\n    \"\"\"\n    A class containing metadata & implementation for a data sink in a pipeline.\n    \"\"\"\n    name: str\n    component: T = Field(description=\"Component that implements the data sink\")\n    @classmethod\n    def from_component(cls, component: BasePydanticVectorStore) -> \"ConfiguredDataSink\":\n        \"\"\"\n        Build a ConfiguredDataSink from a component.",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.data_sinks",
        "documentation": {}
    },
    {
        "label": "build_conifurable_data_sink_enum",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.data_sinks",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.data_sinks",
        "peekOfCode": "def build_conifurable_data_sink_enum() -> ConfigurableComponent:\n    \"\"\"\n    Build an enum of configurable data sinks.\n    But conditional on if the corresponding vector store is available.\n    \"\"\"\n    enum_members = []\n    try:\n        from llama_index.vector_stores.chroma import (\n            ChromaVectorStore,\n        )  # pants: no-infer-dep",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.data_sinks",
        "documentation": {}
    },
    {
        "label": "ConfigurableDataSinks",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.data_sinks",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.data_sinks",
        "peekOfCode": "ConfigurableDataSinks = build_conifurable_data_sink_enum()\nT = TypeVar(\"T\", bound=BasePydanticVectorStore)\nclass ConfiguredDataSink(BaseModel, Generic[T]):\n    \"\"\"\n    A class containing metadata & implementation for a data sink in a pipeline.\n    \"\"\"\n    name: str\n    component: T = Field(description=\"Component that implements the data sink\")\n    @classmethod\n    def from_component(cls, component: BasePydanticVectorStore) -> \"ConfiguredDataSink\":",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.data_sinks",
        "documentation": {}
    },
    {
        "label": "T",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.data_sinks",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.data_sinks",
        "peekOfCode": "T = TypeVar(\"T\", bound=BasePydanticVectorStore)\nclass ConfiguredDataSink(BaseModel, Generic[T]):\n    \"\"\"\n    A class containing metadata & implementation for a data sink in a pipeline.\n    \"\"\"\n    name: str\n    component: T = Field(description=\"Component that implements the data sink\")\n    @classmethod\n    def from_component(cls, component: BasePydanticVectorStore) -> \"ConfiguredDataSink\":\n        \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.data_sinks",
        "documentation": {}
    },
    {
        "label": "DataSource",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.data_sources",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.data_sources",
        "peekOfCode": "class DataSource(BaseModel):\n    \"\"\"\n    A class containing metadata for a type of data source.\n    \"\"\"\n    name: str = Field(\n        description=\"Unique and human-readable name for the type of data source\"\n    )\n    component_type: Type[BaseComponent] = Field(\n        description=\"Type of component that implements the data source\"\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.data_sources",
        "documentation": {}
    },
    {
        "label": "DocumentGroup",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.data_sources",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.data_sources",
        "peekOfCode": "class DocumentGroup(BasePydanticReader):\n    \"\"\"\n    A group of documents, usually separate pages from a single file.\n    \"\"\"\n    file_path: str = Field(description=\"Path to the file containing the documents\")\n    documents: List[Document] = Field(\n        description=\"Sequential group of documents, usually separate pages from a single file.\"\n    )\n    @property\n    def file_name(self) -> str:",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.data_sources",
        "documentation": {}
    },
    {
        "label": "ConfigurableComponent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.data_sources",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.data_sources",
        "peekOfCode": "class ConfigurableComponent(Enum):\n    @classmethod\n    def from_component(cls, component: BaseComponent) -> \"ConfigurableComponent\":\n        component_class = type(component)\n        for component_type in cls:\n            if component_type.value.component_type == component_class:\n                return component_type\n        raise ValueError(\n            f\"Component {component} is not a supported data source component.\"\n        )",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.data_sources",
        "documentation": {}
    },
    {
        "label": "ConfiguredDataSource",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.data_sources",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.data_sources",
        "peekOfCode": "class ConfiguredDataSource(BaseModel, Generic[T]):\n    \"\"\"\n    A class containing metadata & implementation for a data source in a pipeline.\n    \"\"\"\n    name: str\n    component: T = Field(description=\"Component that implements the data source\")\n    @classmethod\n    def from_component(\n        cls, component: BaseComponent, name: Optional[str] = None\n    ) -> \"ConfiguredDataSource\":",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.data_sources",
        "documentation": {}
    },
    {
        "label": "build_configurable_data_source_enum",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.data_sources",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.data_sources",
        "peekOfCode": "def build_configurable_data_source_enum() -> ConfigurableComponent:\n    \"\"\"\n    Build an enum of configurable data sources.\n    But conditional on if the corresponding reader is available.\n    \"\"\"\n    enum_members = []\n    try:\n        from llama_index.readers.discord import DiscordReader  # pants: no-infer-dep\n        enum_members.append(\n            (",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.data_sources",
        "documentation": {}
    },
    {
        "label": "ConfigurableDataSources",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.data_sources",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.data_sources",
        "peekOfCode": "ConfigurableDataSources = build_configurable_data_source_enum()\nT = TypeVar(\"T\", bound=BaseComponent)\nclass ConfiguredDataSource(BaseModel, Generic[T]):\n    \"\"\"\n    A class containing metadata & implementation for a data source in a pipeline.\n    \"\"\"\n    name: str\n    component: T = Field(description=\"Component that implements the data source\")\n    @classmethod\n    def from_component(",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.data_sources",
        "documentation": {}
    },
    {
        "label": "T",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.data_sources",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.data_sources",
        "peekOfCode": "T = TypeVar(\"T\", bound=BaseComponent)\nclass ConfiguredDataSource(BaseModel, Generic[T]):\n    \"\"\"\n    A class containing metadata & implementation for a data source in a pipeline.\n    \"\"\"\n    name: str\n    component: T = Field(description=\"Component that implements the data source\")\n    @classmethod\n    def from_component(\n        cls, component: BaseComponent, name: Optional[str] = None",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.data_sources",
        "documentation": {}
    },
    {
        "label": "DocstoreStrategy",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.pipeline",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.pipeline",
        "peekOfCode": "class DocstoreStrategy(str, Enum):\n    \"\"\"\n    Document de-duplication de-deduplication strategies work by comparing the hashes or ids stored in the document store.\n       They require a document store to be set which must be persisted across pipeline runs.\n    Attributes:\n        UPSERTS:\n            ('upserts') Use upserts to handle duplicates. Checks if the a document is already in the doc store based on its id. If it is not, or if the hash of the document is updated, it will update the document in the doc store and run the transformations.\n        DUPLICATES_ONLY:\n            ('duplicates_only') Only handle duplicates. Checks if the hash of a document is already in the doc store. Only then it will add the document to the doc store and run the transformations\n        UPSERTS_AND_DELETE:",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.pipeline",
        "documentation": {}
    },
    {
        "label": "IngestionPipeline",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.pipeline",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.pipeline",
        "peekOfCode": "class IngestionPipeline(BaseModel):\n    \"\"\"\n    An ingestion pipeline that can be applied to data.\n    Args:\n        name (str, optional):\n            Unique name of the ingestion pipeline. Defaults to DEFAULT_PIPELINE_NAME.\n        project_name (str, optional):\n            Unique name of the project. Defaults to DEFAULT_PROJECT_NAME.\n        transformations (List[TransformComponent], optional):\n            Transformations to apply to the data. Defaults to None.",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.pipeline",
        "documentation": {}
    },
    {
        "label": "remove_unstable_values",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.pipeline",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.pipeline",
        "peekOfCode": "def remove_unstable_values(s: str) -> str:\n    \"\"\"\n    Remove unstable key/value pairs.\n    Examples include:\n    - <__main__.Test object at 0x7fb9f3793f50>\n    - <function test_fn at 0x7fb9f37a8900>\n    \"\"\"\n    pattern = r\"<[\\w\\s_\\. ]+ at 0x[a-z0-9]+>\"\n    return re.sub(pattern, \"\", s)\ndef get_transformation_hash(",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.pipeline",
        "documentation": {}
    },
    {
        "label": "get_transformation_hash",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.pipeline",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.pipeline",
        "peekOfCode": "def get_transformation_hash(\n    nodes: Sequence[BaseNode], transformation: TransformComponent\n) -> str:\n    \"\"\"Get the hash of a transformation.\"\"\"\n    nodes_str = \"\".join(\n        [str(node.get_content(metadata_mode=MetadataMode.ALL)) for node in nodes]\n    )\n    transformation_dict = transformation.to_dict()\n    transform_string = remove_unstable_values(str(transformation_dict))\n    return sha256((nodes_str + transform_string).encode(\"utf-8\")).hexdigest()",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.pipeline",
        "documentation": {}
    },
    {
        "label": "run_transformations",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.pipeline",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.pipeline",
        "peekOfCode": "def run_transformations(\n    nodes: Sequence[BaseNode],\n    transformations: Sequence[TransformComponent],\n    in_place: bool = True,\n    cache: Optional[IngestionCache] = None,\n    cache_collection: Optional[str] = None,\n    **kwargs: Any,\n) -> Sequence[BaseNode]:\n    \"\"\"\n    Run a series of transformations on a set of nodes.",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.pipeline",
        "documentation": {}
    },
    {
        "label": "arun_transformations_wrapper",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.pipeline",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.pipeline",
        "peekOfCode": "def arun_transformations_wrapper(\n    nodes: Sequence[BaseNode],\n    transformations: Sequence[TransformComponent],\n    in_place: bool = True,\n    cache: Optional[IngestionCache] = None,\n    cache_collection: Optional[str] = None,\n    **kwargs: Any,\n) -> Sequence[BaseNode]:\n    \"\"\"\n    Wrapper for async run_transformation. To be used in loop.run_in_executor",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.pipeline",
        "documentation": {}
    },
    {
        "label": "dispatcher",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.pipeline",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.pipeline",
        "peekOfCode": "dispatcher = get_dispatcher(__name__)\ndef remove_unstable_values(s: str) -> str:\n    \"\"\"\n    Remove unstable key/value pairs.\n    Examples include:\n    - <__main__.Test object at 0x7fb9f3793f50>\n    - <function test_fn at 0x7fb9f37a8900>\n    \"\"\"\n    pattern = r\"<[\\w\\s_\\. ]+ at 0x[a-z0-9]+>\"\n    return re.sub(pattern, \"\", s)",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.pipeline",
        "documentation": {}
    },
    {
        "label": "TransformationIOType",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "peekOfCode": "class TransformationIOType(BaseModel):\n    name: str = Field(description=\"Name of the input/output type\")\n    description: str = Field(description=\"Description of the input/output type\")\n    python_type: str = Field(description=\"Python type of the input/output type\")\nclass TransformationIOTypes(Enum):\n    DOCUMENTS = TransformationIOType(\n        name=\"Documents\",\n        description=\"A sequence of Documents\",\n        python_type=str(Sequence[Document]),\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "documentation": {}
    },
    {
        "label": "TransformationIOTypes",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "peekOfCode": "class TransformationIOTypes(Enum):\n    DOCUMENTS = TransformationIOType(\n        name=\"Documents\",\n        description=\"A sequence of Documents\",\n        python_type=str(Sequence[Document]),\n    )\n    NODES = TransformationIOType(\n        name=\"Nodes\",\n        description=\"A sequence of Nodes from a sequence of Documents\",\n        python_type=str(Sequence[BaseNode]),",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "documentation": {}
    },
    {
        "label": "TransformationCategory",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "peekOfCode": "class TransformationCategory(BaseModel):\n    \"\"\"A description for a category of transformation within a pipeline.\"\"\"\n    name: str = Field(description=\"Unique name of the type of transformation\")\n    description: str = Field(description=\"Description for the type of transformation\")\n    input_type: TransformationIOType = Field(\n        description=\"Input type for the transformation type\"\n    )\n    output_type: TransformationIOType = Field(\n        description=\"Output type for the transformation type\"\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "documentation": {}
    },
    {
        "label": "TransformationCategories",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "peekOfCode": "class TransformationCategories(Enum):\n    \"\"\"Supported transformation categories.\"\"\"\n    NODE_PARSER = TransformationCategory(\n        name=\"NodeParser\",\n        description=\"Applies a function to parse nodes from documents\",\n        input_type=TransformationIOTypes.DOCUMENTS.value,\n        output_type=TransformationIOTypes.NODES.value,\n    )\n    EMBEDDING = TransformationCategory(\n        name=\"Embedding\",",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "documentation": {}
    },
    {
        "label": "ConfigurableTransformation",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "peekOfCode": "class ConfigurableTransformation(BaseModel):\n    \"\"\"\n    A class containing metadata for a type of transformation that can be in a pipeline.\n    \"\"\"\n    name: str = Field(\n        description=\"Unique and human-readable name for the type of transformation\"\n    )\n    transformation_category: TransformationCategories = Field(\n        description=\"Type of transformation\"\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "documentation": {}
    },
    {
        "label": "ConfigurableComponent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "peekOfCode": "class ConfigurableComponent(Enum):\n    @classmethod\n    def from_component(cls, component: BaseComponent) -> \"ConfigurableComponent\":\n        component_class = type(component)\n        for component_type in cls:\n            if component_type.value.component_type == component_class:\n                return component_type\n        raise ValueError(\n            f\"Component {component} is not a supported transformation component.\"\n        )",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "documentation": {}
    },
    {
        "label": "ConfiguredTransformation",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "peekOfCode": "class ConfiguredTransformation(BaseModel, Generic[T]):\n    \"\"\"\n    A class containing metadata & implementation for a transformation in a pipeline.\n    \"\"\"\n    name: str\n    component: SerializeAsAny[T] = Field(\n        description=\"Component that implements the transformation\"\n    )\n    @classmethod\n    def from_component(cls, component: BaseComponent) -> \"ConfiguredTransformation\":",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "documentation": {}
    },
    {
        "label": "build_configurable_transformation_enum",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "peekOfCode": "def build_configurable_transformation_enum() -> ConfigurableComponent:\n    \"\"\"\n    Build an enum of configurable transformations.\n    But conditional on if the corresponding component is available.\n    \"\"\"\n    enum_members = []\n    # Node parsers\n    enum_members.append(\n        (\n            \"CODE_NODE_PARSER\",",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "documentation": {}
    },
    {
        "label": "ConfigurableTransformations",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "peekOfCode": "ConfigurableTransformations = build_configurable_transformation_enum()\nT = TypeVar(\"T\", bound=BaseComponent)\nclass ConfiguredTransformation(BaseModel, Generic[T]):\n    \"\"\"\n    A class containing metadata & implementation for a transformation in a pipeline.\n    \"\"\"\n    name: str\n    component: SerializeAsAny[T] = Field(\n        description=\"Component that implements the transformation\"\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "documentation": {}
    },
    {
        "label": "T",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "description": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "peekOfCode": "T = TypeVar(\"T\", bound=BaseComponent)\nclass ConfiguredTransformation(BaseModel, Generic[T]):\n    \"\"\"\n    A class containing metadata & implementation for a transformation in a pipeline.\n    \"\"\"\n    name: str\n    component: SerializeAsAny[T] = Field(\n        description=\"Component that implements the transformation\"\n    )\n    @classmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.ingestion.transformations",
        "documentation": {}
    },
    {
        "label": "AgentRunStepStartEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.agent",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.agent",
        "peekOfCode": "class AgentRunStepStartEvent(BaseEvent):\n    \"\"\"\n    AgentRunStepStartEvent.\n    Args:\n        task_id (str): Task ID.\n        step (Optional[Any]): Task step.\n        input (Optional[str]): Optional input.\n    \"\"\"\n    task_id: str\n    step: Optional[Any]",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.agent",
        "documentation": {}
    },
    {
        "label": "AgentRunStepEndEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.agent",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.agent",
        "peekOfCode": "class AgentRunStepEndEvent(BaseEvent):\n    \"\"\"\n    AgentRunStepEndEvent.\n    Args:\n        step_output (Any): Task step output.\n    \"\"\"\n    step_output: Any\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.agent",
        "documentation": {}
    },
    {
        "label": "AgentChatWithStepStartEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.agent",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.agent",
        "peekOfCode": "class AgentChatWithStepStartEvent(BaseEvent):\n    \"\"\"\n    AgentChatWithStepStartEvent.\n    Args:\n        user_msg (str): User input message.\n    \"\"\"\n    user_msg: str\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.agent",
        "documentation": {}
    },
    {
        "label": "AgentChatWithStepEndEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.agent",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.agent",
        "peekOfCode": "class AgentChatWithStepEndEvent(BaseEvent):\n    \"\"\"\n    AgentChatWithStepEndEvent.\n    Args:\n        response (Optional[AGENT_CHAT_RESPONSE_TYPE]): Agent chat response.\n    \"\"\"\n    response: Optional[AGENT_CHAT_RESPONSE_TYPE]\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_response(cls: Any, values: Any) -> Any:",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.agent",
        "documentation": {}
    },
    {
        "label": "AgentToolCallEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.agent",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.agent",
        "peekOfCode": "class AgentToolCallEvent(BaseEvent):\n    \"\"\"\n    AgentToolCallEvent.\n    Args:\n        arguments (str): Arguments.\n        tool (ToolMetadata): Tool metadata.\n    \"\"\"\n    arguments: str\n    tool: ToolMetadata\n    @classmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.agent",
        "documentation": {}
    },
    {
        "label": "StreamChatStartEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.chat_engine",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.chat_engine",
        "peekOfCode": "class StreamChatStartEvent(BaseEvent):\n    \"\"\"\n    StreamChatStartEvent.\n    Fired at the start of writing to the stream chat-engine queue.\n    \"\"\"\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"StreamChatStartEvent\"\nclass StreamChatEndEvent(BaseEvent):",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.chat_engine",
        "documentation": {}
    },
    {
        "label": "StreamChatEndEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.chat_engine",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.chat_engine",
        "peekOfCode": "class StreamChatEndEvent(BaseEvent):\n    \"\"\"\n    StreamChatEndEvent.\n    Fired at the end of writing to the stream chat-engine queue.\n    \"\"\"\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"StreamChatEndEvent\"\nclass StreamChatErrorEvent(BaseEvent):",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.chat_engine",
        "documentation": {}
    },
    {
        "label": "StreamChatErrorEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.chat_engine",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.chat_engine",
        "peekOfCode": "class StreamChatErrorEvent(BaseEvent):\n    \"\"\"\n    StreamChatErrorEvent.\n    Fired when an exception is raised during the stream chat-engine operation.\n    Args:\n        exception (Exception): Exception raised during the stream chat operation.\n    \"\"\"\n    exception: Exception\n    @classmethod\n    def class_name(cls) -> str:",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.chat_engine",
        "documentation": {}
    },
    {
        "label": "StreamChatDeltaReceivedEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.chat_engine",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.chat_engine",
        "peekOfCode": "class StreamChatDeltaReceivedEvent(BaseEvent):\n    \"\"\"\n    StreamChatDeltaReceivedEvent.\n    Args:\n        delta (str): Delta received from the stream chat.\n    \"\"\"\n    delta: str\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.chat_engine",
        "documentation": {}
    },
    {
        "label": "EmbeddingStartEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.embedding",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.embedding",
        "peekOfCode": "class EmbeddingStartEvent(BaseEvent):\n    \"\"\"\n    EmbeddingStartEvent.\n    Args:\n        model_dict (dict): Model dictionary containing details about the embedding model.\n    \"\"\"\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    model_dict: dict\n    @classmethod\n    def class_name(cls) -> str:",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.embedding",
        "documentation": {}
    },
    {
        "label": "EmbeddingEndEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.embedding",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.embedding",
        "peekOfCode": "class EmbeddingEndEvent(BaseEvent):\n    \"\"\"\n    EmbeddingEndEvent.\n    Args:\n        chunks (List[str]): List of chunks.\n        embeddings (List[List[float]]): List of embeddings.\n    \"\"\"\n    chunks: List[str]\n    embeddings: List[List[float]]\n    @classmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.embedding",
        "documentation": {}
    },
    {
        "label": "SparseEmbeddingStartEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.embedding",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.embedding",
        "peekOfCode": "class SparseEmbeddingStartEvent(BaseEvent):\n    \"\"\"\n    EmbeddingStartEvent.\n    Args:\n        model_dict (dict): Model dictionary containing details about the embedding model.\n    \"\"\"\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    model_dict: dict\n    @classmethod\n    def class_name(cls) -> str:",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.embedding",
        "documentation": {}
    },
    {
        "label": "SparseEmbeddingEndEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.embedding",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.embedding",
        "peekOfCode": "class SparseEmbeddingEndEvent(BaseEvent):\n    \"\"\"\n    EmbeddingEndEvent.\n    Args:\n        chunks (List[str]): List of chunks.\n        embeddings (List[List[float]]): List of embeddings.\n    \"\"\"\n    chunks: List[str]\n    embeddings: List[Dict[int, float]]\n    @classmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.embedding",
        "documentation": {}
    },
    {
        "label": "ExceptionEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.exception",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.exception",
        "peekOfCode": "class ExceptionEvent(BaseEvent):\n    \"\"\"\n    ExceptionEvent.\n    Args:\n        exception (BaseException): exception.\n    \"\"\"\n    exception: BaseException\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.exception",
        "documentation": {}
    },
    {
        "label": "LLMPredictStartEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "peekOfCode": "class LLMPredictStartEvent(BaseEvent):\n    \"\"\"\n    LLMPredictStartEvent.\n    Args:\n        template (BasePromptTemplate): Prompt template.\n        template_args (Optional[dict]): Prompt template arguments.\n    \"\"\"\n    template: SerializeAsAny[BasePromptTemplate]\n    template_args: Optional[dict]\n    @classmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "documentation": {}
    },
    {
        "label": "LLMPredictEndEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "peekOfCode": "class LLMPredictEndEvent(BaseEvent):\n    \"\"\"\n    LLMPredictEndEvent.\n    The result of an llm.predict() call.\n    Args:\n        output (str): Output.\n    \"\"\"\n    output: str\n    @classmethod\n    def class_name(cls) -> str:",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "documentation": {}
    },
    {
        "label": "LLMStructuredPredictStartEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "peekOfCode": "class LLMStructuredPredictStartEvent(BaseEvent):\n    \"\"\"\n    LLMStructuredPredictStartEvent.\n    Args:\n        output_cls (Any): Output class to predict.\n        template (BasePromptTemplate): Prompt template.\n        template_args (Optional[dict]): Prompt template arguments.\n    \"\"\"\n    output_cls: Any\n    template: SerializeAsAny[BasePromptTemplate]",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "documentation": {}
    },
    {
        "label": "LLMStructuredPredictEndEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "peekOfCode": "class LLMStructuredPredictEndEvent(BaseEvent):\n    \"\"\"\n    LLMStructuredPredictEndEvent.\n    Args:\n        output (BaseModel): Predicted output class.\n    \"\"\"\n    output: SerializeAsAny[Any]\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "documentation": {}
    },
    {
        "label": "LLMStructuredPredictInProgressEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "peekOfCode": "class LLMStructuredPredictInProgressEvent(BaseEvent):\n    \"\"\"\n    LLMStructuredPredictInProgressEvent.\n    Args:\n        output (BaseModel): Predicted output class.\n    \"\"\"\n    output: SerializeAsAny[Any]\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "documentation": {}
    },
    {
        "label": "LLMCompletionStartEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "peekOfCode": "class LLMCompletionStartEvent(BaseEvent):\n    \"\"\"\n    LLMCompletionStartEvent.\n    Args:\n        prompt (str): The prompt to be completed.\n        additional_kwargs (dict): Additional keyword arguments.\n        model_dict (dict): Model dictionary.\n    \"\"\"\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    prompt: str",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "documentation": {}
    },
    {
        "label": "LLMCompletionInProgressEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "peekOfCode": "class LLMCompletionInProgressEvent(BaseEvent):\n    \"\"\"\n    LLMCompletionInProgressEvent.\n    Args:\n        prompt (str): The prompt to be completed.\n        response (CompletionResponse): Completion response.\n    \"\"\"\n    prompt: str\n    response: CompletionResponse\n    @classmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "documentation": {}
    },
    {
        "label": "LLMCompletionEndEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "peekOfCode": "class LLMCompletionEndEvent(BaseEvent):\n    \"\"\"\n    LLMCompletionEndEvent.\n    Args:\n        prompt (str): The prompt to be completed.\n        response (CompletionResponse): Completion response.\n    \"\"\"\n    prompt: str\n    response: CompletionResponse\n    @classmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "documentation": {}
    },
    {
        "label": "LLMChatStartEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "peekOfCode": "class LLMChatStartEvent(BaseEvent):\n    \"\"\"\n    LLMChatStartEvent.\n    Args:\n        messages (List[ChatMessage]): List of chat messages.\n        additional_kwargs (dict): Additional keyword arguments.\n        model_dict (dict): Model dictionary.\n    \"\"\"\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    messages: List[ChatMessage]",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "documentation": {}
    },
    {
        "label": "LLMChatInProgressEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "peekOfCode": "class LLMChatInProgressEvent(BaseEvent):\n    \"\"\"\n    LLMChatInProgressEvent.\n    Args:\n        messages (List[ChatMessage]): List of chat messages.\n        response (ChatResponse): Chat response currently being streamed.\n    \"\"\"\n    messages: List[ChatMessage]\n    response: ChatResponse\n    @classmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "documentation": {}
    },
    {
        "label": "LLMChatEndEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "peekOfCode": "class LLMChatEndEvent(BaseEvent):\n    \"\"\"\n    LLMChatEndEvent.\n    Args:\n        messages (List[ChatMessage]): List of chat messages.\n        response (Optional[ChatResponse]): Last chat response.\n    \"\"\"\n    messages: List[ChatMessage]\n    response: Optional[ChatResponse]\n    @classmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.llm",
        "documentation": {}
    },
    {
        "label": "QueryStartEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.query",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.query",
        "peekOfCode": "class QueryStartEvent(BaseEvent):\n    \"\"\"\n    QueryStartEvent.\n    Args:\n        query (QueryType): Query as a string or query bundle.\n    \"\"\"\n    query: QueryType\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.query",
        "documentation": {}
    },
    {
        "label": "QueryEndEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.query",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.query",
        "peekOfCode": "class QueryEndEvent(BaseEvent):\n    \"\"\"\n    QueryEndEvent.\n    Args:\n        query (QueryType): Query as a string or query bundle.\n        response (RESPONSE_TYPE): Response.\n    \"\"\"\n    query: QueryType\n    response: RESPONSE_TYPE\n    @classmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.query",
        "documentation": {}
    },
    {
        "label": "ReRankStartEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.rerank",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.rerank",
        "peekOfCode": "class ReRankStartEvent(BaseEvent):\n    \"\"\"\n    ReRankStartEvent.\n    Args:\n        query (QueryType): Query as a string or query bundle.\n        nodes (List[NodeWithScore]): List of nodes with scores.\n        top_n (int): Number of nodes to return after rerank.\n        model_name (str): Name of the model used for reranking.\n    \"\"\"\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.rerank",
        "documentation": {}
    },
    {
        "label": "ReRankEndEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.rerank",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.rerank",
        "peekOfCode": "class ReRankEndEvent(BaseEvent):\n    \"\"\"\n    ReRankEndEvent.\n    Args:\n        nodes (List[NodeWithScore]): List of returned nodes after rerank.\n    \"\"\"\n    nodes: List[NodeWithScore]\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.rerank",
        "documentation": {}
    },
    {
        "label": "RetrievalStartEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.retrieval",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.retrieval",
        "peekOfCode": "class RetrievalStartEvent(BaseEvent):\n    \"\"\"\n    RetrievalStartEvent.\n    Args:\n        str_or_query_bundle (QueryType): Query bundle.\n    \"\"\"\n    str_or_query_bundle: QueryType\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.retrieval",
        "documentation": {}
    },
    {
        "label": "RetrievalEndEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.retrieval",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.retrieval",
        "peekOfCode": "class RetrievalEndEvent(BaseEvent):\n    \"\"\"\n    RetrievalEndEvent.\n    Args:\n        str_or_query_bundle (QueryType): Query bundle.\n        nodes (List[NodeWithScore]): List of nodes with scores.\n    \"\"\"\n    str_or_query_bundle: QueryType\n    nodes: List[NodeWithScore]\n    @classmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.retrieval",
        "documentation": {}
    },
    {
        "label": "SynthesizeStartEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.synthesis",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.synthesis",
        "peekOfCode": "class SynthesizeStartEvent(BaseEvent):\n    \"\"\"\n    SynthesizeStartEvent.\n    Args:\n        query (QueryType): Query as a string or query bundle.\n    \"\"\"\n    query: QueryType\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.synthesis",
        "documentation": {}
    },
    {
        "label": "SynthesizeEndEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.synthesis",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.synthesis",
        "peekOfCode": "class SynthesizeEndEvent(BaseEvent):\n    \"\"\"\n    SynthesizeEndEvent.\n    Args:\n        query (QueryType): Query as a string or query bundle.\n        response (RESPONSE_TYPE): Response.\n    \"\"\"\n    query: QueryType\n    response: RESPONSE_TYPE\n    @classmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.synthesis",
        "documentation": {}
    },
    {
        "label": "GetResponseStartEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.synthesis",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.synthesis",
        "peekOfCode": "class GetResponseStartEvent(BaseEvent):\n    \"\"\"\n    GetResponseStartEvent.\n    Args:\n        query_str (str): Query string.\n        text_chunks (List[str]): List of text chunks.\n    \"\"\"\n    query_str: str\n    text_chunks: List[str]\n    @classmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.synthesis",
        "documentation": {}
    },
    {
        "label": "GetResponseEndEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.instrumentation.events.synthesis",
        "description": "reference_code.llama-index-core.llama_index.core.instrumentation.events.synthesis",
        "peekOfCode": "class GetResponseEndEvent(BaseEvent):\n    \"\"\"GetResponseEndEvent.\"\"\"\n    # TODO: consumes the first chunk of generators??\n    # response: RESPONSE_TEXT_TYPE\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"GetResponseEndEvent\"",
        "detail": "reference_code.llama-index-core.llama_index.core.instrumentation.events.synthesis",
        "documentation": {}
    },
    {
        "label": "create_llama_agent",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.langchain_helpers.agents.agents",
        "description": "reference_code.llama-index-core.llama_index.core.langchain_helpers.agents.agents",
        "peekOfCode": "def create_llama_agent(\n    toolkit: LlamaToolkit,\n    llm: BaseLLM,\n    agent: Optional[AgentType] = None,\n    callback_manager: Optional[BaseCallbackManager] = None,\n    agent_path: Optional[str] = None,\n    agent_kwargs: Optional[dict] = None,\n    **kwargs: Any,\n) -> AgentExecutor:\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.langchain_helpers.agents.agents",
        "documentation": {}
    },
    {
        "label": "create_llama_chat_agent",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.langchain_helpers.agents.agents",
        "description": "reference_code.llama-index-core.llama_index.core.langchain_helpers.agents.agents",
        "peekOfCode": "def create_llama_chat_agent(\n    toolkit: LlamaToolkit,\n    llm: BaseLLM,\n    callback_manager: Optional[BaseCallbackManager] = None,\n    agent_kwargs: Optional[dict] = None,\n    **kwargs: Any,\n) -> AgentExecutor:\n    \"\"\"\n    Load a chat llama agent given a Llama Toolkit and LLM.\n    Args:",
        "detail": "reference_code.llama-index-core.llama_index.core.langchain_helpers.agents.agents",
        "documentation": {}
    },
    {
        "label": "LlamaToolkit",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.langchain_helpers.agents.toolkits",
        "description": "reference_code.llama-index-core.llama_index.core.langchain_helpers.agents.toolkits",
        "peekOfCode": "class LlamaToolkit(BaseToolkit):\n    \"\"\"Toolkit for interacting with Llama indices.\"\"\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    index_configs: List[IndexToolConfig] = Field(default_factory=list)\n    def get_tools(self) -> List[BaseTool]:\n        \"\"\"Get the tools in the toolkit.\"\"\"\n        index_tools: List[BaseTool] = [\n            LlamaIndexTool.from_tool_config(tool_config=tool_config)\n            for tool_config in self.index_configs\n        ]",
        "detail": "reference_code.llama-index-core.llama_index.core.langchain_helpers.agents.toolkits",
        "documentation": {}
    },
    {
        "label": "IndexToolConfig",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.langchain_helpers.agents.tools",
        "description": "reference_code.llama-index-core.llama_index.core.langchain_helpers.agents.tools",
        "peekOfCode": "class IndexToolConfig(BaseModel):\n    \"\"\"Configuration for LlamaIndex index tool.\"\"\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    query_engine: BaseQueryEngine\n    name: str\n    description: str\n    tool_kwargs: Dict = Field(default_factory=dict)\nclass LlamaIndexTool(BaseTool):\n    \"\"\"Tool for querying a LlamaIndex.\"\"\"\n    # NOTE: name/description still needs to be set",
        "detail": "reference_code.llama-index-core.llama_index.core.langchain_helpers.agents.tools",
        "documentation": {}
    },
    {
        "label": "LlamaIndexTool",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.langchain_helpers.agents.tools",
        "description": "reference_code.llama-index-core.llama_index.core.langchain_helpers.agents.tools",
        "peekOfCode": "class LlamaIndexTool(BaseTool):\n    \"\"\"Tool for querying a LlamaIndex.\"\"\"\n    # NOTE: name/description still needs to be set\n    query_engine: BaseQueryEngine\n    return_sources: bool = False\n    @classmethod\n    def from_tool_config(cls, tool_config: IndexToolConfig) -> \"LlamaIndexTool\":\n        \"\"\"Create a tool from a tool config.\"\"\"\n        return_sources = tool_config.tool_kwargs.pop(\"return_sources\", False)\n        return cls(",
        "detail": "reference_code.llama-index-core.llama_index.core.langchain_helpers.agents.tools",
        "documentation": {}
    },
    {
        "label": "GPTIndexMemory",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.langchain_helpers.memory_wrapper",
        "description": "reference_code.llama-index-core.llama_index.core.langchain_helpers.memory_wrapper",
        "peekOfCode": "class GPTIndexMemory(Memory):\n    \"\"\"\n    Langchain memory wrapper (for LlamaIndex).\n    Args:\n        human_prefix (str): Prefix for human input. Defaults to \"Human\".\n        ai_prefix (str): Prefix for AI output. Defaults to \"AI\".\n        memory_key (str): Key for memory. Defaults to \"history\".\n        index (BaseIndex): LlamaIndex instance.\n        query_kwargs (Dict[str, Any]): Keyword arguments for LlamaIndex query.\n        input_key (Optional[str]): Input key. Defaults to None.",
        "detail": "reference_code.llama-index-core.llama_index.core.langchain_helpers.memory_wrapper",
        "documentation": {}
    },
    {
        "label": "GPTIndexChatMemory",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.langchain_helpers.memory_wrapper",
        "description": "reference_code.llama-index-core.llama_index.core.langchain_helpers.memory_wrapper",
        "peekOfCode": "class GPTIndexChatMemory(BaseChatMemory):\n    \"\"\"\n    Langchain chat memory wrapper (for LlamaIndex).\n    Args:\n        human_prefix (str): Prefix for human input. Defaults to \"Human\".\n        ai_prefix (str): Prefix for AI output. Defaults to \"AI\".\n        memory_key (str): Key for memory. Defaults to \"history\".\n        index (BaseIndex): LlamaIndex instance.\n        query_kwargs (Dict[str, Any]): Keyword arguments for LlamaIndex query.\n        input_key (Optional[str]): Input key. Defaults to None.",
        "detail": "reference_code.llama-index-core.llama_index.core.langchain_helpers.memory_wrapper",
        "documentation": {}
    },
    {
        "label": "get_prompt_input_key",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.langchain_helpers.memory_wrapper",
        "description": "reference_code.llama-index-core.llama_index.core.langchain_helpers.memory_wrapper",
        "peekOfCode": "def get_prompt_input_key(inputs: Dict[str, Any], memory_variables: List[str]) -> str:\n    \"\"\"\n    Get prompt input key.\n    Copied over from langchain.\n    \"\"\"\n    # \"stop\" is a special key that can be passed as input but is not used to\n    # format the prompt.\n    prompt_input_keys = list(set(inputs).difference([*memory_variables, \"stop\"]))\n    if len(prompt_input_keys) != 1:\n        raise ValueError(f\"One input key expected got {prompt_input_keys}\")",
        "detail": "reference_code.llama-index-core.llama_index.core.langchain_helpers.memory_wrapper",
        "documentation": {}
    },
    {
        "label": "StreamingGeneratorCallbackHandler",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.langchain_helpers.streaming",
        "description": "reference_code.llama-index-core.llama_index.core.langchain_helpers.streaming",
        "peekOfCode": "class StreamingGeneratorCallbackHandler(BaseCallbackHandler):\n    \"\"\"Streaming callback handler.\"\"\"\n    def __init__(self) -> None:\n        self._token_queue: Queue = Queue()\n        self._done = Event()\n    def __deepcopy__(self, memo: Any) -> \"StreamingGeneratorCallbackHandler\":\n        # NOTE: hack to bypass deepcopy in langchain\n        return self\n    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:\n        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.langchain_helpers.streaming",
        "documentation": {}
    },
    {
        "label": "EmbeddingQAFinetuneDataset",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.legacy.embedding",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.legacy.embedding",
        "peekOfCode": "class EmbeddingQAFinetuneDataset(BaseModel):\n    \"\"\"\n    Embedding QA Finetuning Dataset.\n    Args:\n        queries (Dict[str, str]): Dict id -> query.\n        corpus (Dict[str, str]): Dict id -> string.\n        relevant_docs (Dict[str, List[str]]): Dict query id -> list of doc ids.\n    \"\"\"\n    queries: Dict[str, str]  # dict id -> query\n    corpus: Dict[str, str]  # dict id -> string",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.legacy.embedding",
        "documentation": {}
    },
    {
        "label": "generate_qa_embedding_pairs",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.legacy.embedding",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.legacy.embedding",
        "peekOfCode": "def generate_qa_embedding_pairs(\n    nodes: List[TextNode],\n    llm: Optional[LLM] = None,\n    qa_generate_prompt_tmpl: str = DEFAULT_QA_GENERATE_PROMPT_TMPL,\n    num_questions_per_chunk: int = 2,\n) -> EmbeddingQAFinetuneDataset:\n    \"\"\"Generate examples given a set of nodes.\"\"\"\n    llm = llm or Settings.llm\n    node_dict = {\n        node.node_id: node.get_content(metadata_mode=MetadataMode.NONE)",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.legacy.embedding",
        "documentation": {}
    },
    {
        "label": "DEFAULT_QA_GENERATE_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.legacy.embedding",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.legacy.embedding",
        "peekOfCode": "DEFAULT_QA_GENERATE_PROMPT_TMPL = \"\"\"\\\nContext information is below.\n---------------------\n{context_str}\n---------------------\nGiven the context information and not prior knowledge.\ngenerate only questions based on the below query.\nYou are a Teacher/ Professor. Your task is to setup \\\n{num_questions_per_chunk} questions for an upcoming \\\nquiz/examination. The questions should be diverse in nature \\",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.legacy.embedding",
        "documentation": {}
    },
    {
        "label": "CreatedByType",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.base",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.base",
        "peekOfCode": "class CreatedByType(str, Enum):\n    \"\"\"The kinds of rag data examples.\"\"\"\n    HUMAN = \"human\"\n    AI = \"ai\"\n    def __str__(self) -> str:\n        return self.value\nclass CreatedBy(BaseModel):\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    model_name: Optional[str] = Field(\n        default_factory=str, description=\"When CreatedByType.AI, specify model name.\"",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.base",
        "documentation": {}
    },
    {
        "label": "CreatedBy",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.base",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.base",
        "peekOfCode": "class CreatedBy(BaseModel):\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    model_name: Optional[str] = Field(\n        default_factory=str, description=\"When CreatedByType.AI, specify model name.\"\n    )\n    type: CreatedByType\n    def __str__(self) -> str:\n        if self.type == \"ai\":\n            return f\"{self.type!s} ({self.model_name})\"\n        else:",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.base",
        "documentation": {}
    },
    {
        "label": "BaseLlamaExamplePrediction",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.base",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.base",
        "peekOfCode": "class BaseLlamaExamplePrediction(BaseModel):\n    \"\"\"Base llama dataset example class.\"\"\"\n    @property\n    @abstractmethod\n    def class_name(self) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"BaseLlamaPrediction\"\nclass BaseLlamaDataExample(BaseModel):\n    \"\"\"Base llama dataset example class.\"\"\"\n    @property",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.base",
        "documentation": {}
    },
    {
        "label": "BaseLlamaDataExample",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.base",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.base",
        "peekOfCode": "class BaseLlamaDataExample(BaseModel):\n    \"\"\"Base llama dataset example class.\"\"\"\n    @property\n    @abstractmethod\n    def class_name(self) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"BaseLlamaDataExample\"\nclass BaseLlamaPredictionDataset(BaseModel):\n    _prediction_type: ClassVar[Type[BaseLlamaExamplePrediction]]\n    predictions: List[BaseLlamaExamplePrediction] = Field(",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.base",
        "documentation": {}
    },
    {
        "label": "BaseLlamaPredictionDataset",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.base",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.base",
        "peekOfCode": "class BaseLlamaPredictionDataset(BaseModel):\n    _prediction_type: ClassVar[Type[BaseLlamaExamplePrediction]]\n    predictions: List[BaseLlamaExamplePrediction] = Field(\n        default_factory=list, description=\"Predictions on train_examples.\"\n    )\n    def __getitem__(\n        self, val: Union[slice, int]\n    ) -> Union[Sequence[BaseLlamaExamplePrediction], BaseLlamaExamplePrediction]:\n        \"\"\"\n        Enable slicing and indexing.",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.base",
        "documentation": {}
    },
    {
        "label": "BaseLlamaDataset",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.base",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.base",
        "peekOfCode": "class BaseLlamaDataset(BaseModel, Generic[P]):\n    _example_type: ClassVar[Type[BaseLlamaDataExample]]\n    examples: List[BaseLlamaDataExample] = Field(\n        default=[], description=\"Data examples of this dataset.\"\n    )\n    _predictions_cache: List[BaseLlamaExamplePrediction] = PrivateAttr(\n        default_factory=list\n    )\n    def __getitem__(\n        self, val: Union[slice, int]",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.base",
        "documentation": {}
    },
    {
        "label": "PredictorType",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.base",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.base",
        "peekOfCode": "PredictorType = Union[BaseQueryEngine, BaseEvaluator, LLM]\nP = TypeVar(\"P\", bound=PredictorType)\nclass CreatedByType(str, Enum):\n    \"\"\"The kinds of rag data examples.\"\"\"\n    HUMAN = \"human\"\n    AI = \"ai\"\n    def __str__(self) -> str:\n        return self.value\nclass CreatedBy(BaseModel):\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.base",
        "documentation": {}
    },
    {
        "label": "P",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.base",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.base",
        "peekOfCode": "P = TypeVar(\"P\", bound=PredictorType)\nclass CreatedByType(str, Enum):\n    \"\"\"The kinds of rag data examples.\"\"\"\n    HUMAN = \"human\"\n    AI = \"ai\"\n    def __str__(self) -> str:\n        return self.value\nclass CreatedBy(BaseModel):\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    model_name: Optional[str] = Field(",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.base",
        "documentation": {}
    },
    {
        "label": "download_llama_dataset",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.download",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.download",
        "peekOfCode": "def download_llama_dataset(\n    llama_dataset_class: str,\n    download_dir: str,\n    llama_datasets_url: str = LLAMA_DATASETS_URL,\n    llama_datasets_lfs_url: str = LLAMA_DATASETS_LFS_URL,\n    llama_datasets_source_files_tree_url: str = LLAMA_DATASETS_SOURCE_FILES_GITHUB_TREE_URL,\n    show_progress: bool = False,\n    load_documents: bool = True,\n) -> Tuple[BaseLlamaDataset, List[Document]]:\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.download",
        "documentation": {}
    },
    {
        "label": "EvaluatorExamplePrediction",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "peekOfCode": "class EvaluatorExamplePrediction(BaseLlamaExamplePrediction):\n    \"\"\"\n    Evaluation example prediction class.\n    Args:\n        feedback (Optional[str]): The evaluator's feedback.\n        score (Optional[float]): The evaluator's score.\n    \"\"\"\n    feedback: str = Field(\n        default_factory=str,\n        description=\"The generated (predicted) response that can be compared to a reference (ground-truth) answer.\",",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "documentation": {}
    },
    {
        "label": "LabelledEvaluatorDataExample",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "peekOfCode": "class LabelledEvaluatorDataExample(BaseLlamaDataExample):\n    \"\"\"\n    Evaluation example class.\n    This data class contains the ingredients to perform a new \"prediction\" i.e.,\n    evaluation. Here an evaluator is meant to evaluate a response against an\n    associated query as well as optionally contexts.\n    Args:\n        query (str): The user query\n        query_by (CreatedBy): Query generated by human or ai (model-name)\n        contexts (Optional[List[str]]): The contexts used for response",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "documentation": {}
    },
    {
        "label": "EvaluatorPredictionDataset",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "peekOfCode": "class EvaluatorPredictionDataset(BaseLlamaPredictionDataset):\n    \"\"\"Evaluation Prediction Dataset Class.\"\"\"\n    _prediction_type = EvaluatorExamplePrediction\n    def to_pandas(self) -> Any:\n        \"\"\"Create pandas dataframe.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\n                \"pandas is required for this function. Please install it with `pip install pandas`.\"",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "documentation": {}
    },
    {
        "label": "LabelledEvaluatorDataset",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "peekOfCode": "class LabelledEvaluatorDataset(BaseLlamaDataset[BaseEvaluator]):\n    \"\"\"LabelledEvalationDataset class.\"\"\"\n    _example_type = LabelledEvaluatorDataExample\n    def to_pandas(self) -> Any:\n        \"\"\"Create pandas dataframe.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\n                \"pandas is required for this function. Please install it with `pip install pandas`.\"",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "documentation": {}
    },
    {
        "label": "PairwiseEvaluatorExamplePrediction",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "peekOfCode": "class PairwiseEvaluatorExamplePrediction(BaseLlamaExamplePrediction):\n    \"\"\"\n    Pairwise evaluation example prediction class.\n    Args:\n        feedback (Optional[str]): The evaluator's feedback.\n        score (Optional[float]): The evaluator's score.\n        evaluation_source (EvaluationSource): If the evaluation came from original order or flipped; or inconclusive.\n    \"\"\"\n    feedback: str = Field(\n        default_factory=str,",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "documentation": {}
    },
    {
        "label": "PairwiseEvaluatorPredictionDataset",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "peekOfCode": "class PairwiseEvaluatorPredictionDataset(BaseLlamaPredictionDataset):\n    \"\"\"Pairwise evaluation predictions dataset class.\"\"\"\n    _prediction_type = PairwiseEvaluatorExamplePrediction\n    def to_pandas(self) -> Any:\n        \"\"\"Create pandas dataframe.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\n                \"pandas is required for this function. Please install it with `pip install pandas`.\"",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "documentation": {}
    },
    {
        "label": "LabelledPairwiseEvaluatorDataExample",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "peekOfCode": "class LabelledPairwiseEvaluatorDataExample(LabelledEvaluatorDataExample):\n    \"\"\"Labelled pairwise evaluation data example class.\"\"\"\n    second_answer: str = Field(\n        default_factory=str,\n        description=\"The second answer to the example that is to be evaluated along versus `answer`.\",\n    )\n    second_answer_by: Optional[CreatedBy] = Field(\n        default=None, description=\"What generated the second answer.\"\n    )\n    @property",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "documentation": {}
    },
    {
        "label": "LabelledPairwiseEvaluatorDataset",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "peekOfCode": "class LabelledPairwiseEvaluatorDataset(BaseLlamaDataset[BaseEvaluator]):\n    \"\"\"\n    Labelled pairwise evaluation dataset. For evaluating the evaluator in\n    performing pairwise evaluations.\n    Args:\n        BaseLlamaDataset (_type_): _description_\n    \"\"\"\n    _example_type = LabelledPairwiseEvaluatorDataExample\n    def to_pandas(self) -> Any:\n        \"\"\"Create pandas dataframe.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "documentation": {}
    },
    {
        "label": "LabeledEvaluatorDataExample",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "peekOfCode": "LabeledEvaluatorDataExample = LabelledEvaluatorDataExample\nLabeledEvaluatorDataset = LabelledEvaluatorDataset\nLabeledPairwiseEvaluatorDataExample = LabelledPairwiseEvaluatorDataExample\nLabeledPairwiseEvaluatorDataset = LabelledPairwiseEvaluatorDataset",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "documentation": {}
    },
    {
        "label": "LabeledEvaluatorDataset",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "peekOfCode": "LabeledEvaluatorDataset = LabelledEvaluatorDataset\nLabeledPairwiseEvaluatorDataExample = LabelledPairwiseEvaluatorDataExample\nLabeledPairwiseEvaluatorDataset = LabelledPairwiseEvaluatorDataset",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "documentation": {}
    },
    {
        "label": "LabeledPairwiseEvaluatorDataExample",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "peekOfCode": "LabeledPairwiseEvaluatorDataExample = LabelledPairwiseEvaluatorDataExample\nLabeledPairwiseEvaluatorDataset = LabelledPairwiseEvaluatorDataset",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "documentation": {}
    },
    {
        "label": "LabeledPairwiseEvaluatorDataset",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "peekOfCode": "LabeledPairwiseEvaluatorDataset = LabelledPairwiseEvaluatorDataset",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.evaluator_evaluation",
        "documentation": {}
    },
    {
        "label": "RagDatasetGenerator",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.generator",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.generator",
        "peekOfCode": "class RagDatasetGenerator(PromptMixin):\n    \"\"\"Generate dataset (question/ question-answer pairs) \\\n    based on the given documents.\n    NOTE: this is a beta feature, subject to change!\n    Args:\n        nodes (List[Node]): List of nodes. (Optional)\n        num_questions_per_chunk: number of question to be \\\n        generated per chunk. Each document is chunked of size 512 words.\n        text_question_template: Question generation template.\n        question_gen_query: Question generation query.",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.generator",
        "documentation": {}
    },
    {
        "label": "DEFAULT_QUESTION_GENERATION_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.generator",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.generator",
        "peekOfCode": "DEFAULT_QUESTION_GENERATION_PROMPT = \"\"\"\\\nContext information is below.\n---------------------\n{context_str}\n---------------------\nGiven the context information and not prior knowledge.\ngenerate only questions based on the below query.\n{query_str}\n\"\"\"\nclass RagDatasetGenerator(PromptMixin):",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.generator",
        "documentation": {}
    },
    {
        "label": "RagExamplePrediction",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.rag",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.rag",
        "peekOfCode": "class RagExamplePrediction(BaseLlamaExamplePrediction):\n    \"\"\"\n    RAG example prediction class.\n    Args:\n        response (str): The response generated by the LLM.\n        contexts (Optional[List[str]]): The retrieved context (text) for generating\n                                        response.\n    \"\"\"\n    response: str = Field(\n        default=\"\",",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.rag",
        "documentation": {}
    },
    {
        "label": "LabelledRagDataExample",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.rag",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.rag",
        "peekOfCode": "class LabelledRagDataExample(BaseLlamaDataExample):\n    \"\"\"\n    RAG example class. Analogous to traditional ML datasets, this dataset contains\n    the \"features\" (i.e., query + context) to make a prediction and the \"label\" (i.e., response)\n    to evaluate the prediction.\n    Args:\n        query (str): The user query\n        query_by (CreatedBy): Query generated by human or ai (model-name)\n        reference_contexts (Optional[List[str]]): The contexts used for response\n        reference_answer ([str]): Reference answer to the query. An answer",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.rag",
        "documentation": {}
    },
    {
        "label": "RagPredictionDataset",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.rag",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.rag",
        "peekOfCode": "class RagPredictionDataset(BaseLlamaPredictionDataset):\n    \"\"\"RagDataset class.\"\"\"\n    _prediction_type = RagExamplePrediction\n    def to_pandas(self) -> Any:\n        \"\"\"Create pandas dataframe.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\n                \"pandas is required for this function. Please install it with `pip install pandas`.\"",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.rag",
        "documentation": {}
    },
    {
        "label": "LabelledRagDataset",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.rag",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.rag",
        "peekOfCode": "class LabelledRagDataset(BaseLlamaDataset[BaseQueryEngine]):\n    \"\"\"RagDataset class.\"\"\"\n    _example_type = LabelledRagDataExample\n    def to_pandas(self) -> Any:\n        \"\"\"Create pandas dataframe.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\n                \"pandas is required for this function. Please install it with `pip install pandas`.\"",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.rag",
        "documentation": {}
    },
    {
        "label": "LabeledRagDataExample",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.rag",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.rag",
        "peekOfCode": "LabeledRagDataExample = LabelledRagDataExample\nLabeledRagDataset = LabelledRagDataset",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.rag",
        "documentation": {}
    },
    {
        "label": "LabeledRagDataset",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.rag",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.rag",
        "peekOfCode": "LabeledRagDataset = LabelledRagDataset",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.rag",
        "documentation": {}
    },
    {
        "label": "SimpleExamplePrediction",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.simple",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.simple",
        "peekOfCode": "class SimpleExamplePrediction(BaseLlamaExamplePrediction):\n    \"\"\"\n    RAG example prediction class.\n    Args:\n        response (str): The response generated by the LLM.\n        contexts (Optional[List[str]]): The retrieved context (text) for generating\n                                        response.\n    \"\"\"\n    label: str = Field(\n        default_factory=str,",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.simple",
        "documentation": {}
    },
    {
        "label": "SimplePredictionDataset",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.simple",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.simple",
        "peekOfCode": "class SimplePredictionDataset(BaseLlamaPredictionDataset):\n    \"\"\"RagDataset class.\"\"\"\n    _prediction_type = SimpleExamplePrediction\n    def to_pandas(self) -> Any:\n        \"\"\"Create pandas dataframe.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\n                \"pandas is required for this function. Please install it with `pip install pandas`.\"",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.simple",
        "documentation": {}
    },
    {
        "label": "LabelledSimpleDataExample",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.simple",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.simple",
        "peekOfCode": "class LabelledSimpleDataExample(BaseLlamaDataExample):\n    reference_label: str = Field(default_factory=str, description=\"Class label\")\n    text: str = Field(default_factory=str, description=\"Text body of example\")\n    text_by: Optional[CreatedBy] = Field(\n        default=None, description=\"What generated the query.\"\n    )\n    @property\n    def class_name(self) -> str:\n        \"\"\"Data example class name.\"\"\"\n        return \"LabelledSimpleDataExample\"",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.simple",
        "documentation": {}
    },
    {
        "label": "LabelledSimpleDataset",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_dataset.simple",
        "description": "reference_code.llama-index-core.llama_index.core.llama_dataset.simple",
        "peekOfCode": "class LabelledSimpleDataset(BaseLlamaDataset[LLM]):\n    _example_type = LabelledSimpleDataExample\n    def _construct_prediction_dataset(  # type: ignore\n        self, predictions: Sequence[SimpleExamplePrediction]\n    ) -> SimplePredictionDataset:\n        \"\"\"\n        Construct the specific prediction dataset.\n        Args:\n            predictions (List[BaseLlamaExamplePrediction]): the list of predictions.\n        Returns:",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_dataset.simple",
        "documentation": {}
    },
    {
        "label": "BaseLlamaPack",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_pack.base",
        "description": "reference_code.llama-index-core.llama_index.core.llama_pack.base",
        "peekOfCode": "class BaseLlamaPack:\n    @abstractmethod\n    def get_modules(self) -> Dict[str, Any]:\n        \"\"\"Get modules.\"\"\"\n    @abstractmethod\n    def run(self, *args: Any, **kwargs: Any) -> Any:\n        \"\"\"Run.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_pack.base",
        "documentation": {}
    },
    {
        "label": "download_llama_pack",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.llama_pack.download",
        "description": "reference_code.llama-index-core.llama_index.core.llama_pack.download",
        "peekOfCode": "def download_llama_pack(\n    llama_pack_class: str,\n    download_dir: Optional[str] = None,\n    llama_pack_url: str = LLAMA_PACKS_CONTENTS_URL,\n    refresh_cache: bool = True,\n) -> Optional[Type[BaseLlamaPack]]:\n    \"\"\"\n    Download a single LlamaPack PyPi Package.\n    Args:\n        llama_pack_class: The name of the LlamaPack class you want to download,",
        "detail": "reference_code.llama-index-core.llama_index.core.llama_pack.download",
        "documentation": {}
    },
    {
        "label": "llm_chat_callback",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.callbacks",
        "description": "reference_code.llama-index-core.llama_index.core.llms.callbacks",
        "peekOfCode": "def llm_chat_callback() -> Callable:\n    def wrap(f: Callable) -> Callable:\n        @contextmanager\n        def wrapper_logic(_self: Any) -> Generator[CallbackManager, None, None]:\n            callback_manager = getattr(_self, \"callback_manager\", None)\n            if not isinstance(callback_manager, CallbackManager):\n                _self.callback_manager = CallbackManager()\n            yield _self.callback_manager  # type: ignore\n        async def wrapped_async_llm_chat(\n            _self: Any, messages: Sequence[ChatMessage], **kwargs: Any",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.callbacks",
        "documentation": {}
    },
    {
        "label": "llm_completion_callback",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.callbacks",
        "description": "reference_code.llama-index-core.llama_index.core.llms.callbacks",
        "peekOfCode": "def llm_completion_callback() -> Callable:\n    def wrap(f: Callable) -> Callable:\n        @contextmanager\n        def wrapper_logic(_self: Any) -> Generator[CallbackManager, None, None]:\n            callback_manager = getattr(_self, \"callback_manager\", None)\n            if not isinstance(callback_manager, CallbackManager):\n                _self.callback_manager = CallbackManager()\n            yield _self.callback_manager\n        def extract_prompt(*args: Any, **kwargs: Any) -> str:\n            if len(args) > 0:",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.callbacks",
        "documentation": {}
    },
    {
        "label": "dispatcher",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.callbacks",
        "description": "reference_code.llama-index-core.llama_index.core.llms.callbacks",
        "peekOfCode": "dispatcher = get_dispatcher(__name__)\ndef llm_chat_callback() -> Callable:\n    def wrap(f: Callable) -> Callable:\n        @contextmanager\n        def wrapper_logic(_self: Any) -> Generator[CallbackManager, None, None]:\n            callback_manager = getattr(_self, \"callback_manager\", None)\n            if not isinstance(callback_manager, CallbackManager):\n                _self.callback_manager = CallbackManager()\n            yield _self.callback_manager  # type: ignore\n        async def wrapped_async_llm_chat(",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.callbacks",
        "documentation": {}
    },
    {
        "label": "messages_to_prompt",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.chatml_utils",
        "description": "reference_code.llama-index-core.llama_index.core.llms.chatml_utils",
        "peekOfCode": "def messages_to_prompt(\n    messages: Sequence[ChatMessage], system_prompt: Optional[str] = None\n) -> str:\n    if len(messages) == 0:\n        raise ValueError(\n            \"At least one message is required to construct the ChatML prompt\"\n        )\n    string_messages: List[str] = []\n    if messages[0].role == MessageRole.SYSTEM:\n        # pull out the system message (if it exists in messages)",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.chatml_utils",
        "documentation": {}
    },
    {
        "label": "completion_to_prompt",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.chatml_utils",
        "description": "reference_code.llama-index-core.llama_index.core.llms.chatml_utils",
        "peekOfCode": "def completion_to_prompt(completion: str, system_prompt: Optional[str] = None) -> str:\n    system_prompt_str = system_prompt or DEFAULT_SYSTEM_PROMPT\n    return (\n        f\"{B_SYS}{system_prompt_str.strip()} {END}\"\n        f\"{B_USER}{completion.strip()} {END}\"\n        f\"{B_ASSISTANT}\"\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.chatml_utils",
        "documentation": {}
    },
    {
        "label": "B_SYS",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.chatml_utils",
        "description": "reference_code.llama-index-core.llama_index.core.llms.chatml_utils",
        "peekOfCode": "B_SYS = \"<|im_start|>system\\n\"\nB_USER = \"<|im_start|>user\\n\"\nB_ASSISTANT = \"<|im_start|>assistant\\n\"\nEND = \"<|im_end|>\\n\"\nDEFAULT_SYSTEM_PROMPT = \"\"\"\\\nYou are a helpful, respectful and honest assistant. \\\nAlways answer as helpfully as possible and follow ALL given instructions. \\\nDo not speculate or make up information. \\\nDo not reference any given instructions or context. \\\n\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.chatml_utils",
        "documentation": {}
    },
    {
        "label": "B_USER",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.chatml_utils",
        "description": "reference_code.llama-index-core.llama_index.core.llms.chatml_utils",
        "peekOfCode": "B_USER = \"<|im_start|>user\\n\"\nB_ASSISTANT = \"<|im_start|>assistant\\n\"\nEND = \"<|im_end|>\\n\"\nDEFAULT_SYSTEM_PROMPT = \"\"\"\\\nYou are a helpful, respectful and honest assistant. \\\nAlways answer as helpfully as possible and follow ALL given instructions. \\\nDo not speculate or make up information. \\\nDo not reference any given instructions or context. \\\n\"\"\"\ndef messages_to_prompt(",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.chatml_utils",
        "documentation": {}
    },
    {
        "label": "B_ASSISTANT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.chatml_utils",
        "description": "reference_code.llama-index-core.llama_index.core.llms.chatml_utils",
        "peekOfCode": "B_ASSISTANT = \"<|im_start|>assistant\\n\"\nEND = \"<|im_end|>\\n\"\nDEFAULT_SYSTEM_PROMPT = \"\"\"\\\nYou are a helpful, respectful and honest assistant. \\\nAlways answer as helpfully as possible and follow ALL given instructions. \\\nDo not speculate or make up information. \\\nDo not reference any given instructions or context. \\\n\"\"\"\ndef messages_to_prompt(\n    messages: Sequence[ChatMessage], system_prompt: Optional[str] = None",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.chatml_utils",
        "documentation": {}
    },
    {
        "label": "END",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.chatml_utils",
        "description": "reference_code.llama-index-core.llama_index.core.llms.chatml_utils",
        "peekOfCode": "END = \"<|im_end|>\\n\"\nDEFAULT_SYSTEM_PROMPT = \"\"\"\\\nYou are a helpful, respectful and honest assistant. \\\nAlways answer as helpfully as possible and follow ALL given instructions. \\\nDo not speculate or make up information. \\\nDo not reference any given instructions or context. \\\n\"\"\"\ndef messages_to_prompt(\n    messages: Sequence[ChatMessage], system_prompt: Optional[str] = None\n) -> str:",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.chatml_utils",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SYSTEM_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.chatml_utils",
        "description": "reference_code.llama-index-core.llama_index.core.llms.chatml_utils",
        "peekOfCode": "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\nYou are a helpful, respectful and honest assistant. \\\nAlways answer as helpfully as possible and follow ALL given instructions. \\\nDo not speculate or make up information. \\\nDo not reference any given instructions or context. \\\n\"\"\"\ndef messages_to_prompt(\n    messages: Sequence[ChatMessage], system_prompt: Optional[str] = None\n) -> str:\n    if len(messages) == 0:",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.chatml_utils",
        "documentation": {}
    },
    {
        "label": "CustomLLM",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.custom",
        "description": "reference_code.llama-index-core.llama_index.core.llms.custom",
        "peekOfCode": "class CustomLLM(LLM):\n    \"\"\"\n    Simple abstract base class for custom LLMs.\n    Subclasses must implement the `__init__`, `_complete`,\n        `_stream_complete`, and `metadata` methods.\n    \"\"\"\n    def __init__(self, *args: Any, **kwargs: Any):\n        super().__init__(*args, **kwargs)\n    @llm_chat_callback()\n    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.custom",
        "documentation": {}
    },
    {
        "label": "FunctionCallingLLM",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.function_calling",
        "description": "reference_code.llama-index-core.llama_index.core.llms.function_calling",
        "peekOfCode": "class FunctionCallingLLM(LLM):\n    \"\"\"\n    Function calling LLMs are LLMs that support function calling.\n    They support an expanded range of capabilities.\n    \"\"\"\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        # Help static checkers understand this class hierarchy\n        super().__init__(*args, **kwargs)\n    def chat_with_tools(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.function_calling",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.function_calling",
        "description": "reference_code.llama-index-core.llama_index.core.llms.function_calling",
        "peekOfCode": "logger = logging.getLogger(__name__)\nif TYPE_CHECKING:\n    from llama_index.core.chat_engine.types import AgentChatResponse\n    from llama_index.core.tools.types import BaseTool\nclass FunctionCallingLLM(LLM):\n    \"\"\"\n    Function calling LLMs are LLMs that support function calling.\n    They support an expanded range of capabilities.\n    \"\"\"\n    def __init__(self, *args: Any, **kwargs: Any) -> None:",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.function_calling",
        "documentation": {}
    },
    {
        "label": "ToolSelection",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "description": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "peekOfCode": "class ToolSelection(BaseModel):\n    \"\"\"Tool selection.\"\"\"\n    tool_id: str = Field(description=\"Tool ID to select.\")\n    tool_name: str = Field(description=\"Tool name to select.\")\n    tool_kwargs: Dict[str, Any] = Field(description=\"Keyword arguments for the tool.\")\n    @field_validator(\"tool_kwargs\", mode=\"wrap\")\n    @classmethod\n    def ignore_non_dict_arguments(cls, v: Any, handler: Any) -> Dict[str, Any]:\n        try:\n            return handler(v)",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "MessagesToPromptType",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "description": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "peekOfCode": "class MessagesToPromptType(Protocol):\n    def __call__(self, messages: Sequence[ChatMessage]) -> str:\n        pass\n@runtime_checkable\nclass CompletionToPromptType(Protocol):\n    def __call__(self, prompt: str) -> str:\n        pass\ndef stream_completion_response_to_tokens(\n    completion_response_gen: CompletionResponseGen,\n) -> TokenGen:",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "CompletionToPromptType",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "description": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "peekOfCode": "class CompletionToPromptType(Protocol):\n    def __call__(self, prompt: str) -> str:\n        pass\ndef stream_completion_response_to_tokens(\n    completion_response_gen: CompletionResponseGen,\n) -> TokenGen:\n    \"\"\"Convert a stream completion response to a stream of tokens.\"\"\"\n    def gen() -> TokenGen:\n        for response in completion_response_gen:\n            yield response.delta or \"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "LLM",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "description": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "peekOfCode": "class LLM(BaseLLM):\n    \"\"\"\n    The LLM class is the main class for interacting with language models.\n    Attributes:\n        system_prompt (Optional[str]):\n            System prompt for LLM calls.\n        messages_to_prompt (Callable):\n            Function to convert a list of messages to an LLM prompt.\n        completion_to_prompt (Callable):\n            Function to convert a completion to an LLM prompt.",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "stream_completion_response_to_tokens",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "description": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "peekOfCode": "def stream_completion_response_to_tokens(\n    completion_response_gen: CompletionResponseGen,\n) -> TokenGen:\n    \"\"\"Convert a stream completion response to a stream of tokens.\"\"\"\n    def gen() -> TokenGen:\n        for response in completion_response_gen:\n            yield response.delta or \"\"\n    return gen()\ndef stream_chat_response_to_tokens(\n    chat_response_gen: ChatResponseGen,",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "stream_chat_response_to_tokens",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "description": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "peekOfCode": "def stream_chat_response_to_tokens(\n    chat_response_gen: ChatResponseGen,\n) -> TokenGen:\n    \"\"\"Convert a stream completion response to a stream of tokens.\"\"\"\n    def gen() -> TokenGen:\n        for response in chat_response_gen:\n            yield response.delta or \"\"\n    return gen()\nasync def astream_completion_response_to_tokens(\n    completion_response_gen: CompletionResponseAsyncGen,",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "default_completion_to_prompt",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "description": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "peekOfCode": "def default_completion_to_prompt(prompt: str) -> str:\n    return prompt\nMessagesToPromptCallable = Annotated[\n    Optional[MessagesToPromptType],\n    WithJsonSchema({\"type\": \"string\"}),\n]\nCompletionToPromptCallable = Annotated[\n    Optional[CompletionToPromptType],\n    WithJsonSchema({\"type\": \"string\"}),\n]",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "dispatcher",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "description": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "peekOfCode": "dispatcher = instrument.get_dispatcher(__name__)\nif TYPE_CHECKING:\n    from llama_index.core.chat_engine.types import AgentChatResponse\n    from llama_index.core.program.utils import FlexibleModel\n    from llama_index.core.tools.types import BaseTool\n    from llama_index.core.llms.structured_llm import StructuredLLM\nclass ToolSelection(BaseModel):\n    \"\"\"Tool selection.\"\"\"\n    tool_id: str = Field(description=\"Tool ID to select.\")\n    tool_name: str = Field(description=\"Tool name to select.\")",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "MessagesToPromptCallable",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "description": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "peekOfCode": "MessagesToPromptCallable = Annotated[\n    Optional[MessagesToPromptType],\n    WithJsonSchema({\"type\": \"string\"}),\n]\nCompletionToPromptCallable = Annotated[\n    Optional[CompletionToPromptType],\n    WithJsonSchema({\"type\": \"string\"}),\n]\nclass LLM(BaseLLM):\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "CompletionToPromptCallable",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "description": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "peekOfCode": "CompletionToPromptCallable = Annotated[\n    Optional[CompletionToPromptType],\n    WithJsonSchema({\"type\": \"string\"}),\n]\nclass LLM(BaseLLM):\n    \"\"\"\n    The LLM class is the main class for interacting with language models.\n    Attributes:\n        system_prompt (Optional[str]):\n            System prompt for LLM calls.",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.llm",
        "documentation": {}
    },
    {
        "label": "load_llm",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.loading",
        "description": "reference_code.llama-index-core.llama_index.core.llms.loading",
        "peekOfCode": "def load_llm(data: dict) -> LLM:\n    \"\"\"Load LLM by name.\"\"\"\n    if isinstance(data, LLM):\n        return data\n    llm_name = data.get(\"class_name\")\n    if llm_name is None:\n        raise ValueError(\"LLM loading requires a class_name\")\n    if llm_name not in RECOGNIZED_LLMS:\n        raise ValueError(f\"Invalid LLM name: {llm_name}\")\n    return RECOGNIZED_LLMS[llm_name].from_dict(data)",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.loading",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.mock",
        "description": "reference_code.llama-index-core.llama_index.core.llms.mock",
        "peekOfCode": "class MockLLM(CustomLLM):\n    max_tokens: Optional[int]\n    def __init__(\n        self,\n        max_tokens: Optional[int] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        system_prompt: Optional[str] = None,\n        messages_to_prompt: Optional[MessagesToPromptType] = None,\n        completion_to_prompt: Optional[CompletionToPromptType] = None,\n        pydantic_program_mode: PydanticProgramMode = PydanticProgramMode.DEFAULT,",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.mock",
        "documentation": {}
    },
    {
        "label": "MockLLMWithNonyieldingChatStream",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.mock",
        "description": "reference_code.llama-index-core.llama_index.core.llms.mock",
        "peekOfCode": "class MockLLMWithNonyieldingChatStream(MockLLM):\n    @llm_chat_callback()\n    def stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n        yield from []",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.mock",
        "documentation": {}
    },
    {
        "label": "StructuredLLM",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.structured_llm",
        "description": "reference_code.llama-index-core.llama_index.core.llms.structured_llm",
        "peekOfCode": "class StructuredLLM(LLM):\n    \"\"\"\n    A structured LLM takes in an inner LLM along with a designated output class,\n    and all methods will return outputs in that structure.\n    \"\"\"\n    llm: SerializeAsAny[LLM]\n    output_cls: Type[BaseModel] = Field(\n        ..., description=\"Output class for the structured LLM.\", exclude=True\n    )\n    @classmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.structured_llm",
        "documentation": {}
    },
    {
        "label": "resolve_llm",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.utils",
        "description": "reference_code.llama-index-core.llama_index.core.llms.utils",
        "peekOfCode": "def resolve_llm(\n    llm: Optional[LLMType] = None, callback_manager: Optional[CallbackManager] = None\n) -> LLM:\n    \"\"\"Resolve LLM from string or LLM instance.\"\"\"\n    from llama_index.core.settings import Settings\n    try:\n        from langchain.base_language import BaseLanguageModel  # pants: no-infer-dep\n    except ImportError:\n        BaseLanguageModel = None  # type: ignore\n    if llm == \"default\":",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.utils",
        "documentation": {}
    },
    {
        "label": "parse_partial_json",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.utils",
        "description": "reference_code.llama-index-core.llama_index.core.llms.utils",
        "peekOfCode": "def parse_partial_json(s: str) -> Dict:\n    \"\"\"\n    Parse an incomplete JSON string into a valid python dictionary.\n    NOTE: This is adapted from\n    https://github.com/OpenInterpreter/open-interpreter/blob/5b6080fae1f8c68938a1e4fa8667e3744084ee21/interpreter/utils/parse_partial_json.py\n    \"\"\"\n    # Attempt to parse the string as-is.\n    try:\n        return json.loads(s)\n    except json.JSONDecodeError:",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.utils",
        "documentation": {}
    },
    {
        "label": "LLMType",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.llms.utils",
        "description": "reference_code.llama-index-core.llama_index.core.llms.utils",
        "peekOfCode": "LLMType = Union[str, LLM, \"BaseLanguageModel\"]\ndef resolve_llm(\n    llm: Optional[LLMType] = None, callback_manager: Optional[CallbackManager] = None\n) -> LLM:\n    \"\"\"Resolve LLM from string or LLM instance.\"\"\"\n    from llama_index.core.settings import Settings\n    try:\n        from langchain.base_language import BaseLanguageModel  # pants: no-infer-dep\n    except ImportError:\n        BaseLanguageModel = None  # type: ignore",
        "detail": "reference_code.llama-index-core.llama_index.core.llms.utils",
        "documentation": {}
    },
    {
        "label": "FactExtractionMemoryBlock",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.memory_blocks.fact",
        "description": "reference_code.llama-index-core.llama_index.core.memory.memory_blocks.fact",
        "peekOfCode": "class FactExtractionMemoryBlock(BaseMemoryBlock[str]):\n    \"\"\"\n    A memory block that extracts key facts from conversation history using an LLM.\n    This block identifies and stores discrete facts disclosed during the conversation,\n    structuring them in XML format for easy parsing and retrieval.\n    \"\"\"\n    name: str = Field(\n        default=\"ExtractedFacts\", description=\"The name of the memory block.\"\n    )\n    llm: LLM = Field(",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.memory_blocks.fact",
        "documentation": {}
    },
    {
        "label": "get_default_llm",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.memory_blocks.fact",
        "description": "reference_code.llama-index-core.llama_index.core.memory.memory_blocks.fact",
        "peekOfCode": "def get_default_llm() -> LLM:\n    return Settings.llm\nclass FactExtractionMemoryBlock(BaseMemoryBlock[str]):\n    \"\"\"\n    A memory block that extracts key facts from conversation history using an LLM.\n    This block identifies and stores discrete facts disclosed during the conversation,\n    structuring them in XML format for easy parsing and retrieval.\n    \"\"\"\n    name: str = Field(\n        default=\"ExtractedFacts\", description=\"The name of the memory block.\"",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.memory_blocks.fact",
        "documentation": {}
    },
    {
        "label": "DEFAULT_FACT_EXTRACT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.memory_blocks.fact",
        "description": "reference_code.llama-index-core.llama_index.core.memory.memory_blocks.fact",
        "peekOfCode": "DEFAULT_FACT_EXTRACT_PROMPT = RichPromptTemplate(\"\"\"You are a precise fact extraction system designed to identify key information from conversations.\nINSTRUCTIONS:\n1. Review the conversation segment provided prior to this message\n2. Extract specific, concrete facts the user has disclosed or important information discovered\n3. Focus on factual information like preferences, personal details, requirements, constraints, or context\n4. Format each fact as a separate <fact> XML tag\n5. Do not include opinions, summaries, or interpretations - only extract explicit information\n6. Do not duplicate facts that are already in the existing facts list\n<existing_facts>\n{{ existing_facts }}",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.memory_blocks.fact",
        "documentation": {}
    },
    {
        "label": "DEFAULT_FACT_CONDENSE_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.memory_blocks.fact",
        "description": "reference_code.llama-index-core.llama_index.core.memory.memory_blocks.fact",
        "peekOfCode": "DEFAULT_FACT_CONDENSE_PROMPT = RichPromptTemplate(\"\"\"You are a precise fact condensing system designed to identify key information from conversations.\nINSTRUCTIONS:\n1. Review the current list of existing facts\n2. Condense the facts into a more concise list, less than {{ max_facts }} facts\n3. Focus on factual information like preferences, personal details, requirements, constraints, or context\n4. Format each fact as a separate <fact> XML tag\n5. Do not include opinions, summaries, or interpretations - only extract explicit information\n6. Do not duplicate facts that are already in the existing facts list\n<existing_facts>\n{{ existing_facts }}",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.memory_blocks.fact",
        "documentation": {}
    },
    {
        "label": "StaticMemoryBlock",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.memory_blocks.static",
        "description": "reference_code.llama-index-core.llama_index.core.memory.memory_blocks.static",
        "peekOfCode": "class StaticMemoryBlock(BaseMemoryBlock[List[ContentBlock]]):\n    \"\"\"\n    A memory block that returns static text.\n    This block is useful for including constant information or instructions\n    in the context without relying on external processing.\n    \"\"\"\n    name: str = Field(\n        default=\"StaticContent\", description=\"The name of the memory block.\"\n    )\n    static_content: Union[List[ContentBlock]] = Field(",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.memory_blocks.static",
        "documentation": {}
    },
    {
        "label": "VectorMemoryBlock",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.memory_blocks.vector",
        "description": "reference_code.llama-index-core.llama_index.core.memory.memory_blocks.vector",
        "peekOfCode": "class VectorMemoryBlock(BaseMemoryBlock[str]):\n    \"\"\"\n    A memory block that retrieves relevant information from a vector store.\n    This block stores conversation history in a vector store and retrieves\n    relevant information based on the most recent messages.\n    \"\"\"\n    name: str = Field(\n        default=\"RetrievedMessages\", description=\"The name of the memory block.\"\n    )\n    vector_store: BasePydanticVectorStore = Field(",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.memory_blocks.vector",
        "documentation": {}
    },
    {
        "label": "get_default_embed_model",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.memory_blocks.vector",
        "description": "reference_code.llama-index-core.llama_index.core.memory.memory_blocks.vector",
        "peekOfCode": "def get_default_embed_model() -> BaseEmbedding:\n    return Settings.embed_model\nclass VectorMemoryBlock(BaseMemoryBlock[str]):\n    \"\"\"\n    A memory block that retrieves relevant information from a vector store.\n    This block stores conversation history in a vector store and retrieves\n    relevant information based on the most recent messages.\n    \"\"\"\n    name: str = Field(\n        default=\"RetrievedMessages\", description=\"The name of the memory block.\"",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.memory_blocks.vector",
        "documentation": {}
    },
    {
        "label": "DEFAULT_RETRIEVED_TEXT_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.memory_blocks.vector",
        "description": "reference_code.llama-index-core.llama_index.core.memory.memory_blocks.vector",
        "peekOfCode": "DEFAULT_RETRIEVED_TEXT_TEMPLATE = RichPromptTemplate(\"{{ text }}\")\ndef get_default_embed_model() -> BaseEmbedding:\n    return Settings.embed_model\nclass VectorMemoryBlock(BaseMemoryBlock[str]):\n    \"\"\"\n    A memory block that retrieves relevant information from a vector store.\n    This block stores conversation history in a vector store and retrieves\n    relevant information based on the most recent messages.\n    \"\"\"\n    name: str = Field(",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.memory_blocks.vector",
        "documentation": {}
    },
    {
        "label": "ChatMemoryBuffer",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.chat_memory_buffer",
        "description": "reference_code.llama-index-core.llama_index.core.memory.chat_memory_buffer",
        "peekOfCode": "class ChatMemoryBuffer(BaseChatStoreMemory):\n    \"\"\"\n    Deprecated: Please use `llama_index.core.memory.Memory` instead.\n    Simple buffer for storing chat history.\n    \"\"\"\n    token_limit: int\n    tokenizer_fn: Callable[[str], List] = Field(\n        default_factory=get_tokenizer,\n        exclude=True,\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.chat_memory_buffer",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TOKEN_LIMIT_RATIO",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.chat_memory_buffer",
        "description": "reference_code.llama-index-core.llama_index.core.memory.chat_memory_buffer",
        "peekOfCode": "DEFAULT_TOKEN_LIMIT_RATIO = 0.75\nDEFAULT_TOKEN_LIMIT = 3000\nclass ChatMemoryBuffer(BaseChatStoreMemory):\n    \"\"\"\n    Deprecated: Please use `llama_index.core.memory.Memory` instead.\n    Simple buffer for storing chat history.\n    \"\"\"\n    token_limit: int\n    tokenizer_fn: Callable[[str], List] = Field(\n        default_factory=get_tokenizer,",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.chat_memory_buffer",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TOKEN_LIMIT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.chat_memory_buffer",
        "description": "reference_code.llama-index-core.llama_index.core.memory.chat_memory_buffer",
        "peekOfCode": "DEFAULT_TOKEN_LIMIT = 3000\nclass ChatMemoryBuffer(BaseChatStoreMemory):\n    \"\"\"\n    Deprecated: Please use `llama_index.core.memory.Memory` instead.\n    Simple buffer for storing chat history.\n    \"\"\"\n    token_limit: int\n    tokenizer_fn: Callable[[str], List] = Field(\n        default_factory=get_tokenizer,\n        exclude=True,",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.chat_memory_buffer",
        "documentation": {}
    },
    {
        "label": "ChatSummaryMemoryBuffer",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.chat_summary_memory_buffer",
        "description": "reference_code.llama-index-core.llama_index.core.memory.chat_summary_memory_buffer",
        "peekOfCode": "class ChatSummaryMemoryBuffer(BaseMemory):\n    \"\"\"\n    Deprecated: Please use `llama_index.core.memory.Memory` instead.\n    Buffer for storing chat history that uses the full text for the latest\n    {token_limit}.\n    All older messages are iteratively summarized using the {llm} provided, with\n    the max number of tokens defined by the {llm}.\n    User can specify whether initial tokens (usually a system prompt)\n    should be counted as part of the {token_limit}\n    using the parameter {count_initial_tokens}.",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.chat_summary_memory_buffer",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TOKEN_LIMIT_RATIO",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.chat_summary_memory_buffer",
        "description": "reference_code.llama-index-core.llama_index.core.memory.chat_summary_memory_buffer",
        "peekOfCode": "DEFAULT_TOKEN_LIMIT_RATIO = 0.75\nDEFAULT_TOKEN_LIMIT = 2000\nSUMMARIZE_PROMPT = \"The following is a conversation between the user and assistant. Write a concise summary about the contents of this conversation.\"\nlogger = logging.getLogger(__name__)\n# TODO: Add option for last N user/assistant history interactions instead of token limit\nclass ChatSummaryMemoryBuffer(BaseMemory):\n    \"\"\"\n    Deprecated: Please use `llama_index.core.memory.Memory` instead.\n    Buffer for storing chat history that uses the full text for the latest\n    {token_limit}.",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.chat_summary_memory_buffer",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TOKEN_LIMIT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.chat_summary_memory_buffer",
        "description": "reference_code.llama-index-core.llama_index.core.memory.chat_summary_memory_buffer",
        "peekOfCode": "DEFAULT_TOKEN_LIMIT = 2000\nSUMMARIZE_PROMPT = \"The following is a conversation between the user and assistant. Write a concise summary about the contents of this conversation.\"\nlogger = logging.getLogger(__name__)\n# TODO: Add option for last N user/assistant history interactions instead of token limit\nclass ChatSummaryMemoryBuffer(BaseMemory):\n    \"\"\"\n    Deprecated: Please use `llama_index.core.memory.Memory` instead.\n    Buffer for storing chat history that uses the full text for the latest\n    {token_limit}.\n    All older messages are iteratively summarized using the {llm} provided, with",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.chat_summary_memory_buffer",
        "documentation": {}
    },
    {
        "label": "SUMMARIZE_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.chat_summary_memory_buffer",
        "description": "reference_code.llama-index-core.llama_index.core.memory.chat_summary_memory_buffer",
        "peekOfCode": "SUMMARIZE_PROMPT = \"The following is a conversation between the user and assistant. Write a concise summary about the contents of this conversation.\"\nlogger = logging.getLogger(__name__)\n# TODO: Add option for last N user/assistant history interactions instead of token limit\nclass ChatSummaryMemoryBuffer(BaseMemory):\n    \"\"\"\n    Deprecated: Please use `llama_index.core.memory.Memory` instead.\n    Buffer for storing chat history that uses the full text for the latest\n    {token_limit}.\n    All older messages are iteratively summarized using the {llm} provided, with\n    the max number of tokens defined by the {llm}.",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.chat_summary_memory_buffer",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.chat_summary_memory_buffer",
        "description": "reference_code.llama-index-core.llama_index.core.memory.chat_summary_memory_buffer",
        "peekOfCode": "logger = logging.getLogger(__name__)\n# TODO: Add option for last N user/assistant history interactions instead of token limit\nclass ChatSummaryMemoryBuffer(BaseMemory):\n    \"\"\"\n    Deprecated: Please use `llama_index.core.memory.Memory` instead.\n    Buffer for storing chat history that uses the full text for the latest\n    {token_limit}.\n    All older messages are iteratively summarized using the {llm} provided, with\n    the max number of tokens defined by the {llm}.\n    User can specify whether initial tokens (usually a system prompt)",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.chat_summary_memory_buffer",
        "documentation": {}
    },
    {
        "label": "InsertMethod",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "description": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "peekOfCode": "class InsertMethod(Enum):\n    SYSTEM = \"system\"\n    USER = \"user\"\ndef generate_chat_store_key() -> str:\n    \"\"\"Generate a unique chat store key.\"\"\"\n    return str(uuid.uuid4())\ndef get_default_chat_store() -> SQLAlchemyChatStore:\n    \"\"\"Get the default chat store.\"\"\"\n    return SQLAlchemyChatStore(table_name=\"llama_index_memory\")\nclass BaseMemoryBlock(BaseModel, Generic[T]):",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "documentation": {}
    },
    {
        "label": "BaseMemoryBlock",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "description": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "peekOfCode": "class BaseMemoryBlock(BaseModel, Generic[T]):\n    \"\"\"\n    A base class for memory blocks.\n    Subclasses must implement the `aget` and `aput` methods.\n    Optionally, subclasses can implement the `atruncate` method, which is used to reduce the size of the memory block.\n    \"\"\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    name: str = Field(description=\"The name/identifier of the memory block.\")\n    description: Optional[str] = Field(\n        default=None, description=\"A description of the memory block.\"",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "documentation": {}
    },
    {
        "label": "Memory",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "description": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "peekOfCode": "class Memory(BaseMemory):\n    \"\"\"\n    A memory module that waterfalls into memory blocks.\n    Works by orchestrating around\n    - a FIFO queue of messages\n    - a list of memory blocks\n    - various parameters (pressure size, token limit, etc.)\n    When the FIFO queue reaches the token limit, the oldest messages within the pressure size are ejected from the FIFO queue.\n    The messages are then processed by each memory block.\n    When pulling messages from this memory, the memory blocks are processed in order, and the messages are injected into the system message or the latest user message.",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "documentation": {}
    },
    {
        "label": "generate_chat_store_key",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "description": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "peekOfCode": "def generate_chat_store_key() -> str:\n    \"\"\"Generate a unique chat store key.\"\"\"\n    return str(uuid.uuid4())\ndef get_default_chat_store() -> SQLAlchemyChatStore:\n    \"\"\"Get the default chat store.\"\"\"\n    return SQLAlchemyChatStore(table_name=\"llama_index_memory\")\nclass BaseMemoryBlock(BaseModel, Generic[T]):\n    \"\"\"\n    A base class for memory blocks.\n    Subclasses must implement the `aget` and `aput` methods.",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "documentation": {}
    },
    {
        "label": "get_default_chat_store",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "description": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "peekOfCode": "def get_default_chat_store() -> SQLAlchemyChatStore:\n    \"\"\"Get the default chat store.\"\"\"\n    return SQLAlchemyChatStore(table_name=\"llama_index_memory\")\nclass BaseMemoryBlock(BaseModel, Generic[T]):\n    \"\"\"\n    A base class for memory blocks.\n    Subclasses must implement the `aget` and `aput` methods.\n    Optionally, subclasses can implement the `atruncate` method, which is used to reduce the size of the memory block.\n    \"\"\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "documentation": {}
    },
    {
        "label": "T",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "description": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "peekOfCode": "T = TypeVar(\"T\", str, List[ContentBlock], List[ChatMessage])\nDEFAULT_TOKEN_LIMIT = 30000\nDEFAULT_FLUSH_SIZE = int(DEFAULT_TOKEN_LIMIT * 0.1)\nDEFAULT_MEMORY_BLOCKS_TEMPLATE = RichPromptTemplate(\n    \"\"\"\n<memory>\n{% for (block_name, block_content) in memory_blocks %}\n<{{ block_name }}>\n  {% for block in block_content %}\n    {% if block.block_type == \"text\" %}",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TOKEN_LIMIT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "description": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "peekOfCode": "DEFAULT_TOKEN_LIMIT = 30000\nDEFAULT_FLUSH_SIZE = int(DEFAULT_TOKEN_LIMIT * 0.1)\nDEFAULT_MEMORY_BLOCKS_TEMPLATE = RichPromptTemplate(\n    \"\"\"\n<memory>\n{% for (block_name, block_content) in memory_blocks %}\n<{{ block_name }}>\n  {% for block in block_content %}\n    {% if block.block_type == \"text\" %}\n{{ block.text }}",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "documentation": {}
    },
    {
        "label": "DEFAULT_FLUSH_SIZE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "description": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "peekOfCode": "DEFAULT_FLUSH_SIZE = int(DEFAULT_TOKEN_LIMIT * 0.1)\nDEFAULT_MEMORY_BLOCKS_TEMPLATE = RichPromptTemplate(\n    \"\"\"\n<memory>\n{% for (block_name, block_content) in memory_blocks %}\n<{{ block_name }}>\n  {% for block in block_content %}\n    {% if block.block_type == \"text\" %}\n{{ block.text }}\n    {% elif block.block_type == \"image\" %}",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MEMORY_BLOCKS_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "description": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "peekOfCode": "DEFAULT_MEMORY_BLOCKS_TEMPLATE = RichPromptTemplate(\n    \"\"\"\n<memory>\n{% for (block_name, block_content) in memory_blocks %}\n<{{ block_name }}>\n  {% for block in block_content %}\n    {% if block.block_type == \"text\" %}\n{{ block.text }}\n    {% elif block.block_type == \"image\" %}\n      {% if block.url %}",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.memory",
        "documentation": {}
    },
    {
        "label": "SimpleComposableMemory",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.simple_composable_memory",
        "description": "reference_code.llama-index-core.llama_index.core.memory.simple_composable_memory",
        "peekOfCode": "class SimpleComposableMemory(BaseMemory):\n    \"\"\"\n    Deprecated: Please use `llama_index.core.memory.Memory` instead.\n    A simple composition of potentially several memory sources.\n    This composable memory considers one of the memory sources as the main\n    one and the others as secondary. The secondary memory sources get added to\n    the chat history only in either the system prompt or to the first user\n    message within the chat history.\n    Args:\n        primary_memory: (BaseMemory) The main memory buffer for agent.",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.simple_composable_memory",
        "documentation": {}
    },
    {
        "label": "DEFAULT_INTRO_HISTORY_MESSAGE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.simple_composable_memory",
        "description": "reference_code.llama-index-core.llama_index.core.memory.simple_composable_memory",
        "peekOfCode": "DEFAULT_INTRO_HISTORY_MESSAGE = \"Below are a set of relevant dialogues retrieved from potentially several memory sources:\"\nDEFAULT_OUTRO_HISTORY_MESSAGE = \"This is the end of the retrieved message dialogues.\"\nclass SimpleComposableMemory(BaseMemory):\n    \"\"\"\n    Deprecated: Please use `llama_index.core.memory.Memory` instead.\n    A simple composition of potentially several memory sources.\n    This composable memory considers one of the memory sources as the main\n    one and the others as secondary. The secondary memory sources get added to\n    the chat history only in either the system prompt or to the first user\n    message within the chat history.",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.simple_composable_memory",
        "documentation": {}
    },
    {
        "label": "DEFAULT_OUTRO_HISTORY_MESSAGE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.simple_composable_memory",
        "description": "reference_code.llama-index-core.llama_index.core.memory.simple_composable_memory",
        "peekOfCode": "DEFAULT_OUTRO_HISTORY_MESSAGE = \"This is the end of the retrieved message dialogues.\"\nclass SimpleComposableMemory(BaseMemory):\n    \"\"\"\n    Deprecated: Please use `llama_index.core.memory.Memory` instead.\n    A simple composition of potentially several memory sources.\n    This composable memory considers one of the memory sources as the main\n    one and the others as secondary. The secondary memory sources get added to\n    the chat history only in either the system prompt or to the first user\n    message within the chat history.\n    Args:",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.simple_composable_memory",
        "documentation": {}
    },
    {
        "label": "BaseMemory",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.types",
        "description": "reference_code.llama-index-core.llama_index.core.memory.types",
        "peekOfCode": "class BaseMemory(BaseComponent):\n    \"\"\"Base class for all memory types.\"\"\"\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Get class name.\"\"\"\n        return \"BaseMemory\"\n    @classmethod\n    @abstractmethod\n    def from_defaults(\n        cls,",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.types",
        "documentation": {}
    },
    {
        "label": "BaseChatStoreMemory",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.types",
        "description": "reference_code.llama-index-core.llama_index.core.memory.types",
        "peekOfCode": "class BaseChatStoreMemory(BaseMemory):\n    \"\"\"Base class for storing multi-tenant chat history.\"\"\"\n    chat_store: SerializeAsAny[BaseChatStore] = Field(default_factory=SimpleChatStore)\n    chat_store_key: str = Field(default=DEFAULT_CHAT_STORE_KEY)\n    @field_serializer(\"chat_store\")\n    def serialize_courses_in_order(self, chat_store: BaseChatStore) -> dict:\n        res = chat_store.model_dump()\n        res.update({\"class_name\": chat_store.class_name()})\n        return res\n    @classmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CHAT_STORE_KEY",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.types",
        "description": "reference_code.llama-index-core.llama_index.core.memory.types",
        "peekOfCode": "DEFAULT_CHAT_STORE_KEY = \"chat_history\"\nclass BaseMemory(BaseComponent):\n    \"\"\"Base class for all memory types.\"\"\"\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Get class name.\"\"\"\n        return \"BaseMemory\"\n    @classmethod\n    @abstractmethod\n    def from_defaults(",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.types",
        "documentation": {}
    },
    {
        "label": "VectorMemory",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.memory.vector_memory",
        "description": "reference_code.llama-index-core.llama_index.core.memory.vector_memory",
        "peekOfCode": "class VectorMemory(BaseMemory):\n    \"\"\"\n    Deprecated: Please use `llama_index.core.memory.Memory` instead.\n    Memory backed by a vector index.\n    NOTE: This class requires the `delete_nodes` method to be implemented\n    by the vector store underlying the vector index. At time of writing (May 2024),\n    Chroma, Qdrant and SimpleVectorStore all support delete_nodes.\n    \"\"\"\n    vector_index: Any\n    retriever_kwargs: Dict[str, Any] = Field(default_factory=dict)",
        "detail": "reference_code.llama-index-core.llama_index.core.memory.vector_memory",
        "documentation": {}
    },
    {
        "label": "MultiModalLLMMetadata",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.base",
        "description": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.base",
        "peekOfCode": "class MultiModalLLMMetadata(BaseModel):\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    context_window: Optional[int] = Field(\n        default=DEFAULT_CONTEXT_WINDOW,\n        description=(\n            \"Total number of tokens the model can be input when generating a response.\"\n        ),\n    )\n    num_output: Optional[int] = Field(\n        default=DEFAULT_NUM_OUTPUTS,",
        "detail": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.base",
        "documentation": {}
    },
    {
        "label": "MultiModalLLM",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.base",
        "description": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.base",
        "peekOfCode": "class MultiModalLLM(BaseComponent, DispatcherSpanMixin):\n    \"\"\"Multi-Modal LLM interface.\"\"\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    callback_manager: CallbackManager = Field(\n        default_factory=CallbackManager, exclude=True\n    )\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        # Help static checkers understand this class hierarchy\n        super().__init__(*args, **kwargs)\n    @property",
        "detail": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.base",
        "documentation": {}
    },
    {
        "label": "load_image_urls",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.generic_utils",
        "peekOfCode": "def load_image_urls(image_urls: List[str]) -> List[ImageDocument]:\n    \"\"\"\n    Convert a list of image URLs into ImageDocument objects.\n    Args:\n        image_urls (List[str]): List of strings containing valid image URLs.\n    Returns:\n        List[ImageDocument]: List of ImageDocument objects.\n    \"\"\"\n    return [ImageDocument(image_url=url) for url in image_urls]\ndef encode_image(image_path: str) -> str:",
        "detail": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "encode_image",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.generic_utils",
        "peekOfCode": "def encode_image(image_path: str) -> str:\n    \"\"\"\n    Create base64 representation of an image.\n    Args:\n        image_path (str): Path to the image file\n    Returns:\n        str: Base64 encoded string of the image\n    Raises:\n        FileNotFoundError: If the `image_path` doesn't exist.\n        IOError: If there's an error reading the file.",
        "detail": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "image_documents_to_base64",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.generic_utils",
        "peekOfCode": "def image_documents_to_base64(\n    image_documents: Sequence[ImageDocument],\n) -> List[str]:\n    \"\"\"\n    Convert ImageDocument objects to base64-encoded strings.\n    Args:\n        image_documents (Sequence[ImageDocument]: Sequence of\n            ImageDocument objects\n    Returns:\n        List[str]: List of base64-encoded image strings",
        "detail": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "infer_image_mimetype_from_file_path",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.generic_utils",
        "peekOfCode": "def infer_image_mimetype_from_file_path(image_file_path: str) -> str:\n    \"\"\"\n    Infer the MIME of an image file based on its file extension.\n    Currently only supports the following types of images:\n        * image/jpeg\n        * image/png\n        * image/gif\n        * image/webp\n    Args:\n        image_file_path (str): Path to the image file.",
        "detail": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "infer_image_mimetype_from_base64",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.generic_utils",
        "peekOfCode": "def infer_image_mimetype_from_base64(base64_string: str) -> Optional[str]:\n    \"\"\"\n    Infer the MIME of an image from the base64 encoding.\n    Args:\n        base64_string (str): Base64-encoded string of the image.\n    Returns:\n        Optional[str]: MIME type of the image: image/jpeg, image/png, image/gif, or image/webp.\n          `None` if the MIME type cannot be inferred.\n    \"\"\"\n    # Decode the base64 string",
        "detail": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "set_base64_and_mimetype_for_image_docs",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.generic_utils",
        "peekOfCode": "def set_base64_and_mimetype_for_image_docs(\n    image_documents: Sequence[ImageDocument],\n) -> Sequence[ImageDocument]:\n    \"\"\"\n    Set the base64 and mimetype fields for the image documents.\n    Args:\n        image_documents (Sequence[ImageDocument]): Sequence of ImageDocument objects.\n    Returns:\n        Sequence[ImageDocument]: ImageDocuments with base64 and detected mimetypes set.\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.generic_utils",
        "description": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.generic_utils",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef load_image_urls(image_urls: List[str]) -> List[ImageDocument]:\n    \"\"\"\n    Convert a list of image URLs into ImageDocument objects.\n    Args:\n        image_urls (List[str]): List of strings containing valid image URLs.\n    Returns:\n        List[ImageDocument]: List of ImageDocument objects.\n    \"\"\"\n    return [ImageDocument(image_url=url) for url in image_urls]",
        "detail": "reference_code.llama-index-core.llama_index.core.multi_modal_llms.generic_utils",
        "documentation": {}
    },
    {
        "label": "HTMLNodeParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.file.html",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.file.html",
        "peekOfCode": "class HTMLNodeParser(NodeParser):\n    \"\"\"\n    HTML node parser.\n    Splits a document into Nodes using custom HTML splitting logic.\n    Args:\n        include_metadata (bool): whether to include metadata in nodes\n        include_prev_next_rel (bool): whether to include prev/next relationships\n    \"\"\"\n    tags: List[str] = Field(\n        default=DEFAULT_TAGS, description=\"HTML tags to extract text from.\"",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.file.html",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TAGS",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.file.html",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.file.html",
        "peekOfCode": "DEFAULT_TAGS = [\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"li\", \"b\", \"i\", \"u\", \"section\"]\nclass HTMLNodeParser(NodeParser):\n    \"\"\"\n    HTML node parser.\n    Splits a document into Nodes using custom HTML splitting logic.\n    Args:\n        include_metadata (bool): whether to include metadata in nodes\n        include_prev_next_rel (bool): whether to include prev/next relationships\n    \"\"\"\n    tags: List[str] = Field(",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.file.html",
        "documentation": {}
    },
    {
        "label": "JSONNodeParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.file.json",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.file.json",
        "peekOfCode": "class JSONNodeParser(NodeParser):\n    \"\"\"\n    JSON node parser.\n    Splits a document into Nodes using custom JSON splitting logic.\n    Args:\n        include_metadata (bool): whether to include metadata in nodes\n        include_prev_next_rel (bool): whether to include prev/next relationships\n    \"\"\"\n    @classmethod\n    def from_defaults(",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.file.json",
        "documentation": {}
    },
    {
        "label": "MarkdownNodeParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.file.markdown",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.file.markdown",
        "peekOfCode": "class MarkdownNodeParser(NodeParser):\n    \"\"\"\n    Markdown node parser.\n    Splits a document into Nodes using Markdown header-based splitting logic.\n    Each node contains its text content and the path of headers leading to it.\n    Args:\n        include_metadata (bool): whether to include metadata in nodes\n        include_prev_next_rel (bool): whether to include prev/next relationships\n        header_path_separator (str): separator char used for section header path metadata\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.file.markdown",
        "documentation": {}
    },
    {
        "label": "SimpleFileNodeParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.file.simple_file",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.file.simple_file",
        "peekOfCode": "class SimpleFileNodeParser(NodeParser):\n    \"\"\"\n    Simple file node parser.\n    Splits a document loaded from a file into Nodes using logic based on the file type\n    automatically detects the NodeParser to use based on file type\n    Args:\n        include_metadata (bool): whether to include metadata in nodes\n        include_prev_next_rel (bool): whether to include prev/next relationships\n    \"\"\"\n    @classmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.file.simple_file",
        "documentation": {}
    },
    {
        "label": "TableColumnOutput",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.relational.base_element",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.relational.base_element",
        "peekOfCode": "class TableColumnOutput(BaseModel):\n    \"\"\"Output from analyzing a table column.\"\"\"\n    col_name: str\n    col_type: str\n    summary: Optional[str] = None\n    def __str__(self) -> str:\n        \"\"\"Convert to string representation.\"\"\"\n        return (\n            f\"Column: {self.col_name}\\nType: {self.col_type}\\nSummary: {self.summary}\"\n        )",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.relational.base_element",
        "documentation": {}
    },
    {
        "label": "TableOutput",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.relational.base_element",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.relational.base_element",
        "peekOfCode": "class TableOutput(BaseModel):\n    \"\"\"Output from analyzing a table.\"\"\"\n    summary: str\n    table_title: Optional[str] = None\n    table_id: Optional[str] = None\n    columns: List[TableColumnOutput]\nclass Element(BaseModel):\n    \"\"\"Element object.\"\"\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    id: str",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.relational.base_element",
        "documentation": {}
    },
    {
        "label": "Element",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.relational.base_element",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.relational.base_element",
        "peekOfCode": "class Element(BaseModel):\n    \"\"\"Element object.\"\"\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    id: str\n    type: str\n    element: Any\n    title_level: Optional[int] = None\n    table_output: Optional[TableOutput] = None\n    table: Optional[Any] = None\n    markdown: Optional[str] = None",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.relational.base_element",
        "documentation": {}
    },
    {
        "label": "BaseElementNodeParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.relational.base_element",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.relational.base_element",
        "peekOfCode": "class BaseElementNodeParser(NodeParser):\n    \"\"\"\n    Splits a document into Text Nodes and Index Nodes corresponding to embedded objects.\n    Supports text and tables currently.\n    \"\"\"\n    callback_manager: CallbackManager = Field(\n        default_factory=lambda: CallbackManager([]), exclude=True\n    )\n    llm: Optional[LLM] = Field(\n        default=None, description=\"LLM model to use for summarization.\"",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.relational.base_element",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SUMMARY_QUERY_STR",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.relational.base_element",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.relational.base_element",
        "peekOfCode": "DEFAULT_SUMMARY_QUERY_STR = \"\"\"\\\nWhat is this table about? Give a very concise summary (imagine you are adding a new caption and summary for this table), \\\nand output the real/existing table title/caption if context provided.\\\nand output the real/existing table id if context provided.\\\nand also output whether or not the table should be kept.\\\n\"\"\"\nclass TableColumnOutput(BaseModel):\n    \"\"\"Output from analyzing a table column.\"\"\"\n    col_name: str\n    col_type: str",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.relational.base_element",
        "documentation": {}
    },
    {
        "label": "HierarchicalNodeParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.relational.hierarchical",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.relational.hierarchical",
        "peekOfCode": "class HierarchicalNodeParser(NodeParser):\n    \"\"\"\n    Hierarchical node parser.\n    Splits a document into a recursive hierarchy Nodes using a NodeParser.\n    NOTE: this will return a hierarchy of nodes in a flat list, where there will be\n    overlap between parent nodes (e.g. with a bigger chunk size), and child nodes\n    per parent (e.g. with a smaller chunk size).\n    For instance, this may return a list of nodes like:\n    - list of top-level nodes with chunk size 2048\n    - list of second-level nodes, where each node is a child of a top-level node,",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.relational.hierarchical",
        "documentation": {}
    },
    {
        "label": "get_leaf_nodes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.relational.hierarchical",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.relational.hierarchical",
        "peekOfCode": "def get_leaf_nodes(nodes: List[BaseNode]) -> List[BaseNode]:\n    \"\"\"Get leaf nodes.\"\"\"\n    leaf_nodes = []\n    for node in nodes:\n        if NodeRelationship.CHILD not in node.relationships:\n            leaf_nodes.append(node)\n    return leaf_nodes\ndef get_root_nodes(nodes: List[BaseNode]) -> List[BaseNode]:\n    \"\"\"Get root nodes.\"\"\"\n    root_nodes = []",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.relational.hierarchical",
        "documentation": {}
    },
    {
        "label": "get_root_nodes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.relational.hierarchical",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.relational.hierarchical",
        "peekOfCode": "def get_root_nodes(nodes: List[BaseNode]) -> List[BaseNode]:\n    \"\"\"Get root nodes.\"\"\"\n    root_nodes = []\n    for node in nodes:\n        if NodeRelationship.PARENT not in node.relationships:\n            root_nodes.append(node)\n    return root_nodes\ndef get_child_nodes(nodes: List[BaseNode], all_nodes: List[BaseNode]) -> List[BaseNode]:\n    \"\"\"Get child nodes of nodes from given all_nodes.\"\"\"\n    children_ids = []",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.relational.hierarchical",
        "documentation": {}
    },
    {
        "label": "get_child_nodes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.relational.hierarchical",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.relational.hierarchical",
        "peekOfCode": "def get_child_nodes(nodes: List[BaseNode], all_nodes: List[BaseNode]) -> List[BaseNode]:\n    \"\"\"Get child nodes of nodes from given all_nodes.\"\"\"\n    children_ids = []\n    for node in nodes:\n        if NodeRelationship.CHILD not in node.relationships:\n            continue\n        children_ids.extend([r.node_id for r in (node.child_nodes or [])])\n    child_nodes = []\n    for candidate_node in all_nodes:\n        if candidate_node.node_id not in children_ids:",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.relational.hierarchical",
        "documentation": {}
    },
    {
        "label": "get_deeper_nodes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.relational.hierarchical",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.relational.hierarchical",
        "peekOfCode": "def get_deeper_nodes(nodes: List[BaseNode], depth: int = 1) -> List[BaseNode]:\n    \"\"\"Get children of root nodes in given nodes that have given depth.\"\"\"\n    if depth < 0:\n        raise ValueError(\"Depth cannot be a negative number!\")\n    root_nodes = get_root_nodes(nodes)\n    if not root_nodes:\n        raise ValueError(\"There is no root nodes in given nodes!\")\n    deeper_nodes = root_nodes\n    for _ in range(depth):\n        deeper_nodes = get_child_nodes(deeper_nodes, nodes)",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.relational.hierarchical",
        "documentation": {}
    },
    {
        "label": "LlamaParseJsonNodeParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.relational.llama_parse_json_element",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.relational.llama_parse_json_element",
        "peekOfCode": "class LlamaParseJsonNodeParser(BaseElementNodeParser):\n    \"\"\"\n    Llama Parse Json format element node parser.\n    Splits a json format document from LlamaParse into Text Nodes and Index Nodes\n    corresponding to embedded objects (e.g. tables).\n    \"\"\"\n    @classmethod\n    def class_name(cls) -> str:\n        return \"LlamaParseJsonNodeParser\"\n    def get_nodes_from_node(self, node: TextNode) -> List[BaseNode]:",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.relational.llama_parse_json_element",
        "documentation": {}
    },
    {
        "label": "MarkdownElementNodeParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.relational.markdown_element",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.relational.markdown_element",
        "peekOfCode": "class MarkdownElementNodeParser(BaseElementNodeParser):\n    \"\"\"\n    Markdown element node parser.\n    Splits a markdown document into Text Nodes and Index Nodes corresponding to embedded objects\n    (e.g. tables).\n    \"\"\"\n    @classmethod\n    def class_name(cls) -> str:\n        return \"MarkdownElementNodeParser\"\n    def get_nodes_from_node(self, node: TextNode) -> List[BaseNode]:",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.relational.markdown_element",
        "documentation": {}
    },
    {
        "label": "UnstructuredElementNodeParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.relational.unstructured_element",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.relational.unstructured_element",
        "peekOfCode": "class UnstructuredElementNodeParser(BaseElementNodeParser):\n    \"\"\"\n    Unstructured element node parser.\n    Splits a document into Text Nodes and Index Nodes corresponding to embedded objects\n    (e.g. tables).\n    \"\"\"\n    partitioning_parameters: Optional[Dict[str, Any]] = Field(\n        default={},\n        description=\"Extra dictionary representing parameters of the partitioning process.\",\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.relational.unstructured_element",
        "documentation": {}
    },
    {
        "label": "md_to_df",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.relational.utils",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.relational.utils",
        "peekOfCode": "def md_to_df(md_str: str) -> Any:\n    \"\"\"Convert Markdown to dataframe.\"\"\"\n    try:\n        import pandas as pd\n    except ImportError:\n        raise ImportError(\n            \"You must install the `pandas` package to use this node parser.\"\n        )\n    # Replace \" by \"\" in md_str\n    md_str = md_str.replace('\"', '\"\"')",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.relational.utils",
        "documentation": {}
    },
    {
        "label": "html_to_df",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.relational.utils",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.relational.utils",
        "peekOfCode": "def html_to_df(html_str: str) -> Any:\n    \"\"\"Convert HTML to dataframe.\"\"\"\n    try:\n        from lxml import html\n    except ImportError:\n        raise ImportError(\n            \"You must install the `lxml` package to use this node parser.\"\n        )\n    try:\n        import pandas as pd",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.relational.utils",
        "documentation": {}
    },
    {
        "label": "CodeSplitter",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.code",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.code",
        "peekOfCode": "class CodeSplitter(TextSplitter):\n    \"\"\"\n    Split code using a AST parser.\n    Thank you to Kevin Lu / SweepAI for suggesting this elegant code splitting solution.\n    https://docs.sweep.dev/blogs/chunking-2m-files\n    \"\"\"\n    language: str = Field(\n        description=\"The programming language of the code being split.\"\n    )\n    chunk_lines: int = Field(",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.code",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CHUNK_LINES",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.code",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.code",
        "peekOfCode": "DEFAULT_CHUNK_LINES = 40\nDEFAULT_LINES_OVERLAP = 15\nDEFAULT_MAX_CHARS = 1500\nclass CodeSplitter(TextSplitter):\n    \"\"\"\n    Split code using a AST parser.\n    Thank you to Kevin Lu / SweepAI for suggesting this elegant code splitting solution.\n    https://docs.sweep.dev/blogs/chunking-2m-files\n    \"\"\"\n    language: str = Field(",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.code",
        "documentation": {}
    },
    {
        "label": "DEFAULT_LINES_OVERLAP",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.code",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.code",
        "peekOfCode": "DEFAULT_LINES_OVERLAP = 15\nDEFAULT_MAX_CHARS = 1500\nclass CodeSplitter(TextSplitter):\n    \"\"\"\n    Split code using a AST parser.\n    Thank you to Kevin Lu / SweepAI for suggesting this elegant code splitting solution.\n    https://docs.sweep.dev/blogs/chunking-2m-files\n    \"\"\"\n    language: str = Field(\n        description=\"The programming language of the code being split.\"",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.code",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MAX_CHARS",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.code",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.code",
        "peekOfCode": "DEFAULT_MAX_CHARS = 1500\nclass CodeSplitter(TextSplitter):\n    \"\"\"\n    Split code using a AST parser.\n    Thank you to Kevin Lu / SweepAI for suggesting this elegant code splitting solution.\n    https://docs.sweep.dev/blogs/chunking-2m-files\n    \"\"\"\n    language: str = Field(\n        description=\"The programming language of the code being split.\"\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.code",
        "documentation": {}
    },
    {
        "label": "LangchainNodeParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.langchain",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.langchain",
        "peekOfCode": "class LangchainNodeParser(TextSplitter):\n    \"\"\"\n    Basic wrapper around langchain's text splitter.\n    TODO: Figure out how to make this metadata aware.\n    \"\"\"\n    _lc_splitter: \"LC_TextSplitter\" = PrivateAttr()\n    def __init__(\n        self,\n        lc_splitter: \"LC_TextSplitter\",\n        callback_manager: Optional[CallbackManager] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.langchain",
        "documentation": {}
    },
    {
        "label": "LanguageConfig",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.semantic_double_merging_splitter",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.semantic_double_merging_splitter",
        "peekOfCode": "class LanguageConfig:\n    def __init__(\n        self,\n        language: str = \"english\",\n        spacy_model: str = \"en_core_web_md\",\n        model_validation: bool = True,\n    ):\n        if language not in LANGUAGES:\n            raise ValueError(\n                f\"{language} language is not supported yet! Available languages: {LANGUAGES}\"",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.semantic_double_merging_splitter",
        "documentation": {}
    },
    {
        "label": "SemanticDoubleMergingSplitterNodeParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.semantic_double_merging_splitter",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.semantic_double_merging_splitter",
        "peekOfCode": "class SemanticDoubleMergingSplitterNodeParser(NodeParser):\n    \"\"\"\n    Semantic double merging text splitter.\n    Splits a document into Nodes, with each node being a group of semantically related sentences.\n    Args:\n        language_config (LanguageConfig): chooses language and spacy language model to be used\n        initial_threshold (float): sets threshold for initializing new chunk\n        appending_threshold (float): sets threshold for appending new sentences to chunk\n        merging_threshold (float): sets threshold for merging whole chunks\n        max_chunk_size (int): maximum size of chunk (in characters)",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.semantic_double_merging_splitter",
        "documentation": {}
    },
    {
        "label": "DEFAULT_OG_TEXT_METADATA_KEY",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.semantic_double_merging_splitter",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.semantic_double_merging_splitter",
        "peekOfCode": "DEFAULT_OG_TEXT_METADATA_KEY = \"original_text\"\n# TODO test more languages\nLANGUAGES: List[str] = [\"english\", \"german\", \"spanish\"]\nLANGUAGE_MODELS: Dict[str, List[str]] = {\n    \"english\": [\"en_core_web_md\", \"en_core_web_lg\"],\n    \"german\": [\"de_core_news_md\", \"de_core_news_lg\"],\n    \"spanish\": [\"es_core_news_md\", \"es_core_news_lg\"],\n}\nclass LanguageConfig:\n    def __init__(",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.semantic_double_merging_splitter",
        "documentation": {}
    },
    {
        "label": "SentenceCombination",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.semantic_splitter",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.semantic_splitter",
        "peekOfCode": "class SentenceCombination(TypedDict):\n    sentence: str\n    index: int\n    combined_sentence: str\n    combined_sentence_embedding: List[float]\nSentenceSplitterCallable = Annotated[\n    Callable[[str], List[str]],\n    WithJsonSchema({\"type\": \"string\"}, mode=\"serialization\"),\n    WithJsonSchema({\"type\": \"string\"}, mode=\"validation\"),\n]",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.semantic_splitter",
        "documentation": {}
    },
    {
        "label": "SemanticSplitterNodeParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.semantic_splitter",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.semantic_splitter",
        "peekOfCode": "class SemanticSplitterNodeParser(NodeParser):\n    \"\"\"\n    Semantic node parser.\n    Splits a document into Nodes, with each node being a group of semantically related sentences.\n    Args:\n        buffer_size (int): number of sentences to group together when evaluating semantic similarity\n        embed_model: (BaseEmbedding): embedding model to use\n        sentence_splitter (Optional[Callable]): splits text into sentences\n        include_metadata (bool): whether to include metadata in nodes\n        include_prev_next_rel (bool): whether to include prev/next relationships",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.semantic_splitter",
        "documentation": {}
    },
    {
        "label": "DEFAULT_OG_TEXT_METADATA_KEY",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.semantic_splitter",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.semantic_splitter",
        "peekOfCode": "DEFAULT_OG_TEXT_METADATA_KEY = \"original_text\"\nclass SentenceCombination(TypedDict):\n    sentence: str\n    index: int\n    combined_sentence: str\n    combined_sentence_embedding: List[float]\nSentenceSplitterCallable = Annotated[\n    Callable[[str], List[str]],\n    WithJsonSchema({\"type\": \"string\"}, mode=\"serialization\"),\n    WithJsonSchema({\"type\": \"string\"}, mode=\"validation\"),",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.semantic_splitter",
        "documentation": {}
    },
    {
        "label": "SentenceSplitterCallable",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.semantic_splitter",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.semantic_splitter",
        "peekOfCode": "SentenceSplitterCallable = Annotated[\n    Callable[[str], List[str]],\n    WithJsonSchema({\"type\": \"string\"}, mode=\"serialization\"),\n    WithJsonSchema({\"type\": \"string\"}, mode=\"validation\"),\n]\nclass SemanticSplitterNodeParser(NodeParser):\n    \"\"\"\n    Semantic node parser.\n    Splits a document into Nodes, with each node being a group of semantically related sentences.\n    Args:",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.semantic_splitter",
        "documentation": {}
    },
    {
        "label": "_Split",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence",
        "peekOfCode": "class _Split:\n    text: str  # the split text\n    is_sentence: bool  # save whether this is a full sentence\n    token_size: int  # token length of split text\nclass SentenceSplitter(MetadataAwareTextSplitter):\n    \"\"\"\n    Parse text with a preference for complete sentences.\n    In general, this class tries to keep sentences and paragraphs together. Therefore\n    compared to the original TokenTextSplitter, there are less likely to be\n    hanging sentences or parts of sentences at the end of the node chunk.",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence",
        "documentation": {}
    },
    {
        "label": "SentenceSplitter",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence",
        "peekOfCode": "class SentenceSplitter(MetadataAwareTextSplitter):\n    \"\"\"\n    Parse text with a preference for complete sentences.\n    In general, this class tries to keep sentences and paragraphs together. Therefore\n    compared to the original TokenTextSplitter, there are less likely to be\n    hanging sentences or parts of sentences at the end of the node chunk.\n    \"\"\"\n    chunk_size: int = Field(\n        default=DEFAULT_CHUNK_SIZE,\n        description=\"The token chunk size for each chunk.\",",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence",
        "documentation": {}
    },
    {
        "label": "SENTENCE_CHUNK_OVERLAP",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence",
        "peekOfCode": "SENTENCE_CHUNK_OVERLAP = 200\nCHUNKING_REGEX = \"[^,.;]+[,.;]?|[,.;]\"\nDEFAULT_PARAGRAPH_SEP = \"\\n\\n\\n\"\n@dataclass\nclass _Split:\n    text: str  # the split text\n    is_sentence: bool  # save whether this is a full sentence\n    token_size: int  # token length of split text\nclass SentenceSplitter(MetadataAwareTextSplitter):\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence",
        "documentation": {}
    },
    {
        "label": "CHUNKING_REGEX",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence",
        "peekOfCode": "CHUNKING_REGEX = \"[^,.;]+[,.;]?|[,.;]\"\nDEFAULT_PARAGRAPH_SEP = \"\\n\\n\\n\"\n@dataclass\nclass _Split:\n    text: str  # the split text\n    is_sentence: bool  # save whether this is a full sentence\n    token_size: int  # token length of split text\nclass SentenceSplitter(MetadataAwareTextSplitter):\n    \"\"\"\n    Parse text with a preference for complete sentences.",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PARAGRAPH_SEP",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence",
        "peekOfCode": "DEFAULT_PARAGRAPH_SEP = \"\\n\\n\\n\"\n@dataclass\nclass _Split:\n    text: str  # the split text\n    is_sentence: bool  # save whether this is a full sentence\n    token_size: int  # token length of split text\nclass SentenceSplitter(MetadataAwareTextSplitter):\n    \"\"\"\n    Parse text with a preference for complete sentences.\n    In general, this class tries to keep sentences and paragraphs together. Therefore",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence",
        "documentation": {}
    },
    {
        "label": "SentenceWindowNodeParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence_window",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence_window",
        "peekOfCode": "class SentenceWindowNodeParser(NodeParser):\n    \"\"\"\n    Sentence window node parser.\n    Splits a document into Nodes, with each node being a sentence.\n    Each node contains a window from the surrounding sentences in the metadata.\n    Args:\n        sentence_splitter (Optional[Callable]): splits text into sentences\n        include_metadata (bool): whether to include metadata in nodes\n        include_prev_next_rel (bool): whether to include prev/next relationships\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence_window",
        "documentation": {}
    },
    {
        "label": "DEFAULT_WINDOW_SIZE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence_window",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence_window",
        "peekOfCode": "DEFAULT_WINDOW_SIZE = 3\nDEFAULT_WINDOW_METADATA_KEY = \"window\"\nDEFAULT_OG_TEXT_METADATA_KEY = \"original_text\"\nclass SentenceWindowNodeParser(NodeParser):\n    \"\"\"\n    Sentence window node parser.\n    Splits a document into Nodes, with each node being a sentence.\n    Each node contains a window from the surrounding sentences in the metadata.\n    Args:\n        sentence_splitter (Optional[Callable]): splits text into sentences",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence_window",
        "documentation": {}
    },
    {
        "label": "DEFAULT_WINDOW_METADATA_KEY",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence_window",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence_window",
        "peekOfCode": "DEFAULT_WINDOW_METADATA_KEY = \"window\"\nDEFAULT_OG_TEXT_METADATA_KEY = \"original_text\"\nclass SentenceWindowNodeParser(NodeParser):\n    \"\"\"\n    Sentence window node parser.\n    Splits a document into Nodes, with each node being a sentence.\n    Each node contains a window from the surrounding sentences in the metadata.\n    Args:\n        sentence_splitter (Optional[Callable]): splits text into sentences\n        include_metadata (bool): whether to include metadata in nodes",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence_window",
        "documentation": {}
    },
    {
        "label": "DEFAULT_OG_TEXT_METADATA_KEY",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence_window",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence_window",
        "peekOfCode": "DEFAULT_OG_TEXT_METADATA_KEY = \"original_text\"\nclass SentenceWindowNodeParser(NodeParser):\n    \"\"\"\n    Sentence window node parser.\n    Splits a document into Nodes, with each node being a sentence.\n    Each node contains a window from the surrounding sentences in the metadata.\n    Args:\n        sentence_splitter (Optional[Callable]): splits text into sentences\n        include_metadata (bool): whether to include metadata in nodes\n        include_prev_next_rel (bool): whether to include prev/next relationships",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.sentence_window",
        "documentation": {}
    },
    {
        "label": "TokenTextSplitter",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.token",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.token",
        "peekOfCode": "class TokenTextSplitter(MetadataAwareTextSplitter):\n    \"\"\"Implementation of splitting text that looks at word tokens.\"\"\"\n    chunk_size: int = Field(\n        default=DEFAULT_CHUNK_SIZE,\n        description=\"The token chunk size for each chunk.\",\n        gt=0,\n    )\n    chunk_overlap: int = Field(\n        default=DEFAULT_CHUNK_OVERLAP,\n        description=\"The token overlap of each chunk when splitting.\",",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.token",
        "documentation": {}
    },
    {
        "label": "_logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.token",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.token",
        "peekOfCode": "_logger = logging.getLogger(__name__)\n# NOTE: this is the number of tokens we reserve for metadata formatting\nDEFAULT_METADATA_FORMAT_LEN = 2\nclass TokenTextSplitter(MetadataAwareTextSplitter):\n    \"\"\"Implementation of splitting text that looks at word tokens.\"\"\"\n    chunk_size: int = Field(\n        default=DEFAULT_CHUNK_SIZE,\n        description=\"The token chunk size for each chunk.\",\n        gt=0,\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.token",
        "documentation": {}
    },
    {
        "label": "DEFAULT_METADATA_FORMAT_LEN",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.token",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.token",
        "peekOfCode": "DEFAULT_METADATA_FORMAT_LEN = 2\nclass TokenTextSplitter(MetadataAwareTextSplitter):\n    \"\"\"Implementation of splitting text that looks at word tokens.\"\"\"\n    chunk_size: int = Field(\n        default=DEFAULT_CHUNK_SIZE,\n        description=\"The token chunk size for each chunk.\",\n        gt=0,\n    )\n    chunk_overlap: int = Field(\n        default=DEFAULT_CHUNK_OVERLAP,",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.token",
        "documentation": {}
    },
    {
        "label": "truncate_text",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "peekOfCode": "def truncate_text(text: str, text_splitter: TextSplitter) -> str:\n    \"\"\"\n    Truncate text to fit within the chunk size.\n    Args:\n        text (str): The text to truncate.\n        text_splitter (TextSplitter): The splitter to use for chunking.\n    Returns:\n        str: The first chunk of the split text that fits within the chunk size.\n    \"\"\"\n    chunks = text_splitter.split_text(text)",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "documentation": {}
    },
    {
        "label": "split_text_keep_separator",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "peekOfCode": "def split_text_keep_separator(text: str, separator: str) -> List[str]:\n    \"\"\"\n    Split text with separator and keep the separator at the end of each split.\n    Args:\n        text (str): The text to split.\n        separator (str): The separator to split on.\n    Returns:\n        List[str]: List of text segments with separators preserved at the end of each split.\n    \"\"\"\n    parts = text.split(separator)",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "documentation": {}
    },
    {
        "label": "split_by_sep",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "peekOfCode": "def split_by_sep(sep: str, keep_sep: bool = True) -> Callable[[str], List[str]]:\n    \"\"\"\n    Create a function that splits text by a separator.\n    Args:\n        sep (str): The separator to split on.\n        keep_sep (bool, optional): Whether to keep the separator in the output. Defaults to True.\n    Returns:\n        Callable[[str], List[str]]: A function that takes a string and returns a list of split strings.\n    \"\"\"\n    if keep_sep:",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "documentation": {}
    },
    {
        "label": "split_by_char",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "peekOfCode": "def split_by_char() -> Callable[[str], List[str]]:\n    \"\"\"\n    Create a function that splits text into individual characters.\n    Returns:\n        Callable[[str], List[str]]: A function that takes a string and returns a list of individual characters.\n    \"\"\"\n    return lambda text: list(text)\ndef split_by_sentence_tokenizer_internal(text: str, tokenizer: Any) -> List[str]:\n    \"\"\"\n    Get the spans and then return the sentences.",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "documentation": {}
    },
    {
        "label": "split_by_sentence_tokenizer_internal",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "peekOfCode": "def split_by_sentence_tokenizer_internal(text: str, tokenizer: Any) -> List[str]:\n    \"\"\"\n    Get the spans and then return the sentences.\n    Using the start index of each span\n    Instead of using end, use the start of the next span if available\n    \"\"\"\n    spans = list(tokenizer.span_tokenize(text))\n    sentences = []\n    for i, span in enumerate(spans):\n        start = span[0]",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "documentation": {}
    },
    {
        "label": "split_by_sentence_tokenizer",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "peekOfCode": "def split_by_sentence_tokenizer() -> Callable[[str], List[str]]:\n    return lambda text: split_by_sentence_tokenizer_internal(\n        text, globals_helper.punkt_tokenizer\n    )\ndef split_by_regex(regex: str) -> Callable[[str], List[str]]:\n    \"\"\"\n    Create a function that splits text using a regular expression pattern.\n    Args:\n        regex (str): The regular expression pattern to use for splitting.\n    Returns:",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "documentation": {}
    },
    {
        "label": "split_by_regex",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "peekOfCode": "def split_by_regex(regex: str) -> Callable[[str], List[str]]:\n    \"\"\"\n    Create a function that splits text using a regular expression pattern.\n    Args:\n        regex (str): The regular expression pattern to use for splitting.\n    Returns:\n        Callable[[str], List[str]]: A function that takes a string and returns a list of matches based on the regex pattern.\n    \"\"\"\n    import re\n    return lambda text: re.findall(regex, text)",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "documentation": {}
    },
    {
        "label": "split_by_phrase_regex",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "peekOfCode": "def split_by_phrase_regex() -> Callable[[str], List[str]]:\n    \"\"\"\n    Split text by phrase regex.\n    This regular expression will split the sentences into phrases,\n    where each phrase is a sequence of one or more non-comma,\n    non-period, and non-semicolon characters, followed by an optional comma,\n    period, or semicolon. The regular expression will also capture the\n    delimiters themselves as separate items in the list of phrases.\n    \"\"\"\n    regex = \"[^,.;]+[,.;]?\"",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef truncate_text(text: str, text_splitter: TextSplitter) -> str:\n    \"\"\"\n    Truncate text to fit within the chunk size.\n    Args:\n        text (str): The text to truncate.\n        text_splitter (TextSplitter): The splitter to use for chunking.\n    Returns:\n        str: The first chunk of the split text that fits within the chunk size.\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.text.utils",
        "documentation": {}
    },
    {
        "label": "NodeParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.interface",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.interface",
        "peekOfCode": "class NodeParser(TransformComponent, ABC):\n    \"\"\"Base interface for node parser.\"\"\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    include_metadata: bool = Field(\n        default=True, description=\"Whether or not to consider metadata when splitting.\"\n    )\n    include_prev_next_rel: bool = Field(\n        default=True, description=\"Include prev/next node relationships.\"\n    )\n    callback_manager: CallbackManager = Field(",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.interface",
        "documentation": {}
    },
    {
        "label": "TextSplitter",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.interface",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.interface",
        "peekOfCode": "class TextSplitter(NodeParser):\n    @abstractmethod\n    def split_text(self, text: str) -> List[str]: ...\n    def split_texts(self, texts: List[str]) -> List[str]:\n        nested_texts = [self.split_text(text) for text in texts]\n        return [item for sublist in nested_texts for item in sublist]\n    def _parse_nodes(\n        self, nodes: Sequence[BaseNode], show_progress: bool = False, **kwargs: Any\n    ) -> List[BaseNode]:\n        all_nodes: List[BaseNode] = []",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.interface",
        "documentation": {}
    },
    {
        "label": "MetadataAwareTextSplitter",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.interface",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.interface",
        "peekOfCode": "class MetadataAwareTextSplitter(TextSplitter):\n    @abstractmethod\n    def split_text_metadata_aware(self, text: str, metadata_str: str) -> List[str]: ...\n    def split_texts_metadata_aware(\n        self, texts: List[str], metadata_strs: List[str]\n    ) -> List[str]:\n        if len(texts) != len(metadata_strs):\n            raise ValueError(\"Texts and metadata_strs must have the same length\")\n        nested_texts = [\n            self.split_text_metadata_aware(text, metadata)",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.interface",
        "documentation": {}
    },
    {
        "label": "IdFuncCallable",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.interface",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.interface",
        "peekOfCode": "IdFuncCallable = Annotated[\n    Callable,\n    Field(validate_default=True),\n    BeforeValidator(_validate_id_func),\n    WithJsonSchema({\"type\": \"string\"}, mode=\"serialization\"),\n    WithJsonSchema({\"type\": \"string\"}, mode=\"validation\"),\n    PlainSerializer(_serialize_id_func),\n]\nclass NodeParser(TransformComponent, ABC):\n    \"\"\"Base interface for node parser.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.interface",
        "documentation": {}
    },
    {
        "label": "load_parser",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.loading",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.loading",
        "peekOfCode": "def load_parser(\n    data: dict,\n) -> NodeParser:\n    if isinstance(data, NodeParser):\n        return data\n    parser_name = data.get(\"class_name\")\n    if parser_name is None:\n        raise ValueError(\"Parser loading requires a class_name\")\n    if parser_name not in all_node_parsers:\n        raise ValueError(f\"Invalid parser name: {parser_name}\")",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.loading",
        "documentation": {}
    },
    {
        "label": "IdFuncCallable",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.node_utils",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.node_utils",
        "peekOfCode": "class IdFuncCallable(Protocol):\n    def __call__(self, i: int, doc: BaseNode) -> str: ...\ndef default_id_func(i: int, doc: BaseNode) -> str:\n    return str(uuid.uuid4())\ndef build_nodes_from_splits(\n    text_splits: List[str],\n    document: BaseNode,\n    ref_doc: Optional[BaseNode] = None,\n    id_func: Optional[IdFuncCallable] = None,\n) -> List[TextNode]:",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.node_utils",
        "documentation": {}
    },
    {
        "label": "default_id_func",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.node_utils",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.node_utils",
        "peekOfCode": "def default_id_func(i: int, doc: BaseNode) -> str:\n    return str(uuid.uuid4())\ndef build_nodes_from_splits(\n    text_splits: List[str],\n    document: BaseNode,\n    ref_doc: Optional[BaseNode] = None,\n    id_func: Optional[IdFuncCallable] = None,\n) -> List[TextNode]:\n    \"\"\"Build nodes from splits.\"\"\"\n    ref_doc = ref_doc or document",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.node_utils",
        "documentation": {}
    },
    {
        "label": "build_nodes_from_splits",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.node_utils",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.node_utils",
        "peekOfCode": "def build_nodes_from_splits(\n    text_splits: List[str],\n    document: BaseNode,\n    ref_doc: Optional[BaseNode] = None,\n    id_func: Optional[IdFuncCallable] = None,\n) -> List[TextNode]:\n    \"\"\"Build nodes from splits.\"\"\"\n    ref_doc = ref_doc or document\n    id_func = id_func or default_id_func\n    nodes: List[TextNode] = []",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.node_utils",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.node_parser.node_utils",
        "description": "reference_code.llama-index-core.llama_index.core.node_parser.node_utils",
        "peekOfCode": "logger = logging.getLogger(__name__)\n@runtime_checkable\nclass IdFuncCallable(Protocol):\n    def __call__(self, i: int, doc: BaseNode) -> str: ...\ndef default_id_func(i: int, doc: BaseNode) -> str:\n    return str(uuid.uuid4())\ndef build_nodes_from_splits(\n    text_splits: List[str],\n    document: BaseNode,\n    ref_doc: Optional[BaseNode] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.node_parser.node_utils",
        "documentation": {}
    },
    {
        "label": "ObjectRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.objects.base",
        "description": "reference_code.llama-index-core.llama_index.core.objects.base",
        "peekOfCode": "class ObjectRetriever(Generic[OT]):\n    \"\"\"Object retriever.\"\"\"\n    def __init__(\n        self,\n        retriever: BaseRetriever,\n        object_node_mapping: BaseObjectNodeMapping[OT],\n        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,\n    ):\n        self._retriever = retriever\n        self._object_node_mapping = object_node_mapping",
        "detail": "reference_code.llama-index-core.llama_index.core.objects.base",
        "documentation": {}
    },
    {
        "label": "ObjectIndex",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.objects.base",
        "description": "reference_code.llama-index-core.llama_index.core.objects.base",
        "peekOfCode": "class ObjectIndex(Generic[OT]):\n    \"\"\"Object index.\"\"\"\n    def __init__(\n        self, index: BaseIndex, object_node_mapping: BaseObjectNodeMapping\n    ) -> None:\n        self._index = index\n        self._object_node_mapping = object_node_mapping\n    @property\n    def index(self) -> BaseIndex:\n        \"\"\"Index.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.objects.base",
        "documentation": {}
    },
    {
        "label": "OT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.objects.base",
        "description": "reference_code.llama-index-core.llama_index.core.objects.base",
        "peekOfCode": "OT = TypeVar(\"OT\")\nclass ObjectRetriever(Generic[OT]):\n    \"\"\"Object retriever.\"\"\"\n    def __init__(\n        self,\n        retriever: BaseRetriever,\n        object_node_mapping: BaseObjectNodeMapping[OT],\n        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,\n    ):\n        self._retriever = retriever",
        "detail": "reference_code.llama-index-core.llama_index.core.objects.base",
        "documentation": {}
    },
    {
        "label": "BaseObjectNodeMapping",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.objects.base_node_mapping",
        "description": "reference_code.llama-index-core.llama_index.core.objects.base_node_mapping",
        "peekOfCode": "class BaseObjectNodeMapping(Generic[OT]):\n    \"\"\"Base object node mapping.\"\"\"\n    @classmethod\n    @abstractmethod\n    def from_objects(\n        cls, objs: Sequence[OT], *args: Any, **kwargs: Any\n    ) -> \"BaseObjectNodeMapping\":\n        \"\"\"\n        Initialize node mapping from a list of objects.\n        Only needs to be specified if the node mapping",
        "detail": "reference_code.llama-index-core.llama_index.core.objects.base_node_mapping",
        "documentation": {}
    },
    {
        "label": "SimpleObjectNodeMapping",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.objects.base_node_mapping",
        "description": "reference_code.llama-index-core.llama_index.core.objects.base_node_mapping",
        "peekOfCode": "class SimpleObjectNodeMapping(BaseObjectNodeMapping[Any]):\n    \"\"\"\n    General node mapping that works for any obj.\n    More specifically, any object with a meaningful string representation.\n    \"\"\"\n    def __init__(self, objs: Optional[Sequence[Any]] = None) -> None:\n        objs = objs or []\n        for obj in objs:\n            self.validate_object(obj)\n        self._objs = {hash(str(obj)): obj for obj in objs}",
        "detail": "reference_code.llama-index-core.llama_index.core.objects.base_node_mapping",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_FNAME",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.objects.base_node_mapping",
        "description": "reference_code.llama-index-core.llama_index.core.objects.base_node_mapping",
        "peekOfCode": "DEFAULT_PERSIST_FNAME = \"object_node_mapping.pickle\"\nOT = TypeVar(\"OT\")\nclass BaseObjectNodeMapping(Generic[OT]):\n    \"\"\"Base object node mapping.\"\"\"\n    @classmethod\n    @abstractmethod\n    def from_objects(\n        cls, objs: Sequence[OT], *args: Any, **kwargs: Any\n    ) -> \"BaseObjectNodeMapping\":\n        \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.objects.base_node_mapping",
        "documentation": {}
    },
    {
        "label": "OT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.objects.base_node_mapping",
        "description": "reference_code.llama-index-core.llama_index.core.objects.base_node_mapping",
        "peekOfCode": "OT = TypeVar(\"OT\")\nclass BaseObjectNodeMapping(Generic[OT]):\n    \"\"\"Base object node mapping.\"\"\"\n    @classmethod\n    @abstractmethod\n    def from_objects(\n        cls, objs: Sequence[OT], *args: Any, **kwargs: Any\n    ) -> \"BaseObjectNodeMapping\":\n        \"\"\"\n        Initialize node mapping from a list of objects.",
        "detail": "reference_code.llama-index-core.llama_index.core.objects.base_node_mapping",
        "documentation": {}
    },
    {
        "label": "FnNodeMapping",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.objects.fn_node_mapping",
        "description": "reference_code.llama-index-core.llama_index.core.objects.fn_node_mapping",
        "peekOfCode": "class FnNodeMapping(BaseObjectNodeMapping[Any]):\n    \"\"\"Fn node mapping.\"\"\"\n    def __init__(\n        self,\n        from_node_fn: Callable[[BaseNode], Any],\n        to_node_fn: Callable[[Any], BaseNode],\n    ) -> None:\n        self._to_node_fn = to_node_fn\n        self._from_node_fn = from_node_fn\n    @classmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.objects.fn_node_mapping",
        "documentation": {}
    },
    {
        "label": "SQLTableSchema",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.objects.table_node_mapping",
        "description": "reference_code.llama-index-core.llama_index.core.objects.table_node_mapping",
        "peekOfCode": "class SQLTableSchema(BaseModel):\n    \"\"\"Lightweight representation of a SQL table.\"\"\"\n    table_name: str\n    context_str: Optional[str] = None\nclass SQLTableNodeMapping(BaseObjectNodeMapping[SQLTableSchema]):\n    \"\"\"SQL Table node mapping.\"\"\"\n    def __init__(self, sql_database: SQLDatabase) -> None:\n        self._sql_database = sql_database\n    @classmethod\n    def from_objects(",
        "detail": "reference_code.llama-index-core.llama_index.core.objects.table_node_mapping",
        "documentation": {}
    },
    {
        "label": "SQLTableNodeMapping",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.objects.table_node_mapping",
        "description": "reference_code.llama-index-core.llama_index.core.objects.table_node_mapping",
        "peekOfCode": "class SQLTableNodeMapping(BaseObjectNodeMapping[SQLTableSchema]):\n    \"\"\"SQL Table node mapping.\"\"\"\n    def __init__(self, sql_database: SQLDatabase) -> None:\n        self._sql_database = sql_database\n    @classmethod\n    def from_objects(\n        cls,\n        objs: Sequence[SQLTableSchema],\n        *args: Any,\n        sql_database: Optional[SQLDatabase] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.objects.table_node_mapping",
        "documentation": {}
    },
    {
        "label": "BaseToolNodeMapping",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.objects.tool_node_mapping",
        "description": "reference_code.llama-index-core.llama_index.core.objects.tool_node_mapping",
        "peekOfCode": "class BaseToolNodeMapping(BaseObjectNodeMapping[BaseTool]):\n    \"\"\"Base Tool node mapping.\"\"\"\n    def validate_object(self, obj: BaseTool) -> None:\n        if not isinstance(obj, BaseTool):\n            raise ValueError(f\"Object must be of type {BaseTool}\")\n    @property\n    def obj_node_mapping(self) -> Dict[int, Any]:\n        \"\"\"The mapping data structure between node and object.\"\"\"\n        raise NotImplementedError(\"Subclasses should implement this!\")\n    def persist(",
        "detail": "reference_code.llama-index-core.llama_index.core.objects.tool_node_mapping",
        "documentation": {}
    },
    {
        "label": "SimpleToolNodeMapping",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.objects.tool_node_mapping",
        "description": "reference_code.llama-index-core.llama_index.core.objects.tool_node_mapping",
        "peekOfCode": "class SimpleToolNodeMapping(BaseToolNodeMapping):\n    \"\"\"\n    Simple Tool mapping.\n    In this setup, we assume that the tool name is unique, and\n    that the list of all tools are stored in memory.\n    \"\"\"\n    def __init__(self, objs: Optional[Sequence[BaseTool]] = None) -> None:\n        objs = objs or []\n        self._tools = {tool.metadata.name: tool for tool in objs}\n    @classmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.objects.tool_node_mapping",
        "documentation": {}
    },
    {
        "label": "BaseQueryToolNodeMapping",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.objects.tool_node_mapping",
        "description": "reference_code.llama-index-core.llama_index.core.objects.tool_node_mapping",
        "peekOfCode": "class BaseQueryToolNodeMapping(BaseObjectNodeMapping[QueryEngineTool]):\n    \"\"\"Base query tool node mapping.\"\"\"\n    @classmethod\n    def from_persist_dir(\n        cls,\n        persist_dir: str = DEFAULT_PERSIST_DIR,\n        obj_node_mapping_fname: str = DEFAULT_PERSIST_FNAME,\n    ) -> \"BaseQueryToolNodeMapping\":\n        raise NotImplementedError(\n            \"This object node mapping does not support persist method.\"",
        "detail": "reference_code.llama-index-core.llama_index.core.objects.tool_node_mapping",
        "documentation": {}
    },
    {
        "label": "SimpleQueryToolNodeMapping",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.objects.tool_node_mapping",
        "description": "reference_code.llama-index-core.llama_index.core.objects.tool_node_mapping",
        "peekOfCode": "class SimpleQueryToolNodeMapping(BaseQueryToolNodeMapping):\n    \"\"\"Simple query tool mapping.\"\"\"\n    def __init__(self, objs: Optional[Sequence[QueryEngineTool]] = None) -> None:\n        objs = objs or []\n        self._tools = {tool.metadata.name: tool for tool in objs}\n    def validate_object(self, obj: QueryEngineTool) -> None:\n        if not isinstance(obj, QueryEngineTool):\n            raise ValueError(f\"Object must be of type {QueryEngineTool}\")\n    @classmethod\n    def from_objects(",
        "detail": "reference_code.llama-index-core.llama_index.core.objects.tool_node_mapping",
        "documentation": {}
    },
    {
        "label": "convert_tool_to_node",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.objects.tool_node_mapping",
        "description": "reference_code.llama-index-core.llama_index.core.objects.tool_node_mapping",
        "peekOfCode": "def convert_tool_to_node(tool: BaseTool) -> TextNode:\n    \"\"\"Function convert Tool to node.\"\"\"\n    node_text = (\n        f\"Tool name: {tool.metadata.name}\\n\"\n        f\"Tool description: {tool.metadata.description}\\n\"\n    )\n    if tool.metadata.fn_schema is not None:\n        node_text += f\"Tool schema: {tool.metadata.fn_schema.model_json_schema()}\\n\"\n    tool_identity = (\n        f\"{tool.metadata.name}{tool.metadata.description}{tool.metadata.fn_schema}\"",
        "detail": "reference_code.llama-index-core.llama_index.core.objects.tool_node_mapping",
        "documentation": {}
    },
    {
        "label": "get_object_mapping",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.objects.utils",
        "description": "reference_code.llama-index-core.llama_index.core.objects.utils",
        "peekOfCode": "def get_object_mapping(\n    objects: Sequence[Any],\n    from_node_fn: Optional[Callable[[BaseNode], Any]] = None,\n    to_node_fn: Optional[Callable[[Any], BaseNode]] = None,\n) -> BaseObjectNodeMapping:\n    \"\"\"Get object mapping according to object.\"\"\"\n    if from_node_fn is not None and to_node_fn is not None:\n        return FnNodeMapping.from_objects(objects, from_node_fn, to_node_fn)\n    elif all(isinstance(obj, BaseTool) for obj in objects):\n        return SimpleToolNodeMapping.from_objects(objects)",
        "detail": "reference_code.llama-index-core.llama_index.core.objects.utils",
        "documentation": {}
    },
    {
        "label": "StructuredOutput",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.output_parsers.base",
        "description": "reference_code.llama-index-core.llama_index.core.output_parsers.base",
        "peekOfCode": "class StructuredOutput:\n    \"\"\"Structured output class.\"\"\"\n    raw_output: str\n    parsed_output: Optional[Any] = None\nclass OutputParserException(Exception):\n    pass",
        "detail": "reference_code.llama-index-core.llama_index.core.output_parsers.base",
        "documentation": {}
    },
    {
        "label": "OutputParserException",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.output_parsers.base",
        "description": "reference_code.llama-index-core.llama_index.core.output_parsers.base",
        "peekOfCode": "class OutputParserException(Exception):\n    pass",
        "detail": "reference_code.llama-index-core.llama_index.core.output_parsers.base",
        "documentation": {}
    },
    {
        "label": "LangchainOutputParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.output_parsers.langchain",
        "description": "reference_code.llama-index-core.llama_index.core.output_parsers.langchain",
        "peekOfCode": "class LangchainOutputParser(BaseOutputParser):\n    \"\"\"Langchain output parser.\"\"\"\n    def __init__(\n        self, output_parser: \"LCOutputParser\", format_key: Optional[str] = None\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self._output_parser = output_parser\n        self._format_key = format_key\n        self._formatter = SafeFormatter()\n    def parse(self, output: str) -> Any:",
        "detail": "reference_code.llama-index-core.llama_index.core.output_parsers.langchain",
        "documentation": {}
    },
    {
        "label": "PydanticOutputParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.output_parsers.pydantic",
        "description": "reference_code.llama-index-core.llama_index.core.output_parsers.pydantic",
        "peekOfCode": "class PydanticOutputParser(BaseOutputParser, Generic[Model]):\n    \"\"\"\n    Pydantic Output Parser.\n    Args:\n        output_cls (BaseModel): Pydantic output class.\n    \"\"\"\n    def __init__(\n        self,\n        output_cls: Type[Model],\n        excluded_schema_keys_from_format: Optional[List] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.output_parsers.pydantic",
        "documentation": {}
    },
    {
        "label": "PYDANTIC_FORMAT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.output_parsers.pydantic",
        "description": "reference_code.llama-index-core.llama_index.core.output_parsers.pydantic",
        "peekOfCode": "PYDANTIC_FORMAT_TMPL = \"\"\"\nHere's a JSON schema to follow:\n{schema}\nOutput a valid JSON object but do not repeat the schema.\n\"\"\"\nclass PydanticOutputParser(BaseOutputParser, Generic[Model]):\n    \"\"\"\n    Pydantic Output Parser.\n    Args:\n        output_cls (BaseModel): Pydantic output class.",
        "detail": "reference_code.llama-index-core.llama_index.core.output_parsers.pydantic",
        "documentation": {}
    },
    {
        "label": "Answer",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.output_parsers.selection",
        "description": "reference_code.llama-index-core.llama_index.core.output_parsers.selection",
        "peekOfCode": "class Answer(DataClassJsonMixin):\n    choice: int\n    reason: str\nclass SelectionOutputParser(BaseOutputParser):\n    REQUIRED_KEYS = frozenset(Answer.__annotations__)\n    def _filter_dict(self, json_dict: dict) -> dict:\n        \"\"\"Filter recursively until a dictionary matches all REQUIRED_KEYS.\"\"\"\n        output_dict = json_dict\n        for key, val in json_dict.items():\n            if key in self.REQUIRED_KEYS:",
        "detail": "reference_code.llama-index-core.llama_index.core.output_parsers.selection",
        "documentation": {}
    },
    {
        "label": "SelectionOutputParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.output_parsers.selection",
        "description": "reference_code.llama-index-core.llama_index.core.output_parsers.selection",
        "peekOfCode": "class SelectionOutputParser(BaseOutputParser):\n    REQUIRED_KEYS = frozenset(Answer.__annotations__)\n    def _filter_dict(self, json_dict: dict) -> dict:\n        \"\"\"Filter recursively until a dictionary matches all REQUIRED_KEYS.\"\"\"\n        output_dict = json_dict\n        for key, val in json_dict.items():\n            if key in self.REQUIRED_KEYS:\n                continue\n            elif isinstance(val, dict):\n                output_dict = self._filter_dict(val)",
        "detail": "reference_code.llama-index-core.llama_index.core.output_parsers.selection",
        "documentation": {}
    },
    {
        "label": "FORMAT_STR",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.output_parsers.selection",
        "description": "reference_code.llama-index-core.llama_index.core.output_parsers.selection",
        "peekOfCode": "FORMAT_STR = \"\"\"The output should be ONLY JSON formatted as a JSON instance.\nHere is an example:\n[\n    {\n        choice: 1,\n        reason: \"<insert reason for choice>\"\n    },\n    ...\n]\n\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.output_parsers.selection",
        "documentation": {}
    },
    {
        "label": "parse_json_markdown",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.output_parsers.utils",
        "description": "reference_code.llama-index-core.llama_index.core.output_parsers.utils",
        "peekOfCode": "def parse_json_markdown(text: str) -> Any:\n    if \"```json\" in text:\n        text = text.split(\"```json\")[1].strip().strip(\"```\").strip()\n    json_string = _marshal_llm_to_json(text)\n    try:\n        json_obj = json.loads(json_string)\n    except json.JSONDecodeError as e_json:\n        try:\n            # NOTE: parsing again with pyyaml\n            #       pyyaml is less strict, and allows for trailing commas",
        "detail": "reference_code.llama-index-core.llama_index.core.output_parsers.utils",
        "documentation": {}
    },
    {
        "label": "parse_code_markdown",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.output_parsers.utils",
        "description": "reference_code.llama-index-core.llama_index.core.output_parsers.utils",
        "peekOfCode": "def parse_code_markdown(text: str, only_last: bool) -> List[str]:\n    # Regular expression pattern to match code within triple-backticks\n    pattern = r\"```(.*?)```\"\n    # Regular expression pattern to match code within triple backticks with\n    # a Python marker. Like: ```python df.columns```\n    python_str_pattern = re.compile(r\"^```python\", re.IGNORECASE)\n    text = python_str_pattern.sub(\"```\", text)\n    # Find all matches of the pattern in the text\n    matches = re.findall(pattern, text, re.DOTALL)\n    # Return the last matched group if requested",
        "detail": "reference_code.llama-index-core.llama_index.core.output_parsers.utils",
        "documentation": {}
    },
    {
        "label": "extract_json_str",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.output_parsers.utils",
        "description": "reference_code.llama-index-core.llama_index.core.output_parsers.utils",
        "peekOfCode": "def extract_json_str(text: str) -> str:\n    \"\"\"Extract JSON string from text.\"\"\"\n    # NOTE: this regex parsing is taken from langchain.output_parsers.pydantic\n    match = re.search(r\"\\{.*\\}\", text.strip(), re.MULTILINE | re.IGNORECASE | re.DOTALL)\n    if not match:\n        raise ValueError(f\"Could not extract json string from output: {text}\")\n    return match.group()",
        "detail": "reference_code.llama-index-core.llama_index.core.output_parsers.utils",
        "documentation": {}
    },
    {
        "label": "Playground",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.playground.base",
        "description": "reference_code.llama-index-core.llama_index.core.playground.base",
        "peekOfCode": "class Playground:\n    \"\"\"Experiment with indices, models, embeddings, retriever_modes, and more.\"\"\"\n    def __init__(\n        self,\n        indices: List[BaseIndex],\n        retriever_modes: INDEX_SPECIFIC_QUERY_MODES_TYPE = DEFAULT_MODES,\n    ):\n        \"\"\"\n        Initialize with indices to experiment with.\n        Args:",
        "detail": "reference_code.llama-index-core.llama_index.core.playground.base",
        "documentation": {}
    },
    {
        "label": "INDEX_SPECIFIC_QUERY_MODES_TYPE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.playground.base",
        "description": "reference_code.llama-index-core.llama_index.core.playground.base",
        "peekOfCode": "INDEX_SPECIFIC_QUERY_MODES_TYPE = Dict[Type[BaseIndex], List[str]]\nDEFAULT_MODES: INDEX_SPECIFIC_QUERY_MODES_TYPE = {\n    TreeIndex: [e.value for e in TreeRetrieverMode],\n    SummaryIndex: [e.value for e in ListRetrieverMode],\n    VectorStoreIndex: [\"default\"],\n}\nclass Playground:\n    \"\"\"Experiment with indices, models, embeddings, retriever_modes, and more.\"\"\"\n    def __init__(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.playground.base",
        "documentation": {}
    },
    {
        "label": "LLMRerank",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.llm_rerank",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.llm_rerank",
        "peekOfCode": "class LLMRerank(BaseNodePostprocessor):\n    \"\"\"LLM-based reranker.\"\"\"\n    top_n: int = Field(description=\"Top N nodes to return.\")\n    choice_select_prompt: SerializeAsAny[BasePromptTemplate] = Field(\n        description=\"Choice select prompt.\"\n    )\n    choice_batch_size: int = Field(description=\"Batch size for choice select.\")\n    llm: LLM = Field(description=\"The LLM to rerank with.\")\n    _format_node_batch_fn: Callable = PrivateAttr()\n    _parse_choice_select_answer_fn: Callable = PrivateAttr()",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.llm_rerank",
        "documentation": {}
    },
    {
        "label": "MetadataReplacementPostProcessor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.metadata_replacement",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.metadata_replacement",
        "peekOfCode": "class MetadataReplacementPostProcessor(BaseNodePostprocessor):\n    target_metadata_key: str = Field(\n        description=\"Target metadata key to replace node content with.\"\n    )\n    def __init__(self, target_metadata_key: str) -> None:\n        super().__init__(target_metadata_key=target_metadata_key)\n    @classmethod\n    def class_name(cls) -> str:\n        return \"MetadataReplacementPostProcessor\"\n    def _postprocess_nodes(",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.metadata_replacement",
        "documentation": {}
    },
    {
        "label": "KeywordNodePostprocessor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "peekOfCode": "class KeywordNodePostprocessor(BaseNodePostprocessor):\n    \"\"\"Keyword-based Node processor.\"\"\"\n    required_keywords: List[str] = Field(default_factory=list)\n    exclude_keywords: List[str] = Field(default_factory=list)\n    lang: str = Field(default=\"en\")\n    @classmethod\n    def class_name(cls) -> str:\n        return \"KeywordNodePostprocessor\"\n    def _postprocess_nodes(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "documentation": {}
    },
    {
        "label": "SimilarityPostprocessor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "peekOfCode": "class SimilarityPostprocessor(BaseNodePostprocessor):\n    \"\"\"Similarity-based Node processor.\"\"\"\n    similarity_cutoff: float = Field(default=0.0)\n    @classmethod\n    def class_name(cls) -> str:\n        return \"SimilarityPostprocessor\"\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "documentation": {}
    },
    {
        "label": "PrevNextNodePostprocessor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "peekOfCode": "class PrevNextNodePostprocessor(BaseNodePostprocessor):\n    \"\"\"\n    Previous/Next Node post-processor.\n    Allows users to fetch additional nodes from the document store,\n    based on the relationships of the nodes.\n    NOTE: this is a beta feature.\n    Args:\n        docstore (BaseDocumentStore): The document store.\n        num_nodes (int): The number of nodes to return (default: 1)\n        mode (str): The mode of the post-processor.",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "documentation": {}
    },
    {
        "label": "AutoPrevNextNodePostprocessor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "peekOfCode": "class AutoPrevNextNodePostprocessor(BaseNodePostprocessor):\n    \"\"\"\n    Previous/Next Node post-processor.\n    Allows users to fetch additional nodes from the document store,\n    based on the prev/next relationships of the nodes.\n    NOTE: difference with PrevNextPostprocessor is that\n    this infers forward/backwards direction.\n    NOTE: this is a beta feature.\n    Args:\n        docstore (BaseDocumentStore): The document store.",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "documentation": {}
    },
    {
        "label": "LongContextReorder",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "peekOfCode": "class LongContextReorder(BaseNodePostprocessor):\n    \"\"\"\n    Models struggle to access significant details found\n    in the center of extended contexts. A study\n    (https://arxiv.org/abs/2307.03172) observed that the best\n    performance typically arises when crucial data is positioned\n    at the start or conclusion of the input context. Additionally,\n    as the input context lengthens, performance drops notably, even\n    in models designed for long contexts.\".\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "documentation": {}
    },
    {
        "label": "get_forward_nodes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "peekOfCode": "def get_forward_nodes(\n    node_with_score: NodeWithScore, num_nodes: int, docstore: BaseDocumentStore\n) -> Dict[str, NodeWithScore]:\n    \"\"\"Get forward nodes.\"\"\"\n    node = node_with_score.node\n    nodes: Dict[str, NodeWithScore] = {node.node_id: node_with_score}\n    cur_count = 0\n    # get forward nodes in an iterative manner\n    while cur_count < num_nodes:\n        if NodeRelationship.NEXT not in node.relationships:",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "documentation": {}
    },
    {
        "label": "get_backward_nodes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "peekOfCode": "def get_backward_nodes(\n    node_with_score: NodeWithScore, num_nodes: int, docstore: BaseDocumentStore\n) -> Dict[str, NodeWithScore]:\n    \"\"\"Get backward nodes.\"\"\"\n    node = node_with_score.node\n    # get backward nodes in an iterative manner\n    nodes: Dict[str, NodeWithScore] = {node.node_id: node_with_score}\n    cur_count = 0\n    while cur_count < num_nodes:\n        prev_node_info = node.prev_node",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass KeywordNodePostprocessor(BaseNodePostprocessor):\n    \"\"\"Keyword-based Node processor.\"\"\"\n    required_keywords: List[str] = Field(default_factory=list)\n    exclude_keywords: List[str] = Field(default_factory=list)\n    lang: str = Field(default=\"en\")\n    @classmethod\n    def class_name(cls) -> str:\n        return \"KeywordNodePostprocessor\"\n    def _postprocess_nodes(",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "documentation": {}
    },
    {
        "label": "DEFAULT_INFER_PREV_NEXT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "peekOfCode": "DEFAULT_INFER_PREV_NEXT_TMPL = (\n    \"The current context information is provided. \\n\"\n    \"A question is also provided. \\n\"\n    \"You are a retrieval agent deciding whether to search the \"\n    \"document store for additional prior context or future context. \\n\"\n    \"Given the context and question, return PREVIOUS or NEXT or NONE. \\n\"\n    \"Examples: \\n\\n\"\n    \"Context: Describes the author's experience at Y Combinator.\"\n    \"Question: What did the author do after his time at Y Combinator? \\n\"\n    \"Answer: NEXT \\n\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "documentation": {}
    },
    {
        "label": "DEFAULT_REFINE_INFER_PREV_NEXT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "peekOfCode": "DEFAULT_REFINE_INFER_PREV_NEXT_TMPL = (\n    \"The current context information is provided. \\n\"\n    \"A question is also provided. \\n\"\n    \"An existing answer is also provided.\\n\"\n    \"You are a retrieval agent deciding whether to search the \"\n    \"document store for additional prior context or future context. \\n\"\n    \"Given the context, question, and previous answer, \"\n    \"return PREVIOUS or NEXT or NONE.\\n\"\n    \"Examples: \\n\\n\"\n    \"Context: {context_msg}\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.node",
        "documentation": {}
    },
    {
        "label": "FixedRecencyPostprocessor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.node_recency",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.node_recency",
        "peekOfCode": "class FixedRecencyPostprocessor(BaseNodePostprocessor):\n    \"\"\"\n    Fixed Recency post-processor.\n    This post-processor does the following steps orders nodes by date.\n    Assumes the date_key corresponds to a date field in the metadata.\n    \"\"\"\n    top_k: int = 1\n    date_key: str = \"date\"\n    @classmethod\n    def class_name(cls) -> str:",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.node_recency",
        "documentation": {}
    },
    {
        "label": "EmbeddingRecencyPostprocessor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.node_recency",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.node_recency",
        "peekOfCode": "class EmbeddingRecencyPostprocessor(BaseNodePostprocessor):\n    \"\"\"Embedding Recency post-processor.\"\"\"\n    embed_model: SerializeAsAny[BaseEmbedding] = Field(\n        default_factory=lambda: Settings.embed_model\n    )\n    date_key: str = \"date\"\n    similarity_cutoff: float = Field(default=0.7)\n    query_embedding_tmpl: str = Field(default=DEFAULT_QUERY_EMBEDDING_TMPL)\n    @classmethod\n    def class_name(cls) -> str:",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.node_recency",
        "documentation": {}
    },
    {
        "label": "TimeWeightedPostprocessor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.node_recency",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.node_recency",
        "peekOfCode": "class TimeWeightedPostprocessor(BaseNodePostprocessor):\n    \"\"\"\n    Time-weighted post-processor.\n    Reranks a set of nodes based on their recency.\n    \"\"\"\n    time_decay: float = Field(default=0.99)\n    last_accessed_key: str = \"__last_accessed__\"\n    time_access_refresh: bool = True\n    # optionally set now (makes it easier to test)\n    now: Optional[float] = None",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.node_recency",
        "documentation": {}
    },
    {
        "label": "DEFAULT_QUERY_EMBEDDING_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.node_recency",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.node_recency",
        "peekOfCode": "DEFAULT_QUERY_EMBEDDING_TMPL = (\n    \"The current document is provided.\\n\"\n    \"----------------\\n\"\n    \"{context_str}\\n\"\n    \"----------------\\n\"\n    \"Given the document, we wish to find documents that contain \\n\"\n    \"similar context. Note that these documents are older \"\n    \"than the current document, meaning that certain details may be changed. \\n\"\n    \"However, the high-level context should be similar.\\n\"\n)",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.node_recency",
        "documentation": {}
    },
    {
        "label": "SentenceEmbeddingOptimizer",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.optimizer",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.optimizer",
        "peekOfCode": "class SentenceEmbeddingOptimizer(BaseNodePostprocessor):\n    \"\"\"Optimization of a text chunk given the query by shortening the input text.\"\"\"\n    percentile_cutoff: Optional[float] = Field(\n        description=\"Percentile cutoff for the top k sentences to use.\"\n    )\n    threshold_cutoff: Optional[float] = Field(\n        description=\"Threshold cutoff for similarity for each sentence to use.\"\n    )\n    _embed_model: BaseEmbedding = PrivateAttr()\n    _tokenizer_fn: Callable[[str], List[str]] = PrivateAttr()",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.optimizer",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.optimizer",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.optimizer",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass SentenceEmbeddingOptimizer(BaseNodePostprocessor):\n    \"\"\"Optimization of a text chunk given the query by shortening the input text.\"\"\"\n    percentile_cutoff: Optional[float] = Field(\n        description=\"Percentile cutoff for the top k sentences to use.\"\n    )\n    threshold_cutoff: Optional[float] = Field(\n        description=\"Threshold cutoff for similarity for each sentence to use.\"\n    )\n    _embed_model: BaseEmbedding = PrivateAttr()",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.optimizer",
        "documentation": {}
    },
    {
        "label": "PIINodePostprocessor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.pii",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.pii",
        "peekOfCode": "class PIINodePostprocessor(BaseNodePostprocessor):\n    \"\"\"\n    PII Node processor.\n    NOTE: this is a beta feature, the API might change.\n    Args:\n        llm (LLM): The local LLM to use for prediction.\n    \"\"\"\n    llm: LLM\n    pii_str_tmpl: str = DEFAULT_PII_TMPL\n    pii_node_info_key: str = \"__pii_node_info__\"",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.pii",
        "documentation": {}
    },
    {
        "label": "NERPIINodePostprocessor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.pii",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.pii",
        "peekOfCode": "class NERPIINodePostprocessor(BaseNodePostprocessor):\n    \"\"\"\n    NER PII Node processor.\n    Uses a HF transformers model.\n    \"\"\"\n    pii_node_info_key: str = \"__pii_node_info__\"\n    @classmethod\n    def class_name(cls) -> str:\n        return \"NERPIINodePostprocessor\"\n    def mask_pii(self, ner: Callable, text: str) -> Tuple[str, Dict]:",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.pii",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PII_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.pii",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.pii",
        "peekOfCode": "DEFAULT_PII_TMPL = (\n    \"The current context information is provided. \\n\"\n    \"A task is also provided to mask the PII within the context. \\n\"\n    \"Return the text, with all PII masked out, and a mapping of the original PII \"\n    \"to the masked PII. \\n\"\n    \"Return the output of the task in JSON. \\n\"\n    \"Context:\\n\"\n    \"Hello Zhang Wei, I am John. \"\n    \"Your AnyCompany Financial Services, \"\n    \"LLC credit card account 1111-0000-1111-0008 \"",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.pii",
        "documentation": {}
    },
    {
        "label": "RankGPTRerank",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.rankGPT_rerank",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.rankGPT_rerank",
        "peekOfCode": "class RankGPTRerank(BaseNodePostprocessor):\n    \"\"\"RankGPT-based reranker.\"\"\"\n    top_n: int = Field(default=5, description=\"Top N nodes to return from reranking.\")\n    llm: LLM = Field(\n        default_factory=get_default_llm,\n        description=\"LLM to use for rankGPT\",\n    )\n    verbose: bool = Field(\n        default=False, description=\"Whether to print intermediate steps.\"\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.rankGPT_rerank",
        "documentation": {}
    },
    {
        "label": "get_default_llm",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.rankGPT_rerank",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.rankGPT_rerank",
        "peekOfCode": "def get_default_llm() -> LLM:\n    from llama_index.llms.openai import OpenAI  # pants: no-infer-dep\n    return OpenAI(model=\"gpt-3.5-turbo-16k\")\nclass RankGPTRerank(BaseNodePostprocessor):\n    \"\"\"RankGPT-based reranker.\"\"\"\n    top_n: int = Field(default=5, description=\"Top N nodes to return from reranking.\")\n    llm: LLM = Field(\n        default_factory=get_default_llm,\n        description=\"LLM to use for rankGPT\",\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.rankGPT_rerank",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.rankGPT_rerank",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.rankGPT_rerank",
        "peekOfCode": "logger = logging.getLogger(__name__)\nlogger.setLevel(logging.WARNING)\ndef get_default_llm() -> LLM:\n    from llama_index.llms.openai import OpenAI  # pants: no-infer-dep\n    return OpenAI(model=\"gpt-3.5-turbo-16k\")\nclass RankGPTRerank(BaseNodePostprocessor):\n    \"\"\"RankGPT-based reranker.\"\"\"\n    top_n: int = Field(default=5, description=\"Top N nodes to return from reranking.\")\n    llm: LLM = Field(\n        default_factory=get_default_llm,",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.rankGPT_rerank",
        "documentation": {}
    },
    {
        "label": "SentenceTransformerRerank",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.sbert_rerank",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.sbert_rerank",
        "peekOfCode": "class SentenceTransformerRerank(BaseNodePostprocessor):\n    model: str = Field(description=\"Sentence transformer model name.\")\n    top_n: int = Field(description=\"Number of nodes to return sorted by score.\")\n    device: str = Field(\n        default=\"cpu\",\n        description=\"Device to use for sentence transformer.\",\n    )\n    keep_retrieval_score: bool = Field(\n        default=False,\n        description=\"Whether to keep the retrieval score in metadata.\",",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.sbert_rerank",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SENTENCE_TRANSFORMER_MAX_LENGTH",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.sbert_rerank",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.sbert_rerank",
        "peekOfCode": "DEFAULT_SENTENCE_TRANSFORMER_MAX_LENGTH = 512\nclass SentenceTransformerRerank(BaseNodePostprocessor):\n    model: str = Field(description=\"Sentence transformer model name.\")\n    top_n: int = Field(description=\"Number of nodes to return sorted by score.\")\n    device: str = Field(\n        default=\"cpu\",\n        description=\"Device to use for sentence transformer.\",\n    )\n    keep_retrieval_score: bool = Field(\n        default=False,",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.sbert_rerank",
        "documentation": {}
    },
    {
        "label": "DocumentWithRelevance",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.structured_llm_rerank",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.structured_llm_rerank",
        "peekOfCode": "class DocumentWithRelevance(BaseModel):\n    \"\"\"\n    Document rankings as selected by model.\n    \"\"\"\n    document_number: int = Field(\n        description=\"The number of the document within the provided list\"\n    )\n    # Put min/max as a json schema extra so that pydantic doesn't enforce it but the model sees it.\n    # Doesn't need to be strictly enforced but is useful for the model.\n    relevance: int = Field(",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.structured_llm_rerank",
        "documentation": {}
    },
    {
        "label": "DocumentRelevanceList",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.structured_llm_rerank",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.structured_llm_rerank",
        "peekOfCode": "class DocumentRelevanceList(BaseModel):\n    \"\"\"\n    List of documents with relevance scores.\n    \"\"\"\n    documents: List[DocumentWithRelevance] = Field(\n        description=\"List of documents with relevance scores\"\n    )\ndef default_parse_structured_choice_select_answer(\n    document_relevance_list: DocumentRelevanceList, num_choices: int\n) -> Tuple[List[int], List[int]]:",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.structured_llm_rerank",
        "documentation": {}
    },
    {
        "label": "StructuredLLMRerank",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.structured_llm_rerank",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.structured_llm_rerank",
        "peekOfCode": "class StructuredLLMRerank(BaseNodePostprocessor):\n    \"\"\"Structured LLM-based reranker.\"\"\"\n    top_n: int = Field(description=\"Top N nodes to return.\")\n    choice_select_prompt: SerializeAsAny[BasePromptTemplate] = Field(\n        description=\"Choice select prompt.\"\n    )\n    choice_batch_size: int = Field(description=\"Batch size for choice select.\")\n    llm: LLM = Field(description=\"The LLM to rerank with.\")\n    _document_relevance_list_cls: type = PrivateAttr()\n    _format_node_batch_fn: Callable = PrivateAttr()",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.structured_llm_rerank",
        "documentation": {}
    },
    {
        "label": "default_parse_structured_choice_select_answer",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.structured_llm_rerank",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.structured_llm_rerank",
        "peekOfCode": "def default_parse_structured_choice_select_answer(\n    document_relevance_list: DocumentRelevanceList, num_choices: int\n) -> Tuple[List[int], List[int]]:\n    \"\"\"\n    Parse the answer from the choice select prompt.\n    \"\"\"\n    documents = [\n        doc\n        for doc in document_relevance_list.documents\n        if doc.document_number <= num_choices",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.structured_llm_rerank",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.structured_llm_rerank",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.structured_llm_rerank",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndispatcher = get_dispatcher(__name__)\nclass DocumentWithRelevance(BaseModel):\n    \"\"\"\n    Document rankings as selected by model.\n    \"\"\"\n    document_number: int = Field(\n        description=\"The number of the document within the provided list\"\n    )\n    # Put min/max as a json schema extra so that pydantic doesn't enforce it but the model sees it.",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.structured_llm_rerank",
        "documentation": {}
    },
    {
        "label": "dispatcher",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.structured_llm_rerank",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.structured_llm_rerank",
        "peekOfCode": "dispatcher = get_dispatcher(__name__)\nclass DocumentWithRelevance(BaseModel):\n    \"\"\"\n    Document rankings as selected by model.\n    \"\"\"\n    document_number: int = Field(\n        description=\"The number of the document within the provided list\"\n    )\n    # Put min/max as a json schema extra so that pydantic doesn't enforce it but the model sees it.\n    # Doesn't need to be strictly enforced but is useful for the model.",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.structured_llm_rerank",
        "documentation": {}
    },
    {
        "label": "BaseNodePostprocessor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.postprocessor.types",
        "description": "reference_code.llama-index-core.llama_index.core.postprocessor.types",
        "peekOfCode": "class BaseNodePostprocessor(BaseComponent, DispatcherSpanMixin, ABC):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    callback_manager: CallbackManager = Field(\n        default_factory=CallbackManager, exclude=True\n    )\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        # set by default since most postprocessors don't require prompts\n        return {}\n    def _update_prompts(self, prompts: PromptDictType) -> None:",
        "detail": "reference_code.llama-index-core.llama_index.core.postprocessor.types",
        "documentation": {}
    },
    {
        "label": "FunctionCallingProgram",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.program.function_program",
        "description": "reference_code.llama-index-core.llama_index.core.program.function_program",
        "peekOfCode": "class FunctionCallingProgram(BasePydanticProgram[Model]):\n    \"\"\"\n    Function Calling Program.\n    Uses function calling LLMs to obtain a structured output.\n    \"\"\"\n    def __init__(\n        self,\n        output_cls: Type[Model],\n        llm: FunctionCallingLLM,\n        prompt: BasePromptTemplate,",
        "detail": "reference_code.llama-index-core.llama_index.core.program.function_program",
        "documentation": {}
    },
    {
        "label": "get_function_tool",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.program.function_program",
        "description": "reference_code.llama-index-core.llama_index.core.program.function_program",
        "peekOfCode": "def get_function_tool(output_cls: Type[Model]) -> FunctionTool:\n    \"\"\"Get function tool.\"\"\"\n    schema = output_cls.model_json_schema()\n    schema_description = schema.get(\"description\", None)\n    # NOTE: this does not specify the schema in the function signature,\n    # so instead we'll directly provide it in the fn_schema in the ToolMetadata\n    def model_fn(**kwargs: Any) -> Model:\n        \"\"\"Model function.\"\"\"\n        return output_cls(**kwargs)\n    return FunctionTool.from_defaults(",
        "detail": "reference_code.llama-index-core.llama_index.core.program.function_program",
        "documentation": {}
    },
    {
        "label": "_logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.program.function_program",
        "description": "reference_code.llama-index-core.llama_index.core.program.function_program",
        "peekOfCode": "_logger = logging.getLogger(__name__)\ndef get_function_tool(output_cls: Type[Model]) -> FunctionTool:\n    \"\"\"Get function tool.\"\"\"\n    schema = output_cls.model_json_schema()\n    schema_description = schema.get(\"description\", None)\n    # NOTE: this does not specify the schema in the function signature,\n    # so instead we'll directly provide it in the fn_schema in the ToolMetadata\n    def model_fn(**kwargs: Any) -> Model:\n        \"\"\"Model function.\"\"\"\n        return output_cls(**kwargs)",
        "detail": "reference_code.llama-index-core.llama_index.core.program.function_program",
        "documentation": {}
    },
    {
        "label": "LLMTextCompletionProgram",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.program.llm_program",
        "description": "reference_code.llama-index-core.llama_index.core.program.llm_program",
        "peekOfCode": "class LLMTextCompletionProgram(BasePydanticProgram[Model]):\n    \"\"\"\n    LLM Text Completion Program.\n    Uses generic LLM text completion + an output parser to generate a structured output.\n    \"\"\"\n    def __init__(\n        self,\n        output_parser: BaseOutputParser,\n        output_cls: Type[Model],\n        prompt: BasePromptTemplate,",
        "detail": "reference_code.llama-index-core.llama_index.core.program.llm_program",
        "documentation": {}
    },
    {
        "label": "BaseLLMFunctionProgram",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.program.llm_prompt_program",
        "description": "reference_code.llama-index-core.llama_index.core.program.llm_prompt_program",
        "peekOfCode": "class BaseLLMFunctionProgram(BasePydanticProgram[BaseModel], Generic[LM]):\n    \"\"\"\n    Base LLM Prompt Program.\n    This is a base class for LLM endpoints that can return\n    a structured output given the prompt.\n    NOTE: this only works for structured endpoints atm\n    (does not work for text completion endpoints.)\n    \"\"\"\n    @classmethod\n    @abstractmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.program.llm_prompt_program",
        "documentation": {}
    },
    {
        "label": "LM",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.program.llm_prompt_program",
        "description": "reference_code.llama-index-core.llama_index.core.program.llm_prompt_program",
        "peekOfCode": "LM = TypeVar(\"LM\")\nclass BaseLLMFunctionProgram(BasePydanticProgram[BaseModel], Generic[LM]):\n    \"\"\"\n    Base LLM Prompt Program.\n    This is a base class for LLM endpoints that can return\n    a structured output given the prompt.\n    NOTE: this only works for structured endpoints atm\n    (does not work for text completion endpoints.)\n    \"\"\"\n    @classmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.program.llm_prompt_program",
        "documentation": {}
    },
    {
        "label": "MultiModalLLMCompletionProgram",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.program.multi_modal_llm_program",
        "description": "reference_code.llama-index-core.llama_index.core.program.multi_modal_llm_program",
        "peekOfCode": "class MultiModalLLMCompletionProgram(BasePydanticProgram[BaseModel]):\n    \"\"\"\n    Multi Modal LLM Completion Program.\n    Uses generic Multi Modal LLM completion + an output parser to generate a structured output.\n    \"\"\"\n    def __init__(\n        self,\n        output_parser: PydanticOutputParser,\n        prompt: BasePromptTemplate,\n        multi_modal_llm: LLM,",
        "detail": "reference_code.llama-index-core.llama_index.core.program.multi_modal_llm_program",
        "documentation": {}
    },
    {
        "label": "process_streaming_content_incremental",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.program.streaming_utils",
        "description": "reference_code.llama-index-core.llama_index.core.program.streaming_utils",
        "peekOfCode": "def process_streaming_content_incremental(\n    chat_response: ChatResponse,\n    output_cls: Type[Model],\n    cur_object: Optional[Union[Model, FlexibleModel]] = None,\n) -> Union[Model, FlexibleModel]:\n    \"\"\"\n    Process streaming response content with true incremental list handling.\n    This version can extract partial progress from incomplete JSON and build\n    lists incrementally (e.g., 1 joke  2 jokes  3 jokes) rather than\n    jumping from empty to complete lists.",
        "detail": "reference_code.llama-index-core.llama_index.core.program.streaming_utils",
        "documentation": {}
    },
    {
        "label": "FlexibleModel",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.program.utils",
        "description": "reference_code.llama-index-core.llama_index.core.program.utils",
        "peekOfCode": "class FlexibleModel(BaseModel):\n    model_config = ConfigDict(extra=\"allow\")\ndef create_flexible_model(model: Type[BaseModel]) -> Type[FlexibleModel]:\n    \"\"\"Create a flexible version of the model that allows any fields.\"\"\"\n    return create_model(\n        f\"Flexible{model.__name__}\",\n        __base__=FlexibleModel,\n        **dict.fromkeys(model.model_fields, (Optional[Any], None)),\n    )  # type: ignore\ndef create_list_model(base_cls: Type[BaseModel]) -> Type[BaseModel]:",
        "detail": "reference_code.llama-index-core.llama_index.core.program.utils",
        "documentation": {}
    },
    {
        "label": "create_flexible_model",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.program.utils",
        "description": "reference_code.llama-index-core.llama_index.core.program.utils",
        "peekOfCode": "def create_flexible_model(model: Type[BaseModel]) -> Type[FlexibleModel]:\n    \"\"\"Create a flexible version of the model that allows any fields.\"\"\"\n    return create_model(\n        f\"Flexible{model.__name__}\",\n        __base__=FlexibleModel,\n        **dict.fromkeys(model.model_fields, (Optional[Any], None)),\n    )  # type: ignore\ndef create_list_model(base_cls: Type[BaseModel]) -> Type[BaseModel]:\n    \"\"\"Create a list version of an existing Pydantic object.\"\"\"\n    # NOTE: this is directly taken from",
        "detail": "reference_code.llama-index-core.llama_index.core.program.utils",
        "documentation": {}
    },
    {
        "label": "create_list_model",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.program.utils",
        "description": "reference_code.llama-index-core.llama_index.core.program.utils",
        "peekOfCode": "def create_list_model(base_cls: Type[BaseModel]) -> Type[BaseModel]:\n    \"\"\"Create a list version of an existing Pydantic object.\"\"\"\n    # NOTE: this is directly taken from\n    # https://github.com/jxnl/openai_function_call/blob/main/examples/streaming_multitask/streaming_multitask.py\n    # all credits go to the openai_function_call repo\n    name = f\"{base_cls.__name__}List\"\n    list_items = (\n        List[base_cls],  # type: ignore\n        Field(\n            default_factory=list,  # type: ignore",
        "detail": "reference_code.llama-index-core.llama_index.core.program.utils",
        "documentation": {}
    },
    {
        "label": "get_program_for_llm",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.program.utils",
        "description": "reference_code.llama-index-core.llama_index.core.program.utils",
        "peekOfCode": "def get_program_for_llm(\n    output_cls: Type[Model],\n    prompt: BasePromptTemplate,\n    llm: LLM,\n    pydantic_program_mode: PydanticProgramMode = PydanticProgramMode.DEFAULT,\n    **kwargs: Any,\n) -> BasePydanticProgram[Model]:\n    \"\"\"Get a program based on the compatible LLM.\"\"\"\n    if pydantic_program_mode == PydanticProgramMode.DEFAULT:\n        if llm.metadata.is_function_calling_model:",
        "detail": "reference_code.llama-index-core.llama_index.core.program.utils",
        "documentation": {}
    },
    {
        "label": "process_streaming_objects",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.program.utils",
        "description": "reference_code.llama-index-core.llama_index.core.program.utils",
        "peekOfCode": "def process_streaming_objects(\n    chat_response: ChatResponse,\n    output_cls: Type[Model],\n    cur_objects: Optional[Sequence[Model]] = None,\n    allow_parallel_tool_calls: bool = False,\n    flexible_mode: bool = True,\n    llm: Optional[FunctionCallingLLM] = None,\n) -> Union[Model, List[Model], FlexibleModel, List[FlexibleModel]]:\n    \"\"\"\n    Process streaming response into structured objects.",
        "detail": "reference_code.llama-index-core.llama_index.core.program.utils",
        "documentation": {}
    },
    {
        "label": "num_valid_fields",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.program.utils",
        "description": "reference_code.llama-index-core.llama_index.core.program.utils",
        "peekOfCode": "def num_valid_fields(\n    obj: Union[BaseModel, Sequence[BaseModel], Dict[str, BaseModel]],\n) -> int:\n    \"\"\"\n    Recursively count the number of fields in a Pydantic object (including nested objects) that aren't None.\n    Args:\n        obj (Any): A Pydantic model instance or any other object.\n    Returns:\n        int: The number of fields that have non-None values.\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.program.utils",
        "documentation": {}
    },
    {
        "label": "_logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.program.utils",
        "description": "reference_code.llama-index-core.llama_index.core.program.utils",
        "peekOfCode": "_logger = logging.getLogger(__name__)\nclass FlexibleModel(BaseModel):\n    model_config = ConfigDict(extra=\"allow\")\ndef create_flexible_model(model: Type[BaseModel]) -> Type[FlexibleModel]:\n    \"\"\"Create a flexible version of the model that allows any fields.\"\"\"\n    return create_model(\n        f\"Flexible{model.__name__}\",\n        __base__=FlexibleModel,\n        **dict.fromkeys(model.model_fields, (Optional[Any], None)),\n    )  # type: ignore",
        "detail": "reference_code.llama-index-core.llama_index.core.program.utils",
        "documentation": {}
    },
    {
        "label": "BasePromptTemplate",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.base",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.base",
        "peekOfCode": "class BasePromptTemplate(BaseModel, ABC):  # type: ignore[no-redef]\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    metadata: Dict[str, Any]\n    template_vars: List[str]\n    kwargs: Dict[str, str]\n    output_parser: Optional[BaseOutputParser]\n    template_var_mappings: Optional[Dict[str, Any]] = Field(\n        default_factory=dict,  # type: ignore\n        description=\"Template variable mappings (Optional).\",\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.base",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.base",
        "peekOfCode": "class PromptTemplate(BasePromptTemplate):  # type: ignore[no-redef]\n    template: str\n    def __init__(\n        self,\n        template: str,\n        prompt_type: str = PromptType.CUSTOM,\n        output_parser: Optional[BaseOutputParser] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        template_var_mappings: Optional[Dict[str, Any]] = None,\n        function_mappings: Optional[Dict[str, Callable]] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.base",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.base",
        "peekOfCode": "class ChatPromptTemplate(BasePromptTemplate):  # type: ignore[no-redef]\n    message_templates: List[ChatMessage]\n    def __init__(\n        self,\n        message_templates: Sequence[ChatMessage],\n        prompt_type: str = PromptType.CUSTOM,\n        output_parser: Optional[BaseOutputParser] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        template_var_mappings: Optional[Dict[str, Any]] = None,\n        function_mappings: Optional[Dict[str, Callable]] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "SelectorPromptTemplate",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.base",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.base",
        "peekOfCode": "class SelectorPromptTemplate(BasePromptTemplate):  # type: ignore[no-redef]\n    default_template: SerializeAsAny[BasePromptTemplate]\n    conditionals: Optional[\n        Sequence[Tuple[Callable[[BaseLLM], bool], BasePromptTemplate]]\n    ] = None\n    def __init__(\n        self,\n        default_template: BasePromptTemplate,\n        conditionals: Optional[\n            Sequence[Tuple[Callable[[BaseLLM], bool], BasePromptTemplate]]",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "LangchainPromptTemplate",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.base",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.base",
        "peekOfCode": "class LangchainPromptTemplate(BasePromptTemplate):  # type: ignore[no-redef]\n    selector: Any\n    requires_langchain_llm: bool = False\n    def __init__(\n        self,\n        template: Optional[\"LangchainTemplate\"] = None,\n        selector: Optional[\"LangchainSelector\"] = None,\n        output_parser: Optional[BaseOutputParser] = None,\n        prompt_type: str = PromptType.CUSTOM,\n        metadata: Optional[Dict[str, Any]] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "AnnotatedCallable",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.base",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.base",
        "peekOfCode": "AnnotatedCallable = Annotated[\n    Callable,\n    WithJsonSchema({\"type\": \"string\"}),\n    WithJsonSchema({\"type\": \"string\"}),\n    PlainSerializer(lambda x: f\"{x.__module__}.{x.__name__}\", return_type=str),\n]\nclass BasePromptTemplate(BaseModel, ABC):  # type: ignore[no-redef]\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    metadata: Dict[str, Any]\n    template_vars: List[str]",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "Prompt",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.base",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.base",
        "peekOfCode": "Prompt = PromptTemplate",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.base",
        "documentation": {}
    },
    {
        "label": "TEXT_QA_SYSTEM_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "peekOfCode": "TEXT_QA_SYSTEM_PROMPT = ChatMessage(\n    content=(\n        \"You are an expert Q&A system that is trusted around the world.\\n\"\n        \"Always answer the query using the provided context information, \"\n        \"and not prior knowledge.\\n\"\n        \"Some rules to follow:\\n\"\n        \"1. Never directly reference the given context in your answer.\\n\"\n        \"2. Avoid statements like 'Based on the context, ...' or \"\n        \"'The context information ...' or anything along \"\n        \"those lines.\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "documentation": {}
    },
    {
        "label": "TEXT_QA_PROMPT_TMPL_MSGS",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "peekOfCode": "TEXT_QA_PROMPT_TMPL_MSGS = [\n    TEXT_QA_SYSTEM_PROMPT,\n    ChatMessage(\n        content=(\n            \"Context information is below.\\n\"\n            \"---------------------\\n\"\n            \"{context_str}\\n\"\n            \"---------------------\\n\"\n            \"Given the context information and not prior knowledge, \"\n            \"answer the query.\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "documentation": {}
    },
    {
        "label": "CHAT_TEXT_QA_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "peekOfCode": "CHAT_TEXT_QA_PROMPT = ChatPromptTemplate(message_templates=TEXT_QA_PROMPT_TMPL_MSGS)\n# Tree Summarize\nTREE_SUMMARIZE_PROMPT_TMPL_MSGS = [\n    TEXT_QA_SYSTEM_PROMPT,\n    ChatMessage(\n        content=(\n            \"Context information from multiple sources is below.\\n\"\n            \"---------------------\\n\"\n            \"{context_str}\\n\"\n            \"---------------------\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "documentation": {}
    },
    {
        "label": "TREE_SUMMARIZE_PROMPT_TMPL_MSGS",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "peekOfCode": "TREE_SUMMARIZE_PROMPT_TMPL_MSGS = [\n    TEXT_QA_SYSTEM_PROMPT,\n    ChatMessage(\n        content=(\n            \"Context information from multiple sources is below.\\n\"\n            \"---------------------\\n\"\n            \"{context_str}\\n\"\n            \"---------------------\\n\"\n            \"Given the information from multiple sources and not prior knowledge, \"\n            \"answer the query.\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "documentation": {}
    },
    {
        "label": "CHAT_TREE_SUMMARIZE_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "peekOfCode": "CHAT_TREE_SUMMARIZE_PROMPT = ChatPromptTemplate(\n    message_templates=TREE_SUMMARIZE_PROMPT_TMPL_MSGS\n)\n# Refine Prompt\nCHAT_REFINE_PROMPT_TMPL_MSGS = [\n    ChatMessage(\n        content=(\n            \"You are an expert Q&A system that strictly operates in two modes \"\n            \"when refining existing answers:\\n\"\n            \"1. **Rewrite** an original answer using the new context.\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "documentation": {}
    },
    {
        "label": "CHAT_REFINE_PROMPT_TMPL_MSGS",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "peekOfCode": "CHAT_REFINE_PROMPT_TMPL_MSGS = [\n    ChatMessage(\n        content=(\n            \"You are an expert Q&A system that strictly operates in two modes \"\n            \"when refining existing answers:\\n\"\n            \"1. **Rewrite** an original answer using the new context.\\n\"\n            \"2. **Repeat** the original answer if the new context isn't useful.\\n\"\n            \"Never reference the original answer or context directly in your answer.\\n\"\n            \"When in doubt, just repeat the original answer.\\n\"\n            \"New Context: {context_msg}\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "documentation": {}
    },
    {
        "label": "CHAT_REFINE_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "peekOfCode": "CHAT_REFINE_PROMPT = ChatPromptTemplate(message_templates=CHAT_REFINE_PROMPT_TMPL_MSGS)\n# Table Context Refine Prompt\nCHAT_REFINE_TABLE_CONTEXT_TMPL_MSGS = [\n    ChatMessage(content=\"{query_str}\", role=MessageRole.USER),\n    ChatMessage(content=\"{existing_answer}\", role=MessageRole.ASSISTANT),\n    ChatMessage(\n        content=(\n            \"We have provided a table schema below. \"\n            \"---------------------\\n\"\n            \"{schema}\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "documentation": {}
    },
    {
        "label": "CHAT_REFINE_TABLE_CONTEXT_TMPL_MSGS",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "peekOfCode": "CHAT_REFINE_TABLE_CONTEXT_TMPL_MSGS = [\n    ChatMessage(content=\"{query_str}\", role=MessageRole.USER),\n    ChatMessage(content=\"{existing_answer}\", role=MessageRole.ASSISTANT),\n    ChatMessage(\n        content=(\n            \"We have provided a table schema below. \"\n            \"---------------------\\n\"\n            \"{schema}\\n\"\n            \"---------------------\\n\"\n            \"We have also provided some context information below. \"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "documentation": {}
    },
    {
        "label": "CHAT_REFINE_TABLE_CONTEXT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "peekOfCode": "CHAT_REFINE_TABLE_CONTEXT_PROMPT = ChatPromptTemplate(\n    message_templates=CHAT_REFINE_TABLE_CONTEXT_TMPL_MSGS\n)",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.chat_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SUMMARY_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_SUMMARY_PROMPT_TMPL = (\n    \"Write a summary of the following. Try to use only the \"\n    \"information provided. \"\n    \"Try to include as many key details as possible.\\n\"\n    \"\\n\"\n    \"\\n\"\n    \"{context_str}\\n\"\n    \"\\n\"\n    \"\\n\"\n    'SUMMARY:\"\"\"\\n'",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SUMMARY_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_SUMMARY_PROMPT = PromptTemplate(\n    DEFAULT_SUMMARY_PROMPT_TMPL, prompt_type=PromptType.SUMMARY\n)\n# insert prompts\nDEFAULT_INSERT_PROMPT_TMPL = (\n    \"Context information is below. It is provided in a numbered list \"\n    \"(1 to {num_chunks}), \"\n    \"where each item in the list corresponds to a summary.\\n\"\n    \"---------------------\\n\"\n    \"{context_list}\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_INSERT_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_INSERT_PROMPT_TMPL = (\n    \"Context information is below. It is provided in a numbered list \"\n    \"(1 to {num_chunks}), \"\n    \"where each item in the list corresponds to a summary.\\n\"\n    \"---------------------\\n\"\n    \"{context_list}\"\n    \"---------------------\\n\"\n    \"Given the context information, here is a new piece of \"\n    \"information: {new_chunk_text}\\n\"\n    \"Answer with the number corresponding to the summary that should be updated. \"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_INSERT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_INSERT_PROMPT = PromptTemplate(\n    DEFAULT_INSERT_PROMPT_TMPL, prompt_type=PromptType.TREE_INSERT\n)\n# # single choice\nDEFAULT_QUERY_PROMPT_TMPL = (\n    \"Some choices are given below. It is provided in a numbered list \"\n    \"(1 to {num_chunks}), \"\n    \"where each item in the list corresponds to a summary.\\n\"\n    \"---------------------\\n\"\n    \"{context_list}\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_QUERY_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_QUERY_PROMPT_TMPL = (\n    \"Some choices are given below. It is provided in a numbered list \"\n    \"(1 to {num_chunks}), \"\n    \"where each item in the list corresponds to a summary.\\n\"\n    \"---------------------\\n\"\n    \"{context_list}\"\n    \"\\n---------------------\\n\"\n    \"Using only the choices above and not prior knowledge, return \"\n    \"the choice that is most relevant to the question: '{query_str}'\\n\"\n    \"Provide choice in the following format: 'ANSWER: <number>' and explain why \"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_QUERY_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_QUERY_PROMPT = PromptTemplate(\n    DEFAULT_QUERY_PROMPT_TMPL, prompt_type=PromptType.TREE_SELECT\n)\n# multiple choice\nDEFAULT_QUERY_PROMPT_MULTIPLE_TMPL = (\n    \"Some choices are given below. It is provided in a numbered \"\n    \"list (1 to {num_chunks}), \"\n    \"where each item in the list corresponds to a summary.\\n\"\n    \"---------------------\\n\"\n    \"{context_list}\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_QUERY_PROMPT_MULTIPLE_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_QUERY_PROMPT_MULTIPLE_TMPL = (\n    \"Some choices are given below. It is provided in a numbered \"\n    \"list (1 to {num_chunks}), \"\n    \"where each item in the list corresponds to a summary.\\n\"\n    \"---------------------\\n\"\n    \"{context_list}\"\n    \"\\n---------------------\\n\"\n    \"Using only the choices above and not prior knowledge, return the top choices \"\n    \"(no more than {branching_factor}, ranked by most relevant to least) that \"\n    \"are most relevant to the question: '{query_str}'\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_QUERY_PROMPT_MULTIPLE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_QUERY_PROMPT_MULTIPLE = PromptTemplate(\n    DEFAULT_QUERY_PROMPT_MULTIPLE_TMPL, prompt_type=PromptType.TREE_SELECT_MULTIPLE\n)\nDEFAULT_REFINE_PROMPT_TMPL = (\n    \"The original query is as follows: {query_str}\\n\"\n    \"We have provided an existing answer: {existing_answer}\\n\"\n    \"We have the opportunity to refine the existing answer \"\n    \"(only if needed) with some more context below.\\n\"\n    \"------------\\n\"\n    \"{context_msg}\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_REFINE_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_REFINE_PROMPT_TMPL = (\n    \"The original query is as follows: {query_str}\\n\"\n    \"We have provided an existing answer: {existing_answer}\\n\"\n    \"We have the opportunity to refine the existing answer \"\n    \"(only if needed) with some more context below.\\n\"\n    \"------------\\n\"\n    \"{context_msg}\\n\"\n    \"------------\\n\"\n    \"Given the new context, refine the original answer to better \"\n    \"answer the query. \"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_REFINE_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_REFINE_PROMPT = PromptTemplate(\n    DEFAULT_REFINE_PROMPT_TMPL, prompt_type=PromptType.REFINE\n)\nDEFAULT_TEXT_QA_PROMPT_TMPL = (\n    \"Context information is below.\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\\n\"\n    \"---------------------\\n\"\n    \"Given the context information and not prior knowledge, \"\n    \"answer the query.\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TEXT_QA_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_TEXT_QA_PROMPT_TMPL = (\n    \"Context information is below.\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\\n\"\n    \"---------------------\\n\"\n    \"Given the context information and not prior knowledge, \"\n    \"answer the query.\\n\"\n    \"Query: {query_str}\\n\"\n    \"Answer: \"\n)",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TEXT_QA_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_TEXT_QA_PROMPT = PromptTemplate(\n    DEFAULT_TEXT_QA_PROMPT_TMPL, prompt_type=PromptType.QUESTION_ANSWER\n)\nDEFAULT_TREE_SUMMARIZE_TMPL = (\n    \"Context information from multiple sources is below.\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\\n\"\n    \"---------------------\\n\"\n    \"Given the information from multiple sources and not prior knowledge, \"\n    \"answer the query.\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TREE_SUMMARIZE_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_TREE_SUMMARIZE_TMPL = (\n    \"Context information from multiple sources is below.\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\\n\"\n    \"---------------------\\n\"\n    \"Given the information from multiple sources and not prior knowledge, \"\n    \"answer the query.\\n\"\n    \"Query: {query_str}\\n\"\n    \"Answer: \"\n)",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TREE_SUMMARIZE_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_TREE_SUMMARIZE_PROMPT = PromptTemplate(\n    DEFAULT_TREE_SUMMARIZE_TMPL, prompt_type=PromptType.SUMMARY\n)\n############################################\n# Keyword Table\n############################################\nDEFAULT_KEYWORD_EXTRACT_TEMPLATE_TMPL = (\n    \"Some text is provided below. Given the text, extract up to {max_keywords} \"\n    \"keywords from the text. Avoid stopwords.\"\n    \"---------------------\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_KEYWORD_EXTRACT_TEMPLATE_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_KEYWORD_EXTRACT_TEMPLATE_TMPL = (\n    \"Some text is provided below. Given the text, extract up to {max_keywords} \"\n    \"keywords from the text. Avoid stopwords.\"\n    \"---------------------\\n\"\n    \"{text}\\n\"\n    \"---------------------\\n\"\n    \"Provide keywords in the following comma-separated format: 'KEYWORDS: <keywords>'\\n\"\n)\nDEFAULT_KEYWORD_EXTRACT_TEMPLATE = PromptTemplate(\n    DEFAULT_KEYWORD_EXTRACT_TEMPLATE_TMPL, prompt_type=PromptType.KEYWORD_EXTRACT",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_KEYWORD_EXTRACT_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_KEYWORD_EXTRACT_TEMPLATE = PromptTemplate(\n    DEFAULT_KEYWORD_EXTRACT_TEMPLATE_TMPL, prompt_type=PromptType.KEYWORD_EXTRACT\n)\n# NOTE: the keyword extraction for queries can be the same as\n# the one used to build the index, but here we tune it to see if performance is better.\nDEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE_TMPL = (\n    \"A question is provided below. Given the question, extract up to {max_keywords} \"\n    \"keywords from the text. Focus on extracting the keywords that we can use \"\n    \"to best lookup answers to the question. Avoid stopwords.\\n\"\n    \"---------------------\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE_TMPL = (\n    \"A question is provided below. Given the question, extract up to {max_keywords} \"\n    \"keywords from the text. Focus on extracting the keywords that we can use \"\n    \"to best lookup answers to the question. Avoid stopwords.\\n\"\n    \"---------------------\\n\"\n    \"{question}\\n\"\n    \"---------------------\\n\"\n    \"Provide keywords in the following comma-separated format: 'KEYWORDS: <keywords>'\\n\"\n)\nDEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE = PromptTemplate(",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE = PromptTemplate(\n    DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE_TMPL,\n    prompt_type=PromptType.QUERY_KEYWORD_EXTRACT,\n)\n############################################\n# Structured Store\n############################################\nDEFAULT_SCHEMA_EXTRACT_TMPL = (\n    \"We wish to extract relevant fields from an unstructured text chunk into \"\n    \"a structured schema. We first provide the unstructured text, and then \"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SCHEMA_EXTRACT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_SCHEMA_EXTRACT_TMPL = (\n    \"We wish to extract relevant fields from an unstructured text chunk into \"\n    \"a structured schema. We first provide the unstructured text, and then \"\n    \"we provide the schema that we wish to extract. \"\n    \"-----------text-----------\\n\"\n    \"{text}\\n\"\n    \"-----------schema-----------\\n\"\n    \"{schema}\\n\"\n    \"---------------------\\n\"\n    \"Given the text and schema, extract the relevant fields from the text in \"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SCHEMA_EXTRACT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_SCHEMA_EXTRACT_PROMPT = PromptTemplate(\n    DEFAULT_SCHEMA_EXTRACT_TMPL, prompt_type=PromptType.SCHEMA_EXTRACT\n)\n# NOTE: taken from langchain and adapted\n# https://github.com/langchain-ai/langchain/blob/v0.0.303/libs/langchain/langchain/chains/sql_database/prompt.py\nDEFAULT_TEXT_TO_SQL_TMPL = (\n    \"Given an input question, first create a syntactically correct {dialect} \"\n    \"query to run, then look at the results of the query and return the answer. \"\n    \"You can order the results by a relevant column to return the most \"\n    \"interesting examples in the database.\\n\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TEXT_TO_SQL_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_TEXT_TO_SQL_TMPL = (\n    \"Given an input question, first create a syntactically correct {dialect} \"\n    \"query to run, then look at the results of the query and return the answer. \"\n    \"You can order the results by a relevant column to return the most \"\n    \"interesting examples in the database.\\n\\n\"\n    \"Never query for all the columns from a specific table, only ask for a \"\n    \"few relevant columns given the question.\\n\\n\"\n    \"Pay attention to use only the column names that you can see in the schema \"\n    \"description. \"\n    \"Be careful to not query for columns that do not exist. \"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TEXT_TO_SQL_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_TEXT_TO_SQL_PROMPT = PromptTemplate(\n    DEFAULT_TEXT_TO_SQL_TMPL,\n    prompt_type=PromptType.TEXT_TO_SQL,\n)\nDEFAULT_TEXT_TO_SQL_PGVECTOR_TMPL = \"\"\"\\\nGiven an input question, first create a syntactically correct {dialect} \\\nquery to run, then look at the results of the query and return the answer. \\\nYou can order the results by a relevant column to return the most \\\ninteresting examples in the database.\nPay attention to use only the column names that you can see in the schema \\",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TEXT_TO_SQL_PGVECTOR_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_TEXT_TO_SQL_PGVECTOR_TMPL = \"\"\"\\\nGiven an input question, first create a syntactically correct {dialect} \\\nquery to run, then look at the results of the query and return the answer. \\\nYou can order the results by a relevant column to return the most \\\ninteresting examples in the database.\nPay attention to use only the column names that you can see in the schema \\\ndescription. Be careful to not query for columns that do not exist. \\\nPay attention to which column is in which table. Also, qualify column names \\\nwith the table name when needed.\nIMPORTANT NOTE: you can use specialized pgvector syntax (`<->`) to do nearest \\",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TEXT_TO_SQL_PGVECTOR_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_TEXT_TO_SQL_PGVECTOR_PROMPT = PromptTemplate(\n    DEFAULT_TEXT_TO_SQL_PGVECTOR_TMPL,\n    prompt_type=PromptType.TEXT_TO_SQL,\n)\n# NOTE: by partially filling schema, we can reduce to a QuestionAnswer prompt\n# that we can feed to ur table\nDEFAULT_TABLE_CONTEXT_TMPL = (\n    \"We have provided a table schema below. \"\n    \"---------------------\\n\"\n    \"{schema}\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TABLE_CONTEXT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_TABLE_CONTEXT_TMPL = (\n    \"We have provided a table schema below. \"\n    \"---------------------\\n\"\n    \"{schema}\\n\"\n    \"---------------------\\n\"\n    \"We have also provided context information below. \"\n    \"{context_str}\\n\"\n    \"---------------------\\n\"\n    \"Given the context information and the table schema, \"\n    \"give a response to the following task: {query_str}\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TABLE_CONTEXT_QUERY",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_TABLE_CONTEXT_QUERY = (\n    \"Provide a high-level description of the table, \"\n    \"as well as a description of each column in the table. \"\n    \"Provide answers in the following format:\\n\"\n    \"TableDescription: <description>\\n\"\n    \"Column1Description: <description>\\n\"\n    \"Column2Description: <description>\\n\"\n    \"...\\n\\n\"\n)\nDEFAULT_TABLE_CONTEXT_PROMPT = PromptTemplate(",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TABLE_CONTEXT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_TABLE_CONTEXT_PROMPT = PromptTemplate(\n    DEFAULT_TABLE_CONTEXT_TMPL, prompt_type=PromptType.TABLE_CONTEXT\n)\n# NOTE: by partially filling schema, we can reduce to a refine prompt\n# that we can feed to ur table\nDEFAULT_REFINE_TABLE_CONTEXT_TMPL = (\n    \"We have provided a table schema below. \"\n    \"---------------------\\n\"\n    \"{schema}\\n\"\n    \"---------------------\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_REFINE_TABLE_CONTEXT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_REFINE_TABLE_CONTEXT_TMPL = (\n    \"We have provided a table schema below. \"\n    \"---------------------\\n\"\n    \"{schema}\\n\"\n    \"---------------------\\n\"\n    \"We have also provided some context information below. \"\n    \"{context_msg}\\n\"\n    \"---------------------\\n\"\n    \"Given the context information and the table schema, \"\n    \"give a response to the following task: {query_str}\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_REFINE_TABLE_CONTEXT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_REFINE_TABLE_CONTEXT_PROMPT = PromptTemplate(\n    DEFAULT_REFINE_TABLE_CONTEXT_TMPL, prompt_type=PromptType.TABLE_CONTEXT\n)\n############################################\n# Knowledge-Graph Table\n############################################\nDEFAULT_KG_TRIPLET_EXTRACT_TMPL = (\n    \"Some text is provided below. Given the text, extract up to \"\n    \"{max_knowledge_triplets} \"\n    \"knowledge triplets in the form of (subject, predicate, object). Avoid stopwords.\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_KG_TRIPLET_EXTRACT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_KG_TRIPLET_EXTRACT_TMPL = (\n    \"Some text is provided below. Given the text, extract up to \"\n    \"{max_knowledge_triplets} \"\n    \"knowledge triplets in the form of (subject, predicate, object). Avoid stopwords.\\n\"\n    \"---------------------\\n\"\n    \"Example:\"\n    \"Text: Alice is Bob's mother.\"\n    \"Triplets:\\n(Alice, is mother of, Bob)\\n\"\n    \"Text: Philz is a coffee shop founded in Berkeley in 1982.\\n\"\n    \"Triplets:\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_KG_TRIPLET_EXTRACT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_KG_TRIPLET_EXTRACT_PROMPT = PromptTemplate(\n    DEFAULT_KG_TRIPLET_EXTRACT_TMPL,\n    prompt_type=PromptType.KNOWLEDGE_TRIPLET_EXTRACT,\n)\nDEFAULT_DYNAMIC_EXTRACT_TMPL = (\n    \"Extract up to {max_knowledge_triplets} knowledge triplets from the given text. \"\n    \"Each triplet should be in the form of (head, relation, tail) with their respective types.\\n\"\n    \"---------------------\\n\"\n    \"INITIAL ONTOLOGY:\\n\"\n    \"Entity Types: {allowed_entity_types}\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_DYNAMIC_EXTRACT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_DYNAMIC_EXTRACT_TMPL = (\n    \"Extract up to {max_knowledge_triplets} knowledge triplets from the given text. \"\n    \"Each triplet should be in the form of (head, relation, tail) with their respective types.\\n\"\n    \"---------------------\\n\"\n    \"INITIAL ONTOLOGY:\\n\"\n    \"Entity Types: {allowed_entity_types}\\n\"\n    \"Relation Types: {allowed_relation_types}\\n\"\n    \"\\n\"\n    \"Use these types as a starting point, but introduce new types if necessary based on the context.\\n\"\n    \"\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_DYNAMIC_EXTRACT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_DYNAMIC_EXTRACT_PROMPT = PromptTemplate(\n    DEFAULT_DYNAMIC_EXTRACT_TMPL, prompt_type=PromptType.KNOWLEDGE_TRIPLET_EXTRACT\n)\nDEFAULT_DYNAMIC_EXTRACT_PROPS_TMPL = (\n    \"Extract up to {max_knowledge_triplets} knowledge triplets from the given text. \"\n    \"Each triplet should be in the form of (head, relation, tail) with their respective types and properties.\\n\"\n    \"---------------------\\n\"\n    \"INITIAL ONTOLOGY:\\n\"\n    \"Entity Types: {allowed_entity_types}\\n\"\n    \"Entity Properties: {allowed_entity_properties}\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_DYNAMIC_EXTRACT_PROPS_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_DYNAMIC_EXTRACT_PROPS_TMPL = (\n    \"Extract up to {max_knowledge_triplets} knowledge triplets from the given text. \"\n    \"Each triplet should be in the form of (head, relation, tail) with their respective types and properties.\\n\"\n    \"---------------------\\n\"\n    \"INITIAL ONTOLOGY:\\n\"\n    \"Entity Types: {allowed_entity_types}\\n\"\n    \"Entity Properties: {allowed_entity_properties}\\n\"\n    \"Relation Types: {allowed_relation_types}\\n\"\n    \"Relation Properties: {allowed_relation_properties}\\n\"\n    \"\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_DYNAMIC_EXTRACT_PROPS_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_DYNAMIC_EXTRACT_PROPS_PROMPT = PromptTemplate(\n    DEFAULT_DYNAMIC_EXTRACT_PROPS_TMPL, prompt_type=PromptType.KNOWLEDGE_TRIPLET_EXTRACT\n)\n############################################\n# HYDE\n##############################################\nHYDE_TMPL = (\n    \"Please write a passage to answer the question\\n\"\n    \"Try to include as many key details as possible.\\n\"\n    \"\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "HYDE_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "HYDE_TMPL = (\n    \"Please write a passage to answer the question\\n\"\n    \"Try to include as many key details as possible.\\n\"\n    \"\\n\"\n    \"\\n\"\n    \"{context_str}\\n\"\n    \"\\n\"\n    \"\\n\"\n    'Passage:\"\"\"\\n'\n)",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_HYDE_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_HYDE_PROMPT = PromptTemplate(HYDE_TMPL, prompt_type=PromptType.SUMMARY)\n############################################\n# Simple Input\n############################################\nDEFAULT_SIMPLE_INPUT_TMPL = \"{query_str}\"\nDEFAULT_SIMPLE_INPUT_PROMPT = PromptTemplate(\n    DEFAULT_SIMPLE_INPUT_TMPL, prompt_type=PromptType.SIMPLE_INPUT\n)\n############################################\n# JSON Path",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SIMPLE_INPUT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_SIMPLE_INPUT_TMPL = \"{query_str}\"\nDEFAULT_SIMPLE_INPUT_PROMPT = PromptTemplate(\n    DEFAULT_SIMPLE_INPUT_TMPL, prompt_type=PromptType.SIMPLE_INPUT\n)\n############################################\n# JSON Path\n############################################\nDEFAULT_JSON_PATH_TMPL = (\n    \"We have provided a JSON schema below:\\n\"\n    \"{schema}\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SIMPLE_INPUT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_SIMPLE_INPUT_PROMPT = PromptTemplate(\n    DEFAULT_SIMPLE_INPUT_TMPL, prompt_type=PromptType.SIMPLE_INPUT\n)\n############################################\n# JSON Path\n############################################\nDEFAULT_JSON_PATH_TMPL = (\n    \"We have provided a JSON schema below:\\n\"\n    \"{schema}\\n\"\n    \"Given a task, respond with a JSON Path query that \"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_JSON_PATH_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_JSON_PATH_TMPL = (\n    \"We have provided a JSON schema below:\\n\"\n    \"{schema}\\n\"\n    \"Given a task, respond with a JSON Path query that \"\n    \"can retrieve data from a JSON value that matches the schema.\\n\"\n    \"Provide the JSON Path query in the following format: 'JSONPath: <JSONPath>'\\n\"\n    \"You must include the value 'JSONPath:' before the provided JSON Path query.\"\n    \"Example Format:\\n\"\n    \"Task: What is John's age?\\n\"\n    \"Response: JSONPath: $.John.age\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_JSON_PATH_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_JSON_PATH_PROMPT = PromptTemplate(\n    DEFAULT_JSON_PATH_TMPL, prompt_type=PromptType.JSON_PATH\n)\n############################################\n# Choice Select\n############################################\nDEFAULT_CHOICE_SELECT_PROMPT_TMPL = (\n    \"A list of documents is shown below. Each document has a number next to it along \"\n    \"with a summary of the document. A question is also provided. \\n\"\n    \"Respond with the numbers of the documents \"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CHOICE_SELECT_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_CHOICE_SELECT_PROMPT_TMPL = (\n    \"A list of documents is shown below. Each document has a number next to it along \"\n    \"with a summary of the document. A question is also provided. \\n\"\n    \"Respond with the numbers of the documents \"\n    \"you should consult to answer the question, in order of relevance, as well \\n\"\n    \"as the relevance score. The relevance score is a number from 1-10 based on \"\n    \"how relevant you think the document is to the question.\\n\"\n    \"Do not include any documents that are not relevant to the question. \\n\"\n    \"Example format: \\n\"\n    \"Document 1:\\n<summary of document 1>\\n\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CHOICE_SELECT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_CHOICE_SELECT_PROMPT = PromptTemplate(\n    DEFAULT_CHOICE_SELECT_PROMPT_TMPL, prompt_type=PromptType.CHOICE_SELECT\n)\n############################################\n# Structured Choice Select\n############################################\nSTRUCTURED_CHOICE_SELECT_PROMPT_TMPL = (\n    \"A list of documents is shown below. Each document has a number next to it along \"\n    \"with a summary of the document. A question is also provided. \\n\"\n    \"Respond with the numbers of the documents \"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "STRUCTURED_CHOICE_SELECT_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "STRUCTURED_CHOICE_SELECT_PROMPT_TMPL = (\n    \"A list of documents is shown below. Each document has a number next to it along \"\n    \"with a summary of the document. A question is also provided. \\n\"\n    \"Respond with the numbers of the documents \"\n    \"you should consult to answer the question, in order of relevance, as well \\n\"\n    \"as the relevance score. The relevance score is a number from 1-10 based on \"\n    \"how relevant you think the document is to the question.\\n\"\n    \"Do not include any documents that are not relevant to the question. \\n\"\n    \"Let's try this now: \\n\\n\"\n    \"{context_str}\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "STRUCTURED_CHOICE_SELECT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "STRUCTURED_CHOICE_SELECT_PROMPT = PromptTemplate(\n    STRUCTURED_CHOICE_SELECT_PROMPT_TMPL, prompt_type=PromptType.CHOICE_SELECT\n)\n############################################\n# RankGPT Rerank template\n############################################\nRANKGPT_RERANK_PROMPT_TMPL = (\n    \"Search Query: {query}. \\nRank the {num} passages above \"\n    \"based on their relevance to the search query. The passages \"\n    \"should be listed in descending order using identifiers. \"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "RANKGPT_RERANK_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "RANKGPT_RERANK_PROMPT_TMPL = (\n    \"Search Query: {query}. \\nRank the {num} passages above \"\n    \"based on their relevance to the search query. The passages \"\n    \"should be listed in descending order using identifiers. \"\n    \"The most relevant passages should be listed first. \"\n    \"The output format should be [] > [], e.g., [1] > [2]. \"\n    \"Only response the ranking results, \"\n    \"do not say any word or explain.\"\n)\nRANKGPT_RERANK_PROMPT = PromptTemplate(",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "RANKGPT_RERANK_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "RANKGPT_RERANK_PROMPT = PromptTemplate(\n    RANKGPT_RERANK_PROMPT_TMPL, prompt_type=PromptType.RANKGPT_RERANK\n)\n############################################\n# JSONalyze Query Template\n############################################\nDEFAULT_JSONALYZE_PROMPT_TMPL = (\n    \"You are given a table named: '{table_name}' with schema, \"\n    \"generate SQLite SQL query to answer the given question.\\n\"\n    \"Table schema:\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_JSONALYZE_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_JSONALYZE_PROMPT_TMPL = (\n    \"You are given a table named: '{table_name}' with schema, \"\n    \"generate SQLite SQL query to answer the given question.\\n\"\n    \"Table schema:\\n\"\n    \"{table_schema}\\n\"\n    \"Question: {question}\\n\\n\"\n    \"SQLQuery: \"\n)\nDEFAULT_JSONALYZE_PROMPT = PromptTemplate(\n    DEFAULT_JSONALYZE_PROMPT_TMPL, prompt_type=PromptType.TEXT_TO_SQL",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_JSONALYZE_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "peekOfCode": "DEFAULT_JSONALYZE_PROMPT = PromptTemplate(\n    DEFAULT_JSONALYZE_PROMPT_TMPL, prompt_type=PromptType.TEXT_TO_SQL\n)",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompts",
        "documentation": {}
    },
    {
        "label": "default_text_qa_conditionals",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompt_selectors",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompt_selectors",
        "peekOfCode": "default_text_qa_conditionals = [(is_chat_model, CHAT_TEXT_QA_PROMPT)]\nif COHERE_QA_TEMPLATE is not None:\n    default_text_qa_conditionals = [\n        (is_cohere_model, COHERE_QA_TEMPLATE),\n        (is_chat_model, CHAT_TEXT_QA_PROMPT),\n    ]\nDEFAULT_TEXT_QA_PROMPT_SEL = SelectorPromptTemplate(\n    default_template=DEFAULT_TEXT_QA_PROMPT,\n    conditionals=default_text_qa_conditionals,\n)",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompt_selectors",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TEXT_QA_PROMPT_SEL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompt_selectors",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompt_selectors",
        "peekOfCode": "DEFAULT_TEXT_QA_PROMPT_SEL = SelectorPromptTemplate(\n    default_template=DEFAULT_TEXT_QA_PROMPT,\n    conditionals=default_text_qa_conditionals,\n)\n# Tree Summarize\ndefault_tree_summarize_conditionals = [(is_chat_model, CHAT_TREE_SUMMARIZE_PROMPT)]\nif COHERE_TREE_SUMMARIZE_TEMPLATE is not None:\n    default_tree_summarize_conditionals = [\n        (is_cohere_model, COHERE_TREE_SUMMARIZE_TEMPLATE),\n        (is_chat_model, CHAT_TREE_SUMMARIZE_PROMPT),",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompt_selectors",
        "documentation": {}
    },
    {
        "label": "default_tree_summarize_conditionals",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompt_selectors",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompt_selectors",
        "peekOfCode": "default_tree_summarize_conditionals = [(is_chat_model, CHAT_TREE_SUMMARIZE_PROMPT)]\nif COHERE_TREE_SUMMARIZE_TEMPLATE is not None:\n    default_tree_summarize_conditionals = [\n        (is_cohere_model, COHERE_TREE_SUMMARIZE_TEMPLATE),\n        (is_chat_model, CHAT_TREE_SUMMARIZE_PROMPT),\n    ]\nDEFAULT_TREE_SUMMARIZE_PROMPT_SEL = SelectorPromptTemplate(\n    default_template=DEFAULT_TREE_SUMMARIZE_PROMPT,\n    conditionals=default_tree_summarize_conditionals,\n)",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompt_selectors",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TREE_SUMMARIZE_PROMPT_SEL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompt_selectors",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompt_selectors",
        "peekOfCode": "DEFAULT_TREE_SUMMARIZE_PROMPT_SEL = SelectorPromptTemplate(\n    default_template=DEFAULT_TREE_SUMMARIZE_PROMPT,\n    conditionals=default_tree_summarize_conditionals,\n)\n# Refine\ndefault_refine_conditionals = [(is_chat_model, CHAT_REFINE_PROMPT)]\nif COHERE_REFINE_TEMPLATE is not None:\n    default_refine_conditionals = [\n        (is_cohere_model, COHERE_REFINE_TEMPLATE),\n        (is_chat_model, CHAT_REFINE_PROMPT),",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompt_selectors",
        "documentation": {}
    },
    {
        "label": "default_refine_conditionals",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompt_selectors",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompt_selectors",
        "peekOfCode": "default_refine_conditionals = [(is_chat_model, CHAT_REFINE_PROMPT)]\nif COHERE_REFINE_TEMPLATE is not None:\n    default_refine_conditionals = [\n        (is_cohere_model, COHERE_REFINE_TEMPLATE),\n        (is_chat_model, CHAT_REFINE_PROMPT),\n    ]\nDEFAULT_REFINE_PROMPT_SEL = SelectorPromptTemplate(\n    default_template=DEFAULT_REFINE_PROMPT,\n    conditionals=default_refine_conditionals,\n)",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompt_selectors",
        "documentation": {}
    },
    {
        "label": "DEFAULT_REFINE_PROMPT_SEL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompt_selectors",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompt_selectors",
        "peekOfCode": "DEFAULT_REFINE_PROMPT_SEL = SelectorPromptTemplate(\n    default_template=DEFAULT_REFINE_PROMPT,\n    conditionals=default_refine_conditionals,\n)\n# Refine Table Context\ndefault_refine_table_conditionals = [(is_chat_model, CHAT_REFINE_TABLE_CONTEXT_PROMPT)]\nif COHERE_REFINE_TABLE_CONTEXT_PROMPT is not None:\n    default_refine_table_conditionals = [\n        (is_cohere_model, COHERE_REFINE_TABLE_CONTEXT_PROMPT),\n        (is_chat_model, CHAT_REFINE_TABLE_CONTEXT_PROMPT),",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompt_selectors",
        "documentation": {}
    },
    {
        "label": "default_refine_table_conditionals",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompt_selectors",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompt_selectors",
        "peekOfCode": "default_refine_table_conditionals = [(is_chat_model, CHAT_REFINE_TABLE_CONTEXT_PROMPT)]\nif COHERE_REFINE_TABLE_CONTEXT_PROMPT is not None:\n    default_refine_table_conditionals = [\n        (is_cohere_model, COHERE_REFINE_TABLE_CONTEXT_PROMPT),\n        (is_chat_model, CHAT_REFINE_TABLE_CONTEXT_PROMPT),\n    ]\nDEFAULT_REFINE_TABLE_CONTEXT_PROMPT_SEL = SelectorPromptTemplate(\n    default_template=DEFAULT_REFINE_TABLE_CONTEXT_PROMPT,\n    conditionals=default_refine_table_conditionals,\n)",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompt_selectors",
        "documentation": {}
    },
    {
        "label": "DEFAULT_REFINE_TABLE_CONTEXT_PROMPT_SEL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.default_prompt_selectors",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.default_prompt_selectors",
        "peekOfCode": "DEFAULT_REFINE_TABLE_CONTEXT_PROMPT_SEL = SelectorPromptTemplate(\n    default_template=DEFAULT_REFINE_TABLE_CONTEXT_PROMPT,\n    conditionals=default_refine_table_conditionals,\n)",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.default_prompt_selectors",
        "documentation": {}
    },
    {
        "label": "display_prompt_dict",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.display_utils",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.display_utils",
        "peekOfCode": "def display_prompt_dict(prompts_dict: PromptDictType) -> None:\n    \"\"\"\n    Display prompt dict.\n    Args:\n        prompts_dict: prompt dict\n    \"\"\"\n    from IPython.display import Markdown, display\n    for k, p in prompts_dict.items():\n        text_md = f\"**Prompt Key**: {k}<br>**Text:** <br>\"\n        display(Markdown(text_md))",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.display_utils",
        "documentation": {}
    },
    {
        "label": "convert_to_handlebars",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.guidance_utils",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.guidance_utils",
        "peekOfCode": "def convert_to_handlebars(text: str) -> str:\n    \"\"\"\n    Convert a python format string to handlebars-style template.\n    In python format string, single braces {} are used for variable substitution,\n        and double braces {{}} are used for escaping actual braces (e.g. for JSON dict)\n    In handlebars template, double braces {{}} are used for variable substitution,\n        and single braces are actual braces (e.g. for JSON dict)\n    This is currently only used to convert a python format string based prompt template\n    to a guidance program template.\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.guidance_utils",
        "documentation": {}
    },
    {
        "label": "wrap_json_markdown",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.guidance_utils",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.guidance_utils",
        "peekOfCode": "def wrap_json_markdown(text: str) -> str:\n    \"\"\"Wrap text in json markdown formatting block.\"\"\"\n    return \"```json\\n\" + text + \"\\n```\"\ndef pydantic_to_guidance_output_template(cls: Type[BaseModel]) -> str:\n    \"\"\"Convert a pydantic model to guidance output template.\"\"\"\n    return json_schema_to_guidance_output_template(\n        cls.model_json_schema(), root=cls.model_json_schema()\n    )\ndef pydantic_to_guidance_output_template_markdown(cls: Type[BaseModel]) -> str:\n    \"\"\"Convert a pydantic model to guidance output template wrapped in json markdown.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.guidance_utils",
        "documentation": {}
    },
    {
        "label": "pydantic_to_guidance_output_template",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.guidance_utils",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.guidance_utils",
        "peekOfCode": "def pydantic_to_guidance_output_template(cls: Type[BaseModel]) -> str:\n    \"\"\"Convert a pydantic model to guidance output template.\"\"\"\n    return json_schema_to_guidance_output_template(\n        cls.model_json_schema(), root=cls.model_json_schema()\n    )\ndef pydantic_to_guidance_output_template_markdown(cls: Type[BaseModel]) -> str:\n    \"\"\"Convert a pydantic model to guidance output template wrapped in json markdown.\"\"\"\n    output = json_schema_to_guidance_output_template(\n        cls.model_json_schema(), root=cls.model_json_schema()\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.guidance_utils",
        "documentation": {}
    },
    {
        "label": "pydantic_to_guidance_output_template_markdown",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.guidance_utils",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.guidance_utils",
        "peekOfCode": "def pydantic_to_guidance_output_template_markdown(cls: Type[BaseModel]) -> str:\n    \"\"\"Convert a pydantic model to guidance output template wrapped in json markdown.\"\"\"\n    output = json_schema_to_guidance_output_template(\n        cls.model_json_schema(), root=cls.model_json_schema()\n    )\n    return wrap_json_markdown(output)\ndef json_schema_to_guidance_output_template(\n    schema: dict,\n    key: Optional[str] = None,\n    indent: int = 0,",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.guidance_utils",
        "documentation": {}
    },
    {
        "label": "json_schema_to_guidance_output_template",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.guidance_utils",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.guidance_utils",
        "peekOfCode": "def json_schema_to_guidance_output_template(\n    schema: dict,\n    key: Optional[str] = None,\n    indent: int = 0,\n    root: Optional[dict] = None,\n    use_pattern_control: bool = False,\n) -> str:\n    \"\"\"\n    Convert a json schema to guidance output template.\n    Implementation based on https://github.com/microsoft/guidance/\\",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.guidance_utils",
        "documentation": {}
    },
    {
        "label": "parse_pydantic_from_guidance_program",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.guidance_utils",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.guidance_utils",
        "peekOfCode": "def parse_pydantic_from_guidance_program(\n    response: str, cls: Type[Model], verbose: bool = False\n) -> Model:\n    \"\"\"\n    Parse output from guidance program.\n    This is a temporary solution for parsing a pydantic object out of an executed\n    guidance program.\n    NOTE: right now we assume the output is the last markdown formatted json block\n    NOTE: a better way is to extract via Program.variables, but guidance does not\n          support extracting nested objects right now.",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.guidance_utils",
        "documentation": {}
    },
    {
        "label": "Model",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.guidance_utils",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.guidance_utils",
        "peekOfCode": "Model = TypeVar(\"Model\", bound=BaseModel)\ndef parse_pydantic_from_guidance_program(\n    response: str, cls: Type[Model], verbose: bool = False\n) -> Model:\n    \"\"\"\n    Parse output from guidance program.\n    This is a temporary solution for parsing a pydantic object out of an executed\n    guidance program.\n    NOTE: right now we assume the output is the last markdown formatted json block\n    NOTE: a better way is to extract via Program.variables, but guidance does not",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.guidance_utils",
        "documentation": {}
    },
    {
        "label": "PromptMixin",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.mixin",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.mixin",
        "peekOfCode": "class PromptMixin(ABC):\n    \"\"\"\n    Prompt mixin.\n    This mixin is used in other modules, like query engines, response synthesizers.\n    This shows that the module supports getting, setting prompts,\n    both within the immediate module as well as child modules.\n    \"\"\"\n    def _validate_prompts(\n        self,\n        prompts_dict: PromptDictType,",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "HasPromptType",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.mixin",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.mixin",
        "peekOfCode": "HasPromptType = Union[\"PromptMixin\", BasePromptTemplate]\nPromptDictType = Dict[str, BasePromptTemplate]\nPromptMixinType = Dict[str, \"PromptMixin\"]\nclass PromptMixin(ABC):\n    \"\"\"\n    Prompt mixin.\n    This mixin is used in other modules, like query engines, response synthesizers.\n    This shows that the module supports getting, setting prompts,\n    both within the immediate module as well as child modules.\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptDictType",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.mixin",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.mixin",
        "peekOfCode": "PromptDictType = Dict[str, BasePromptTemplate]\nPromptMixinType = Dict[str, \"PromptMixin\"]\nclass PromptMixin(ABC):\n    \"\"\"\n    Prompt mixin.\n    This mixin is used in other modules, like query engines, response synthesizers.\n    This shows that the module supports getting, setting prompts,\n    both within the immediate module as well as child modules.\n    \"\"\"\n    def _validate_prompts(",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "PromptMixinType",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.mixin",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.mixin",
        "peekOfCode": "PromptMixinType = Dict[str, \"PromptMixin\"]\nclass PromptMixin(ABC):\n    \"\"\"\n    Prompt mixin.\n    This mixin is used in other modules, like query engines, response synthesizers.\n    This shows that the module supports getting, setting prompts,\n    both within the immediate module as well as child modules.\n    \"\"\"\n    def _validate_prompts(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.mixin",
        "documentation": {}
    },
    {
        "label": "SummaryPrompt",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "peekOfCode": "SummaryPrompt = PromptTemplate\n\"\"\"Tree Insert prompt.\nPromptTemplate to insert a new chunk of text `new_chunk_text` into the tree index.\nMore specifically, this prompt has the LLM select the relevant candidate\nchild node to continue tree traversal.\nRequired template variables: `num_chunks`, `context_list`, `new_chunk_text`\n\"\"\"\nTreeInsertPrompt = PromptTemplate\n\"\"\"Tree select prompt.\nPromptTemplate to select a candidate child node out of all child nodes",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "documentation": {}
    },
    {
        "label": "TreeInsertPrompt",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "peekOfCode": "TreeInsertPrompt = PromptTemplate\n\"\"\"Tree select prompt.\nPromptTemplate to select a candidate child node out of all child nodes\nprovided in `context_list`, given a query `query_str`. `num_chunks` is\nthe number of child nodes in `context_list`.\nRequired template variables: `num_chunks`, `context_list`, `query_str`\n\"\"\"\nTreeSelectPrompt = PromptTemplate\n\"\"\"Tree select multiple prompt.\nPromptTemplate to select multiple candidate child nodes out of all",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "documentation": {}
    },
    {
        "label": "TreeSelectPrompt",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "peekOfCode": "TreeSelectPrompt = PromptTemplate\n\"\"\"Tree select multiple prompt.\nPromptTemplate to select multiple candidate child nodes out of all\nchild nodes provided in `context_list`, given a query `query_str`.\n`branching_factor` refers to the number of child nodes to select, and\n`num_chunks` is the number of child nodes in `context_list`.\nRequired template variables: `num_chunks`, `context_list`, `query_str`,\n    `branching_factor`\n\"\"\"\nTreeSelectMultiplePrompt = PromptTemplate",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "documentation": {}
    },
    {
        "label": "TreeSelectMultiplePrompt",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "peekOfCode": "TreeSelectMultiplePrompt = PromptTemplate\n\"\"\"Refine prompt.\nPromptTemplate to refine an existing answer `existing_answer`\ngiven a context `context_msg`, and a query `query_str`.\nRequired template variables: `query_str`, `existing_answer`, `context_msg`\n\"\"\"\nRefinePrompt = PromptTemplate\n\"\"\"Question Answer prompt.\nPromptTemplate to answer a question `query_str` given a context `context_str`.\nRequired template variables: `context_str`, `query_str`",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "documentation": {}
    },
    {
        "label": "RefinePrompt",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "peekOfCode": "RefinePrompt = PromptTemplate\n\"\"\"Question Answer prompt.\nPromptTemplate to answer a question `query_str` given a context `context_str`.\nRequired template variables: `context_str`, `query_str`\n\"\"\"\nQuestionAnswerPrompt = PromptTemplate\n\"\"\"Keyword extract prompt.\nPromptTemplate to extract keywords from a text `text` with a maximum of\n`max_keywords` keywords.\nRequired template variables: `text`, `max_keywords`",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "documentation": {}
    },
    {
        "label": "QuestionAnswerPrompt",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "peekOfCode": "QuestionAnswerPrompt = PromptTemplate\n\"\"\"Keyword extract prompt.\nPromptTemplate to extract keywords from a text `text` with a maximum of\n`max_keywords` keywords.\nRequired template variables: `text`, `max_keywords`\n\"\"\"\nKeywordExtractPrompt = PromptTemplate\n\"\"\"Query keyword extract prompt.\nPromptTemplate to extract keywords from a query `query_str` with a maximum\nof `max_keywords` keywords.",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "documentation": {}
    },
    {
        "label": "KeywordExtractPrompt",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "peekOfCode": "KeywordExtractPrompt = PromptTemplate\n\"\"\"Query keyword extract prompt.\nPromptTemplate to extract keywords from a query `query_str` with a maximum\nof `max_keywords` keywords.\nRequired template variables: `query_str`, `max_keywords`\n\"\"\"\nQueryKeywordExtractPrompt = PromptTemplate\n\"\"\"Schema extract prompt.\nPromptTemplate to extract schema from unstructured text `text`.\nRequired template variables: `text`, `schema`",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "documentation": {}
    },
    {
        "label": "QueryKeywordExtractPrompt",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "peekOfCode": "QueryKeywordExtractPrompt = PromptTemplate\n\"\"\"Schema extract prompt.\nPromptTemplate to extract schema from unstructured text `text`.\nRequired template variables: `text`, `schema`\n\"\"\"\nSchemaExtractPrompt = PromptTemplate\n\"\"\"Text to SQL prompt.\nPromptTemplate to translate a natural language query into SQL in the dialect\n`dialect` given a schema `schema`.\nRequired template variables: `query_str`, `schema`, `dialect`",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "documentation": {}
    },
    {
        "label": "SchemaExtractPrompt",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "peekOfCode": "SchemaExtractPrompt = PromptTemplate\n\"\"\"Text to SQL prompt.\nPromptTemplate to translate a natural language query into SQL in the dialect\n`dialect` given a schema `schema`.\nRequired template variables: `query_str`, `schema`, `dialect`\n\"\"\"\nTextToSQLPrompt = PromptTemplate\n\"\"\"Table context prompt.\nPromptTemplate to generate a table context given a table schema `schema`,\nas well as unstructured text context `context_str`, and",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "documentation": {}
    },
    {
        "label": "TextToSQLPrompt",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "peekOfCode": "TextToSQLPrompt = PromptTemplate\n\"\"\"Table context prompt.\nPromptTemplate to generate a table context given a table schema `schema`,\nas well as unstructured text context `context_str`, and\na task `query_str`.\nThis includes both a high-level description of the table\nas well as a description of each column in the table.\n\"\"\"\nTableContextPrompt = PromptTemplate\n\"\"\"Refine Table context prompt.",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "documentation": {}
    },
    {
        "label": "TableContextPrompt",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "peekOfCode": "TableContextPrompt = PromptTemplate\n\"\"\"Refine Table context prompt.\nPromptTemplate to refine a table context given a table schema `schema`,\nas well as unstructured text context `context_msg`, and\na task `query_str`.\nThis includes both a high-level description of the table\nas well as a description of each column in the table.\n\"\"\"\nRefineTableContextPrompt = PromptTemplate\n\"\"\"Define the knowledge graph triplet extraction prompt.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "documentation": {}
    },
    {
        "label": "RefineTableContextPrompt",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "peekOfCode": "RefineTableContextPrompt = PromptTemplate\n\"\"\"Define the knowledge graph triplet extraction prompt.\"\"\"\nKnowledgeGraphPrompt = PromptTemplate\n\"\"\"Simple Input prompt.\nRequired template variables: `query_str`.\n\"\"\"\nSimpleInputPrompt = PromptTemplate\n\"\"\"Pandas prompt. Convert query to python code.\nRequired template variables: `query_str`, `df_str`, `instruction_str`.\n\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "documentation": {}
    },
    {
        "label": "KnowledgeGraphPrompt",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "peekOfCode": "KnowledgeGraphPrompt = PromptTemplate\n\"\"\"Simple Input prompt.\nRequired template variables: `query_str`.\n\"\"\"\nSimpleInputPrompt = PromptTemplate\n\"\"\"Pandas prompt. Convert query to python code.\nRequired template variables: `query_str`, `df_str`, `instruction_str`.\n\"\"\"\nPandasPrompt = PromptTemplate\n\"\"\"Choice select prompt. Select from a list of choices.",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "documentation": {}
    },
    {
        "label": "SimpleInputPrompt",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "peekOfCode": "SimpleInputPrompt = PromptTemplate\n\"\"\"Pandas prompt. Convert query to python code.\nRequired template variables: `query_str`, `df_str`, `instruction_str`.\n\"\"\"\nPandasPrompt = PromptTemplate\n\"\"\"Choice select prompt. Select from a list of choices.\nRequired template variables: `context_str`, `query_str`.\n\"\"\"\nChoiceSelectPrompt = PromptTemplate",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "documentation": {}
    },
    {
        "label": "PandasPrompt",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "peekOfCode": "PandasPrompt = PromptTemplate\n\"\"\"Choice select prompt. Select from a list of choices.\nRequired template variables: `context_str`, `query_str`.\n\"\"\"\nChoiceSelectPrompt = PromptTemplate",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "documentation": {}
    },
    {
        "label": "ChoiceSelectPrompt",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "peekOfCode": "ChoiceSelectPrompt = PromptTemplate",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.prompts",
        "documentation": {}
    },
    {
        "label": "PromptType",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.prompt_type",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.prompt_type",
        "peekOfCode": "class PromptType(str, Enum):\n    \"\"\"Prompt type.\"\"\"\n    # summarization\n    SUMMARY = \"summary\"\n    # tree insert node\n    TREE_INSERT = \"insert\"\n    # tree select query prompt\n    TREE_SELECT = \"tree_select\"\n    # tree select query prompt (multiple)\n    TREE_SELECT_MULTIPLE = \"tree_select_multiple\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.prompt_type",
        "documentation": {}
    },
    {
        "label": "get_empty_prompt_txt",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.prompt_utils",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.prompt_utils",
        "peekOfCode": "def get_empty_prompt_txt(prompt: BasePromptTemplate) -> str:\n    \"\"\"\n    Get empty prompt text.\n    Substitute empty strings in parts of the prompt that have\n    not yet been filled out. Skip variables that have already\n    been partially formatted. This is used to compute the initial tokens.\n    \"\"\"\n    partial_kargs = prompt.kwargs\n    empty_kwargs = {v: \"\" for v in prompt.template_vars if v not in partial_kargs}\n    all_kwargs = {**partial_kargs, **empty_kwargs}",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.prompt_utils",
        "documentation": {}
    },
    {
        "label": "get_biggest_prompt",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.prompt_utils",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.prompt_utils",
        "peekOfCode": "def get_biggest_prompt(prompts: List[BasePromptTemplate]) -> BasePromptTemplate:\n    \"\"\"\n    Get biggest prompt.\n    Oftentimes we need to fetch the biggest prompt, in order to\n    be the most conservative about chunking text. This\n    is a helper utility for that.\n    \"\"\"\n    return max(prompts, key=lambda p: len(get_empty_prompt_txt(p)))",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.prompt_utils",
        "documentation": {}
    },
    {
        "label": "RichPromptTemplate",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.rich",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.rich",
        "peekOfCode": "class RichPromptTemplate(BasePromptTemplate):  # type: ignore[no-redef]\n    template_str: str = Field(description=\"The template string for the prompt.\")\n    def __init__(\n        self,\n        template_str: str,\n        metadata: Optional[Dict[str, Any]] = None,\n        output_parser: Optional[BaseOutputParser] = None,\n        template_vars: Optional[List[str]] = None,\n        template_var_mappings: Optional[Dict[str, Any]] = None,\n        function_mappings: Optional[Dict[str, Callable]] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.rich",
        "documentation": {}
    },
    {
        "label": "SHAKESPEARE_WRITING_ASSISTANT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.system",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.system",
        "peekOfCode": "SHAKESPEARE_WRITING_ASSISTANT = \"\"\"\\\nYou are a Shakespearean writing assistant who speaks in a Shakespearean style. \\\nYou help people come up with creative ideas and content like stories, poems, \\\nand songs that use Shakespearean style of writing style, including words like \\\n\"thou\" and \"hath.\nHere are some example of Shakespeare's style:\n - Romeo, Romeo! Wherefore art thou Romeo?\n - Love looks not with the eyes, but with the mind; and therefore is winged Cupid \\\npainted blind.\n - Shall I compare thee to a summer's day? Thou art more lovely and more temperate.",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.system",
        "documentation": {}
    },
    {
        "label": "IRS_TAX_CHATBOT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.system",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.system",
        "peekOfCode": "IRS_TAX_CHATBOT = \"\"\"\\\n\tYou are an IRS chatbot whose primary goal is to help users with filing their tax \\\nreturns for the 2022 year.\n\tProvide concise replies that are polite and professional.\n\tAnswer questions truthfully based on official government information, with \\\nconsideration to context provided below on changes for 2022 that can affect \\\ntax refund.\n\tDo not answer questions that are not related to United States tax procedures and \\\nrespond with \"I can only help with any tax-related questions you may have.\".\n\tIf you do not know the answer to a question, respond by saying I do not know the \\",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.system",
        "documentation": {}
    },
    {
        "label": "MARKETING_WRITING_ASSISTANT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.system",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.system",
        "peekOfCode": "MARKETING_WRITING_ASSISTANT = \"\"\"\\\nYou are a marketing writing assistant. You help come up with creative content ideas \\\nand content like marketing emails, blog posts, tweets, ad copy and product \\\ndescriptions. You write in a friendly yet professional tone but can tailor \\\nyour writing style that best works for a user-specified audience.\\\nIf you do not know the answer to a question, respond by saying \\\n\"I do not know the answer to your question.\"\n\"\"\"\nXBOX_CUSTOMER_SUPPORT_AGENT = \"\"\"\\\nYou are an Xbox customer support agent whose primary goal is to help users with issues \\",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.system",
        "documentation": {}
    },
    {
        "label": "XBOX_CUSTOMER_SUPPORT_AGENT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.system",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.system",
        "peekOfCode": "XBOX_CUSTOMER_SUPPORT_AGENT = \"\"\"\\\nYou are an Xbox customer support agent whose primary goal is to help users with issues \\\nthey are experiencing with their Xbox devices. You are friendly and concise. \\\nYou only provide factual answers to queries, and do not provide answers \\\nthat are not related to Xbox.\n\"\"\"\nHIKING_RECOMMENDATION_CHATBOT = \"\"\"\\\nI am a hiking enthusiast named Forest who helps people discover fun hikes in their \\\narea. I am upbeat and friendly. I introduce myself when first saying hello. \\\nWhen helping people out, I always ask them for this information to inform the \\",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.system",
        "documentation": {}
    },
    {
        "label": "HIKING_RECOMMENDATION_CHATBOT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.system",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.system",
        "peekOfCode": "HIKING_RECOMMENDATION_CHATBOT = \"\"\"\\\nI am a hiking enthusiast named Forest who helps people discover fun hikes in their \\\narea. I am upbeat and friendly. I introduce myself when first saying hello. \\\nWhen helping people out, I always ask them for this information to inform the \\\nhiking recommendation I provide:\n1.\tWhere they are located\n2.\tWhat hiking intensity they are looking for\nI will then provide three suggestions for nearby hikes that vary in length after I get \\\nthis information. I will also share an interesting fact about the local nature on \\\nthe hikes when making a recommendation.",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.system",
        "documentation": {}
    },
    {
        "label": "JSON_FORMATTER_ASSISTANT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.system",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.system",
        "peekOfCode": "JSON_FORMATTER_ASSISTANT = \"\"\"\\\nAssistant is an AI chatbot that helps users turn a natural language list into JSON \\\nformat. After users input a list they want in JSON format, it will provide \\\nsuggested list of attribute labels if the user has not provided any, \\\nthen ask the user to confirm them before creating the list.\n\"\"\"\nDEFAULT = \"\"\"\\\nYou are an AI assistant that helps people find information.\n\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.system",
        "documentation": {}
    },
    {
        "label": "DEFAULT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.system",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.system",
        "peekOfCode": "DEFAULT = \"\"\"\\\nYou are an AI assistant that helps people find information.\n\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.system",
        "documentation": {}
    },
    {
        "label": "SafeFormatter",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.utils",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.utils",
        "peekOfCode": "class SafeFormatter:\n    \"\"\"Safe string formatter that does not raise KeyError if key is missing.\"\"\"\n    def __init__(self, format_dict: Optional[Dict[str, str]] = None):\n        self.format_dict = format_dict or {}\n    def format(self, format_string: str) -> str:\n        return re.sub(r\"\\{([^{}]+)\\}\", self._replace_match, format_string)\n    def parse(self, format_string: str) -> List[str]:\n        return re.findall(\n            r\"\\{([a-zA-Z_][a-zA-Z0-9_]*(?:\\.[a-zA-Z_][a-zA-Z0-9_]*)*)\\}\", format_string\n        )",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.utils",
        "documentation": {}
    },
    {
        "label": "format_string",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.utils",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.utils",
        "peekOfCode": "def format_string(string_to_format: str, **kwargs: str) -> str:\n    \"\"\"Format a string with kwargs.\"\"\"\n    formatter = SafeFormatter(format_dict=kwargs)\n    return formatter.format(string_to_format)\ndef format_content_blocks(\n    content_blocks: List[ContentBlock], **kwargs: str\n) -> List[ContentBlock]:\n    \"\"\"Format content blocks with kwargs.\"\"\"\n    formatter = SafeFormatter(format_dict=kwargs)\n    formatted_blocks: List[ContentBlock] = []",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.utils",
        "documentation": {}
    },
    {
        "label": "format_content_blocks",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.utils",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.utils",
        "peekOfCode": "def format_content_blocks(\n    content_blocks: List[ContentBlock], **kwargs: str\n) -> List[ContentBlock]:\n    \"\"\"Format content blocks with kwargs.\"\"\"\n    formatter = SafeFormatter(format_dict=kwargs)\n    formatted_blocks: List[ContentBlock] = []\n    for block in content_blocks:\n        if isinstance(block, TextBlock):\n            formatted_blocks.append(TextBlock(text=formatter.format(block.text)))\n        else:",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.utils",
        "documentation": {}
    },
    {
        "label": "get_template_vars",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.utils",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.utils",
        "peekOfCode": "def get_template_vars(template_str: str) -> List[str]:\n    \"\"\"Get template variables from a template string.\"\"\"\n    variables = []\n    formatter = SafeFormatter()\n    for variable_name in formatter.parse(template_str):\n        if variable_name:\n            variables.append(variable_name)\n    return variables\ndef is_chat_model(llm: BaseLLM) -> bool:\n    return llm.metadata.is_chat_model",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.utils",
        "documentation": {}
    },
    {
        "label": "is_chat_model",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.prompts.utils",
        "description": "reference_code.llama-index-core.llama_index.core.prompts.utils",
        "peekOfCode": "def is_chat_model(llm: BaseLLM) -> bool:\n    return llm.metadata.is_chat_model",
        "detail": "reference_code.llama-index-core.llama_index.core.prompts.utils",
        "documentation": {}
    },
    {
        "label": "BaseLookaheadAnswerInserter",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.flare.answer_inserter",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.flare.answer_inserter",
        "peekOfCode": "class BaseLookaheadAnswerInserter(PromptMixin):\n    \"\"\"\n    Lookahead answer inserter.\n    These are responsible for insert answers into a lookahead answer template.\n    E.g.\n    lookahead answer: Red is for [Search(What is the meaning of Ghana's\n        flag being red?)], green for forests, and gold for mineral wealth.\n    query: What is the meaning of Ghana's flag being red?\n    query answer: \"the blood of those who died in the country's struggle\n        for independence\"",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.flare.answer_inserter",
        "documentation": {}
    },
    {
        "label": "LLMLookaheadAnswerInserter",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.flare.answer_inserter",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.flare.answer_inserter",
        "peekOfCode": "class LLMLookaheadAnswerInserter(BaseLookaheadAnswerInserter):\n    \"\"\"\n    LLM Lookahead answer inserter.\n    Takes in a lookahead response and a list of query tasks, and the\n        lookahead answers, and inserts the answers into the lookahead response.\n    \"\"\"\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        answer_insert_prompt: Optional[BasePromptTemplate] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.flare.answer_inserter",
        "documentation": {}
    },
    {
        "label": "DirectLookaheadAnswerInserter",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.flare.answer_inserter",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.flare.answer_inserter",
        "peekOfCode": "class DirectLookaheadAnswerInserter(BaseLookaheadAnswerInserter):\n    \"\"\"\n    Direct lookahead answer inserter.\n    Simple inserter module that directly inserts answers into\n        the [Search(query)] tags in the lookahead response.\n    \"\"\"\n    def _get_prompts(self) -> Dict[str, Any]:\n        \"\"\"Get prompts.\"\"\"\n        return {}\n    def _update_prompts(self, prompts: PromptDictType) -> None:",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.flare.answer_inserter",
        "documentation": {}
    },
    {
        "label": "DEFAULT_ANSWER_INSERT_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.flare.answer_inserter",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.flare.answer_inserter",
        "peekOfCode": "DEFAULT_ANSWER_INSERT_PROMPT_TMPL = \"\"\"\nAn existing 'lookahead response' is given below. The lookahead response\ncontains `[Search(query)]` tags. Some queries have been executed and the\nresponse retrieved. The queries and answers are also given below.\nAlso the previous response (the response before the lookahead response)\nis given below.\nGiven the lookahead template, previous response, and also queries and answers,\nplease 'fill in' the lookahead template with the appropriate answers.\nNOTE: Please make sure that the final response grammatically follows\nthe previous response + lookahead template. For example, if the previous",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.flare.answer_inserter",
        "documentation": {}
    },
    {
        "label": "DEFAULT_ANSWER_INSERT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.flare.answer_inserter",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.flare.answer_inserter",
        "peekOfCode": "DEFAULT_ANSWER_INSERT_PROMPT = PromptTemplate(DEFAULT_ANSWER_INSERT_PROMPT_TMPL)\nclass LLMLookaheadAnswerInserter(BaseLookaheadAnswerInserter):\n    \"\"\"\n    LLM Lookahead answer inserter.\n    Takes in a lookahead response and a list of query tasks, and the\n        lookahead answers, and inserts the answers into the lookahead response.\n    \"\"\"\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.flare.answer_inserter",
        "documentation": {}
    },
    {
        "label": "FLAREInstructQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.flare.base",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.flare.base",
        "peekOfCode": "class FLAREInstructQueryEngine(BaseQueryEngine):\n    \"\"\"\n    FLARE Instruct query engine.\n    This is the version of FLARE that uses retrieval-encouraging instructions.\n    NOTE: this is a beta feature. Interfaces might change, and it might not\n    always give correct answers.\n    Args:\n        query_engine (BaseQueryEngine): query engine to use\n        llm (Optional[LLM]): LLM model. Defaults to None.\n        instruct_prompt (Optional[PromptTemplate]): instruct prompt. Defaults to None.",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.flare.base",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EXAMPLES",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.flare.base",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.flare.base",
        "peekOfCode": "DEFAULT_EXAMPLES = \"\"\"\nQuery: But what are the risks during production of nanomaterials?\nAnswer: [Search(What are some nanomaterial production risks?)]\nQuery: The colors on the flag of Ghana have the following meanings.\nAnswer: Red is for [Search(What is the meaning of Ghana's flag being red?)], \\\n    green for forests, and gold for mineral wealth.\nQuery: What did the author do during his time in college?\nAnswer: The author took classes in [Search(What classes did the author take in \\\n    college?)].\n\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.flare.base",
        "documentation": {}
    },
    {
        "label": "DEFAULT_FIRST_SKILL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.flare.base",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.flare.base",
        "peekOfCode": "DEFAULT_FIRST_SKILL = f\"\"\"\\\nSkill 1. Use the Search API to look up relevant information by writing \\\n    \"[Search(query)]\" where \"query\" is the search query you want to look up. \\\n    For example:\n{DEFAULT_EXAMPLES}\n\"\"\"\nDEFAULT_SECOND_SKILL = \"\"\"\\\nSkill 2. Solve more complex generation tasks by thinking step by step. For example:\nQuery: Give a summary of the author's life and career.\nAnswer: The author was born in 1990. Growing up, he [Search(What did the \\",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.flare.base",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SECOND_SKILL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.flare.base",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.flare.base",
        "peekOfCode": "DEFAULT_SECOND_SKILL = \"\"\"\\\nSkill 2. Solve more complex generation tasks by thinking step by step. For example:\nQuery: Give a summary of the author's life and career.\nAnswer: The author was born in 1990. Growing up, he [Search(What did the \\\n    author do during his childhood?)].\nQuery: Can you write a summary of the Great Gatsby.\nAnswer: The Great Gatsby is a novel written by F. Scott Fitzgerald. It is about \\\n    [Search(What is the Great Gatsby about?)].\n\"\"\"\nDEFAULT_END = \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.flare.base",
        "documentation": {}
    },
    {
        "label": "DEFAULT_END",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.flare.base",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.flare.base",
        "peekOfCode": "DEFAULT_END = \"\"\"\nNow given the following task, and the stub of an existing answer, generate the \\\nnext portion of the answer. You may use the Search API \\\n\"[Search(query)]\" whenever possible.\nIf the answer is complete and no longer contains any \"[Search(query)]\" tags, write \\\n    \"done\" to finish the task.\nDo not write \"done\" if the answer still contains \"[Search(query)]\" tags.\nDo not make up answers. It is better to generate one \"[Search(query)]\" tag and stop \\\ngeneration\nthan to fill in the answer with made up information with no \"[Search(query)]\" tags",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.flare.base",
        "documentation": {}
    },
    {
        "label": "DEFAULT_INSTRUCT_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.flare.base",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.flare.base",
        "peekOfCode": "DEFAULT_INSTRUCT_PROMPT_TMPL = (\n    DEFAULT_FIRST_SKILL\n    + DEFAULT_SECOND_SKILL\n    + DEFAULT_END\n    + (\n        \"\"\"\nQuery: {query_str}\nExisting Answer: {existing_answer}\nAnswer: \"\"\"\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.flare.base",
        "documentation": {}
    },
    {
        "label": "DEFAULT_INSTRUCT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.flare.base",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.flare.base",
        "peekOfCode": "DEFAULT_INSTRUCT_PROMPT = PromptTemplate(DEFAULT_INSTRUCT_PROMPT_TMPL)\nclass FLAREInstructQueryEngine(BaseQueryEngine):\n    \"\"\"\n    FLARE Instruct query engine.\n    This is the version of FLARE that uses retrieval-encouraging instructions.\n    NOTE: this is a beta feature. Interfaces might change, and it might not\n    always give correct answers.\n    Args:\n        query_engine (BaseQueryEngine): query engine to use\n        llm (Optional[LLM]): LLM model. Defaults to None.",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.flare.base",
        "documentation": {}
    },
    {
        "label": "IsDoneOutputParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.flare.output_parser",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.flare.output_parser",
        "peekOfCode": "class IsDoneOutputParser(BaseOutputParser):\n    \"\"\"Is done output parser.\"\"\"\n    def __init__(\n        self,\n        is_done_fn: Optional[Callable[[str], bool]] = None,\n        fmt_answer_fn: Optional[Callable[[str], str]] = None,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self._is_done_fn = is_done_fn or default_parse_is_done_fn\n        self._fmt_answer_fn = fmt_answer_fn or default_format_done_answer",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.flare.output_parser",
        "documentation": {}
    },
    {
        "label": "QueryTaskOutputParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.flare.output_parser",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.flare.output_parser",
        "peekOfCode": "class QueryTaskOutputParser(BaseOutputParser):\n    \"\"\"\n    QueryTask output parser.\n    By default, parses output that contains \"[Search(query)]\" tags.\n    \"\"\"\n    def parse(self, output: str) -> Any:\n        \"\"\"Parse output.\"\"\"\n        query_tasks = []\n        for idx, char in enumerate(output):\n            if char == \"[\":",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.flare.output_parser",
        "documentation": {}
    },
    {
        "label": "default_parse_is_done_fn",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.flare.output_parser",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.flare.output_parser",
        "peekOfCode": "def default_parse_is_done_fn(response: str) -> bool:\n    \"\"\"Default parse is done function.\"\"\"\n    return \"done\" in response.lower()\ndef default_format_done_answer(response: str) -> str:\n    \"\"\"Default format done answer.\"\"\"\n    return response.replace(\"done\", \"\").strip()\nclass IsDoneOutputParser(BaseOutputParser):\n    \"\"\"Is done output parser.\"\"\"\n    def __init__(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.flare.output_parser",
        "documentation": {}
    },
    {
        "label": "default_format_done_answer",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.flare.output_parser",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.flare.output_parser",
        "peekOfCode": "def default_format_done_answer(response: str) -> str:\n    \"\"\"Default format done answer.\"\"\"\n    return response.replace(\"done\", \"\").strip()\nclass IsDoneOutputParser(BaseOutputParser):\n    \"\"\"Is done output parser.\"\"\"\n    def __init__(\n        self,\n        is_done_fn: Optional[Callable[[str], bool]] = None,\n        fmt_answer_fn: Optional[Callable[[str], str]] = None,\n    ) -> None:",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.flare.output_parser",
        "documentation": {}
    },
    {
        "label": "QueryTask",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.flare.schema",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.flare.schema",
        "peekOfCode": "class QueryTask:\n    \"\"\"Query task.\"\"\"\n    query_str: str\n    start_idx: int\n    end_idx: int",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.flare.schema",
        "documentation": {}
    },
    {
        "label": "JSONalyzeQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.jsonalyze.jsonalyze_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.jsonalyze.jsonalyze_query_engine",
        "peekOfCode": "class JSONalyzeQueryEngine:\n    \"\"\"\n    JSONalyze query engine.\n    DEPRECATED: Use `JSONalyzeQueryEngine` from `llama-index-experimental` instead.\n    \"\"\"\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        raise DeprecationWarning(\n            \"JSONalyzeQueryEngine has been moved to `llama-index-experimental`.\\n\"\n            \"`pip install llama-index-experimental`\\n\"\n            \"`from llama_index.experimental.query_engine import JSONalyzeQueryEngine`\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.jsonalyze.jsonalyze_query_engine",
        "documentation": {}
    },
    {
        "label": "PandasInstructionParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.pandas.output_parser",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.pandas.output_parser",
        "peekOfCode": "class PandasInstructionParser:\n    \"\"\"\n    Pandas instruction parser.\n    DEPRECATED: This class has been moved to `llama-index-experimental`.\n    \"\"\"\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        raise DeprecationWarning(\n            \"PandasInstructionParser has been moved to `llama-index-experimental`.\\n\"\n            \"`pip install llama-index-experimental`\\n\"\n            \"`from llama_index.experimental.query_engine.pandas import PandasInstructionParser`\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.pandas.output_parser",
        "documentation": {}
    },
    {
        "label": "PandasQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.pandas.pandas_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.pandas.pandas_query_engine",
        "peekOfCode": "class PandasQueryEngine:\n    \"\"\"\n    Pandas query engine.\n    DEPRECATED: Use `PandasQueryEngine` from `llama-index-experimental` instead.\n    \"\"\"\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        raise DeprecationWarning(\n            \"PandasQueryEngine has been moved to `llama-index-experimental`.\\n\"\n            \"`pip install llama-index-experimental`\\n\"\n            \"`from llama_index.experimental.query_engine import PandasQueryEngine`\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.pandas.pandas_query_engine",
        "documentation": {}
    },
    {
        "label": "NLPandasQueryEngine",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.pandas.pandas_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.pandas.pandas_query_engine",
        "peekOfCode": "NLPandasQueryEngine = PandasQueryEngine\nGPTNLPandasQueryEngine = PandasQueryEngine",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.pandas.pandas_query_engine",
        "documentation": {}
    },
    {
        "label": "GPTNLPandasQueryEngine",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.pandas.pandas_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.pandas.pandas_query_engine",
        "peekOfCode": "GPTNLPandasQueryEngine = PandasQueryEngine",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.pandas.pandas_query_engine",
        "documentation": {}
    },
    {
        "label": "CitationQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.citation_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.citation_query_engine",
        "peekOfCode": "class CitationQueryEngine(BaseQueryEngine):\n    \"\"\"\n    Citation query engine.\n    Args:\n        retriever (BaseRetriever): A retriever object.\n        response_synthesizer (Optional[BaseSynthesizer]):\n            A BaseSynthesizer object.\n        citation_chunk_size (int):\n            Size of citation chunks, default=512. Useful for controlling\n            granularity of sources.",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.citation_query_engine",
        "documentation": {}
    },
    {
        "label": "CITATION_QA_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.citation_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.citation_query_engine",
        "peekOfCode": "CITATION_QA_TEMPLATE = PromptTemplate(\n    \"Please provide an answer based solely on the provided sources. \"\n    \"When referencing information from a source, \"\n    \"cite the appropriate source(s) using their corresponding numbers. \"\n    \"Every answer should include at least one source citation. \"\n    \"Only cite a source when you are explicitly referencing it. \"\n    \"If none of the sources are helpful, you should indicate that. \"\n    \"For example:\\n\"\n    \"Source 1:\\n\"\n    \"The sky is red in the evening and blue in the morning.\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.citation_query_engine",
        "documentation": {}
    },
    {
        "label": "CITATION_REFINE_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.citation_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.citation_query_engine",
        "peekOfCode": "CITATION_REFINE_TEMPLATE = PromptTemplate(\n    \"Please provide an answer based solely on the provided sources. \"\n    \"When referencing information from a source, \"\n    \"cite the appropriate source(s) using their corresponding numbers. \"\n    \"Every answer should include at least one source citation. \"\n    \"Only cite a source when you are explicitly referencing it. \"\n    \"If none of the sources are helpful, you should indicate that. \"\n    \"For example:\\n\"\n    \"Source 1:\\n\"\n    \"The sky is red in the evening and blue in the morning.\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.citation_query_engine",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CITATION_CHUNK_SIZE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.citation_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.citation_query_engine",
        "peekOfCode": "DEFAULT_CITATION_CHUNK_SIZE = 512\nDEFAULT_CITATION_CHUNK_OVERLAP = 20\nclass CitationQueryEngine(BaseQueryEngine):\n    \"\"\"\n    Citation query engine.\n    Args:\n        retriever (BaseRetriever): A retriever object.\n        response_synthesizer (Optional[BaseSynthesizer]):\n            A BaseSynthesizer object.\n        citation_chunk_size (int):",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.citation_query_engine",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CITATION_CHUNK_OVERLAP",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.citation_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.citation_query_engine",
        "peekOfCode": "DEFAULT_CITATION_CHUNK_OVERLAP = 20\nclass CitationQueryEngine(BaseQueryEngine):\n    \"\"\"\n    Citation query engine.\n    Args:\n        retriever (BaseRetriever): A retriever object.\n        response_synthesizer (Optional[BaseSynthesizer]):\n            A BaseSynthesizer object.\n        citation_chunk_size (int):\n            Size of citation chunks, default=512. Useful for controlling",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.citation_query_engine",
        "documentation": {}
    },
    {
        "label": "CogniswitchQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.cogniswitch_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.cogniswitch_query_engine",
        "peekOfCode": "class CogniswitchQueryEngine(BaseQueryEngine):\n    def __init__(self, cs_token: str, OAI_token: str, apiKey: str) -> None:\n        \"\"\"\n        The required fields.\n        Args:\n            cs_token (str): Cogniswitch token.\n            OAI_token (str): OpenAI token.\n            apiKey (str): Oauth token.\n        \"\"\"\n        self.cs_token = cs_token",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.cogniswitch_query_engine",
        "documentation": {}
    },
    {
        "label": "CustomQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.custom",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.custom",
        "peekOfCode": "class CustomQueryEngine(BaseModel, BaseQueryEngine):\n    \"\"\"\n    Custom query engine.\n    Subclasses can define additional attributes as Pydantic fields.\n    Subclasses must implement the `custom_query` method, which takes a query string\n    and returns either a Response object or a string as output.\n    They can optionally implement the `acustom_query` method for async support.\n    \"\"\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    callback_manager: CallbackManager = Field(",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.custom",
        "documentation": {}
    },
    {
        "label": "STR_OR_RESPONSE_TYPE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.custom",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.custom",
        "peekOfCode": "STR_OR_RESPONSE_TYPE = Union[RESPONSE_TYPE, str]\nclass CustomQueryEngine(BaseModel, BaseQueryEngine):\n    \"\"\"\n    Custom query engine.\n    Subclasses can define additional attributes as Pydantic fields.\n    Subclasses must implement the `custom_query` method, which takes a query string\n    and returns either a Response object or a string as output.\n    They can optionally implement the `acustom_query` method for async support.\n    \"\"\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.custom",
        "documentation": {}
    },
    {
        "label": "ComposableGraphQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.graph_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.graph_query_engine",
        "peekOfCode": "class ComposableGraphQueryEngine(BaseQueryEngine):\n    \"\"\"\n    Composable graph query engine.\n    This query engine can operate over a ComposableGraph.\n    It can take in custom query engines for its sub-indices.\n    Args:\n        graph (ComposableGraph): A ComposableGraph object.\n        custom_query_engines (Optional[Dict[str, BaseQueryEngine]]): A dictionary of\n            custom query engines.\n        recursive (bool): Whether to recursively query the graph.",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.graph_query_engine",
        "documentation": {}
    },
    {
        "label": "dispatcher",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.graph_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.graph_query_engine",
        "peekOfCode": "dispatcher = instrument.get_dispatcher(__name__)\nclass ComposableGraphQueryEngine(BaseQueryEngine):\n    \"\"\"\n    Composable graph query engine.\n    This query engine can operate over a ComposableGraph.\n    It can take in custom query engines for its sub-indices.\n    Args:\n        graph (ComposableGraph): A ComposableGraph object.\n        custom_query_engines (Optional[Dict[str, BaseQueryEngine]]): A dictionary of\n            custom query engines.",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.graph_query_engine",
        "documentation": {}
    },
    {
        "label": "KnowledgeGraphQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.knowledge_graph_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.knowledge_graph_query_engine",
        "peekOfCode": "class KnowledgeGraphQueryEngine(BaseQueryEngine):\n    \"\"\"\n    Knowledge graph query engine.\n    Query engine to call a knowledge graph.\n    Args:\n        storage_context (Optional[StorageContext]): A storage context to use.\n        refresh_schema (bool): Whether to refresh the schema.\n        verbose (bool): Whether to print intermediate results.\n        response_synthesizer (Optional[BaseSynthesizer]):\n            A BaseSynthesizer object.",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.knowledge_graph_query_engine",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.knowledge_graph_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.knowledge_graph_query_engine",
        "peekOfCode": "logger = logging.getLogger(__name__)\nDEFAULT_KG_RESPONSE_ANSWER_PROMPT_TMPL = \"\"\"\nThe original question is given below.\nThis question has been translated into a Graph Database query.\nBoth the Graph query and the response are given below.\nGiven the Graph Query response, synthesise a response to the original question.\nOriginal question: {query_str}\nGraph query: {kg_query_str}\nGraph response: {kg_response_str}\nResponse:",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.knowledge_graph_query_engine",
        "documentation": {}
    },
    {
        "label": "DEFAULT_KG_RESPONSE_ANSWER_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.knowledge_graph_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.knowledge_graph_query_engine",
        "peekOfCode": "DEFAULT_KG_RESPONSE_ANSWER_PROMPT_TMPL = \"\"\"\nThe original question is given below.\nThis question has been translated into a Graph Database query.\nBoth the Graph query and the response are given below.\nGiven the Graph Query response, synthesise a response to the original question.\nOriginal question: {query_str}\nGraph query: {kg_query_str}\nGraph response: {kg_response_str}\nResponse:\n\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.knowledge_graph_query_engine",
        "documentation": {}
    },
    {
        "label": "DEFAULT_KG_RESPONSE_ANSWER_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.knowledge_graph_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.knowledge_graph_query_engine",
        "peekOfCode": "DEFAULT_KG_RESPONSE_ANSWER_PROMPT = PromptTemplate(\n    DEFAULT_KG_RESPONSE_ANSWER_PROMPT_TMPL,\n    prompt_type=PromptType.QUESTION_ANSWER,\n)\n@deprecated.deprecated(\n    version=\"0.10.53\",\n    reason=(\n        \"KnowledgeGraphQueryEngine is deprecated. It is recommended to use \"\n        \"the PropertyGraphIndex and associated retrievers instead.\"\n    ),",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.knowledge_graph_query_engine",
        "documentation": {}
    },
    {
        "label": "MultiStepQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.multistep_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.multistep_query_engine",
        "peekOfCode": "class MultiStepQueryEngine(BaseQueryEngine):\n    \"\"\"\n    Multi-step query engine.\n    This query engine can operate over an existing base query engine,\n    along with the multi-step query transform.\n    Args:\n        query_engine (BaseQueryEngine): A BaseQueryEngine object.\n        query_transform (StepDecomposeQueryTransform): A StepDecomposeQueryTransform\n            object.\n        response_synthesizer (Optional[BaseSynthesizer]): A BaseSynthesizer",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.multistep_query_engine",
        "documentation": {}
    },
    {
        "label": "default_stop_fn",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.multistep_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.multistep_query_engine",
        "peekOfCode": "def default_stop_fn(stop_dict: Dict) -> bool:\n    \"\"\"Stop function for multi-step query combiner.\"\"\"\n    query_bundle = cast(QueryBundle, stop_dict.get(\"query_bundle\"))\n    if query_bundle is None:\n        raise ValueError(\"Response must be provided to stop function.\")\n    return \"none\" in query_bundle.query_str.lower()\nclass MultiStepQueryEngine(BaseQueryEngine):\n    \"\"\"\n    Multi-step query engine.\n    This query engine can operate over an existing base query engine,",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.multistep_query_engine",
        "documentation": {}
    },
    {
        "label": "SimpleMultiModalQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.multi_modal",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.multi_modal",
        "peekOfCode": "class SimpleMultiModalQueryEngine(BaseQueryEngine):\n    \"\"\"\n    Simple Multi Modal Retriever query engine.\n    Assumes that retrieved text context fits within context window of LLM, along with images.\n    Args:\n        retriever (MultiModalVectorIndexRetriever): A retriever object.\n        multi_modal_llm (Optional[LLM]): An LLM model.\n        text_qa_template (Optional[BasePromptTemplate]): Text QA Prompt Template.\n        image_qa_template (Optional[BasePromptTemplate]): Image QA Prompt Template.\n        node_postprocessors (Optional[List[BaseNodePostprocessor]]): Node Postprocessors.",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.multi_modal",
        "documentation": {}
    },
    {
        "label": "RetrieverQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.retriever_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.retriever_query_engine",
        "peekOfCode": "class RetrieverQueryEngine(BaseQueryEngine):\n    \"\"\"\n    Retriever query engine.\n    Args:\n        retriever (BaseRetriever): A retriever object.\n        response_synthesizer (Optional[BaseSynthesizer]): A BaseSynthesizer\n            object.\n        callback_manager (Optional[CallbackManager]): A callback manager.\n    \"\"\"\n    def __init__(",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.retriever_query_engine",
        "documentation": {}
    },
    {
        "label": "dispatcher",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.retriever_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.retriever_query_engine",
        "peekOfCode": "dispatcher = instrument.get_dispatcher(__name__)\nclass RetrieverQueryEngine(BaseQueryEngine):\n    \"\"\"\n    Retriever query engine.\n    Args:\n        retriever (BaseRetriever): A retriever object.\n        response_synthesizer (Optional[BaseSynthesizer]): A BaseSynthesizer\n            object.\n        callback_manager (Optional[CallbackManager]): A callback manager.\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.retriever_query_engine",
        "documentation": {}
    },
    {
        "label": "RetryQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.retry_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.retry_query_engine",
        "peekOfCode": "class RetryQueryEngine(BaseQueryEngine):\n    \"\"\"\n    Does retry on query engine if it fails evaluation.\n    Args:\n        query_engine (BaseQueryEngine): A query engine object\n        evaluator (BaseEvaluator): An evaluator object\n        max_retries (int): Maximum number of retries\n        callback_manager (Optional[CallbackManager]): A callback manager object\n    \"\"\"\n    def __init__(",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.retry_query_engine",
        "documentation": {}
    },
    {
        "label": "RetryGuidelineQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.retry_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.retry_query_engine",
        "peekOfCode": "class RetryGuidelineQueryEngine(BaseQueryEngine):\n    \"\"\"\n    Does retry with evaluator feedback\n    if query engine fails evaluation.\n    Args:\n        query_engine (BaseQueryEngine): A query engine object\n        guideline_evaluator (GuidelineEvaluator): A guideline evaluator object\n        resynthesize_query (bool): Whether to resynthesize query\n        max_retries (int): Maximum number of retries\n        callback_manager (Optional[CallbackManager]): A callback manager object",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.retry_query_engine",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.retry_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.retry_query_engine",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass RetryQueryEngine(BaseQueryEngine):\n    \"\"\"\n    Does retry on query engine if it fails evaluation.\n    Args:\n        query_engine (BaseQueryEngine): A query engine object\n        evaluator (BaseEvaluator): An evaluator object\n        max_retries (int): Maximum number of retries\n        callback_manager (Optional[CallbackManager]): A callback manager object\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.retry_query_engine",
        "documentation": {}
    },
    {
        "label": "RetrySourceQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.retry_source_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.retry_source_query_engine",
        "peekOfCode": "class RetrySourceQueryEngine(BaseQueryEngine):\n    \"\"\"Retry with different source nodes.\"\"\"\n    def __init__(\n        self,\n        query_engine: RetrieverQueryEngine,\n        evaluator: BaseEvaluator,\n        llm: Optional[LLM] = None,\n        max_retries: int = 3,\n        callback_manager: Optional[CallbackManager] = None,\n    ) -> None:",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.retry_source_query_engine",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.retry_source_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.retry_source_query_engine",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass RetrySourceQueryEngine(BaseQueryEngine):\n    \"\"\"Retry with different source nodes.\"\"\"\n    def __init__(\n        self,\n        query_engine: RetrieverQueryEngine,\n        evaluator: BaseEvaluator,\n        llm: Optional[LLM] = None,\n        max_retries: int = 3,\n        callback_manager: Optional[CallbackManager] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.retry_source_query_engine",
        "documentation": {}
    },
    {
        "label": "RouterQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.router_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.router_query_engine",
        "peekOfCode": "class RouterQueryEngine(BaseQueryEngine):\n    \"\"\"\n    Router query engine.\n    Selects one out of several candidate query engines to execute a query.\n    Args:\n        selector (BaseSelector): A selector that chooses one out of many options based\n            on each candidate's metadata and query.\n        query_engine_tools (Sequence[QueryEngineTool]): A sequence of candidate\n            query engines. They must be wrapped as tools to expose metadata to\n            the selector.",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.router_query_engine",
        "documentation": {}
    },
    {
        "label": "RetrieverRouterQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.router_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.router_query_engine",
        "peekOfCode": "class RetrieverRouterQueryEngine(BaseQueryEngine):\n    \"\"\"\n    Retriever-based router query engine.\n    NOTE: this is deprecated, please use our new ToolRetrieverRouterQueryEngine\n    Use a retriever to select a set of Nodes. Each node will be converted\n    into a ToolMetadata object, and also used to retrieve a query engine, to form\n    a QueryEngineTool.\n    NOTE: this is a beta feature. We are figuring out the right interface\n    between the retriever and query engine.\n    Args:",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.router_query_engine",
        "documentation": {}
    },
    {
        "label": "ToolRetrieverRouterQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.router_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.router_query_engine",
        "peekOfCode": "class ToolRetrieverRouterQueryEngine(BaseQueryEngine):\n    \"\"\"\n    Tool Retriever router query engine.\n    Selects a set of candidate query engines to execute a query.\n    Args:\n        retriever (ObjectRetriever): A retriever that retrieves a set of\n            query engine tools.\n        summarizer (Optional[TreeSummarize]): Tree summarizer to summarize sub-results.\n    \"\"\"\n    def __init__(",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.router_query_engine",
        "documentation": {}
    },
    {
        "label": "combine_responses",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.router_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.router_query_engine",
        "peekOfCode": "def combine_responses(\n    summarizer: TreeSummarize, responses: List[RESPONSE_TYPE], query_bundle: QueryBundle\n) -> RESPONSE_TYPE:\n    \"\"\"Combine multiple response from sub-engines.\"\"\"\n    logger.info(\"Combining responses from multiple query engines.\")\n    response_strs = []\n    source_nodes = []\n    for response in responses:\n        if isinstance(response, (StreamingResponse, PydanticResponse)):\n            response_obj = response.get_response()",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.router_query_engine",
        "documentation": {}
    },
    {
        "label": "default_node_to_metadata_fn",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.router_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.router_query_engine",
        "peekOfCode": "def default_node_to_metadata_fn(node: BaseNode) -> ToolMetadata:\n    \"\"\"\n    Default node to metadata function.\n    We use the node's text as the Tool description.\n    \"\"\"\n    metadata = node.metadata or {}\n    if \"tool_name\" not in metadata:\n        raise ValueError(\"Node must have a tool_name in metadata.\")\n    return ToolMetadata(name=metadata[\"tool_name\"], description=node.get_content())\nclass RetrieverRouterQueryEngine(BaseQueryEngine):",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.router_query_engine",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.router_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.router_query_engine",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef combine_responses(\n    summarizer: TreeSummarize, responses: List[RESPONSE_TYPE], query_bundle: QueryBundle\n) -> RESPONSE_TYPE:\n    \"\"\"Combine multiple response from sub-engines.\"\"\"\n    logger.info(\"Combining responses from multiple query engines.\")\n    response_strs = []\n    source_nodes = []\n    for response in responses:\n        if isinstance(response, (StreamingResponse, PydanticResponse)):",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.router_query_engine",
        "documentation": {}
    },
    {
        "label": "SQLAugmentQueryTransform",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.sql_join_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.sql_join_query_engine",
        "peekOfCode": "class SQLAugmentQueryTransform(BaseQueryTransform):\n    \"\"\"\n    SQL Augment Query Transform.\n    This query transform will transform the query into a more specific query\n    after augmenting with SQL results.\n    Args:\n        llm (LLM): LLM to use for query transformation.\n        sql_augment_transform_prompt (BasePromptTemplate): PromptTemplate to use\n            for query transformation.\n        check_stop_parser (Optional[Callable[[str], bool]]): Check stop function.",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.sql_join_query_engine",
        "documentation": {}
    },
    {
        "label": "SQLJoinQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.sql_join_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.sql_join_query_engine",
        "peekOfCode": "class SQLJoinQueryEngine(BaseQueryEngine):\n    \"\"\"\n    SQL Join Query Engine.\n    This query engine can \"Join\" a SQL database results\n    with another query engine.\n    It can decide it needs to query the SQL database or the other query engine.\n    If it decides to query the SQL database, it will first query the SQL database,\n    whether to augment information with retrieved results from the other query engine.\n    Args:\n        sql_query_tool (QueryEngineTool): Query engine tool for SQL database.",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.sql_join_query_engine",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.sql_join_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.sql_join_query_engine",
        "peekOfCode": "logger = logging.getLogger(__name__)\nDEFAULT_SQL_JOIN_SYNTHESIS_PROMPT_TMPL = \"\"\"\nThe original question is given below.\nThis question has been translated into a SQL query. Both the SQL query and \\\nthe response are given below.\nGiven the SQL response, the question has also been transformed into a more \\\ndetailed query,\nand executed against another query engine.\nThe transformed query and query engine response are also given below.\nGiven SQL query, SQL response, transformed query, and query engine response, \\",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.sql_join_query_engine",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SQL_JOIN_SYNTHESIS_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.sql_join_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.sql_join_query_engine",
        "peekOfCode": "DEFAULT_SQL_JOIN_SYNTHESIS_PROMPT_TMPL = \"\"\"\nThe original question is given below.\nThis question has been translated into a SQL query. Both the SQL query and \\\nthe response are given below.\nGiven the SQL response, the question has also been transformed into a more \\\ndetailed query,\nand executed against another query engine.\nThe transformed query and query engine response are also given below.\nGiven SQL query, SQL response, transformed query, and query engine response, \\\nplease synthesize a response to the original question.",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.sql_join_query_engine",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SQL_JOIN_SYNTHESIS_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.sql_join_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.sql_join_query_engine",
        "peekOfCode": "DEFAULT_SQL_JOIN_SYNTHESIS_PROMPT = PromptTemplate(\n    DEFAULT_SQL_JOIN_SYNTHESIS_PROMPT_TMPL\n)\nDEFAULT_SQL_AUGMENT_TRANSFORM_PROMPT_TMPL = \"\"\"\n\"The original question is given below.\nThis question has been translated into a SQL query. Both the SQL query and the \\\nresponse are given below.\nThe SQL response either answers the question, or should provide additional context \\\nthat can be used to make the question more specific.\nYour job is to come up with a more specific question that needs to be answered to \\",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.sql_join_query_engine",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SQL_AUGMENT_TRANSFORM_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.sql_join_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.sql_join_query_engine",
        "peekOfCode": "DEFAULT_SQL_AUGMENT_TRANSFORM_PROMPT_TMPL = \"\"\"\n\"The original question is given below.\nThis question has been translated into a SQL query. Both the SQL query and the \\\nresponse are given below.\nThe SQL response either answers the question, or should provide additional context \\\nthat can be used to make the question more specific.\nYour job is to come up with a more specific question that needs to be answered to \\\nfully answer the original question, or 'None' if the original question has already \\\nbeen fully answered from the SQL response. Do not create a new question that is \\\nirrelevant to the original question; in that case return None instead.",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.sql_join_query_engine",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SQL_AUGMENT_TRANSFORM_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.sql_join_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.sql_join_query_engine",
        "peekOfCode": "DEFAULT_SQL_AUGMENT_TRANSFORM_PROMPT = PromptTemplate(\n    DEFAULT_SQL_AUGMENT_TRANSFORM_PROMPT_TMPL\n)\ndef _default_check_stop(query_bundle: QueryBundle) -> bool:\n    \"\"\"Default check stop function.\"\"\"\n    return query_bundle.query_str.lower() == \"none\"\ndef _format_sql_query(sql_query: str) -> str:\n    \"\"\"Format SQL query.\"\"\"\n    return sql_query.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\nclass SQLAugmentQueryTransform(BaseQueryTransform):",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.sql_join_query_engine",
        "documentation": {}
    },
    {
        "label": "SQLAutoVectorQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.sql_vector_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.sql_vector_query_engine",
        "peekOfCode": "class SQLAutoVectorQueryEngine(SQLJoinQueryEngine):\n    \"\"\"\n    SQL + Vector Index Auto Retriever Query Engine.\n    This query engine can query both a SQL database\n    as well as a vector database. It will first decide\n    whether it needs to query the SQL database or vector store.\n    If it decides to query the SQL database, it will also decide\n    whether to augment information with retrieved results from the vector store.\n    We use the VectorIndexAutoRetriever to retrieve results.\n    Args:",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.sql_vector_query_engine",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.sql_vector_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.sql_vector_query_engine",
        "peekOfCode": "logger = logging.getLogger(__name__)\nDEFAULT_SQL_VECTOR_SYNTHESIS_PROMPT_TMPL = \"\"\"\nThe original question is given below.\nThis question has been translated into a SQL query. \\\nBoth the SQL query and the response are given below.\nGiven the SQL response, the question has also been translated into a vector store query.\nThe vector store query and response is given below.\nGiven SQL query, SQL response, transformed vector store query, and vector store \\\nresponse, please synthesize a response to the original question.\nOriginal question: {query_str}",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.sql_vector_query_engine",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SQL_VECTOR_SYNTHESIS_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.sql_vector_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.sql_vector_query_engine",
        "peekOfCode": "DEFAULT_SQL_VECTOR_SYNTHESIS_PROMPT_TMPL = \"\"\"\nThe original question is given below.\nThis question has been translated into a SQL query. \\\nBoth the SQL query and the response are given below.\nGiven the SQL response, the question has also been translated into a vector store query.\nThe vector store query and response is given below.\nGiven SQL query, SQL response, transformed vector store query, and vector store \\\nresponse, please synthesize a response to the original question.\nOriginal question: {query_str}\nSQL query: {sql_query_str}",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.sql_vector_query_engine",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SQL_VECTOR_SYNTHESIS_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.sql_vector_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.sql_vector_query_engine",
        "peekOfCode": "DEFAULT_SQL_VECTOR_SYNTHESIS_PROMPT = PromptTemplate(\n    DEFAULT_SQL_VECTOR_SYNTHESIS_PROMPT_TMPL\n)\n# NOTE: maintain for backwards compatibility\nclass SQLAutoVectorQueryEngine(SQLJoinQueryEngine):\n    \"\"\"\n    SQL + Vector Index Auto Retriever Query Engine.\n    This query engine can query both a SQL database\n    as well as a vector database. It will first decide\n    whether it needs to query the SQL database or vector store.",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.sql_vector_query_engine",
        "documentation": {}
    },
    {
        "label": "SubQuestionAnswerPair",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.sub_question_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.sub_question_query_engine",
        "peekOfCode": "class SubQuestionAnswerPair(BaseModel):\n    \"\"\"\n    Pair of the sub question and optionally its answer (if its been answered yet).\n    \"\"\"\n    sub_q: SubQuestion\n    answer: Optional[str] = None\n    sources: List[NodeWithScore] = Field(default_factory=list)\nclass SubQuestionQueryEngine(BaseQueryEngine):\n    \"\"\"\n    Sub question query engine.",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.sub_question_query_engine",
        "documentation": {}
    },
    {
        "label": "SubQuestionQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.sub_question_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.sub_question_query_engine",
        "peekOfCode": "class SubQuestionQueryEngine(BaseQueryEngine):\n    \"\"\"\n    Sub question query engine.\n    A query engine that breaks down a complex query (e.g. compare and contrast) into\n        many sub questions and their target query engine for execution.\n        After executing all sub questions, all responses are gathered and sent to\n        response synthesizer to produce the final response.\n    Args:\n        question_gen (BaseQuestionGenerator): A module for generating sub questions\n            given a complex question and tools.",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.sub_question_query_engine",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.sub_question_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.sub_question_query_engine",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass SubQuestionAnswerPair(BaseModel):\n    \"\"\"\n    Pair of the sub question and optionally its answer (if its been answered yet).\n    \"\"\"\n    sub_q: SubQuestion\n    answer: Optional[str] = None\n    sources: List[NodeWithScore] = Field(default_factory=list)\nclass SubQuestionQueryEngine(BaseQueryEngine):\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.sub_question_query_engine",
        "documentation": {}
    },
    {
        "label": "TransformQueryEngine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.query_engine.transform_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.query_engine.transform_query_engine",
        "peekOfCode": "class TransformQueryEngine(BaseQueryEngine):\n    \"\"\"\n    Transform query engine.\n    Applies a query transform to a query bundle before passing\n        it to a query engine.\n    Args:\n        query_engine (BaseQueryEngine): A query engine object.\n        query_transform (BaseQueryTransform): A query transform object.\n        transform_metadata (Optional[dict]): metadata to pass to the\n            query transform.",
        "detail": "reference_code.llama-index-core.llama_index.core.query_engine.transform_query_engine",
        "documentation": {}
    },
    {
        "label": "LLMQuestionGenerator",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.question_gen.llm_generators",
        "description": "reference_code.llama-index-core.llama_index.core.question_gen.llm_generators",
        "peekOfCode": "class LLMQuestionGenerator(BaseQuestionGenerator):\n    def __init__(\n        self,\n        llm: LLM,\n        prompt: BasePromptTemplate,\n    ) -> None:\n        self._llm = llm\n        self._prompt = prompt\n        if self._prompt.output_parser is None:\n            raise ValueError(\"Prompt should have output parser.\")",
        "detail": "reference_code.llama-index-core.llama_index.core.question_gen.llm_generators",
        "documentation": {}
    },
    {
        "label": "SubQuestionOutputParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.question_gen.output_parser",
        "description": "reference_code.llama-index-core.llama_index.core.question_gen.output_parser",
        "peekOfCode": "class SubQuestionOutputParser(BaseOutputParser):\n    def parse(self, output: str) -> Any:\n        json_dict = parse_json_markdown(output)\n        if not json_dict:\n            raise ValueError(f\"No valid JSON found in output: {output}\")\n        # example code includes an 'items' key, which breaks\n        # the parsing from open-source LLMs such as Zephyr.\n        # This gets the actual subquestions and recommended tools directly\n        if \"items\" in json_dict:\n            json_dict = json_dict[\"items\"]",
        "detail": "reference_code.llama-index-core.llama_index.core.question_gen.output_parser",
        "documentation": {}
    },
    {
        "label": "build_tools_text",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "peekOfCode": "def build_tools_text(tools: Sequence[ToolMetadata]) -> str:\n    tools_dict = {}\n    for tool in tools:\n        tools_dict[tool.name] = tool.description\n    return json.dumps(tools_dict, indent=4)\nPREFIX = \"\"\"\\\nGiven a user question, and a list of tools, output a list of relevant sub-questions \\\nin json markdown that when composed can help answer the full user question:\n\"\"\"\nexample_query_str = (",
        "detail": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "documentation": {}
    },
    {
        "label": "SubQuestionPrompt",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "peekOfCode": "SubQuestionPrompt = PromptTemplate\ndef build_tools_text(tools: Sequence[ToolMetadata]) -> str:\n    tools_dict = {}\n    for tool in tools:\n        tools_dict[tool.name] = tool.description\n    return json.dumps(tools_dict, indent=4)\nPREFIX = \"\"\"\\\nGiven a user question, and a list of tools, output a list of relevant sub-questions \\\nin json markdown that when composed can help answer the full user question:\n\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "documentation": {}
    },
    {
        "label": "PREFIX",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "peekOfCode": "PREFIX = \"\"\"\\\nGiven a user question, and a list of tools, output a list of relevant sub-questions \\\nin json markdown that when composed can help answer the full user question:\n\"\"\"\nexample_query_str = (\n    \"Compare and contrast the revenue growth and EBITDA of Uber and Lyft for year 2021\"\n)\nexample_tools = [\n    ToolMetadata(\n        name=\"uber_10k\",",
        "detail": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "documentation": {}
    },
    {
        "label": "example_query_str",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "peekOfCode": "example_query_str = (\n    \"Compare and contrast the revenue growth and EBITDA of Uber and Lyft for year 2021\"\n)\nexample_tools = [\n    ToolMetadata(\n        name=\"uber_10k\",\n        description=\"Provides information about Uber financials for year 2021\",\n    ),\n    ToolMetadata(\n        name=\"lyft_10k\",",
        "detail": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "documentation": {}
    },
    {
        "label": "example_tools",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "peekOfCode": "example_tools = [\n    ToolMetadata(\n        name=\"uber_10k\",\n        description=\"Provides information about Uber financials for year 2021\",\n    ),\n    ToolMetadata(\n        name=\"lyft_10k\",\n        description=\"Provides information about Lyft financials for year 2021\",\n    ),\n]",
        "detail": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "documentation": {}
    },
    {
        "label": "example_tools_str",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "peekOfCode": "example_tools_str = build_tools_text(example_tools)\nexample_output = [\n    SubQuestion(\n        sub_question=\"What is the revenue growth of Uber\", tool_name=\"uber_10k\"\n    ),\n    SubQuestion(sub_question=\"What is the EBITDA of Uber\", tool_name=\"uber_10k\"),\n    SubQuestion(\n        sub_question=\"What is the revenue growth of Lyft\", tool_name=\"lyft_10k\"\n    ),\n    SubQuestion(sub_question=\"What is the EBITDA of Lyft\", tool_name=\"lyft_10k\"),",
        "detail": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "documentation": {}
    },
    {
        "label": "example_output",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "peekOfCode": "example_output = [\n    SubQuestion(\n        sub_question=\"What is the revenue growth of Uber\", tool_name=\"uber_10k\"\n    ),\n    SubQuestion(sub_question=\"What is the EBITDA of Uber\", tool_name=\"uber_10k\"),\n    SubQuestion(\n        sub_question=\"What is the revenue growth of Lyft\", tool_name=\"lyft_10k\"\n    ),\n    SubQuestion(sub_question=\"What is the EBITDA of Lyft\", tool_name=\"lyft_10k\"),\n]",
        "detail": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "documentation": {}
    },
    {
        "label": "example_output_str",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "peekOfCode": "example_output_str = json.dumps(\n    {\"items\": [x.model_dump() for x in example_output]}, indent=4\n)\nEXAMPLES = f\"\"\"\\\n# Example 1\n<Tools>\n```json\n{example_tools_str}\n```\n<User Question>",
        "detail": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "documentation": {}
    },
    {
        "label": "EXAMPLES",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "peekOfCode": "EXAMPLES = f\"\"\"\\\n# Example 1\n<Tools>\n```json\n{example_tools_str}\n```\n<User Question>\n{example_query_str}\n<Output>\n```json",
        "detail": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "documentation": {}
    },
    {
        "label": "SUFFIX",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "peekOfCode": "SUFFIX = \"\"\"\\\n# Example 2\n<Tools>\n```json\n{tools_str}\n```\n<User Question>\n{query_str}\n<Output>\n\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SUB_QUESTION_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "peekOfCode": "DEFAULT_SUB_QUESTION_PROMPT_TMPL = PREFIX + EXAMPLES + SUFFIX",
        "detail": "reference_code.llama-index-core.llama_index.core.question_gen.prompts",
        "documentation": {}
    },
    {
        "label": "SubQuestion",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.question_gen.types",
        "description": "reference_code.llama-index-core.llama_index.core.question_gen.types",
        "peekOfCode": "class SubQuestion(BaseModel):\n    sub_question: str\n    tool_name: str\nclass SubQuestionList(BaseModel):\n    \"\"\"\n    A pydantic object wrapping a list of sub-questions.\n    This is mostly used to make getting a json schema easier.\n    \"\"\"\n    items: List[SubQuestion]\nclass BaseQuestionGenerator(PromptMixin, DispatcherSpanMixin):",
        "detail": "reference_code.llama-index-core.llama_index.core.question_gen.types",
        "documentation": {}
    },
    {
        "label": "SubQuestionList",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.question_gen.types",
        "description": "reference_code.llama-index-core.llama_index.core.question_gen.types",
        "peekOfCode": "class SubQuestionList(BaseModel):\n    \"\"\"\n    A pydantic object wrapping a list of sub-questions.\n    This is mostly used to make getting a json schema easier.\n    \"\"\"\n    items: List[SubQuestion]\nclass BaseQuestionGenerator(PromptMixin, DispatcherSpanMixin):\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt modules.\"\"\"\n        return {}",
        "detail": "reference_code.llama-index-core.llama_index.core.question_gen.types",
        "documentation": {}
    },
    {
        "label": "BaseQuestionGenerator",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.question_gen.types",
        "description": "reference_code.llama-index-core.llama_index.core.question_gen.types",
        "peekOfCode": "class BaseQuestionGenerator(PromptMixin, DispatcherSpanMixin):\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt modules.\"\"\"\n        return {}\n    @abstractmethod\n    def generate(\n        self, tools: Sequence[ToolMetadata], query: QueryBundle\n    ) -> List[SubQuestion]:\n        pass\n    @abstractmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.question_gen.types",
        "documentation": {}
    },
    {
        "label": "FileSystemReaderMixin",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.readers.file.base",
        "description": "reference_code.llama-index-core.llama_index.core.readers.file.base",
        "peekOfCode": "class FileSystemReaderMixin(ABC):\n    @abstractmethod\n    def read_file_content(self, input_file: Path, **kwargs: Any) -> bytes:\n        \"\"\"\n        Read the bytes content of a file.\n        Args:\n            input_file (Path): Path to the file.\n        Returns:\n            bytes: File content.\n        \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.readers.file.base",
        "documentation": {}
    },
    {
        "label": "_DefaultFileMetadataFunc",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.readers.file.base",
        "description": "reference_code.llama-index-core.llama_index.core.readers.file.base",
        "peekOfCode": "class _DefaultFileMetadataFunc:\n    \"\"\"\n    Default file metadata function wrapper which stores the fs.\n    Allows for pickling of the function.\n    \"\"\"\n    def __init__(self, fs: fsspec.AbstractFileSystem | None = None):\n        self.fs = fs or get_default_fs()\n    def __call__(self, file_path: str) -> dict:\n        return default_file_metadata_func(file_path, self.fs)\ndef get_default_fs() -> fsspec.AbstractFileSystem:",
        "detail": "reference_code.llama-index-core.llama_index.core.readers.file.base",
        "documentation": {}
    },
    {
        "label": "SimpleDirectoryReader",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.readers.file.base",
        "description": "reference_code.llama-index-core.llama_index.core.readers.file.base",
        "peekOfCode": "class SimpleDirectoryReader(BaseReader, ResourcesReaderMixin, FileSystemReaderMixin):\n    \"\"\"\n    Simple directory reader.\n    Load files from file directory.\n    Automatically select the best file reader given file extensions.\n    Args:\n        input_dir (Union[Path, str]): Path to the directory.\n        input_files (List): List of file paths to read\n            (Optional; overrides input_dir, exclude)\n        exclude (List): glob of python file paths to exclude (Optional)",
        "detail": "reference_code.llama-index-core.llama_index.core.readers.file.base",
        "documentation": {}
    },
    {
        "label": "default_file_metadata_func",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.readers.file.base",
        "description": "reference_code.llama-index-core.llama_index.core.readers.file.base",
        "peekOfCode": "def default_file_metadata_func(\n    file_path: str, fs: fsspec.AbstractFileSystem | None = None\n) -> dict:\n    \"\"\"\n    Get some handy metadata from filesystem.\n    Args:\n        file_path: str: file path in str\n    \"\"\"\n    fs = fs or get_default_fs()\n    stat_result = fs.stat(file_path)",
        "detail": "reference_code.llama-index-core.llama_index.core.readers.file.base",
        "documentation": {}
    },
    {
        "label": "get_default_fs",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.readers.file.base",
        "description": "reference_code.llama-index-core.llama_index.core.readers.file.base",
        "peekOfCode": "def get_default_fs() -> fsspec.AbstractFileSystem:\n    return LocalFileSystem()\ndef is_default_fs(fs: fsspec.AbstractFileSystem) -> bool:\n    return isinstance(fs, LocalFileSystem) and not fs.auto_mkdir\nclass SimpleDirectoryReader(BaseReader, ResourcesReaderMixin, FileSystemReaderMixin):\n    \"\"\"\n    Simple directory reader.\n    Load files from file directory.\n    Automatically select the best file reader given file extensions.\n    Args:",
        "detail": "reference_code.llama-index-core.llama_index.core.readers.file.base",
        "documentation": {}
    },
    {
        "label": "is_default_fs",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.readers.file.base",
        "description": "reference_code.llama-index-core.llama_index.core.readers.file.base",
        "peekOfCode": "def is_default_fs(fs: fsspec.AbstractFileSystem) -> bool:\n    return isinstance(fs, LocalFileSystem) and not fs.auto_mkdir\nclass SimpleDirectoryReader(BaseReader, ResourcesReaderMixin, FileSystemReaderMixin):\n    \"\"\"\n    Simple directory reader.\n    Load files from file directory.\n    Automatically select the best file reader given file extensions.\n    Args:\n        input_dir (Union[Path, str]): Path to the directory.\n        input_files (List): List of file paths to read",
        "detail": "reference_code.llama-index-core.llama_index.core.readers.file.base",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.readers.file.base",
        "description": "reference_code.llama-index-core.llama_index.core.readers.file.base",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass FileSystemReaderMixin(ABC):\n    @abstractmethod\n    def read_file_content(self, input_file: Path, **kwargs: Any) -> bytes:\n        \"\"\"\n        Read the bytes content of a file.\n        Args:\n            input_file (Path): Path to the file.\n        Returns:\n            bytes: File content.",
        "detail": "reference_code.llama-index-core.llama_index.core.readers.file.base",
        "documentation": {}
    },
    {
        "label": "BaseReader",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.readers.base",
        "description": "reference_code.llama-index-core.llama_index.core.readers.base",
        "peekOfCode": "class BaseReader(ABC):  # pragma: no cover\n    \"\"\"Utilities for loading data from a directory.\"\"\"\n    def lazy_load_data(self, *args: Any, **load_kwargs: Any) -> Iterable[Document]:\n        \"\"\"Load data from the input directory lazily.\"\"\"\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} does not provide lazy_load_data method currently\"\n        )\n    async def alazy_load_data(\n        self, *args: Any, **load_kwargs: Any\n    ) -> Iterable[Document]:",
        "detail": "reference_code.llama-index-core.llama_index.core.readers.base",
        "documentation": {}
    },
    {
        "label": "BasePydanticReader",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.readers.base",
        "description": "reference_code.llama-index-core.llama_index.core.readers.base",
        "peekOfCode": "class BasePydanticReader(BaseReader, BaseComponent):\n    \"\"\"Serialiable Data Loader with Pydantic.\"\"\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    is_remote: bool = Field(\n        default=False,\n        description=\"Whether the data is loaded from a remote API or a local file.\",\n    )\nclass ResourcesReaderMixin(ABC):  # pragma: no cover\n    \"\"\"\n    Mixin for readers that provide access to different types of resources.",
        "detail": "reference_code.llama-index-core.llama_index.core.readers.base",
        "documentation": {}
    },
    {
        "label": "ResourcesReaderMixin",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.readers.base",
        "description": "reference_code.llama-index-core.llama_index.core.readers.base",
        "peekOfCode": "class ResourcesReaderMixin(ABC):  # pragma: no cover\n    \"\"\"\n    Mixin for readers that provide access to different types of resources.\n    Resources refer to specific data entities that can be accessed by the reader.\n    Examples of resources include files for a filesystem reader, channel IDs for a Slack reader, or pages for a Notion reader.\n    \"\"\"\n    @abstractmethod\n    def list_resources(self, *args: Any, **kwargs: Any) -> List[str]:\n        \"\"\"\n        List of identifiers for the specific type of resources available in the reader.",
        "detail": "reference_code.llama-index-core.llama_index.core.readers.base",
        "documentation": {}
    },
    {
        "label": "ReaderConfig",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.readers.base",
        "description": "reference_code.llama-index-core.llama_index.core.readers.base",
        "peekOfCode": "class ReaderConfig(BaseComponent):  # pragma: no cover\n    \"\"\"Represents a reader and it's input arguments.\"\"\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    reader: BasePydanticReader = Field(..., description=\"Reader to use.\")\n    reader_args: List[Any] = Field(default_factory=list, description=\"Reader args.\")\n    reader_kwargs: Dict[str, Any] = Field(\n        default_factory=dict, description=\"Reader kwargs.\"\n    )\n    @classmethod\n    def class_name(cls) -> str:",
        "detail": "reference_code.llama-index-core.llama_index.core.readers.base",
        "documentation": {}
    },
    {
        "label": "download_loader",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.readers.download",
        "description": "reference_code.llama-index-core.llama_index.core.readers.download",
        "peekOfCode": "def download_loader(\n    loader_class: str,\n    loader_hub_url: str = \"\",\n    refresh_cache: bool = False,\n    use_gpt_index_import: bool = False,\n    custom_path: Optional[str] = None,\n) -> Type[BaseReader]:  # pragma: no cover\n    \"\"\"\n    Download a single loader from the Loader Hub.\n    Args:",
        "detail": "reference_code.llama-index-core.llama_index.core.readers.download",
        "documentation": {}
    },
    {
        "label": "JSONReader",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.readers.json",
        "description": "reference_code.llama-index-core.llama_index.core.readers.json",
        "peekOfCode": "class JSONReader(BaseReader):\n    \"\"\"\n    JSON reader.\n    Reads JSON documents with options to help suss out relationships between nodes.\n    Args:\n        levels_back (int): the number of levels to go back in the JSON tree, 0\n          if you want all levels. If levels_back is None, then we just format the\n          JSON and make each line an embedding\n        collapse_length (int): the maximum number of characters a JSON fragment\n          would be collapsed in the output (levels_back needs to be not None)",
        "detail": "reference_code.llama-index-core.llama_index.core.readers.json",
        "documentation": {}
    },
    {
        "label": "load_reader",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.readers.loading",
        "description": "reference_code.llama-index-core.llama_index.core.readers.loading",
        "peekOfCode": "def load_reader(data: Dict[str, Any]) -> BasePydanticReader:\n    if isinstance(data, BasePydanticReader):\n        return data\n    class_name = data.get(\"class_name\")\n    if class_name is None:\n        raise ValueError(\"Must specify `class_name` in reader data.\")\n    if class_name not in ALL_READERS:\n        raise ValueError(f\"Reader class name {class_name} not found.\")\n    # remove static attribute\n    data.pop(\"is_remote\", None)",
        "detail": "reference_code.llama-index-core.llama_index.core.readers.loading",
        "documentation": {}
    },
    {
        "label": "StringIterableReader",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.readers.string_iterable",
        "description": "reference_code.llama-index-core.llama_index.core.readers.string_iterable",
        "peekOfCode": "class StringIterableReader(BasePydanticReader):\n    \"\"\"\n    String Iterable Reader.\n    Gets a list of documents, given an iterable (e.g. list) of strings.\n    Example:\n        .. code-block:: python\n            from llama_index.core.legacy import StringIterableReader, TreeIndex\n            documents = StringIterableReader().load_data(\n                texts=[\"I went to the store\", \"I bought an apple\"]\n            )",
        "detail": "reference_code.llama-index-core.llama_index.core.readers.string_iterable",
        "documentation": {}
    },
    {
        "label": "display_image",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "description": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "peekOfCode": "def display_image(img_str: str, size: Tuple[int, int] = DEFAULT_THUMBNAIL_SIZE) -> None:\n    \"\"\"Display base64 encoded image str as image for jupyter notebook.\"\"\"\n    img = b64_2_img(img_str)\n    img.thumbnail(size)\n    display(img)\ndef display_image_uris(\n    image_paths: List[str],\n    image_matrix: Tuple[int, int] = DEFAULT_IMAGE_MATRIX,\n    top_k: int = DEFAULT_SHOW_TOP_K,\n) -> None:",
        "detail": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "documentation": {}
    },
    {
        "label": "display_image_uris",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "description": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "peekOfCode": "def display_image_uris(\n    image_paths: List[str],\n    image_matrix: Tuple[int, int] = DEFAULT_IMAGE_MATRIX,\n    top_k: int = DEFAULT_SHOW_TOP_K,\n) -> None:\n    \"\"\"Display base64 encoded image str as image for jupyter notebook.\"\"\"\n    images_shown = 0\n    plt.figure(figsize=(16, 9))\n    for img_path in image_paths[:top_k]:\n        if os.path.isfile(img_path):",
        "detail": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "documentation": {}
    },
    {
        "label": "display_source_node",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "description": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "peekOfCode": "def display_source_node(\n    source_node: NodeWithScore,\n    source_length: int = 100,\n    show_source_metadata: bool = False,\n    metadata_mode: MetadataMode = MetadataMode.NONE,\n) -> None:\n    \"\"\"Display source node for jupyter notebook.\"\"\"\n    source_text_fmt = truncate_text(\n        source_node.node.get_content(metadata_mode=metadata_mode).strip(), source_length\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "documentation": {}
    },
    {
        "label": "display_metadata",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "description": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "peekOfCode": "def display_metadata(metadata: Dict[str, Any]) -> None:\n    \"\"\"Display metadata for jupyter notebook.\"\"\"\n    display(metadata)\ndef display_response(\n    response: Response,\n    source_length: int = 100,\n    show_source: bool = False,\n    show_metadata: bool = False,\n    show_source_metadata: bool = False,\n) -> None:",
        "detail": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "documentation": {}
    },
    {
        "label": "display_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "description": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "peekOfCode": "def display_response(\n    response: Response,\n    source_length: int = 100,\n    show_source: bool = False,\n    show_metadata: bool = False,\n    show_source_metadata: bool = False,\n) -> None:\n    \"\"\"Display response for jupyter notebook.\"\"\"\n    if response.response is None:\n        response_text = \"None\"",
        "detail": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "documentation": {}
    },
    {
        "label": "display_query_and_multimodal_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "description": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "peekOfCode": "def display_query_and_multimodal_response(\n    query_str: str, response: Response, plot_height: int = 2, plot_width: int = 5\n) -> None:\n    \"\"\"For displaying a query and its multi-modal response.\"\"\"\n    if response.metadata:\n        image_nodes = response.metadata[\"image_nodes\"] or []\n    else:\n        image_nodes = []\n    num_subplots = len(image_nodes)\n    f, axarr = plt.subplots(1, num_subplots)",
        "detail": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "documentation": {}
    },
    {
        "label": "DEFAULT_THUMBNAIL_SIZE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "description": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "peekOfCode": "DEFAULT_THUMBNAIL_SIZE = (512, 512)\nDEFAULT_IMAGE_MATRIX = (3, 3)\nDEFAULT_SHOW_TOP_K = 3\ndef display_image(img_str: str, size: Tuple[int, int] = DEFAULT_THUMBNAIL_SIZE) -> None:\n    \"\"\"Display base64 encoded image str as image for jupyter notebook.\"\"\"\n    img = b64_2_img(img_str)\n    img.thumbnail(size)\n    display(img)\ndef display_image_uris(\n    image_paths: List[str],",
        "detail": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "documentation": {}
    },
    {
        "label": "DEFAULT_IMAGE_MATRIX",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "description": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "peekOfCode": "DEFAULT_IMAGE_MATRIX = (3, 3)\nDEFAULT_SHOW_TOP_K = 3\ndef display_image(img_str: str, size: Tuple[int, int] = DEFAULT_THUMBNAIL_SIZE) -> None:\n    \"\"\"Display base64 encoded image str as image for jupyter notebook.\"\"\"\n    img = b64_2_img(img_str)\n    img.thumbnail(size)\n    display(img)\ndef display_image_uris(\n    image_paths: List[str],\n    image_matrix: Tuple[int, int] = DEFAULT_IMAGE_MATRIX,",
        "detail": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SHOW_TOP_K",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "description": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "peekOfCode": "DEFAULT_SHOW_TOP_K = 3\ndef display_image(img_str: str, size: Tuple[int, int] = DEFAULT_THUMBNAIL_SIZE) -> None:\n    \"\"\"Display base64 encoded image str as image for jupyter notebook.\"\"\"\n    img = b64_2_img(img_str)\n    img.thumbnail(size)\n    display(img)\ndef display_image_uris(\n    image_paths: List[str],\n    image_matrix: Tuple[int, int] = DEFAULT_IMAGE_MATRIX,\n    top_k: int = DEFAULT_SHOW_TOP_K,",
        "detail": "reference_code.llama-index-core.llama_index.core.response.notebook_utils",
        "documentation": {}
    },
    {
        "label": "pprint_metadata",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.response.pprint_utils",
        "description": "reference_code.llama-index-core.llama_index.core.response.pprint_utils",
        "peekOfCode": "def pprint_metadata(metadata: Dict[str, Any]) -> None:\n    \"\"\"Display metadata for jupyter notebook.\"\"\"\n    pprint(metadata)\ndef pprint_source_node(\n    source_node: NodeWithScore, source_length: int = 350, wrap_width: int = 70\n) -> None:\n    \"\"\"Display source node for jupyter notebook.\"\"\"\n    source_text_fmt = truncate_text(\n        source_node.node.get_content().strip(), source_length\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.response.pprint_utils",
        "documentation": {}
    },
    {
        "label": "pprint_source_node",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.response.pprint_utils",
        "description": "reference_code.llama-index-core.llama_index.core.response.pprint_utils",
        "peekOfCode": "def pprint_source_node(\n    source_node: NodeWithScore, source_length: int = 350, wrap_width: int = 70\n) -> None:\n    \"\"\"Display source node for jupyter notebook.\"\"\"\n    source_text_fmt = truncate_text(\n        source_node.node.get_content().strip(), source_length\n    )\n    print(f\"Node ID: {source_node.node.node_id}\")\n    print(f\"Similarity: {source_node.score}\")\n    print(textwrap.fill(f\"Text: {source_text_fmt}\\n\", width=wrap_width))",
        "detail": "reference_code.llama-index-core.llama_index.core.response.pprint_utils",
        "documentation": {}
    },
    {
        "label": "pprint_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.response.pprint_utils",
        "description": "reference_code.llama-index-core.llama_index.core.response.pprint_utils",
        "peekOfCode": "def pprint_response(\n    response: Response,\n    source_length: int = 350,\n    wrap_width: int = 70,\n    show_source: bool = False,\n) -> None:\n    \"\"\"Pretty print response for jupyter notebook.\"\"\"\n    if response.response is None:\n        response_text = \"None\"\n    else:",
        "detail": "reference_code.llama-index-core.llama_index.core.response.pprint_utils",
        "documentation": {}
    },
    {
        "label": "get_response_text",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.response.utils",
        "description": "reference_code.llama-index-core.llama_index.core.response.utils",
        "peekOfCode": "def get_response_text(response_gen: Generator) -> str:\n    \"\"\"Get response text.\"\"\"\n    response_text = \"\"\n    for response in response_gen:\n        response_text += response\n    return response_text\nasync def aget_response_text(response_gen: AsyncGenerator) -> str:\n    \"\"\"Get response text.\"\"\"\n    response_text = \"\"\n    async for response in response_gen:",
        "detail": "reference_code.llama-index-core.llama_index.core.response.utils",
        "documentation": {}
    },
    {
        "label": "Accumulate",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.response_synthesizers.accumulate",
        "description": "reference_code.llama-index-core.llama_index.core.response_synthesizers.accumulate",
        "peekOfCode": "class Accumulate(BaseSynthesizer):\n    \"\"\"Accumulate responses from multiple text chunks.\"\"\"\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        text_qa_template: Optional[BasePromptTemplate] = None,\n        output_cls: Optional[Type[BaseModel]] = None,\n        streaming: bool = False,",
        "detail": "reference_code.llama-index-core.llama_index.core.response_synthesizers.accumulate",
        "documentation": {}
    },
    {
        "label": "BaseSynthesizer",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.response_synthesizers.base",
        "description": "reference_code.llama-index-core.llama_index.core.response_synthesizers.base",
        "peekOfCode": "class BaseSynthesizer(PromptMixin, DispatcherSpanMixin):\n    \"\"\"Response builder class.\"\"\"\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        streaming: bool = False,\n        output_cls: Optional[Type[BaseModel]] = None,\n    ) -> None:",
        "detail": "reference_code.llama-index-core.llama_index.core.response_synthesizers.base",
        "documentation": {}
    },
    {
        "label": "empty_response_generator",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.response_synthesizers.base",
        "description": "reference_code.llama-index-core.llama_index.core.response_synthesizers.base",
        "peekOfCode": "def empty_response_generator() -> Generator[str, None, None]:\n    yield \"Empty Response\"\nasync def empty_response_agenerator() -> AsyncGenerator[str, None]:\n    yield \"Empty Response\"\nclass BaseSynthesizer(PromptMixin, DispatcherSpanMixin):\n    \"\"\"Response builder class.\"\"\"\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        callback_manager: Optional[CallbackManager] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.response_synthesizers.base",
        "documentation": {}
    },
    {
        "label": "dispatcher",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.response_synthesizers.base",
        "description": "reference_code.llama-index-core.llama_index.core.response_synthesizers.base",
        "peekOfCode": "dispatcher = instrument.get_dispatcher(__name__)\nlogger = logging.getLogger(__name__)\nQueryTextType = QueryType\ndef empty_response_generator() -> Generator[str, None, None]:\n    yield \"Empty Response\"\nasync def empty_response_agenerator() -> AsyncGenerator[str, None]:\n    yield \"Empty Response\"\nclass BaseSynthesizer(PromptMixin, DispatcherSpanMixin):\n    \"\"\"Response builder class.\"\"\"\n    def __init__(",
        "detail": "reference_code.llama-index-core.llama_index.core.response_synthesizers.base",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.response_synthesizers.base",
        "description": "reference_code.llama-index-core.llama_index.core.response_synthesizers.base",
        "peekOfCode": "logger = logging.getLogger(__name__)\nQueryTextType = QueryType\ndef empty_response_generator() -> Generator[str, None, None]:\n    yield \"Empty Response\"\nasync def empty_response_agenerator() -> AsyncGenerator[str, None]:\n    yield \"Empty Response\"\nclass BaseSynthesizer(PromptMixin, DispatcherSpanMixin):\n    \"\"\"Response builder class.\"\"\"\n    def __init__(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.response_synthesizers.base",
        "documentation": {}
    },
    {
        "label": "QueryTextType",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.response_synthesizers.base",
        "description": "reference_code.llama-index-core.llama_index.core.response_synthesizers.base",
        "peekOfCode": "QueryTextType = QueryType\ndef empty_response_generator() -> Generator[str, None, None]:\n    yield \"Empty Response\"\nasync def empty_response_agenerator() -> AsyncGenerator[str, None]:\n    yield \"Empty Response\"\nclass BaseSynthesizer(PromptMixin, DispatcherSpanMixin):\n    \"\"\"Response builder class.\"\"\"\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.response_synthesizers.base",
        "documentation": {}
    },
    {
        "label": "CompactAndAccumulate",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.response_synthesizers.compact_and_accumulate",
        "description": "reference_code.llama-index-core.llama_index.core.response_synthesizers.compact_and_accumulate",
        "peekOfCode": "class CompactAndAccumulate(Accumulate):\n    \"\"\"Accumulate responses across compact text chunks.\"\"\"\n    async def aget_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        separator: str = \"\\n---------------------\\n\",\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Get compact response.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.response_synthesizers.compact_and_accumulate",
        "documentation": {}
    },
    {
        "label": "CompactAndRefine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.response_synthesizers.compact_and_refine",
        "description": "reference_code.llama-index-core.llama_index.core.response_synthesizers.compact_and_refine",
        "peekOfCode": "class CompactAndRefine(Refine):\n    \"\"\"Refine responses across compact text chunks.\"\"\"\n    @dispatcher.span\n    async def aget_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        prev_response: Optional[RESPONSE_TEXT_TYPE] = None,\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:",
        "detail": "reference_code.llama-index-core.llama_index.core.response_synthesizers.compact_and_refine",
        "documentation": {}
    },
    {
        "label": "dispatcher",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.response_synthesizers.compact_and_refine",
        "description": "reference_code.llama-index-core.llama_index.core.response_synthesizers.compact_and_refine",
        "peekOfCode": "dispatcher = instrument.get_dispatcher(__name__)\nclass CompactAndRefine(Refine):\n    \"\"\"Refine responses across compact text chunks.\"\"\"\n    @dispatcher.span\n    async def aget_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        prev_response: Optional[RESPONSE_TEXT_TYPE] = None,\n        **response_kwargs: Any,",
        "detail": "reference_code.llama-index-core.llama_index.core.response_synthesizers.compact_and_refine",
        "documentation": {}
    },
    {
        "label": "ContextOnly",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.response_synthesizers.context_only",
        "description": "reference_code.llama-index-core.llama_index.core.response_synthesizers.context_only",
        "peekOfCode": "class ContextOnly(BaseSynthesizer):\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {}\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],",
        "detail": "reference_code.llama-index-core.llama_index.core.response_synthesizers.context_only",
        "documentation": {}
    },
    {
        "label": "get_response_synthesizer",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.response_synthesizers.factory",
        "description": "reference_code.llama-index-core.llama_index.core.response_synthesizers.factory",
        "peekOfCode": "def get_response_synthesizer(\n    llm: Optional[LLM] = None,\n    prompt_helper: Optional[PromptHelper] = None,\n    text_qa_template: Optional[BasePromptTemplate] = None,\n    refine_template: Optional[BasePromptTemplate] = None,\n    summary_template: Optional[BasePromptTemplate] = None,\n    simple_template: Optional[BasePromptTemplate] = None,\n    response_mode: ResponseMode = ResponseMode.COMPACT,\n    callback_manager: Optional[CallbackManager] = None,\n    use_async: bool = False,",
        "detail": "reference_code.llama-index-core.llama_index.core.response_synthesizers.factory",
        "documentation": {}
    },
    {
        "label": "Generation",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.response_synthesizers.generation",
        "description": "reference_code.llama-index-core.llama_index.core.response_synthesizers.generation",
        "peekOfCode": "class Generation(BaseSynthesizer):\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        simple_template: Optional[BasePromptTemplate] = None,\n        streaming: bool = False,\n    ) -> None:\n        super().__init__(",
        "detail": "reference_code.llama-index-core.llama_index.core.response_synthesizers.generation",
        "documentation": {}
    },
    {
        "label": "dispatcher",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.response_synthesizers.generation",
        "description": "reference_code.llama-index-core.llama_index.core.response_synthesizers.generation",
        "peekOfCode": "dispatcher = instrument.get_dispatcher(__name__)\nclass Generation(BaseSynthesizer):\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        simple_template: Optional[BasePromptTemplate] = None,\n        streaming: bool = False,\n    ) -> None:",
        "detail": "reference_code.llama-index-core.llama_index.core.response_synthesizers.generation",
        "documentation": {}
    },
    {
        "label": "NoText",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.response_synthesizers.no_text",
        "description": "reference_code.llama-index-core.llama_index.core.response_synthesizers.no_text",
        "peekOfCode": "class NoText(BaseSynthesizer):\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {}\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],",
        "detail": "reference_code.llama-index-core.llama_index.core.response_synthesizers.no_text",
        "documentation": {}
    },
    {
        "label": "StructuredRefineResponse",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.response_synthesizers.refine",
        "description": "reference_code.llama-index-core.llama_index.core.response_synthesizers.refine",
        "peekOfCode": "class StructuredRefineResponse(BaseModel):\n    \"\"\"\n    Used to answer a given query based on the provided context.\n    Also indicates if the query was satisfied with the provided answer.\n    \"\"\"\n    answer: str = Field(\n        description=\"The answer for the given query, based on the context and not \"\n        \"prior knowledge.\"\n    )\n    query_satisfied: bool = Field(",
        "detail": "reference_code.llama-index-core.llama_index.core.response_synthesizers.refine",
        "documentation": {}
    },
    {
        "label": "DefaultRefineProgram",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.response_synthesizers.refine",
        "description": "reference_code.llama-index-core.llama_index.core.response_synthesizers.refine",
        "peekOfCode": "class DefaultRefineProgram(BasePydanticProgram):\n    \"\"\"\n    Runs the query on the LLM as normal and always returns the answer with\n    query_satisfied=True. In effect, doesn't do any answer filtering.\n    \"\"\"\n    def __init__(\n        self,\n        prompt: BasePromptTemplate,\n        llm: LLM,\n        output_cls: Optional[Type[BaseModel]] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.response_synthesizers.refine",
        "documentation": {}
    },
    {
        "label": "Refine",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.response_synthesizers.refine",
        "description": "reference_code.llama-index-core.llama_index.core.response_synthesizers.refine",
        "peekOfCode": "class Refine(BaseSynthesizer):\n    \"\"\"Refine a response to a query across text chunks.\"\"\"\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        text_qa_template: Optional[BasePromptTemplate] = None,\n        refine_template: Optional[BasePromptTemplate] = None,\n        output_cls: Optional[Type[BaseModel]] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.response_synthesizers.refine",
        "documentation": {}
    },
    {
        "label": "dispatcher",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.response_synthesizers.refine",
        "description": "reference_code.llama-index-core.llama_index.core.response_synthesizers.refine",
        "peekOfCode": "dispatcher = instrument.get_dispatcher(__name__)\nlogger = logging.getLogger(__name__)\nclass StructuredRefineResponse(BaseModel):\n    \"\"\"\n    Used to answer a given query based on the provided context.\n    Also indicates if the query was satisfied with the provided answer.\n    \"\"\"\n    answer: str = Field(\n        description=\"The answer for the given query, based on the context and not \"\n        \"prior knowledge.\"",
        "detail": "reference_code.llama-index-core.llama_index.core.response_synthesizers.refine",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.response_synthesizers.refine",
        "description": "reference_code.llama-index-core.llama_index.core.response_synthesizers.refine",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass StructuredRefineResponse(BaseModel):\n    \"\"\"\n    Used to answer a given query based on the provided context.\n    Also indicates if the query was satisfied with the provided answer.\n    \"\"\"\n    answer: str = Field(\n        description=\"The answer for the given query, based on the context and not \"\n        \"prior knowledge.\"\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.response_synthesizers.refine",
        "documentation": {}
    },
    {
        "label": "SimpleSummarize",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.response_synthesizers.simple_summarize",
        "description": "reference_code.llama-index-core.llama_index.core.response_synthesizers.simple_summarize",
        "peekOfCode": "class SimpleSummarize(BaseSynthesizer):\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        text_qa_template: Optional[BasePromptTemplate] = None,\n        streaming: bool = False,\n    ) -> None:\n        super().__init__(",
        "detail": "reference_code.llama-index-core.llama_index.core.response_synthesizers.simple_summarize",
        "documentation": {}
    },
    {
        "label": "TreeSummarize",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.response_synthesizers.tree_summarize",
        "description": "reference_code.llama-index-core.llama_index.core.response_synthesizers.tree_summarize",
        "peekOfCode": "class TreeSummarize(BaseSynthesizer):\n    \"\"\"\n    Tree summarize response builder.\n    This response builder recursively merges text chunks and summarizes them\n    in a bottom-up fashion (i.e. building a tree from leaves to root).\n    More concretely, at each recursively step:\n    1. we repack the text chunks so that each chunk fills the context window of the LLM\n    2. if there is only one chunk, we give the final response\n    3. otherwise, we summarize each chunk and recursively summarize the summaries.\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.response_synthesizers.tree_summarize",
        "documentation": {}
    },
    {
        "label": "ResponseMode",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.response_synthesizers.type",
        "description": "reference_code.llama-index-core.llama_index.core.response_synthesizers.type",
        "peekOfCode": "class ResponseMode(str, Enum):\n    \"\"\"Response modes of the response builder (and synthesizer).\"\"\"\n    REFINE = \"refine\"\n    \"\"\"\n    Refine is an iterative way of generating a response.\n    We first use the context in the first node, along with the query, to generate an \\\n    initial answer.\n    We then pass this answer, the query, and the context of the second node as input \\\n    into a refine prompt to generate a refined answer. We refine through N-1 nodes, \\\n    where N is the total number of nodes.",
        "detail": "reference_code.llama-index-core.llama_index.core.response_synthesizers.type",
        "documentation": {}
    },
    {
        "label": "AutoMergingRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.retrievers.auto_merging_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.retrievers.auto_merging_retriever",
        "peekOfCode": "class AutoMergingRetriever(BaseRetriever):\n    \"\"\"\n    This retriever will try to merge context into parent context.\n    The retriever first retrieves chunks from a vector store.\n    Then, it will try to merge the chunks into a single context.\n    \"\"\"\n    def __init__(\n        self,\n        vector_retriever: VectorIndexRetriever,\n        storage_context: StorageContext,",
        "detail": "reference_code.llama-index-core.llama_index.core.retrievers.auto_merging_retriever",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.retrievers.auto_merging_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.retrievers.auto_merging_retriever",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass AutoMergingRetriever(BaseRetriever):\n    \"\"\"\n    This retriever will try to merge context into parent context.\n    The retriever first retrieves chunks from a vector store.\n    Then, it will try to merge the chunks into a single context.\n    \"\"\"\n    def __init__(\n        self,\n        vector_retriever: VectorIndexRetriever,",
        "detail": "reference_code.llama-index-core.llama_index.core.retrievers.auto_merging_retriever",
        "documentation": {}
    },
    {
        "label": "FUSION_MODES",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.retrievers.fusion_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.retrievers.fusion_retriever",
        "peekOfCode": "class FUSION_MODES(str, Enum):\n    \"\"\"Enum for different fusion modes.\"\"\"\n    RECIPROCAL_RANK = \"reciprocal_rerank\"  # apply reciprocal rank fusion\n    RELATIVE_SCORE = \"relative_score\"  # apply relative score fusion\n    DIST_BASED_SCORE = \"dist_based_score\"  # apply distance-based score fusion\n    SIMPLE = \"simple\"  # simple re-ordering of results based on original scores\nclass QueryFusionRetriever(BaseRetriever):\n    def __init__(\n        self,\n        retrievers: List[BaseRetriever],",
        "detail": "reference_code.llama-index-core.llama_index.core.retrievers.fusion_retriever",
        "documentation": {}
    },
    {
        "label": "QueryFusionRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.retrievers.fusion_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.retrievers.fusion_retriever",
        "peekOfCode": "class QueryFusionRetriever(BaseRetriever):\n    def __init__(\n        self,\n        retrievers: List[BaseRetriever],\n        llm: Optional[LLMType] = None,\n        query_gen_prompt: Optional[str] = None,\n        mode: FUSION_MODES = FUSION_MODES.SIMPLE,\n        similarity_top_k: int = DEFAULT_SIMILARITY_TOP_K,\n        num_queries: int = 4,\n        use_async: bool = True,",
        "detail": "reference_code.llama-index-core.llama_index.core.retrievers.fusion_retriever",
        "documentation": {}
    },
    {
        "label": "QUERY_GEN_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.retrievers.fusion_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.retrievers.fusion_retriever",
        "peekOfCode": "QUERY_GEN_PROMPT = (\n    \"You are a helpful assistant that generates multiple search queries based on a \"\n    \"single input query. Generate {num_queries} search queries, one on each line, \"\n    \"related to the following input query:\\n\"\n    \"Query: {query}\\n\"\n    \"Queries:\\n\"\n)\nclass FUSION_MODES(str, Enum):\n    \"\"\"Enum for different fusion modes.\"\"\"\n    RECIPROCAL_RANK = \"reciprocal_rerank\"  # apply reciprocal rank fusion",
        "detail": "reference_code.llama-index-core.llama_index.core.retrievers.fusion_retriever",
        "documentation": {}
    },
    {
        "label": "RecursiveRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.retrievers.recursive_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.retrievers.recursive_retriever",
        "peekOfCode": "class RecursiveRetriever(BaseRetriever):\n    \"\"\"\n    Recursive retriever.\n    This retriever will recursively explore links from nodes to other\n    retrievers/query engines.\n    For any retrieved nodes, if any of the nodes are IndexNodes,\n    then it will explore the linked retriever/query engine, and query that.\n    Args:\n        root_id (str): The root id of the query graph.\n        retriever_dict (Optional[Dict[str, BaseRetriever]]): A dictionary",
        "detail": "reference_code.llama-index-core.llama_index.core.retrievers.recursive_retriever",
        "documentation": {}
    },
    {
        "label": "DEFAULT_QUERY_RESPONSE_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.retrievers.recursive_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.retrievers.recursive_retriever",
        "peekOfCode": "DEFAULT_QUERY_RESPONSE_TMPL = \"Query: {query_str}\\nResponse: {response}\"\nRQN_TYPE = Union[BaseRetriever, BaseQueryEngine, BaseNode]\nclass RecursiveRetriever(BaseRetriever):\n    \"\"\"\n    Recursive retriever.\n    This retriever will recursively explore links from nodes to other\n    retrievers/query engines.\n    For any retrieved nodes, if any of the nodes are IndexNodes,\n    then it will explore the linked retriever/query engine, and query that.\n    Args:",
        "detail": "reference_code.llama-index-core.llama_index.core.retrievers.recursive_retriever",
        "documentation": {}
    },
    {
        "label": "RQN_TYPE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.retrievers.recursive_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.retrievers.recursive_retriever",
        "peekOfCode": "RQN_TYPE = Union[BaseRetriever, BaseQueryEngine, BaseNode]\nclass RecursiveRetriever(BaseRetriever):\n    \"\"\"\n    Recursive retriever.\n    This retriever will recursively explore links from nodes to other\n    retrievers/query engines.\n    For any retrieved nodes, if any of the nodes are IndexNodes,\n    then it will explore the linked retriever/query engine, and query that.\n    Args:\n        root_id (str): The root id of the query graph.",
        "detail": "reference_code.llama-index-core.llama_index.core.retrievers.recursive_retriever",
        "documentation": {}
    },
    {
        "label": "RouterRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.retrievers.router_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.retrievers.router_retriever",
        "peekOfCode": "class RouterRetriever(BaseRetriever):\n    \"\"\"\n    Router retriever.\n    Selects one (or multiple) out of several candidate retrievers to execute a query.\n    Args:\n        selector (BaseSelector): A selector that chooses one out of many options based\n            on each candidate's metadata and query.\n        retriever_tools (Sequence[RetrieverTool]): A sequence of candidate\n            retrievers. They must be wrapped as tools to expose metadata to\n            the selector.",
        "detail": "reference_code.llama-index-core.llama_index.core.retrievers.router_retriever",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.retrievers.router_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.retrievers.router_retriever",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass RouterRetriever(BaseRetriever):\n    \"\"\"\n    Router retriever.\n    Selects one (or multiple) out of several candidate retrievers to execute a query.\n    Args:\n        selector (BaseSelector): A selector that chooses one out of many options based\n            on each candidate's metadata and query.\n        retriever_tools (Sequence[RetrieverTool]): A sequence of candidate\n            retrievers. They must be wrapped as tools to expose metadata to",
        "detail": "reference_code.llama-index-core.llama_index.core.retrievers.router_retriever",
        "documentation": {}
    },
    {
        "label": "TransformRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.retrievers.transform_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.retrievers.transform_retriever",
        "peekOfCode": "class TransformRetriever(BaseRetriever):\n    \"\"\"\n    Transform Retriever.\n    Takes in an existing retriever and a query transform and runs the query transform\n    before running the retriever.\n    \"\"\"\n    def __init__(\n        self,\n        retriever: BaseRetriever,\n        query_transform: BaseQueryTransform,",
        "detail": "reference_code.llama-index-core.llama_index.core.retrievers.transform_retriever",
        "documentation": {}
    },
    {
        "label": "EmbeddingSingleSelector",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.selectors.embedding_selectors",
        "description": "reference_code.llama-index-core.llama_index.core.selectors.embedding_selectors",
        "peekOfCode": "class EmbeddingSingleSelector(BaseSelector):\n    \"\"\"\n    Embedding selector.\n    Embedding selector that chooses one out of many options.\n    Args:\n        embed_model (BaseEmbedding): An embedding model.\n    \"\"\"\n    def __init__(\n        self,\n        embed_model: BaseEmbedding,",
        "detail": "reference_code.llama-index-core.llama_index.core.selectors.embedding_selectors",
        "documentation": {}
    },
    {
        "label": "LLMSingleSelector",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.selectors.llm_selectors",
        "description": "reference_code.llama-index-core.llama_index.core.selectors.llm_selectors",
        "peekOfCode": "class LLMSingleSelector(BaseSelector):\n    \"\"\"\n    LLM single selector.\n    LLM-based selector that chooses one out of many options.\n    Args:\n        LLM (LLM): An LLM.\n        prompt (SingleSelectPrompt): A LLM prompt for selecting one out of many options.\n    \"\"\"\n    def __init__(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.selectors.llm_selectors",
        "documentation": {}
    },
    {
        "label": "LLMMultiSelector",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.selectors.llm_selectors",
        "description": "reference_code.llama-index-core.llama_index.core.selectors.llm_selectors",
        "peekOfCode": "class LLMMultiSelector(BaseSelector):\n    \"\"\"\n    LLM multi selector.\n    LLM-based selector that chooses multiple out of many options.\n    Args:\n        llm (LLM): An LLM.\n        prompt (SingleSelectPrompt): A LLM prompt for selecting multiple out of many\n            options.\n    \"\"\"\n    def __init__(",
        "detail": "reference_code.llama-index-core.llama_index.core.selectors.llm_selectors",
        "documentation": {}
    },
    {
        "label": "SingleSelectPrompt",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.selectors.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.selectors.prompts",
        "peekOfCode": "SingleSelectPrompt = PromptTemplate\n\"\"\"Multiple select prompt.\nPromptTemplate to select multiple candidates (up to `max_outputs`) out of `num_choices`\noptions provided in `context_list`, given a query `query_str`.\nRequired template variables: `num_chunks`, `context_list`, `query_str`,\n    `max_outputs`\n\"\"\"\nMultiSelectPrompt = PromptTemplate\n# single select\nDEFAULT_SINGLE_SELECT_PROMPT_TMPL = (",
        "detail": "reference_code.llama-index-core.llama_index.core.selectors.prompts",
        "documentation": {}
    },
    {
        "label": "MultiSelectPrompt",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.selectors.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.selectors.prompts",
        "peekOfCode": "MultiSelectPrompt = PromptTemplate\n# single select\nDEFAULT_SINGLE_SELECT_PROMPT_TMPL = (\n    \"Some choices are given below. It is provided in a numbered list \"\n    \"(1 to {num_choices}), \"\n    \"where each item in the list corresponds to a summary.\\n\"\n    \"---------------------\\n\"\n    \"{context_list}\"\n    \"\\n---------------------\\n\"\n    \"Using only the choices above and not prior knowledge, return \"",
        "detail": "reference_code.llama-index-core.llama_index.core.selectors.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SINGLE_SELECT_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.selectors.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.selectors.prompts",
        "peekOfCode": "DEFAULT_SINGLE_SELECT_PROMPT_TMPL = (\n    \"Some choices are given below. It is provided in a numbered list \"\n    \"(1 to {num_choices}), \"\n    \"where each item in the list corresponds to a summary.\\n\"\n    \"---------------------\\n\"\n    \"{context_list}\"\n    \"\\n---------------------\\n\"\n    \"Using only the choices above and not prior knowledge, return \"\n    \"the choice that is most relevant to the question: '{query_str}'\\n\"\n)",
        "detail": "reference_code.llama-index-core.llama_index.core.selectors.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SINGLE_SELECT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.selectors.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.selectors.prompts",
        "peekOfCode": "DEFAULT_SINGLE_SELECT_PROMPT = PromptTemplate(\n    template=DEFAULT_SINGLE_SELECT_PROMPT_TMPL, prompt_type=PromptType.SINGLE_SELECT\n)\n# multiple select\nDEFAULT_MULTI_SELECT_PROMPT_TMPL = (\n    \"Some choices are given below. It is provided in a numbered \"\n    \"list (1 to {num_choices}), \"\n    \"where each item in the list corresponds to a summary.\\n\"\n    \"---------------------\\n\"\n    \"{context_list}\"",
        "detail": "reference_code.llama-index-core.llama_index.core.selectors.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MULTI_SELECT_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.selectors.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.selectors.prompts",
        "peekOfCode": "DEFAULT_MULTI_SELECT_PROMPT_TMPL = (\n    \"Some choices are given below. It is provided in a numbered \"\n    \"list (1 to {num_choices}), \"\n    \"where each item in the list corresponds to a summary.\\n\"\n    \"---------------------\\n\"\n    \"{context_list}\"\n    \"\\n---------------------\\n\"\n    \"Using only the choices above and not prior knowledge, return the top choices \"\n    \"(no more than {max_outputs}, but only select what is needed) that \"\n    \"are most relevant to the question: '{query_str}'\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.selectors.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MULTIPLE_SELECT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.selectors.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.selectors.prompts",
        "peekOfCode": "DEFAULT_MULTIPLE_SELECT_PROMPT = PromptTemplate(\n    template=DEFAULT_MULTI_SELECT_PROMPT_TMPL, prompt_type=PromptType.MULTI_SELECT\n)\n# single pydantic select\nDEFAULT_SINGLE_PYD_SELECT_PROMPT_TMPL = (\n    \"Some choices are given below. It is provided in a numbered list \"\n    \"(1 to {num_choices}), \"\n    \"where each item in the list corresponds to a summary.\\n\"\n    \"---------------------\\n\"\n    \"{context_list}\"",
        "detail": "reference_code.llama-index-core.llama_index.core.selectors.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SINGLE_PYD_SELECT_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.selectors.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.selectors.prompts",
        "peekOfCode": "DEFAULT_SINGLE_PYD_SELECT_PROMPT_TMPL = (\n    \"Some choices are given below. It is provided in a numbered list \"\n    \"(1 to {num_choices}), \"\n    \"where each item in the list corresponds to a summary.\\n\"\n    \"---------------------\\n\"\n    \"{context_list}\"\n    \"\\n---------------------\\n\"\n    \"Using only the choices above and not prior knowledge, generate \"\n    \"the selection object and reason that is most relevant to the \"\n    \"question: '{query_str}'\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.selectors.prompts",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MULTI_PYD_SELECT_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.selectors.prompts",
        "description": "reference_code.llama-index-core.llama_index.core.selectors.prompts",
        "peekOfCode": "DEFAULT_MULTI_PYD_SELECT_PROMPT_TMPL = (\n    \"Some choices are given below. It is provided in a numbered \"\n    \"list (1 to {num_choices}), \"\n    \"where each item in the list corresponds to a summary.\\n\"\n    \"---------------------\\n\"\n    \"{context_list}\"\n    \"\\n---------------------\\n\"\n    \"Using only the choices above and not prior knowledge, return the top choice(s) \"\n    \"(no more than {max_outputs}, but only select what is needed) by generating \"\n    \"the selection object and reasons that are most relevant to the \"",
        "detail": "reference_code.llama-index-core.llama_index.core.selectors.prompts",
        "documentation": {}
    },
    {
        "label": "PydanticSingleSelector",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.selectors.pydantic_selectors",
        "description": "reference_code.llama-index-core.llama_index.core.selectors.pydantic_selectors",
        "peekOfCode": "class PydanticSingleSelector(BaseSelector):\n    def __init__(self, selector_program: BasePydanticProgram) -> None:\n        self._selector_program = selector_program\n    @classmethod\n    def from_defaults(\n        cls,\n        program: Optional[BasePydanticProgram] = None,\n        llm: Optional[\"OpenAI\"] = None,\n        prompt_template_str: str = DEFAULT_SINGLE_PYD_SELECT_PROMPT_TMPL,\n        verbose: bool = False,",
        "detail": "reference_code.llama-index-core.llama_index.core.selectors.pydantic_selectors",
        "documentation": {}
    },
    {
        "label": "PydanticMultiSelector",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.selectors.pydantic_selectors",
        "description": "reference_code.llama-index-core.llama_index.core.selectors.pydantic_selectors",
        "peekOfCode": "class PydanticMultiSelector(BaseSelector):\n    def __init__(\n        self, selector_program: BasePydanticProgram, max_outputs: Optional[int] = None\n    ) -> None:\n        self._selector_program = selector_program\n        self._max_outputs = max_outputs\n    @classmethod\n    def from_defaults(\n        cls,\n        program: Optional[BasePydanticProgram] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.selectors.pydantic_selectors",
        "documentation": {}
    },
    {
        "label": "get_selector_from_llm",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.selectors.utils",
        "description": "reference_code.llama-index-core.llama_index.core.selectors.utils",
        "peekOfCode": "def get_selector_from_llm(llm: LLM, is_multi: bool = False) -> BaseSelector:\n    \"\"\"Get a selector from a service context. Prefers Pydantic selectors if possible.\"\"\"\n    selector: Optional[BaseSelector] = None\n    if is_multi:\n        try:\n            selector = PydanticMultiSelector.from_defaults(llm=llm)  # type: ignore\n        except ValueError:\n            selector = LLMMultiSelector.from_defaults(llm=llm)\n    else:\n        try:",
        "detail": "reference_code.llama-index-core.llama_index.core.selectors.utils",
        "documentation": {}
    },
    {
        "label": "LlamaLogger",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.service_context_elements.llama_logger",
        "description": "reference_code.llama-index-core.llama_index.core.service_context_elements.llama_logger",
        "peekOfCode": "class LlamaLogger:\n    \"\"\"Logger class.\"\"\"\n    def __init__(self) -> None:\n        \"\"\"Init params.\"\"\"\n        self._logs: List[Dict] = []\n        self._metadata: Dict[str, Any] = {}\n    def reset(self) -> None:\n        \"\"\"Reset logs.\"\"\"\n        self._logs = []\n    def set_metadata(self, metadata: Dict) -> None:",
        "detail": "reference_code.llama-index-core.llama_index.core.service_context_elements.llama_logger",
        "documentation": {}
    },
    {
        "label": "BaseLLMPredictor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.service_context_elements.llm_predictor",
        "description": "reference_code.llama-index-core.llama_index.core.service_context_elements.llm_predictor",
        "peekOfCode": "class BaseLLMPredictor(BaseComponent, DispatcherSpanMixin, ABC):\n    \"\"\"Base LLM Predictor.\"\"\"\n    def model_dump(self, **kwargs: Any) -> Dict[str, Any]:\n        print(\"here\", flush=True)\n        data = super().model_dump(**kwargs)\n        data[\"llm\"] = self.llm.to_dict()\n        return data\n    def dict(self, **kwargs: Any) -> Dict[str, Any]:\n        \"\"\"Keep for backwards compatibility.\"\"\"\n        return self.model_dump(**kwargs)",
        "detail": "reference_code.llama-index-core.llama_index.core.service_context_elements.llm_predictor",
        "documentation": {}
    },
    {
        "label": "LLMPredictor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.service_context_elements.llm_predictor",
        "description": "reference_code.llama-index-core.llama_index.core.service_context_elements.llm_predictor",
        "peekOfCode": "class LLMPredictor(BaseLLMPredictor):\n    \"\"\"\n    LLM predictor class.\n    NOTE: Deprecated. Use any LLM class directly.\n    \"\"\"\n    def __init__(\n        self,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.service_context_elements.llm_predictor",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.service_context_elements.llm_predictor",
        "description": "reference_code.llama-index-core.llama_index.core.service_context_elements.llm_predictor",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass BaseLLMPredictor(BaseComponent, DispatcherSpanMixin, ABC):\n    \"\"\"Base LLM Predictor.\"\"\"\n    def model_dump(self, **kwargs: Any) -> Dict[str, Any]:\n        print(\"here\", flush=True)\n        data = super().model_dump(**kwargs)\n        data[\"llm\"] = self.llm.to_dict()\n        return data\n    def dict(self, **kwargs: Any) -> Dict[str, Any]:\n        \"\"\"Keep for backwards compatibility.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.service_context_elements.llm_predictor",
        "documentation": {}
    },
    {
        "label": "MockSparseEmbedding",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.sparse_embeddings.mock_sparse_embedding",
        "description": "reference_code.llama-index-core.llama_index.core.sparse_embeddings.mock_sparse_embedding",
        "peekOfCode": "class MockSparseEmbedding(BaseSparseEmbedding):\n    \"\"\"A mock sparse embedding model for testing.\"\"\"\n    default_embedding: SparseEmbedding = Field(\n        default_factory=lambda: {0: 1.0},\n        description=\"The default embedding to return.\",\n    )\n    text_to_embedding: Optional[Dict[str, SparseEmbedding]] = Field(\n        default=None,\n        description=\"The mapping of text to embeddings for lookup.\",\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.sparse_embeddings.mock_sparse_embedding",
        "documentation": {}
    },
    {
        "label": "BaseChatStore",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.chat_store.base",
        "description": "reference_code.llama-index-core.llama_index.core.storage.chat_store.base",
        "peekOfCode": "class BaseChatStore(BaseComponent):\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Get class name.\"\"\"\n        return \"BaseChatStore\"\n    @abstractmethod\n    def set_messages(self, key: str, messages: List[ChatMessage]) -> None:\n        \"\"\"Set messages for a key.\"\"\"\n        ...\n    @abstractmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.chat_store.base",
        "documentation": {}
    },
    {
        "label": "MessageStatus",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.chat_store.base_db",
        "description": "reference_code.llama-index-core.llama_index.core.storage.chat_store.base_db",
        "peekOfCode": "class MessageStatus(str, Enum):\n    \"\"\"Status of a message in the chat store.\"\"\"\n    # Message is in the active FIFO queue\n    ACTIVE = \"active\"\n    # Message has been processed and is archived, removed from the active queue\n    ARCHIVED = \"archived\"\nclass AsyncDBChatStore(BaseModel):\n    \"\"\"\n    Base class for DB-based chat stores.\n    Meant to implement a FIFO queue to manage short-term memory and",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.chat_store.base_db",
        "documentation": {}
    },
    {
        "label": "AsyncDBChatStore",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.chat_store.base_db",
        "description": "reference_code.llama-index-core.llama_index.core.storage.chat_store.base_db",
        "peekOfCode": "class AsyncDBChatStore(BaseModel):\n    \"\"\"\n    Base class for DB-based chat stores.\n    Meant to implement a FIFO queue to manage short-term memory and\n    general conversation history.\n    \"\"\"\n    @abstractmethod\n    async def get_messages(\n        self,\n        key: str,",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.chat_store.base_db",
        "documentation": {}
    },
    {
        "label": "load_chat_store",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.chat_store.loading",
        "description": "reference_code.llama-index-core.llama_index.core.storage.chat_store.loading",
        "peekOfCode": "def load_chat_store(data: dict) -> BaseChatStore:\n    \"\"\"Load a chat store from a dict.\"\"\"\n    chat_store_name = data.get(\"class_name\")\n    if chat_store_name is None:\n        raise ValueError(\"ChatStore loading requires a class_name\")\n    if chat_store_name not in RECOGNIZED_CHAT_STORES:\n        raise ValueError(f\"Invalid ChatStore name: {chat_store_name}\")\n    return RECOGNIZED_CHAT_STORES[chat_store_name].from_dict(data)",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.chat_store.loading",
        "documentation": {}
    },
    {
        "label": "RECOGNIZED_CHAT_STORES",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.chat_store.loading",
        "description": "reference_code.llama-index-core.llama_index.core.storage.chat_store.loading",
        "peekOfCode": "RECOGNIZED_CHAT_STORES = {\n    SimpleChatStore.class_name(): SimpleChatStore,\n}\ndef load_chat_store(data: dict) -> BaseChatStore:\n    \"\"\"Load a chat store from a dict.\"\"\"\n    chat_store_name = data.get(\"class_name\")\n    if chat_store_name is None:\n        raise ValueError(\"ChatStore loading requires a class_name\")\n    if chat_store_name not in RECOGNIZED_CHAT_STORES:\n        raise ValueError(f\"Invalid ChatStore name: {chat_store_name}\")",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.chat_store.loading",
        "documentation": {}
    },
    {
        "label": "SimpleChatStore",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.chat_store.simple_chat_store",
        "description": "reference_code.llama-index-core.llama_index.core.storage.chat_store.simple_chat_store",
        "peekOfCode": "class SimpleChatStore(BaseChatStore):\n    \"\"\"Simple chat store. Async methods provide same functionality as sync methods in this class.\"\"\"\n    store: Dict[str, List[AnnotatedChatMessage]] = Field(default_factory=dict)\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Get class name.\"\"\"\n        return \"SimpleChatStore\"\n    def set_messages(self, key: str, messages: List[ChatMessage]) -> None:\n        \"\"\"Set messages for a key.\"\"\"\n        self.store[key] = messages",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.chat_store.simple_chat_store",
        "documentation": {}
    },
    {
        "label": "chat_message_serialization",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.chat_store.simple_chat_store",
        "description": "reference_code.llama-index-core.llama_index.core.storage.chat_store.simple_chat_store",
        "peekOfCode": "def chat_message_serialization(\n    chat_message: Any, handler: Any, info: Any\n) -> Dict[str, Any]:\n    partial_result = handler(chat_message, info)\n    for key, value in partial_result.get(\"additional_kwargs\", {}).items():\n        value = chat_message._recursive_serialization(value)\n        if not isinstance(value, (str, int, float, bool, dict, list, type(None))):\n            raise ValueError(f\"Failed to serialize additional_kwargs value: {value}\")\n        partial_result[\"additional_kwargs\"][key] = value\n    return partial_result",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.chat_store.simple_chat_store",
        "documentation": {}
    },
    {
        "label": "AnnotatedChatMessage",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.chat_store.simple_chat_store",
        "description": "reference_code.llama-index-core.llama_index.core.storage.chat_store.simple_chat_store",
        "peekOfCode": "AnnotatedChatMessage = Annotated[\n    ChatMessage, WrapSerializer(chat_message_serialization)\n]\nclass SimpleChatStore(BaseChatStore):\n    \"\"\"Simple chat store. Async methods provide same functionality as sync methods in this class.\"\"\"\n    store: Dict[str, List[AnnotatedChatMessage]] = Field(default_factory=dict)\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Get class name.\"\"\"\n        return \"SimpleChatStore\"",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.chat_store.simple_chat_store",
        "documentation": {}
    },
    {
        "label": "SQLAlchemyChatStore",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.chat_store.sql",
        "description": "reference_code.llama-index-core.llama_index.core.storage.chat_store.sql",
        "peekOfCode": "class SQLAlchemyChatStore(AsyncDBChatStore):\n    \"\"\"\n    Base class for SQLAlchemy-based chat stores.\n    This class provides a foundation for creating chat stores that use SQLAlchemy\n    to interact with SQL databases. It handles common operations like managing\n    sessions, creating tables, and CRUD operations on chat messages.\n    Enhanced with status tracking for better FIFO queue management for short-term memory.\n    This class is meant to replace all other chat store classes.\n    \"\"\"\n    table_name: str = Field(description=\"Name of the table to store messages\")",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.chat_store.sql",
        "documentation": {}
    },
    {
        "label": "DEFAULT_ASYNC_DATABASE_URI",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.chat_store.sql",
        "description": "reference_code.llama-index-core.llama_index.core.storage.chat_store.sql",
        "peekOfCode": "DEFAULT_ASYNC_DATABASE_URI = \"sqlite+aiosqlite:///:memory:\"\nBase = declarative_base()\nclass SQLAlchemyChatStore(AsyncDBChatStore):\n    \"\"\"\n    Base class for SQLAlchemy-based chat stores.\n    This class provides a foundation for creating chat stores that use SQLAlchemy\n    to interact with SQL databases. It handles common operations like managing\n    sessions, creating tables, and CRUD operations on chat messages.\n    Enhanced with status tracking for better FIFO queue management for short-term memory.\n    This class is meant to replace all other chat store classes.",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.chat_store.sql",
        "documentation": {}
    },
    {
        "label": "Base",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.chat_store.sql",
        "description": "reference_code.llama-index-core.llama_index.core.storage.chat_store.sql",
        "peekOfCode": "Base = declarative_base()\nclass SQLAlchemyChatStore(AsyncDBChatStore):\n    \"\"\"\n    Base class for SQLAlchemy-based chat stores.\n    This class provides a foundation for creating chat stores that use SQLAlchemy\n    to interact with SQL databases. It handles common operations like managing\n    sessions, creating tables, and CRUD operations on chat messages.\n    Enhanced with status tracking for better FIFO queue management for short-term memory.\n    This class is meant to replace all other chat store classes.\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.chat_store.sql",
        "documentation": {}
    },
    {
        "label": "KVDocumentStore",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.docstore.keyval_docstore",
        "description": "reference_code.llama-index-core.llama_index.core.storage.docstore.keyval_docstore",
        "peekOfCode": "class KVDocumentStore(BaseDocumentStore):\n    \"\"\"\n    Document (Node) store.\n    NOTE: at the moment, this store is primarily used to store Node objects.\n    Each node will be assigned an ID.\n    The same docstore can be reused across index structures. This\n    allows you to reuse the same storage for multiple index structures;\n    otherwise, each index would create a docstore under the hood.\n    .. code-block:: python\n        nodes = SentenceSplitter().get_nodes_from_documents()",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.docstore.keyval_docstore",
        "documentation": {}
    },
    {
        "label": "DEFAULT_NAMESPACE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.docstore.keyval_docstore",
        "description": "reference_code.llama-index-core.llama_index.core.storage.docstore.keyval_docstore",
        "peekOfCode": "DEFAULT_NAMESPACE = \"docstore\"\n# The nodes collection contains the content of each node, along with metadata specific\n# to each node, including associated attributes like excluded metadata and relationships.\nDEFAULT_COLLECTION_DATA_SUFFIX = \"/data\"\n# Contains mappings from each document to the list of node IDs that belong to it\n# including the document's metadata.\nDEFAULT_REF_DOC_COLLECTION_SUFFIX = \"/ref_doc_info\"\n# Contains references from each node to its corresponding document,\n# including the node's document hash and reference document ID.\nDEFAULT_METADATA_COLLECTION_SUFFIX = \"/metadata\"",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.docstore.keyval_docstore",
        "documentation": {}
    },
    {
        "label": "DEFAULT_COLLECTION_DATA_SUFFIX",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.docstore.keyval_docstore",
        "description": "reference_code.llama-index-core.llama_index.core.storage.docstore.keyval_docstore",
        "peekOfCode": "DEFAULT_COLLECTION_DATA_SUFFIX = \"/data\"\n# Contains mappings from each document to the list of node IDs that belong to it\n# including the document's metadata.\nDEFAULT_REF_DOC_COLLECTION_SUFFIX = \"/ref_doc_info\"\n# Contains references from each node to its corresponding document,\n# including the node's document hash and reference document ID.\nDEFAULT_METADATA_COLLECTION_SUFFIX = \"/metadata\"\nclass KVDocumentStore(BaseDocumentStore):\n    \"\"\"\n    Document (Node) store.",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.docstore.keyval_docstore",
        "documentation": {}
    },
    {
        "label": "DEFAULT_REF_DOC_COLLECTION_SUFFIX",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.docstore.keyval_docstore",
        "description": "reference_code.llama-index-core.llama_index.core.storage.docstore.keyval_docstore",
        "peekOfCode": "DEFAULT_REF_DOC_COLLECTION_SUFFIX = \"/ref_doc_info\"\n# Contains references from each node to its corresponding document,\n# including the node's document hash and reference document ID.\nDEFAULT_METADATA_COLLECTION_SUFFIX = \"/metadata\"\nclass KVDocumentStore(BaseDocumentStore):\n    \"\"\"\n    Document (Node) store.\n    NOTE: at the moment, this store is primarily used to store Node objects.\n    Each node will be assigned an ID.\n    The same docstore can be reused across index structures. This",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.docstore.keyval_docstore",
        "documentation": {}
    },
    {
        "label": "DEFAULT_METADATA_COLLECTION_SUFFIX",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.docstore.keyval_docstore",
        "description": "reference_code.llama-index-core.llama_index.core.storage.docstore.keyval_docstore",
        "peekOfCode": "DEFAULT_METADATA_COLLECTION_SUFFIX = \"/metadata\"\nclass KVDocumentStore(BaseDocumentStore):\n    \"\"\"\n    Document (Node) store.\n    NOTE: at the moment, this store is primarily used to store Node objects.\n    Each node will be assigned an ID.\n    The same docstore can be reused across index structures. This\n    allows you to reuse the same storage for multiple index structures;\n    otherwise, each index would create a docstore under the hood.\n    .. code-block:: python",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.docstore.keyval_docstore",
        "documentation": {}
    },
    {
        "label": "DocumentStoreType",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.docstore.registry",
        "description": "reference_code.llama-index-core.llama_index.core.storage.docstore.registry",
        "peekOfCode": "class DocumentStoreType(str, Enum):\n    MONGO = \"mongo\"\n    SIMPLE = \"simple\"\nDOCSTORE_TYPE_TO_CLASS: Dict[DocumentStoreType, Type[BaseDocumentStore]] = {\n    DocumentStoreType.SIMPLE: SimpleDocumentStore,\n}\nDOCSTORE_CLASS_TO_TYPE: Dict[Type[BaseDocumentStore], DocumentStoreType] = {\n    cls_: type_ for type_, cls_ in DOCSTORE_TYPE_TO_CLASS.items()\n}\ndef get_default_docstore() -> BaseDocumentStore:",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.docstore.registry",
        "documentation": {}
    },
    {
        "label": "get_default_docstore",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.docstore.registry",
        "description": "reference_code.llama-index-core.llama_index.core.storage.docstore.registry",
        "peekOfCode": "def get_default_docstore() -> BaseDocumentStore:\n    return SimpleDocumentStore()",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.docstore.registry",
        "documentation": {}
    },
    {
        "label": "SimpleDocumentStore",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.docstore.simple_docstore",
        "description": "reference_code.llama-index-core.llama_index.core.storage.docstore.simple_docstore",
        "peekOfCode": "class SimpleDocumentStore(KVDocumentStore):\n    \"\"\"\n    Simple Document (Node) store.\n    An in-memory store for Document and Node objects.\n    Args:\n        simple_kvstore (SimpleKVStore): simple key-value store\n        namespace (str): namespace for the docstore\n    \"\"\"\n    def __init__(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.docstore.simple_docstore",
        "documentation": {}
    },
    {
        "label": "DocumentStore",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.docstore.simple_docstore",
        "description": "reference_code.llama-index-core.llama_index.core.storage.docstore.simple_docstore",
        "peekOfCode": "DocumentStore = SimpleDocumentStore",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.docstore.simple_docstore",
        "documentation": {}
    },
    {
        "label": "RefDocInfo",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.docstore.types",
        "description": "reference_code.llama-index-core.llama_index.core.storage.docstore.types",
        "peekOfCode": "class RefDocInfo(DataClassJsonMixin):\n    \"\"\"Dataclass to represent ingested documents.\"\"\"\n    node_ids: List = field(default_factory=list)\n    metadata: Dict[str, Any] = field(default_factory=dict)\nclass BaseDocumentStore(ABC):\n    # ===== Save/load =====\n    def persist(\n        self,\n        persist_path: str = DEFAULT_PERSIST_PATH,\n        fs: Optional[fsspec.AbstractFileSystem] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "BaseDocumentStore",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.docstore.types",
        "description": "reference_code.llama-index-core.llama_index.core.storage.docstore.types",
        "peekOfCode": "class BaseDocumentStore(ABC):\n    # ===== Save/load =====\n    def persist(\n        self,\n        persist_path: str = DEFAULT_PERSIST_PATH,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n    ) -> None:\n        \"\"\"Persist the docstore to a file.\"\"\"\n    # ===== Main interface =====\n    @property",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_FNAME",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.docstore.types",
        "description": "reference_code.llama-index-core.llama_index.core.storage.docstore.types",
        "peekOfCode": "DEFAULT_PERSIST_FNAME = \"docstore.json\"\nDEFAULT_PERSIST_DIR = \"./storage\"\nDEFAULT_PERSIST_PATH = os.path.join(DEFAULT_PERSIST_DIR, DEFAULT_PERSIST_FNAME)\n@dataclass\nclass RefDocInfo(DataClassJsonMixin):\n    \"\"\"Dataclass to represent ingested documents.\"\"\"\n    node_ids: List = field(default_factory=list)\n    metadata: Dict[str, Any] = field(default_factory=dict)\nclass BaseDocumentStore(ABC):\n    # ===== Save/load =====",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_DIR",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.docstore.types",
        "description": "reference_code.llama-index-core.llama_index.core.storage.docstore.types",
        "peekOfCode": "DEFAULT_PERSIST_DIR = \"./storage\"\nDEFAULT_PERSIST_PATH = os.path.join(DEFAULT_PERSIST_DIR, DEFAULT_PERSIST_FNAME)\n@dataclass\nclass RefDocInfo(DataClassJsonMixin):\n    \"\"\"Dataclass to represent ingested documents.\"\"\"\n    node_ids: List = field(default_factory=list)\n    metadata: Dict[str, Any] = field(default_factory=dict)\nclass BaseDocumentStore(ABC):\n    # ===== Save/load =====\n    def persist(",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_PATH",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.docstore.types",
        "description": "reference_code.llama-index-core.llama_index.core.storage.docstore.types",
        "peekOfCode": "DEFAULT_PERSIST_PATH = os.path.join(DEFAULT_PERSIST_DIR, DEFAULT_PERSIST_FNAME)\n@dataclass\nclass RefDocInfo(DataClassJsonMixin):\n    \"\"\"Dataclass to represent ingested documents.\"\"\"\n    node_ids: List = field(default_factory=list)\n    metadata: Dict[str, Any] = field(default_factory=dict)\nclass BaseDocumentStore(ABC):\n    # ===== Save/load =====\n    def persist(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.docstore.types",
        "documentation": {}
    },
    {
        "label": "doc_to_json",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.docstore.utils",
        "description": "reference_code.llama-index-core.llama_index.core.storage.docstore.utils",
        "peekOfCode": "def doc_to_json(doc: BaseNode) -> dict:\n    return {\n        DATA_KEY: doc.to_dict(),\n        TYPE_KEY: doc.get_type(),\n    }\ndef json_to_doc(doc_dict: dict) -> BaseNode:\n    doc_type = doc_dict[TYPE_KEY]\n    data_dict = doc_dict[DATA_KEY]\n    doc: BaseNode\n    if \"extra_info\" in data_dict:",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.docstore.utils",
        "documentation": {}
    },
    {
        "label": "json_to_doc",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.docstore.utils",
        "description": "reference_code.llama-index-core.llama_index.core.storage.docstore.utils",
        "peekOfCode": "def json_to_doc(doc_dict: dict) -> BaseNode:\n    doc_type = doc_dict[TYPE_KEY]\n    data_dict = doc_dict[DATA_KEY]\n    doc: BaseNode\n    if \"extra_info\" in data_dict:\n        return legacy_json_to_doc(doc_dict)\n    else:\n        if doc_type == Document.get_type():\n            if data_dict[\"class_name\"] == ImageDocument.class_name():\n                doc = ImageDocument.from_dict(data_dict)",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.docstore.utils",
        "documentation": {}
    },
    {
        "label": "legacy_json_to_doc",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.docstore.utils",
        "description": "reference_code.llama-index-core.llama_index.core.storage.docstore.utils",
        "peekOfCode": "def legacy_json_to_doc(doc_dict: dict) -> BaseNode:\n    \"\"\"Todo: Deprecated legacy support for old node versions.\"\"\"\n    doc_type = doc_dict[TYPE_KEY]\n    data_dict = doc_dict[DATA_KEY]\n    doc: BaseNode\n    text = data_dict.get(\"text\", \"\")\n    metadata = data_dict.get(\"extra_info\", {}) or {}\n    id_ = data_dict.get(\"doc_id\", None)\n    relationships = data_dict.get(\"relationships\", {})\n    relationships = {",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.docstore.utils",
        "documentation": {}
    },
    {
        "label": "KVIndexStore",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.index_store.keyval_index_store",
        "description": "reference_code.llama-index-core.llama_index.core.storage.index_store.keyval_index_store",
        "peekOfCode": "class KVIndexStore(BaseIndexStore):\n    \"\"\"\n    Key-Value Index store.\n    Args:\n        kvstore (BaseKVStore): key-value store\n        namespace (str): namespace for the index store\n        collection_suffix (str): suffix for the collection name\n    \"\"\"\n    def __init__(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.index_store.keyval_index_store",
        "documentation": {}
    },
    {
        "label": "DEFAULT_NAMESPACE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.index_store.keyval_index_store",
        "description": "reference_code.llama-index-core.llama_index.core.storage.index_store.keyval_index_store",
        "peekOfCode": "DEFAULT_NAMESPACE = \"index_store\"\nDEFAULT_COLLECTION_SUFFIX = \"/data\"\nclass KVIndexStore(BaseIndexStore):\n    \"\"\"\n    Key-Value Index store.\n    Args:\n        kvstore (BaseKVStore): key-value store\n        namespace (str): namespace for the index store\n        collection_suffix (str): suffix for the collection name\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.index_store.keyval_index_store",
        "documentation": {}
    },
    {
        "label": "DEFAULT_COLLECTION_SUFFIX",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.index_store.keyval_index_store",
        "description": "reference_code.llama-index-core.llama_index.core.storage.index_store.keyval_index_store",
        "peekOfCode": "DEFAULT_COLLECTION_SUFFIX = \"/data\"\nclass KVIndexStore(BaseIndexStore):\n    \"\"\"\n    Key-Value Index store.\n    Args:\n        kvstore (BaseKVStore): key-value store\n        namespace (str): namespace for the index store\n        collection_suffix (str): suffix for the collection name\n    \"\"\"\n    def __init__(",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.index_store.keyval_index_store",
        "documentation": {}
    },
    {
        "label": "SimpleIndexStore",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.index_store.simple_index_store",
        "description": "reference_code.llama-index-core.llama_index.core.storage.index_store.simple_index_store",
        "peekOfCode": "class SimpleIndexStore(KVIndexStore):\n    \"\"\"\n    Simple in-memory Index store.\n    Args:\n        simple_kvstore (SimpleKVStore): simple key-value store\n    \"\"\"\n    def __init__(\n        self,\n        simple_kvstore: Optional[SimpleKVStore] = None,\n    ) -> None:",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.index_store.simple_index_store",
        "documentation": {}
    },
    {
        "label": "BaseIndexStore",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.index_store.types",
        "description": "reference_code.llama-index-core.llama_index.core.storage.index_store.types",
        "peekOfCode": "class BaseIndexStore(ABC):\n    @abstractmethod\n    def index_structs(self) -> List[IndexStruct]:\n        pass\n    @abstractmethod\n    async def async_index_structs(self) -> List[IndexStruct]:\n        pass\n    @abstractmethod\n    def add_index_struct(self, index_struct: IndexStruct) -> None:\n        pass",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.index_store.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_DIR",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.index_store.types",
        "description": "reference_code.llama-index-core.llama_index.core.storage.index_store.types",
        "peekOfCode": "DEFAULT_PERSIST_DIR = \"./storage\"\nDEFAULT_PERSIST_FNAME = \"index_store.json\"\nDEFAULT_PERSIST_PATH = os.path.join(DEFAULT_PERSIST_DIR, DEFAULT_PERSIST_FNAME)\nclass BaseIndexStore(ABC):\n    @abstractmethod\n    def index_structs(self) -> List[IndexStruct]:\n        pass\n    @abstractmethod\n    async def async_index_structs(self) -> List[IndexStruct]:\n        pass",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.index_store.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_FNAME",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.index_store.types",
        "description": "reference_code.llama-index-core.llama_index.core.storage.index_store.types",
        "peekOfCode": "DEFAULT_PERSIST_FNAME = \"index_store.json\"\nDEFAULT_PERSIST_PATH = os.path.join(DEFAULT_PERSIST_DIR, DEFAULT_PERSIST_FNAME)\nclass BaseIndexStore(ABC):\n    @abstractmethod\n    def index_structs(self) -> List[IndexStruct]:\n        pass\n    @abstractmethod\n    async def async_index_structs(self) -> List[IndexStruct]:\n        pass\n    @abstractmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.index_store.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_PATH",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.index_store.types",
        "description": "reference_code.llama-index-core.llama_index.core.storage.index_store.types",
        "peekOfCode": "DEFAULT_PERSIST_PATH = os.path.join(DEFAULT_PERSIST_DIR, DEFAULT_PERSIST_FNAME)\nclass BaseIndexStore(ABC):\n    @abstractmethod\n    def index_structs(self) -> List[IndexStruct]:\n        pass\n    @abstractmethod\n    async def async_index_structs(self) -> List[IndexStruct]:\n        pass\n    @abstractmethod\n    def add_index_struct(self, index_struct: IndexStruct) -> None:",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.index_store.types",
        "documentation": {}
    },
    {
        "label": "index_struct_to_json",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.index_store.utils",
        "description": "reference_code.llama-index-core.llama_index.core.storage.index_store.utils",
        "peekOfCode": "def index_struct_to_json(index_struct: IndexStruct) -> dict:\n    return {\n        TYPE_KEY: index_struct.get_type(),\n        DATA_KEY: index_struct.to_json(),\n    }\ndef json_to_index_struct(struct_dict: dict) -> IndexStruct:\n    type = struct_dict[TYPE_KEY]\n    data_dict = struct_dict[DATA_KEY]\n    cls = INDEX_STRUCT_TYPE_TO_INDEX_STRUCT_CLASS[type]\n    try:",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.index_store.utils",
        "documentation": {}
    },
    {
        "label": "json_to_index_struct",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.index_store.utils",
        "description": "reference_code.llama-index-core.llama_index.core.storage.index_store.utils",
        "peekOfCode": "def json_to_index_struct(struct_dict: dict) -> IndexStruct:\n    type = struct_dict[TYPE_KEY]\n    data_dict = struct_dict[DATA_KEY]\n    cls = INDEX_STRUCT_TYPE_TO_INDEX_STRUCT_CLASS[type]\n    try:\n        return cls.from_json(data_dict)\n    except TypeError:\n        return cls.from_dict(data_dict)",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.index_store.utils",
        "documentation": {}
    },
    {
        "label": "SimpleKVStore",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.kvstore.simple_kvstore",
        "description": "reference_code.llama-index-core.llama_index.core.storage.kvstore.simple_kvstore",
        "peekOfCode": "class SimpleKVStore(MutableMappingKVStore[dict]):\n    \"\"\"\n    Simple in-memory Key-Value store.\n    Args:\n        data (Optional[DATA_TYPE]): data to initialize the store with\n    \"\"\"\n    def __init__(\n        self,\n        data: Optional[DATA_TYPE] = None,\n    ) -> None:",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.kvstore.simple_kvstore",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.kvstore.simple_kvstore",
        "description": "reference_code.llama-index-core.llama_index.core.storage.kvstore.simple_kvstore",
        "peekOfCode": "logger = logging.getLogger(__name__)\nDATA_TYPE = Dict[str, Dict[str, dict]]\nclass SimpleKVStore(MutableMappingKVStore[dict]):\n    \"\"\"\n    Simple in-memory Key-Value store.\n    Args:\n        data (Optional[DATA_TYPE]): data to initialize the store with\n    \"\"\"\n    def __init__(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.kvstore.simple_kvstore",
        "documentation": {}
    },
    {
        "label": "DATA_TYPE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.kvstore.simple_kvstore",
        "description": "reference_code.llama-index-core.llama_index.core.storage.kvstore.simple_kvstore",
        "peekOfCode": "DATA_TYPE = Dict[str, Dict[str, dict]]\nclass SimpleKVStore(MutableMappingKVStore[dict]):\n    \"\"\"\n    Simple in-memory Key-Value store.\n    Args:\n        data (Optional[DATA_TYPE]): data to initialize the store with\n    \"\"\"\n    def __init__(\n        self,\n        data: Optional[DATA_TYPE] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.kvstore.simple_kvstore",
        "documentation": {}
    },
    {
        "label": "BaseKVStore",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.kvstore.types",
        "description": "reference_code.llama-index-core.llama_index.core.storage.kvstore.types",
        "peekOfCode": "class BaseKVStore(ABC):\n    \"\"\"Base key-value store.\"\"\"\n    @abstractmethod\n    def put(self, key: str, val: dict, collection: str = DEFAULT_COLLECTION) -> None:\n        pass\n    @abstractmethod\n    async def aput(\n        self, key: str, val: dict, collection: str = DEFAULT_COLLECTION\n    ) -> None:\n        pass",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.kvstore.types",
        "documentation": {}
    },
    {
        "label": "BaseInMemoryKVStore",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.kvstore.types",
        "description": "reference_code.llama-index-core.llama_index.core.storage.kvstore.types",
        "peekOfCode": "class BaseInMemoryKVStore(BaseKVStore):\n    \"\"\"Base in-memory key-value store.\"\"\"\n    @abstractmethod\n    def persist(\n        self, persist_path: str, fs: Optional[fsspec.AbstractFileSystem] = None\n    ) -> None:\n        pass\n    @classmethod\n    @abstractmethod\n    def from_persist_path(cls, persist_path: str) -> \"BaseInMemoryKVStore\":",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.kvstore.types",
        "documentation": {}
    },
    {
        "label": "MutableMappingKVStore",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.kvstore.types",
        "description": "reference_code.llama-index-core.llama_index.core.storage.kvstore.types",
        "peekOfCode": "class MutableMappingKVStore(Generic[MutableMappingT], BaseKVStore):\n    \"\"\"\n    MutableMapping Key-Value store.\n    Args:\n        mapping_factory (Callable[[], MutableMapping[str, dict]): the mutable mapping factory\n    \"\"\"\n    def __init__(self, mapping_factory: Callable[[], MutableMappingT]) -> None:\n        \"\"\"Initialize a MutableMappingKVStore.\"\"\"\n        self._collections_mappings: Dict[str, MutableMappingT] = {}\n        self._mapping_factory = mapping_factory",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.kvstore.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_COLLECTION",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.kvstore.types",
        "description": "reference_code.llama-index-core.llama_index.core.storage.kvstore.types",
        "peekOfCode": "DEFAULT_COLLECTION = \"data\"\nDEFAULT_BATCH_SIZE = 1\nclass BaseKVStore(ABC):\n    \"\"\"Base key-value store.\"\"\"\n    @abstractmethod\n    def put(self, key: str, val: dict, collection: str = DEFAULT_COLLECTION) -> None:\n        pass\n    @abstractmethod\n    async def aput(\n        self, key: str, val: dict, collection: str = DEFAULT_COLLECTION",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.kvstore.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_BATCH_SIZE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.kvstore.types",
        "description": "reference_code.llama-index-core.llama_index.core.storage.kvstore.types",
        "peekOfCode": "DEFAULT_BATCH_SIZE = 1\nclass BaseKVStore(ABC):\n    \"\"\"Base key-value store.\"\"\"\n    @abstractmethod\n    def put(self, key: str, val: dict, collection: str = DEFAULT_COLLECTION) -> None:\n        pass\n    @abstractmethod\n    async def aput(\n        self, key: str, val: dict, collection: str = DEFAULT_COLLECTION\n    ) -> None:",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.kvstore.types",
        "documentation": {}
    },
    {
        "label": "MutableMappingT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.kvstore.types",
        "description": "reference_code.llama-index-core.llama_index.core.storage.kvstore.types",
        "peekOfCode": "MutableMappingT = TypeVar(\"MutableMappingT\", bound=MutableMapping[str, dict])\nclass MutableMappingKVStore(Generic[MutableMappingT], BaseKVStore):\n    \"\"\"\n    MutableMapping Key-Value store.\n    Args:\n        mapping_factory (Callable[[], MutableMapping[str, dict]): the mutable mapping factory\n    \"\"\"\n    def __init__(self, mapping_factory: Callable[[], MutableMappingT]) -> None:\n        \"\"\"Initialize a MutableMappingKVStore.\"\"\"\n        self._collections_mappings: Dict[str, MutableMappingT] = {}",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.kvstore.types",
        "documentation": {}
    },
    {
        "label": "StorageContext",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.storage_context",
        "description": "reference_code.llama-index-core.llama_index.core.storage.storage_context",
        "peekOfCode": "class StorageContext:\n    \"\"\"\n    Storage context.\n    The storage context container is a utility container for storing nodes,\n    indices, and vectors. It contains the following:\n    - docstore: BaseDocumentStore\n    - index_store: BaseIndexStore\n    - vector_store: BasePydanticVectorStore\n    - graph_store: GraphStore\n    - property_graph_store: PropertyGraphStore (lazily initialized)",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_DIR",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.storage_context",
        "description": "reference_code.llama-index-core.llama_index.core.storage.storage_context",
        "peekOfCode": "DEFAULT_PERSIST_DIR = \"./storage\"\nIMAGE_STORE_FNAME = \"image_store.json\"\nIMAGE_VECTOR_STORE_NAMESPACE = \"image\"\n@dataclass\nclass StorageContext:\n    \"\"\"\n    Storage context.\n    The storage context container is a utility container for storing nodes,\n    indices, and vectors. It contains the following:\n    - docstore: BaseDocumentStore",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "IMAGE_STORE_FNAME",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.storage_context",
        "description": "reference_code.llama-index-core.llama_index.core.storage.storage_context",
        "peekOfCode": "IMAGE_STORE_FNAME = \"image_store.json\"\nIMAGE_VECTOR_STORE_NAMESPACE = \"image\"\n@dataclass\nclass StorageContext:\n    \"\"\"\n    Storage context.\n    The storage context container is a utility container for storing nodes,\n    indices, and vectors. It contains the following:\n    - docstore: BaseDocumentStore\n    - index_store: BaseIndexStore",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "IMAGE_VECTOR_STORE_NAMESPACE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.storage.storage_context",
        "description": "reference_code.llama-index-core.llama_index.core.storage.storage_context",
        "peekOfCode": "IMAGE_VECTOR_STORE_NAMESPACE = \"image\"\n@dataclass\nclass StorageContext:\n    \"\"\"\n    Storage context.\n    The storage context container is a utility container for storing nodes,\n    indices, and vectors. It contains the following:\n    - docstore: BaseDocumentStore\n    - index_store: BaseIndexStore\n    - vector_store: BasePydanticVectorStore",
        "detail": "reference_code.llama-index-core.llama_index.core.storage.storage_context",
        "documentation": {}
    },
    {
        "label": "LoadAndSearchToolSpec",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.tool_spec.load_and_search.base",
        "description": "reference_code.llama-index-core.llama_index.core.tools.tool_spec.load_and_search.base",
        "peekOfCode": "class LoadAndSearchToolSpec(BaseToolSpec):\n    \"\"\"\n    Load and Search Tool.\n    This tool can be used with other tools that load large amounts of\n    information. Compared to OndemandLoaderTool this returns two tools,\n    one to retrieve data to an index and another to allow the Agent to search\n    the retrieved data with a natural language query string.\n    \"\"\"\n    loader_prompt = \"\"\"\n        Use this tool to load data from the following function. It must then be read from",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.tool_spec.load_and_search.base",
        "documentation": {}
    },
    {
        "label": "BaseToolSpec",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.tool_spec.base",
        "description": "reference_code.llama-index-core.llama_index.core.tools.tool_spec.base",
        "peekOfCode": "class BaseToolSpec:\n    \"\"\"Base tool spec class.\"\"\"\n    # list of functions that you'd want to convert to spec\n    spec_functions: List[SPEC_FUNCTION_TYPE]\n    def get_fn_schema_from_fn_name(\n        self, fn_name: str, spec_functions: Optional[List[SPEC_FUNCTION_TYPE]] = None\n    ) -> Optional[Type[BaseModel]]:\n        \"\"\"\n        NOTE: This function is deprecated and kept only for backwards compatibility.\n        Return map from function name.",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.tool_spec.base",
        "documentation": {}
    },
    {
        "label": "AsyncCallable",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.tool_spec.base",
        "description": "reference_code.llama-index-core.llama_index.core.tools.tool_spec.base",
        "peekOfCode": "AsyncCallable = Callable[..., Awaitable[Any]]\n# TODO: deprecate the Tuple (there's no use for it)\nSPEC_FUNCTION_TYPE = Union[str, Tuple[str, str]]\nclass BaseToolSpec:\n    \"\"\"Base tool spec class.\"\"\"\n    # list of functions that you'd want to convert to spec\n    spec_functions: List[SPEC_FUNCTION_TYPE]\n    def get_fn_schema_from_fn_name(\n        self, fn_name: str, spec_functions: Optional[List[SPEC_FUNCTION_TYPE]] = None\n    ) -> Optional[Type[BaseModel]]:",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.tool_spec.base",
        "documentation": {}
    },
    {
        "label": "SPEC_FUNCTION_TYPE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.tool_spec.base",
        "description": "reference_code.llama-index-core.llama_index.core.tools.tool_spec.base",
        "peekOfCode": "SPEC_FUNCTION_TYPE = Union[str, Tuple[str, str]]\nclass BaseToolSpec:\n    \"\"\"Base tool spec class.\"\"\"\n    # list of functions that you'd want to convert to spec\n    spec_functions: List[SPEC_FUNCTION_TYPE]\n    def get_fn_schema_from_fn_name(\n        self, fn_name: str, spec_functions: Optional[List[SPEC_FUNCTION_TYPE]] = None\n    ) -> Optional[Type[BaseModel]]:\n        \"\"\"\n        NOTE: This function is deprecated and kept only for backwards compatibility.",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.tool_spec.base",
        "documentation": {}
    },
    {
        "label": "call_tool",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.calling",
        "description": "reference_code.llama-index-core.llama_index.core.tools.calling",
        "peekOfCode": "def call_tool(tool: BaseTool, arguments: dict) -> ToolOutput:\n    \"\"\"Call a tool with arguments.\"\"\"\n    try:\n        if (\n            len(tool.metadata.get_parameters_dict()[\"properties\"]) == 1\n            and len(arguments) == 1\n        ):\n            try:\n                single_arg = arguments[next(iter(arguments))]\n                return tool(single_arg)",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.calling",
        "documentation": {}
    },
    {
        "label": "call_tool_with_selection",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.calling",
        "description": "reference_code.llama-index-core.llama_index.core.tools.calling",
        "peekOfCode": "def call_tool_with_selection(\n    tool_call: ToolSelection,\n    tools: Sequence[\"BaseTool\"],\n    verbose: bool = False,\n) -> ToolOutput:\n    from llama_index.core.tools.calling import call_tool\n    tools_by_name = {tool.metadata.name: tool for tool in tools}\n    name = tool_call.tool_name\n    if verbose:\n        arguments_str = json.dumps(tool_call.tool_kwargs)",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.calling",
        "documentation": {}
    },
    {
        "label": "download_tool",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.download",
        "description": "reference_code.llama-index-core.llama_index.core.tools.download",
        "peekOfCode": "def download_tool(\n    tool_class: str,\n    llama_hub_url: str = \"\",\n    refresh_cache: bool = False,\n    custom_path: Optional[str] = None,\n) -> Type[BaseToolSpec]:\n    \"\"\"\n    Download a single tool from Llama Hub.\n    Args:\n        tool_class: The name of the tool class you want to download,",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.download",
        "documentation": {}
    },
    {
        "label": "EvalQueryEngineTool",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.eval_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.tools.eval_query_engine",
        "peekOfCode": "class EvalQueryEngineTool(QueryEngineTool):\n    \"\"\"\n    Evaluating query engine tool.\n    A tool that makes use of a query engine and an evaluator, where the\n    evaluation of the query engine response will determine the tool output.\n    Args:\n        evaluator (BaseEvaluator): A query engine.\n        query_engine (BaseQueryEngine): A query engine.\n        metadata (ToolMetadata): The associated metadata of the query engine.\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.eval_query_engine",
        "documentation": {}
    },
    {
        "label": "DEFAULT_NAME",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.eval_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.tools.eval_query_engine",
        "peekOfCode": "DEFAULT_NAME = \"query_engine_tool\"\nDEFAULT_DESCRIPTION = \"\"\"Useful for running a natural language query\nagainst a knowledge base and get back a natural language response.\n\"\"\"\nFAILED_TOOL_OUTPUT_TEMPLATE = (\n    \"Could not use tool {tool_name} because it failed evaluation.\\nReason: {reason}\"\n)\nclass EvalQueryEngineTool(QueryEngineTool):\n    \"\"\"\n    Evaluating query engine tool.",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.eval_query_engine",
        "documentation": {}
    },
    {
        "label": "DEFAULT_DESCRIPTION",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.eval_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.tools.eval_query_engine",
        "peekOfCode": "DEFAULT_DESCRIPTION = \"\"\"Useful for running a natural language query\nagainst a knowledge base and get back a natural language response.\n\"\"\"\nFAILED_TOOL_OUTPUT_TEMPLATE = (\n    \"Could not use tool {tool_name} because it failed evaluation.\\nReason: {reason}\"\n)\nclass EvalQueryEngineTool(QueryEngineTool):\n    \"\"\"\n    Evaluating query engine tool.\n    A tool that makes use of a query engine and an evaluator, where the",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.eval_query_engine",
        "documentation": {}
    },
    {
        "label": "FAILED_TOOL_OUTPUT_TEMPLATE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.eval_query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.tools.eval_query_engine",
        "peekOfCode": "FAILED_TOOL_OUTPUT_TEMPLATE = (\n    \"Could not use tool {tool_name} because it failed evaluation.\\nReason: {reason}\"\n)\nclass EvalQueryEngineTool(QueryEngineTool):\n    \"\"\"\n    Evaluating query engine tool.\n    A tool that makes use of a query engine and an evaluator, where the\n    evaluation of the query engine response will determine the tool output.\n    Args:\n        evaluator (BaseEvaluator): A query engine.",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.eval_query_engine",
        "documentation": {}
    },
    {
        "label": "FunctionTool",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.function_tool",
        "description": "reference_code.llama-index-core.llama_index.core.tools.function_tool",
        "peekOfCode": "class FunctionTool(AsyncBaseTool):\n    \"\"\"\n    Function Tool.\n    A tool that takes in a function, optionally handles workflow context,\n    and allows the use of callbacks. The callback can return a new ToolOutput\n    to override the default one or a string that will be used as the final content.\n    \"\"\"\n    def __init__(\n        self,\n        fn: Optional[Callable[..., Any]] = None,",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.function_tool",
        "documentation": {}
    },
    {
        "label": "sync_to_async",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.function_tool",
        "description": "reference_code.llama-index-core.llama_index.core.tools.function_tool",
        "peekOfCode": "def sync_to_async(fn: Callable[..., Any]) -> AsyncCallable:\n    \"\"\"Sync to async.\"\"\"\n    async def _async_wrapped_fn(*args: Any, **kwargs: Any) -> Any:\n        loop = asyncio.get_running_loop()\n        return await loop.run_in_executor(None, lambda: fn(*args, **kwargs))\n    return _async_wrapped_fn\ndef async_to_sync(func_async: AsyncCallable) -> Callable:\n    \"\"\"Async to sync.\"\"\"\n    def _sync_wrapped_fn(*args: Any, **kwargs: Any) -> Any:\n        return asyncio_run(func_async(*args, **kwargs))  # type: ignore[arg-type]",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.function_tool",
        "documentation": {}
    },
    {
        "label": "async_to_sync",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.function_tool",
        "description": "reference_code.llama-index-core.llama_index.core.tools.function_tool",
        "peekOfCode": "def async_to_sync(func_async: AsyncCallable) -> Callable:\n    \"\"\"Async to sync.\"\"\"\n    def _sync_wrapped_fn(*args: Any, **kwargs: Any) -> Any:\n        return asyncio_run(func_async(*args, **kwargs))  # type: ignore[arg-type]\n    return _sync_wrapped_fn\n# The type that the callback can return: either a ToolOutput instance or a string to override the content.\nCallbackReturn = Optional[Union[ToolOutput, str]]\nclass FunctionTool(AsyncBaseTool):\n    \"\"\"\n    Function Tool.",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.function_tool",
        "documentation": {}
    },
    {
        "label": "AsyncCallable",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.function_tool",
        "description": "reference_code.llama-index-core.llama_index.core.tools.function_tool",
        "peekOfCode": "AsyncCallable = Callable[..., Awaitable[Any]]\ndef _is_context_param(param_annotation: Any) -> bool:\n    \"\"\"Check if a parameter annotation is Context or Context[SomeType].\"\"\"\n    return param_annotation == Context or (get_origin(param_annotation) is Context)\ndef sync_to_async(fn: Callable[..., Any]) -> AsyncCallable:\n    \"\"\"Sync to async.\"\"\"\n    async def _async_wrapped_fn(*args: Any, **kwargs: Any) -> Any:\n        loop = asyncio.get_running_loop()\n        return await loop.run_in_executor(None, lambda: fn(*args, **kwargs))\n    return _async_wrapped_fn",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.function_tool",
        "documentation": {}
    },
    {
        "label": "CallbackReturn",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.function_tool",
        "description": "reference_code.llama-index-core.llama_index.core.tools.function_tool",
        "peekOfCode": "CallbackReturn = Optional[Union[ToolOutput, str]]\nclass FunctionTool(AsyncBaseTool):\n    \"\"\"\n    Function Tool.\n    A tool that takes in a function, optionally handles workflow context,\n    and allows the use of callbacks. The callback can return a new ToolOutput\n    to override the default one or a string that will be used as the final content.\n    \"\"\"\n    def __init__(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.function_tool",
        "documentation": {}
    },
    {
        "label": "OnDemandLoaderTool",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.ondemand_loader_tool",
        "description": "reference_code.llama-index-core.llama_index.core.tools.ondemand_loader_tool",
        "peekOfCode": "class OnDemandLoaderTool(AsyncBaseTool):\n    \"\"\"\n    On-demand data loader tool.\n    Loads data with by calling the provided loader function,\n    stores in index, and queries for relevant data with a\n    natural language query string.\n    \"\"\"\n    def __init__(\n        self,\n        loader: Callable[..., List[Document]],",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.ondemand_loader_tool",
        "documentation": {}
    },
    {
        "label": "QueryEngineTool",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.tools.query_engine",
        "peekOfCode": "class QueryEngineTool(AsyncBaseTool):\n    \"\"\"\n    Query engine tool.\n    A tool making use of a query engine.\n    Args:\n        query_engine (BaseQueryEngine): A query engine.\n        metadata (ToolMetadata): The associated metadata of the query engine.\n    \"\"\"\n    def __init__(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.query_engine",
        "documentation": {}
    },
    {
        "label": "DEFAULT_NAME",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.tools.query_engine",
        "peekOfCode": "DEFAULT_NAME = \"query_engine_tool\"\nDEFAULT_DESCRIPTION = \"\"\"Useful for running a natural language query\nagainst a knowledge base and get back a natural language response.\n\"\"\"\nclass QueryEngineTool(AsyncBaseTool):\n    \"\"\"\n    Query engine tool.\n    A tool making use of a query engine.\n    Args:\n        query_engine (BaseQueryEngine): A query engine.",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.query_engine",
        "documentation": {}
    },
    {
        "label": "DEFAULT_DESCRIPTION",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.query_engine",
        "description": "reference_code.llama-index-core.llama_index.core.tools.query_engine",
        "peekOfCode": "DEFAULT_DESCRIPTION = \"\"\"Useful for running a natural language query\nagainst a knowledge base and get back a natural language response.\n\"\"\"\nclass QueryEngineTool(AsyncBaseTool):\n    \"\"\"\n    Query engine tool.\n    A tool making use of a query engine.\n    Args:\n        query_engine (BaseQueryEngine): A query engine.\n        metadata (ToolMetadata): The associated metadata of the query engine.",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.query_engine",
        "documentation": {}
    },
    {
        "label": "QueryNode",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.query_plan",
        "description": "reference_code.llama-index-core.llama_index.core.tools.query_plan",
        "peekOfCode": "class QueryNode(BaseModel):\n    \"\"\"\n    Query node.\n    A query node represents a query (query_str) that must be answered.\n    It can either be answered by a tool (tool_name), or by a list of child nodes\n    (child_nodes).\n    The tool_name and child_nodes fields are mutually exclusive.\n    \"\"\"\n    # NOTE: inspired from https://github.com/jxnl/openai_function_call/pull/3/files\n    id: int = Field(..., description=\"ID of the query node.\")",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.query_plan",
        "documentation": {}
    },
    {
        "label": "QueryPlan",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.query_plan",
        "description": "reference_code.llama-index-core.llama_index.core.tools.query_plan",
        "peekOfCode": "class QueryPlan(BaseModel):\n    \"\"\"\n    Query plan.\n    Contains a list of QueryNode objects (which is a recursive object).\n    Out of the list of QueryNode objects, one of them must be the root node.\n    The root node is the one that isn't a dependency of any other node.\n    \"\"\"\n    nodes: List[QueryNode] = Field(\n        ...,\n        description=\"The original question we are asking.\",",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.query_plan",
        "documentation": {}
    },
    {
        "label": "QueryPlanTool",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.query_plan",
        "description": "reference_code.llama-index-core.llama_index.core.tools.query_plan",
        "peekOfCode": "class QueryPlanTool(BaseTool):\n    \"\"\"\n    Query plan tool.\n    A tool that takes in a list of tools and executes a query plan.\n    \"\"\"\n    def __init__(\n        self,\n        query_engine_tools: List[BaseTool],\n        response_synthesizer: BaseSynthesizer,\n        name: str,",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.query_plan",
        "documentation": {}
    },
    {
        "label": "DEFAULT_NAME",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.query_plan",
        "description": "reference_code.llama-index-core.llama_index.core.tools.query_plan",
        "peekOfCode": "DEFAULT_NAME = \"query_plan_tool\"\nQUERYNODE_QUERY_STR_DESC = \"\"\"\\\nQuestion we are asking. This is the query string that will be executed. \\\n\"\"\"\nQUERYNODE_TOOL_NAME_DESC = \"\"\"\\\nName of the tool to execute the `query_str`. \\\nShould NOT be specified if there are subquestions to be specified, in which \\\ncase child_nodes should be nonempty instead.\\\n\"\"\"\nQUERYNODE_DEPENDENCIES_DESC = \"\"\"\\",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.query_plan",
        "documentation": {}
    },
    {
        "label": "QUERYNODE_QUERY_STR_DESC",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.query_plan",
        "description": "reference_code.llama-index-core.llama_index.core.tools.query_plan",
        "peekOfCode": "QUERYNODE_QUERY_STR_DESC = \"\"\"\\\nQuestion we are asking. This is the query string that will be executed. \\\n\"\"\"\nQUERYNODE_TOOL_NAME_DESC = \"\"\"\\\nName of the tool to execute the `query_str`. \\\nShould NOT be specified if there are subquestions to be specified, in which \\\ncase child_nodes should be nonempty instead.\\\n\"\"\"\nQUERYNODE_DEPENDENCIES_DESC = \"\"\"\\\nList of sub-questions that need to be answered in order \\",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.query_plan",
        "documentation": {}
    },
    {
        "label": "QUERYNODE_TOOL_NAME_DESC",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.query_plan",
        "description": "reference_code.llama-index-core.llama_index.core.tools.query_plan",
        "peekOfCode": "QUERYNODE_TOOL_NAME_DESC = \"\"\"\\\nName of the tool to execute the `query_str`. \\\nShould NOT be specified if there are subquestions to be specified, in which \\\ncase child_nodes should be nonempty instead.\\\n\"\"\"\nQUERYNODE_DEPENDENCIES_DESC = \"\"\"\\\nList of sub-questions that need to be answered in order \\\nto answer the question given by `query_str`.\\\nShould be blank if there are no sub-questions to be specified, in which case \\\n`tool_name` is specified.\\",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.query_plan",
        "documentation": {}
    },
    {
        "label": "QUERYNODE_DEPENDENCIES_DESC",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.query_plan",
        "description": "reference_code.llama-index-core.llama_index.core.tools.query_plan",
        "peekOfCode": "QUERYNODE_DEPENDENCIES_DESC = \"\"\"\\\nList of sub-questions that need to be answered in order \\\nto answer the question given by `query_str`.\\\nShould be blank if there are no sub-questions to be specified, in which case \\\n`tool_name` is specified.\\\n\"\"\"\nclass QueryNode(BaseModel):\n    \"\"\"\n    Query node.\n    A query node represents a query (query_str) that must be answered.",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.query_plan",
        "documentation": {}
    },
    {
        "label": "DEFAULT_DESCRIPTION_PREFIX",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.query_plan",
        "description": "reference_code.llama-index-core.llama_index.core.tools.query_plan",
        "peekOfCode": "DEFAULT_DESCRIPTION_PREFIX = \"\"\"\\\nThis is a query plan tool that takes in a list of tools and executes a \\\nquery plan over these tools to answer a query. The query plan is a DAG of query nodes.\nGiven a list of tool names and the query plan schema, you \\\ncan choose to generate a query plan to answer a question.\nThe tool names and descriptions are as follows:\n\"\"\"\nclass QueryPlanTool(BaseTool):\n    \"\"\"\n    Query plan tool.",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.query_plan",
        "documentation": {}
    },
    {
        "label": "RetrieverTool",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.retriever_tool",
        "description": "reference_code.llama-index-core.llama_index.core.tools.retriever_tool",
        "peekOfCode": "class RetrieverTool(AsyncBaseTool):\n    \"\"\"\n    Retriever tool.\n    A tool making use of a retriever.\n    Args:\n        retriever (BaseRetriever): A retriever.\n        metadata (ToolMetadata): The associated metadata of the query engine.\n        node_postprocessors (Optional[List[BaseNodePostprocessor]]): A list of\n            node postprocessors.\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.retriever_tool",
        "documentation": {}
    },
    {
        "label": "DEFAULT_NAME",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.retriever_tool",
        "description": "reference_code.llama-index-core.llama_index.core.tools.retriever_tool",
        "peekOfCode": "DEFAULT_NAME = \"retriever_tool\"\nDEFAULT_DESCRIPTION = \"\"\"Useful for running a natural language query\nagainst a knowledge base and retrieving a set of relevant documents.\n\"\"\"\nclass RetrieverTool(AsyncBaseTool):\n    \"\"\"\n    Retriever tool.\n    A tool making use of a retriever.\n    Args:\n        retriever (BaseRetriever): A retriever.",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.retriever_tool",
        "documentation": {}
    },
    {
        "label": "DEFAULT_DESCRIPTION",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.retriever_tool",
        "description": "reference_code.llama-index-core.llama_index.core.tools.retriever_tool",
        "peekOfCode": "DEFAULT_DESCRIPTION = \"\"\"Useful for running a natural language query\nagainst a knowledge base and retrieving a set of relevant documents.\n\"\"\"\nclass RetrieverTool(AsyncBaseTool):\n    \"\"\"\n    Retriever tool.\n    A tool making use of a retriever.\n    Args:\n        retriever (BaseRetriever): A retriever.\n        metadata (ToolMetadata): The associated metadata of the query engine.",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.retriever_tool",
        "documentation": {}
    },
    {
        "label": "DefaultToolFnSchema",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.types",
        "description": "reference_code.llama-index-core.llama_index.core.tools.types",
        "peekOfCode": "class DefaultToolFnSchema(BaseModel):\n    \"\"\"Default tool function Schema.\"\"\"\n    input: str\n@dataclass\nclass ToolMetadata:\n    description: str\n    name: Optional[str] = None\n    fn_schema: Optional[Type[BaseModel]] = DefaultToolFnSchema\n    return_direct: bool = False\n    def get_parameters_dict(self) -> dict:",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "ToolMetadata",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.types",
        "description": "reference_code.llama-index-core.llama_index.core.tools.types",
        "peekOfCode": "class ToolMetadata:\n    description: str\n    name: Optional[str] = None\n    fn_schema: Optional[Type[BaseModel]] = DefaultToolFnSchema\n    return_direct: bool = False\n    def get_parameters_dict(self) -> dict:\n        if self.fn_schema is None:\n            parameters = {\n                \"type\": \"object\",\n                \"properties\": {",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "ToolOutput",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.types",
        "description": "reference_code.llama-index-core.llama_index.core.tools.types",
        "peekOfCode": "class ToolOutput(BaseModel):\n    \"\"\"Tool output.\"\"\"\n    blocks: List[ContentBlock]\n    tool_name: str\n    raw_input: Dict[str, Any]\n    raw_output: Any\n    is_error: bool = False\n    def __init__(\n        self,\n        tool_name: str,",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "BaseTool",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.types",
        "description": "reference_code.llama-index-core.llama_index.core.tools.types",
        "peekOfCode": "class BaseTool(DispatcherSpanMixin):\n    @property\n    @abstractmethod\n    def metadata(self) -> ToolMetadata:\n        pass\n    @abstractmethod\n    def __call__(self, input: Any) -> ToolOutput:\n        pass\n    def _process_langchain_tool_kwargs(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "AsyncBaseTool",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.types",
        "description": "reference_code.llama-index-core.llama_index.core.tools.types",
        "peekOfCode": "class AsyncBaseTool(BaseTool):\n    \"\"\"\n    Base-level tool class that is backwards compatible with the old tool spec but also\n    supports async.\n    \"\"\"\n    def __call__(self, *args: Any, **kwargs: Any) -> ToolOutput:\n        return self.call(*args, **kwargs)\n    @abstractmethod\n    def call(self, input: Any) -> ToolOutput:\n        \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "BaseToolAsyncAdapter",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.types",
        "description": "reference_code.llama-index-core.llama_index.core.tools.types",
        "peekOfCode": "class BaseToolAsyncAdapter(AsyncBaseTool):\n    \"\"\"\n    Adapter class that allows a synchronous tool to be used as an async tool.\n    \"\"\"\n    def __init__(self, tool: BaseTool):\n        self.base_tool = tool\n    @property\n    def metadata(self) -> ToolMetadata:\n        return self.base_tool.metadata\n    def call(self, input: Any) -> ToolOutput:",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "adapt_to_async_tool",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.types",
        "description": "reference_code.llama-index-core.llama_index.core.tools.types",
        "peekOfCode": "def adapt_to_async_tool(tool: BaseTool) -> AsyncBaseTool:\n    \"\"\"\n    Converts a synchronous tool to an async tool.\n    \"\"\"\n    if isinstance(tool, AsyncBaseTool):\n        return tool\n    else:\n        return BaseToolAsyncAdapter(tool)",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.types",
        "documentation": {}
    },
    {
        "label": "create_schema_from_function",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.tools.utils",
        "description": "reference_code.llama-index-core.llama_index.core.tools.utils",
        "peekOfCode": "def create_schema_from_function(\n    name: str,\n    func: Union[Callable[..., Any], Callable[..., Awaitable[Any]]],\n    additional_fields: Optional[\n        List[Union[Tuple[str, Type, Any], Tuple[str, Type]]]\n    ] = None,\n    ignore_fields: Optional[List[str]] = None,\n) -> Type[BaseModel]:\n    \"\"\"\n    Create schema from function.",
        "detail": "reference_code.llama-index-core.llama_index.core.tools.utils",
        "documentation": {}
    },
    {
        "label": "get_aws_service_client",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.utilities.aws_utils",
        "description": "reference_code.llama-index-core.llama_index.core.utilities.aws_utils",
        "peekOfCode": "def get_aws_service_client(\n    service_name: Optional[str] = None,\n    region_name: Optional[str] = None,\n    aws_access_key_id: Optional[str] = None,\n    aws_secret_access_key: Optional[str] = None,\n    aws_session_token: Optional[str] = None,\n    profile_name: Optional[str] = None,\n    max_retries: Optional[int] = 3,\n    timeout: Optional[float] = 60.0,\n) -> \"botocore.client.BaseClient\":",
        "detail": "reference_code.llama-index-core.llama_index.core.utilities.aws_utils",
        "documentation": {}
    },
    {
        "label": "merge_neighboring_same_role_messages",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.utilities.gemini_utils",
        "description": "reference_code.llama-index-core.llama_index.core.utilities.gemini_utils",
        "peekOfCode": "def merge_neighboring_same_role_messages(\n    messages: Sequence[ChatMessage],\n) -> Sequence[ChatMessage]:\n    if len(messages) < 2:\n        # Nothing to merge\n        return messages\n    # Gemini does not support multiple messages of the same role in a row, so we merge them\n    merged_messages = []\n    i = 0\n    while i < len(messages):",
        "detail": "reference_code.llama-index-core.llama_index.core.utilities.gemini_utils",
        "documentation": {}
    },
    {
        "label": "SQLDatabase",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.utilities.sql_wrapper",
        "description": "reference_code.llama-index-core.llama_index.core.utilities.sql_wrapper",
        "peekOfCode": "class SQLDatabase:\n    \"\"\"\n    SQL Database.\n    This class provides a wrapper around the SQLAlchemy engine to interact with a SQL\n    database.\n    It provides methods to execute SQL commands, insert data into tables, and retrieve\n    information about the database schema.\n    It also supports optional features such as including or excluding specific tables,\n    sampling rows for table info,\n    including indexes in table info, and supporting views.",
        "detail": "reference_code.llama-index-core.llama_index.core.utilities.sql_wrapper",
        "documentation": {}
    },
    {
        "label": "TokenCounter",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.utilities.token_counting",
        "description": "reference_code.llama-index-core.llama_index.core.utilities.token_counting",
        "peekOfCode": "class TokenCounter:\n    \"\"\"\n    Token counter class.\n    Attributes:\n        model (Optional[str]): The model to use for token counting.\n    \"\"\"\n    def __init__(self, tokenizer: Optional[Callable[[str], list]] = None) -> None:\n        self.tokenizer = tokenizer or get_tokenizer()\n    def get_string_tokens(self, string: str) -> int:\n        \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.utilities.token_counting",
        "documentation": {}
    },
    {
        "label": "SimpleVectorStoreData",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.simple",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.simple",
        "peekOfCode": "class SimpleVectorStoreData(DataClassJsonMixin):\n    \"\"\"\n    Simple Vector Store Data container.\n    Args:\n        embedding_dict (Optional[dict]): dict mapping node_ids to embeddings.\n        text_id_to_ref_doc_id (Optional[dict]):\n            dict mapping text_ids/node_ids to ref_doc_ids.\n    \"\"\"\n    embedding_dict: Dict[str, List[float]] = field(default_factory=dict)\n    text_id_to_ref_doc_id: Dict[str, str] = field(default_factory=dict)",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.simple",
        "documentation": {}
    },
    {
        "label": "SimpleVectorStore",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.simple",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.simple",
        "peekOfCode": "class SimpleVectorStore(BasePydanticVectorStore):\n    \"\"\"\n    Simple Vector Store.\n    In this vector store, embeddings are stored within a simple, in-memory dictionary.\n    Args:\n        simple_vector_store_data_dict (Optional[dict]): data dict\n            containing the embeddings and doc_ids. See SimpleVectorStoreData\n            for more details.\n    \"\"\"\n    stores_text: bool = False",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.simple",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.simple",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.simple",
        "peekOfCode": "logger = logging.getLogger(__name__)\nLEARNER_MODES = {\n    VectorStoreQueryMode.SVM,\n    VectorStoreQueryMode.LINEAR_REGRESSION,\n    VectorStoreQueryMode.LOGISTIC_REGRESSION,\n}\nMMR_MODE = VectorStoreQueryMode.MMR\nNAMESPACE_SEP = \"__\"\nDEFAULT_VECTOR_STORE = \"default\"\n@dataclass",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.simple",
        "documentation": {}
    },
    {
        "label": "LEARNER_MODES",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.simple",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.simple",
        "peekOfCode": "LEARNER_MODES = {\n    VectorStoreQueryMode.SVM,\n    VectorStoreQueryMode.LINEAR_REGRESSION,\n    VectorStoreQueryMode.LOGISTIC_REGRESSION,\n}\nMMR_MODE = VectorStoreQueryMode.MMR\nNAMESPACE_SEP = \"__\"\nDEFAULT_VECTOR_STORE = \"default\"\n@dataclass\nclass SimpleVectorStoreData(DataClassJsonMixin):",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.simple",
        "documentation": {}
    },
    {
        "label": "MMR_MODE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.simple",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.simple",
        "peekOfCode": "MMR_MODE = VectorStoreQueryMode.MMR\nNAMESPACE_SEP = \"__\"\nDEFAULT_VECTOR_STORE = \"default\"\n@dataclass\nclass SimpleVectorStoreData(DataClassJsonMixin):\n    \"\"\"\n    Simple Vector Store Data container.\n    Args:\n        embedding_dict (Optional[dict]): dict mapping node_ids to embeddings.\n        text_id_to_ref_doc_id (Optional[dict]):",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.simple",
        "documentation": {}
    },
    {
        "label": "NAMESPACE_SEP",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.simple",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.simple",
        "peekOfCode": "NAMESPACE_SEP = \"__\"\nDEFAULT_VECTOR_STORE = \"default\"\n@dataclass\nclass SimpleVectorStoreData(DataClassJsonMixin):\n    \"\"\"\n    Simple Vector Store Data container.\n    Args:\n        embedding_dict (Optional[dict]): dict mapping node_ids to embeddings.\n        text_id_to_ref_doc_id (Optional[dict]):\n            dict mapping text_ids/node_ids to ref_doc_ids.",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.simple",
        "documentation": {}
    },
    {
        "label": "DEFAULT_VECTOR_STORE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.simple",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.simple",
        "peekOfCode": "DEFAULT_VECTOR_STORE = \"default\"\n@dataclass\nclass SimpleVectorStoreData(DataClassJsonMixin):\n    \"\"\"\n    Simple Vector Store Data container.\n    Args:\n        embedding_dict (Optional[dict]): dict mapping node_ids to embeddings.\n        text_id_to_ref_doc_id (Optional[dict]):\n            dict mapping text_ids/node_ids to ref_doc_ids.\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.simple",
        "documentation": {}
    },
    {
        "label": "VectorStoreQueryResult",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "peekOfCode": "class VectorStoreQueryResult:\n    \"\"\"Vector store query result.\"\"\"\n    nodes: Optional[Sequence[BaseNode]] = None\n    similarities: Optional[List[float]] = None\n    ids: Optional[List[str]] = None\nclass VectorStoreQueryMode(str, Enum):\n    \"\"\"Vector store query mode.\"\"\"\n    DEFAULT = \"default\"\n    SPARSE = \"sparse\"\n    HYBRID = \"hybrid\"",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQueryMode",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "peekOfCode": "class VectorStoreQueryMode(str, Enum):\n    \"\"\"Vector store query mode.\"\"\"\n    DEFAULT = \"default\"\n    SPARSE = \"sparse\"\n    HYBRID = \"hybrid\"\n    TEXT_SEARCH = \"text_search\"\n    SEMANTIC_HYBRID = \"semantic_hybrid\"\n    # fit learners\n    SVM = \"svm\"\n    LOGISTIC_REGRESSION = \"logistic_regression\"",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "FilterOperator",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "peekOfCode": "class FilterOperator(str, Enum):\n    \"\"\"Vector store filter operator.\"\"\"\n    # TODO add more operators\n    EQ = \"==\"  # default operator (string, int, float)\n    GT = \">\"  # greater than (int, float)\n    LT = \"<\"  # less than (int, float)\n    NE = \"!=\"  # not equal to (string, int, float)\n    GTE = \">=\"  # greater than or equal to (int, float)\n    LTE = \"<=\"  # less than or equal to (int, float)\n    IN = \"in\"  # In array (string or number)",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "FilterCondition",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "peekOfCode": "class FilterCondition(str, Enum):\n    \"\"\"Vector store filter conditions to combine different filters.\"\"\"\n    # TODO add more conditions\n    AND = \"and\"\n    OR = \"or\"\n    NOT = \"not\"  # negates the filter condition\nclass MetadataFilter(BaseModel):\n    r\"\"\"\n    Comprehensive metadata filter for vector stores to support more operators.\n    Value uses Strict types, as int, float and str are compatible types and were all",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "MetadataFilter",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "peekOfCode": "class MetadataFilter(BaseModel):\n    r\"\"\"\n    Comprehensive metadata filter for vector stores to support more operators.\n    Value uses Strict types, as int, float and str are compatible types and were all\n    converted to string before.\n    See: https://docs.pydantic.dev/latest/usage/types/#strict-types\n    \"\"\"\n    key: str\n    value: Optional[\n        Union[",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "MetadataFilters",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "peekOfCode": "class MetadataFilters(BaseModel):\n    \"\"\"Metadata filters for vector stores.\"\"\"\n    # Exact match filters and Advanced filters with operators like >, <, >=, <=, !=, etc.\n    filters: List[Union[MetadataFilter, ExactMatchFilter, \"MetadataFilters\"]]\n    # and/or such conditions for combining different filters\n    condition: Optional[FilterCondition] = FilterCondition.AND\n    @classmethod\n    @deprecated(\n        \"`from_dict()` is deprecated. \"\n        \"Please use `MetadataFilters(filters=.., condition='and')` directly instead.\"",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQuerySpec",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "peekOfCode": "class VectorStoreQuerySpec(BaseModel):\n    \"\"\"\n    Schema for a structured request for vector store\n    (i.e. to be converted to a VectorStoreQuery).\n    Currently only used by VectorIndexAutoRetriever.\n    \"\"\"\n    query: str\n    filters: List[MetadataFilter]\n    top_k: Optional[int] = None\nclass MetadataInfo(BaseModel):",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "MetadataInfo",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "peekOfCode": "class MetadataInfo(BaseModel):\n    \"\"\"\n    Information about a metadata filter supported by a vector store.\n    Currently only used by VectorIndexAutoRetriever.\n    \"\"\"\n    name: str\n    type: str\n    description: str\nclass VectorStoreInfo(BaseModel):\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreInfo",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "peekOfCode": "class VectorStoreInfo(BaseModel):\n    \"\"\"\n    Information about a vector store (content and supported metadata filters).\n    Currently only used by VectorIndexAutoRetriever.\n    \"\"\"\n    metadata_info: List[MetadataInfo]\n    content_info: str\n@dataclass\nclass VectorStoreQuery:\n    \"\"\"Vector store query.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStoreQuery",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "peekOfCode": "class VectorStoreQuery:\n    \"\"\"Vector store query.\"\"\"\n    query_embedding: Optional[List[float]] = None\n    similarity_top_k: int = 1\n    doc_ids: Optional[List[str]] = None\n    node_ids: Optional[List[str]] = None\n    query_str: Optional[str] = None\n    output_fields: Optional[List[str]] = None\n    embedding_field: Optional[str] = None\n    mode: VectorStoreQueryMode = VectorStoreQueryMode.DEFAULT",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "VectorStore",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "peekOfCode": "class VectorStore(Protocol):\n    \"\"\"Abstract vector store protocol.\"\"\"\n    stores_text: bool\n    is_embedding_query: bool = True\n    @property\n    def client(self) -> Any:\n        \"\"\"Get client.\"\"\"\n        ...\n    def add(\n        self,",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "BasePydanticVectorStore",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "peekOfCode": "class BasePydanticVectorStore(BaseComponent, ABC):\n    \"\"\"Abstract vector store protocol.\"\"\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    stores_text: bool\n    is_embedding_query: bool = True\n    @property\n    @abstractmethod\n    def client(self) -> Any:\n        \"\"\"Get client.\"\"\"\n    def get_nodes(",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_DIR",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "peekOfCode": "DEFAULT_PERSIST_DIR = \"./storage\"\nDEFAULT_PERSIST_FNAME = \"vector_store.json\"\n# legacy: kept for backward compatibility\nNodeWithEmbedding = TextNode\n@dataclass\nclass VectorStoreQueryResult:\n    \"\"\"Vector store query result.\"\"\"\n    nodes: Optional[Sequence[BaseNode]] = None\n    similarities: Optional[List[float]] = None\n    ids: Optional[List[str]] = None",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PERSIST_FNAME",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "peekOfCode": "DEFAULT_PERSIST_FNAME = \"vector_store.json\"\n# legacy: kept for backward compatibility\nNodeWithEmbedding = TextNode\n@dataclass\nclass VectorStoreQueryResult:\n    \"\"\"Vector store query result.\"\"\"\n    nodes: Optional[Sequence[BaseNode]] = None\n    similarities: Optional[List[float]] = None\n    ids: Optional[List[str]] = None\nclass VectorStoreQueryMode(str, Enum):",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "NodeWithEmbedding",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "peekOfCode": "NodeWithEmbedding = TextNode\n@dataclass\nclass VectorStoreQueryResult:\n    \"\"\"Vector store query result.\"\"\"\n    nodes: Optional[Sequence[BaseNode]] = None\n    similarities: Optional[List[float]] = None\n    ids: Optional[List[str]] = None\nclass VectorStoreQueryMode(str, Enum):\n    \"\"\"Vector store query mode.\"\"\"\n    DEFAULT = \"default\"",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "ExactMatchFilter",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "peekOfCode": "ExactMatchFilter = MetadataFilter\nclass MetadataFilters(BaseModel):\n    \"\"\"Metadata filters for vector stores.\"\"\"\n    # Exact match filters and Advanced filters with operators like >, <, >=, <=, !=, etc.\n    filters: List[Union[MetadataFilter, ExactMatchFilter, \"MetadataFilters\"]]\n    # and/or such conditions for combining different filters\n    condition: Optional[FilterCondition] = FilterCondition.AND\n    @classmethod\n    @deprecated(\n        \"`from_dict()` is deprecated. \"",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.types",
        "documentation": {}
    },
    {
        "label": "node_to_metadata_dict",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.utils",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.utils",
        "peekOfCode": "def node_to_metadata_dict(\n    node: BaseNode,\n    remove_text: bool = False,\n    text_field: str = DEFAULT_TEXT_KEY,\n    text_resource_field: str = DEFAULT_TEXT_RESOURCE_KEY,\n    flat_metadata: bool = False,\n) -> Dict[str, Any]:\n    \"\"\"Common logic for saving Node data into metadata dict.\"\"\"\n    # Using mode=\"json\" here because BaseNode may have fields of type bytes (e.g. images in ImageBlock),\n    # which would cause serialization issues.",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.utils",
        "documentation": {}
    },
    {
        "label": "metadata_dict_to_node",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.utils",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.utils",
        "peekOfCode": "def metadata_dict_to_node(metadata: dict, text: Optional[str] = None) -> BaseNode:\n    \"\"\"Common logic for loading Node data from metadata dict.\"\"\"\n    node_json = metadata.get(\"_node_content\")\n    node_type = metadata.get(\"_node_type\")\n    if node_json is None:\n        raise ValueError(\"Node content not found in metadata dict.\")\n    node: BaseNode\n    if node_type == Node.class_name():\n        node = Node.from_json(node_json)\n    elif node_type == IndexNode.class_name():",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.utils",
        "documentation": {}
    },
    {
        "label": "build_metadata_filter_fn",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.utils",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.utils",
        "peekOfCode": "def build_metadata_filter_fn(\n    metadata_lookup_fn: Callable[[str], Mapping[str, Any]],\n    metadata_filters: Optional[MetadataFilters] = None,\n) -> Callable[[str], bool]:\n    \"\"\"Build metadata filter function.\"\"\"\n    filter_list = metadata_filters.filters if metadata_filters else []\n    if not filter_list or not metadata_filters:\n        return lambda _: True\n    filter_condition = cast(MetadataFilters, metadata_filters.condition)\n    def filter_fn(node_id: str) -> bool:",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.utils",
        "documentation": {}
    },
    {
        "label": "legacy_metadata_dict_to_node",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.utils",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.utils",
        "peekOfCode": "def legacy_metadata_dict_to_node(\n    metadata: dict, text_key: str = DEFAULT_TEXT_KEY\n) -> Tuple[dict, dict, dict]:\n    \"\"\"Common logic for loading Node data from metadata dict.\"\"\"\n    # make a copy first\n    if metadata is None:\n        metadata = {}\n    else:\n        metadata = metadata.copy()\n    # load node_info from json string",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.utils",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TEXT_KEY",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.utils",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.utils",
        "peekOfCode": "DEFAULT_TEXT_KEY = \"text\"\nDEFAULT_TEXT_RESOURCE_KEY = \"text_resource\"\nDEFAULT_EMBEDDING_KEY = \"embedding\"\nDEFAULT_DOC_ID_KEY = \"doc_id\"\ndef _validate_is_flat_dict(metadata_dict: dict) -> None:\n    \"\"\"\n    Validate that metadata dict is flat,\n    and key is str, and value is one of (str, int, float, None).\n    \"\"\"\n    for key, val in metadata_dict.items():",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.utils",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TEXT_RESOURCE_KEY",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.utils",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.utils",
        "peekOfCode": "DEFAULT_TEXT_RESOURCE_KEY = \"text_resource\"\nDEFAULT_EMBEDDING_KEY = \"embedding\"\nDEFAULT_DOC_ID_KEY = \"doc_id\"\ndef _validate_is_flat_dict(metadata_dict: dict) -> None:\n    \"\"\"\n    Validate that metadata dict is flat,\n    and key is str, and value is one of (str, int, float, None).\n    \"\"\"\n    for key, val in metadata_dict.items():\n        if not isinstance(key, str):",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.utils",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EMBEDDING_KEY",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.utils",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.utils",
        "peekOfCode": "DEFAULT_EMBEDDING_KEY = \"embedding\"\nDEFAULT_DOC_ID_KEY = \"doc_id\"\ndef _validate_is_flat_dict(metadata_dict: dict) -> None:\n    \"\"\"\n    Validate that metadata dict is flat,\n    and key is str, and value is one of (str, int, float, None).\n    \"\"\"\n    for key, val in metadata_dict.items():\n        if not isinstance(key, str):\n            raise ValueError(\"Metadata key must be str!\")",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.utils",
        "documentation": {}
    },
    {
        "label": "DEFAULT_DOC_ID_KEY",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.vector_stores.utils",
        "description": "reference_code.llama-index-core.llama_index.core.vector_stores.utils",
        "peekOfCode": "DEFAULT_DOC_ID_KEY = \"doc_id\"\ndef _validate_is_flat_dict(metadata_dict: dict) -> None:\n    \"\"\"\n    Validate that metadata dict is flat,\n    and key is str, and value is one of (str, int, float, None).\n    \"\"\"\n    for key, val in metadata_dict.items():\n        if not isinstance(key, str):\n            raise ValueError(\"Metadata key must be str!\")\n        if not isinstance(val, (str, int, float, type(None))):",
        "detail": "reference_code.llama-index-core.llama_index.core.vector_stores.utils",
        "documentation": {}
    },
    {
        "label": "BaseVoiceAgent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.voice_agents.base",
        "description": "reference_code.llama-index-core.llama_index.core.voice_agents.base",
        "peekOfCode": "class BaseVoiceAgent(ABC):\n    \"\"\"\n    Abstract class that serves as base for any Voice Agent.\n    Attributes:\n        ws (BaseVoiceAgentWebSocket): The websocket underlying the agent and providing the voice service.\n        interface (BaseVoiceAgentInterface): The audio input/output interface.\n        api_key (Optional[str]): API key (if needed). Defaults to None.\n        tools (Optional[List[BaseTool]]): List of tools for the agent to use (tool use should be adapted to the specific integration). Defaults to None.\n        _messages (List[ChatMessage]): Private attribute initialized as an empty list of ChatMessage, it should be populated with chat messages as the conversation goes on.\n        _events (List[BaseVoiceAgentEvent]): Private attribute initialized as an empty list of BaseVoiceAgentEvent, it should be populated with events as the conversation goes on.",
        "detail": "reference_code.llama-index-core.llama_index.core.voice_agents.base",
        "documentation": {}
    },
    {
        "label": "BaseVoiceAgentEvent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.voice_agents.events",
        "description": "reference_code.llama-index-core.llama_index.core.voice_agents.events",
        "peekOfCode": "class BaseVoiceAgentEvent(BaseModel):\n    \"\"\"\n    Base class to represent events in Voice Agents conversations.\n    Attributes:\n        type_t (str): Event type (serialized as 'type')\n    \"\"\"\n    type_t: str = Field(serialization_alias=\"type\")",
        "detail": "reference_code.llama-index-core.llama_index.core.voice_agents.events",
        "documentation": {}
    },
    {
        "label": "BaseVoiceAgentInterface",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.voice_agents.interface",
        "description": "reference_code.llama-index-core.llama_index.core.voice_agents.interface",
        "peekOfCode": "class BaseVoiceAgentInterface(ABC):\n    \"\"\"\n    Abstract base class for a voice agent audio input/output interface.\n    \"\"\"\n    @abstractmethod\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Please implement this method by initializing the class with arbitrary attributes.\"\"\"\n        ...\n    @abstractmethod\n    def _speaker_callback(self, *args: Any, **kwargs: Any) -> Any:",
        "detail": "reference_code.llama-index-core.llama_index.core.voice_agents.interface",
        "documentation": {}
    },
    {
        "label": "BaseVoiceAgentWebsocket",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.voice_agents.websocket",
        "description": "reference_code.llama-index-core.llama_index.core.voice_agents.websocket",
        "peekOfCode": "class BaseVoiceAgentWebsocket(ABC):\n    \"\"\"\n    Abstract base class for a voice agent websocket.\n    Attributes:\n        uri (str): URL of the websocket.\n        ws (Optional[ClientConnection]): Private attribute, initialized as None, represents the websocket client.\n    \"\"\"\n    def __init__(\n        self,\n        uri: str,",
        "detail": "reference_code.llama-index-core.llama_index.core.voice_agents.websocket",
        "documentation": {}
    },
    {
        "label": "JsonPickleSerializer",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.workflow.context_serializers",
        "description": "reference_code.llama-index-core.llama_index.core.workflow.context_serializers",
        "peekOfCode": "JsonPickleSerializer = PickleSerializer",
        "detail": "reference_code.llama-index-core.llama_index.core.workflow.context_serializers",
        "documentation": {}
    },
    {
        "label": "step",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.workflow.decorators",
        "description": "reference_code.llama-index-core.llama_index.core.workflow.decorators",
        "peekOfCode": "def step(*args: Any, **kwargs: Any) -> Callable:\n    # Remove old, unused parameter\n    kwargs.pop(\"pass_context\", None)\n    return upstream_step(*args, **kwargs)",
        "detail": "reference_code.llama-index-core.llama_index.core.workflow.decorators",
        "documentation": {}
    },
    {
        "label": "draw_all_possible_flows",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.workflow.drawing",
        "description": "reference_code.llama-index-core.llama_index.core.workflow.drawing",
        "peekOfCode": "def draw_all_possible_flows(\n    workflow: Workflow,\n    filename: str = \"workflow_all_flows.html\",\n    notebook: bool = False,\n) -> None:\n    \"\"\"Draws all possible flows of the workflow.\"\"\"\n    from pyvis.network import Network\n    net = Network(directed=True, height=\"750px\", width=\"100%\")\n    # Add the nodes + edge for stop events\n    net.add_node(",
        "detail": "reference_code.llama-index-core.llama_index.core.workflow.drawing",
        "documentation": {}
    },
    {
        "label": "draw_most_recent_execution",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.workflow.drawing",
        "description": "reference_code.llama-index-core.llama_index.core.workflow.drawing",
        "peekOfCode": "def draw_most_recent_execution(\n    workflow: Workflow,\n    filename: str = \"workflow_recent_execution.html\",\n    notebook: bool = False,\n) -> None:\n    \"\"\"Draws the most recent execution of the workflow.\"\"\"\n    from pyvis.network import Network\n    net = Network(directed=True, height=\"750px\", width=\"100%\")\n    # Add nodes and edges based on execution history\n    existing_context = next(iter(workflow._contexts), None)",
        "detail": "reference_code.llama-index-core.llama_index.core.workflow.drawing",
        "documentation": {}
    },
    {
        "label": "asyncio_module",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.async_utils",
        "description": "reference_code.llama-index-core.llama_index.core.async_utils",
        "peekOfCode": "def asyncio_module(show_progress: bool = False) -> Any:\n    if show_progress:\n        from tqdm.asyncio import tqdm_asyncio\n        module = tqdm_asyncio\n    else:\n        module = asyncio\n    return module\ndef asyncio_run(coro: Coroutine) -> Any:\n    \"\"\"\n    Gets an existing event loop to run the coroutine.",
        "detail": "reference_code.llama-index-core.llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "asyncio_run",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.async_utils",
        "description": "reference_code.llama-index-core.llama_index.core.async_utils",
        "peekOfCode": "def asyncio_run(coro: Coroutine) -> Any:\n    \"\"\"\n    Gets an existing event loop to run the coroutine.\n    If there is no existing event loop, creates a new one.\n    If an event loop is already running, uses threading to run in a separate thread.\n    \"\"\"\n    try:\n        # Check if there's an existing event loop\n        loop = asyncio.get_event_loop()\n        # Check if the loop is already running",
        "detail": "reference_code.llama-index-core.llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "run_async_tasks",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.async_utils",
        "description": "reference_code.llama-index-core.llama_index.core.async_utils",
        "peekOfCode": "def run_async_tasks(\n    tasks: List[Coroutine],\n    show_progress: bool = False,\n    progress_bar_desc: str = \"Running async tasks\",\n) -> List[Any]:\n    \"\"\"Run a list of async tasks.\"\"\"\n    tasks_to_execute: List[Any] = tasks\n    if show_progress:\n        try:\n            import nest_asyncio",
        "detail": "reference_code.llama-index-core.llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "chunks",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.async_utils",
        "description": "reference_code.llama-index-core.llama_index.core.async_utils",
        "peekOfCode": "def chunks(iterable: Iterable, size: int) -> Iterable:\n    args = [iter(iterable)] * size\n    return zip_longest(*args, fillvalue=None)\nasync def batch_gather(\n    tasks: List[Coroutine], batch_size: int = 10, verbose: bool = False\n) -> List[Any]:\n    output: List[Any] = []\n    for task_chunk in chunks(tasks, batch_size):\n        task_chunk = (task for task in task_chunk if task is not None)\n        output_chunk = await asyncio.gather(*task_chunk)",
        "detail": "reference_code.llama-index-core.llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "get_asyncio_module",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.async_utils",
        "description": "reference_code.llama-index-core.llama_index.core.async_utils",
        "peekOfCode": "def get_asyncio_module(show_progress: bool = False) -> Any:\n    if show_progress:\n        from tqdm.asyncio import tqdm_asyncio\n        module = tqdm_asyncio\n    else:\n        module = asyncio\n    return module\nDEFAULT_NUM_WORKERS = 4\nT = TypeVar(\"T\")\n@dispatcher.span",
        "detail": "reference_code.llama-index-core.llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "dispatcher",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.async_utils",
        "description": "reference_code.llama-index-core.llama_index.core.async_utils",
        "peekOfCode": "dispatcher = instrument.get_dispatcher(__name__)\ndef asyncio_module(show_progress: bool = False) -> Any:\n    if show_progress:\n        from tqdm.asyncio import tqdm_asyncio\n        module = tqdm_asyncio\n    else:\n        module = asyncio\n    return module\ndef asyncio_run(coro: Coroutine) -> Any:\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "DEFAULT_NUM_WORKERS",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.async_utils",
        "description": "reference_code.llama-index-core.llama_index.core.async_utils",
        "peekOfCode": "DEFAULT_NUM_WORKERS = 4\nT = TypeVar(\"T\")\n@dispatcher.span\nasync def run_jobs(\n    jobs: List[Coroutine[Any, Any, T]],\n    show_progress: bool = False,\n    workers: int = DEFAULT_NUM_WORKERS,\n    desc: Optional[str] = None,\n) -> List[T]:\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "T",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.async_utils",
        "description": "reference_code.llama-index-core.llama_index.core.async_utils",
        "peekOfCode": "T = TypeVar(\"T\")\n@dispatcher.span\nasync def run_jobs(\n    jobs: List[Coroutine[Any, Any, T]],\n    show_progress: bool = False,\n    workers: int = DEFAULT_NUM_WORKERS,\n    desc: Optional[str] = None,\n) -> List[T]:\n    \"\"\"\n    Run jobs.",
        "detail": "reference_code.llama-index-core.llama_index.core.async_utils",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TEMPERATURE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.constants",
        "description": "reference_code.llama-index-core.llama_index.core.constants",
        "peekOfCode": "DEFAULT_TEMPERATURE = 0.1\nDEFAULT_CONTEXT_WINDOW = 3900  # tokens\nDEFAULT_NUM_OUTPUTS = 256  # tokens\nDEFAULT_NUM_INPUT_FILES = 10  # files\nDEFAULT_EMBED_BATCH_SIZE = 10\nDEFAULT_CHUNK_SIZE = 1024  # tokens\nDEFAULT_CHUNK_OVERLAP = 20  # tokens\nDEFAULT_SIMILARITY_TOP_K = 2\nDEFAULT_IMAGE_SIMILARITY_TOP_K = 2\n# NOTE: for text-embedding-ada-002",
        "detail": "reference_code.llama-index-core.llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CONTEXT_WINDOW",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.constants",
        "description": "reference_code.llama-index-core.llama_index.core.constants",
        "peekOfCode": "DEFAULT_CONTEXT_WINDOW = 3900  # tokens\nDEFAULT_NUM_OUTPUTS = 256  # tokens\nDEFAULT_NUM_INPUT_FILES = 10  # files\nDEFAULT_EMBED_BATCH_SIZE = 10\nDEFAULT_CHUNK_SIZE = 1024  # tokens\nDEFAULT_CHUNK_OVERLAP = 20  # tokens\nDEFAULT_SIMILARITY_TOP_K = 2\nDEFAULT_IMAGE_SIMILARITY_TOP_K = 2\n# NOTE: for text-embedding-ada-002\nDEFAULT_EMBEDDING_DIM = 1536",
        "detail": "reference_code.llama-index-core.llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_NUM_OUTPUTS",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.constants",
        "description": "reference_code.llama-index-core.llama_index.core.constants",
        "peekOfCode": "DEFAULT_NUM_OUTPUTS = 256  # tokens\nDEFAULT_NUM_INPUT_FILES = 10  # files\nDEFAULT_EMBED_BATCH_SIZE = 10\nDEFAULT_CHUNK_SIZE = 1024  # tokens\nDEFAULT_CHUNK_OVERLAP = 20  # tokens\nDEFAULT_SIMILARITY_TOP_K = 2\nDEFAULT_IMAGE_SIMILARITY_TOP_K = 2\n# NOTE: for text-embedding-ada-002\nDEFAULT_EMBEDDING_DIM = 1536\n# context window size for llm predictor",
        "detail": "reference_code.llama-index-core.llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_NUM_INPUT_FILES",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.constants",
        "description": "reference_code.llama-index-core.llama_index.core.constants",
        "peekOfCode": "DEFAULT_NUM_INPUT_FILES = 10  # files\nDEFAULT_EMBED_BATCH_SIZE = 10\nDEFAULT_CHUNK_SIZE = 1024  # tokens\nDEFAULT_CHUNK_OVERLAP = 20  # tokens\nDEFAULT_SIMILARITY_TOP_K = 2\nDEFAULT_IMAGE_SIMILARITY_TOP_K = 2\n# NOTE: for text-embedding-ada-002\nDEFAULT_EMBEDDING_DIM = 1536\n# context window size for llm predictor\nCOHERE_CONTEXT_WINDOW = 2048",
        "detail": "reference_code.llama-index-core.llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EMBED_BATCH_SIZE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.constants",
        "description": "reference_code.llama-index-core.llama_index.core.constants",
        "peekOfCode": "DEFAULT_EMBED_BATCH_SIZE = 10\nDEFAULT_CHUNK_SIZE = 1024  # tokens\nDEFAULT_CHUNK_OVERLAP = 20  # tokens\nDEFAULT_SIMILARITY_TOP_K = 2\nDEFAULT_IMAGE_SIMILARITY_TOP_K = 2\n# NOTE: for text-embedding-ada-002\nDEFAULT_EMBEDDING_DIM = 1536\n# context window size for llm predictor\nCOHERE_CONTEXT_WINDOW = 2048\nAI21_J2_CONTEXT_WINDOW = 8192",
        "detail": "reference_code.llama-index-core.llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CHUNK_SIZE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.constants",
        "description": "reference_code.llama-index-core.llama_index.core.constants",
        "peekOfCode": "DEFAULT_CHUNK_SIZE = 1024  # tokens\nDEFAULT_CHUNK_OVERLAP = 20  # tokens\nDEFAULT_SIMILARITY_TOP_K = 2\nDEFAULT_IMAGE_SIMILARITY_TOP_K = 2\n# NOTE: for text-embedding-ada-002\nDEFAULT_EMBEDDING_DIM = 1536\n# context window size for llm predictor\nCOHERE_CONTEXT_WINDOW = 2048\nAI21_J2_CONTEXT_WINDOW = 8192\nTYPE_KEY = \"__type__\"",
        "detail": "reference_code.llama-index-core.llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CHUNK_OVERLAP",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.constants",
        "description": "reference_code.llama-index-core.llama_index.core.constants",
        "peekOfCode": "DEFAULT_CHUNK_OVERLAP = 20  # tokens\nDEFAULT_SIMILARITY_TOP_K = 2\nDEFAULT_IMAGE_SIMILARITY_TOP_K = 2\n# NOTE: for text-embedding-ada-002\nDEFAULT_EMBEDDING_DIM = 1536\n# context window size for llm predictor\nCOHERE_CONTEXT_WINDOW = 2048\nAI21_J2_CONTEXT_WINDOW = 8192\nTYPE_KEY = \"__type__\"\nDATA_KEY = \"__data__\"",
        "detail": "reference_code.llama-index-core.llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SIMILARITY_TOP_K",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.constants",
        "description": "reference_code.llama-index-core.llama_index.core.constants",
        "peekOfCode": "DEFAULT_SIMILARITY_TOP_K = 2\nDEFAULT_IMAGE_SIMILARITY_TOP_K = 2\n# NOTE: for text-embedding-ada-002\nDEFAULT_EMBEDDING_DIM = 1536\n# context window size for llm predictor\nCOHERE_CONTEXT_WINDOW = 2048\nAI21_J2_CONTEXT_WINDOW = 8192\nTYPE_KEY = \"__type__\"\nDATA_KEY = \"__data__\"\nVECTOR_STORE_KEY = \"vector_store\"",
        "detail": "reference_code.llama-index-core.llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_IMAGE_SIMILARITY_TOP_K",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.constants",
        "description": "reference_code.llama-index-core.llama_index.core.constants",
        "peekOfCode": "DEFAULT_IMAGE_SIMILARITY_TOP_K = 2\n# NOTE: for text-embedding-ada-002\nDEFAULT_EMBEDDING_DIM = 1536\n# context window size for llm predictor\nCOHERE_CONTEXT_WINDOW = 2048\nAI21_J2_CONTEXT_WINDOW = 8192\nTYPE_KEY = \"__type__\"\nDATA_KEY = \"__data__\"\nVECTOR_STORE_KEY = \"vector_store\"\nIMAGE_STORE_KEY = \"image_store\"",
        "detail": "reference_code.llama-index-core.llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_EMBEDDING_DIM",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.constants",
        "description": "reference_code.llama-index-core.llama_index.core.constants",
        "peekOfCode": "DEFAULT_EMBEDDING_DIM = 1536\n# context window size for llm predictor\nCOHERE_CONTEXT_WINDOW = 2048\nAI21_J2_CONTEXT_WINDOW = 8192\nTYPE_KEY = \"__type__\"\nDATA_KEY = \"__data__\"\nVECTOR_STORE_KEY = \"vector_store\"\nIMAGE_STORE_KEY = \"image_store\"\nGRAPH_STORE_KEY = \"graph_store\"\nINDEX_STORE_KEY = \"index_store\"",
        "detail": "reference_code.llama-index-core.llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "COHERE_CONTEXT_WINDOW",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.constants",
        "description": "reference_code.llama-index-core.llama_index.core.constants",
        "peekOfCode": "COHERE_CONTEXT_WINDOW = 2048\nAI21_J2_CONTEXT_WINDOW = 8192\nTYPE_KEY = \"__type__\"\nDATA_KEY = \"__data__\"\nVECTOR_STORE_KEY = \"vector_store\"\nIMAGE_STORE_KEY = \"image_store\"\nGRAPH_STORE_KEY = \"graph_store\"\nINDEX_STORE_KEY = \"index_store\"\nDOC_STORE_KEY = \"doc_store\"\nPG_STORE_KEY = \"property_graph_store\"",
        "detail": "reference_code.llama-index-core.llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "AI21_J2_CONTEXT_WINDOW",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.constants",
        "description": "reference_code.llama-index-core.llama_index.core.constants",
        "peekOfCode": "AI21_J2_CONTEXT_WINDOW = 8192\nTYPE_KEY = \"__type__\"\nDATA_KEY = \"__data__\"\nVECTOR_STORE_KEY = \"vector_store\"\nIMAGE_STORE_KEY = \"image_store\"\nGRAPH_STORE_KEY = \"graph_store\"\nINDEX_STORE_KEY = \"index_store\"\nDOC_STORE_KEY = \"doc_store\"\nPG_STORE_KEY = \"property_graph_store\"\n# llama-cloud constants",
        "detail": "reference_code.llama-index-core.llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "TYPE_KEY",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.constants",
        "description": "reference_code.llama-index-core.llama_index.core.constants",
        "peekOfCode": "TYPE_KEY = \"__type__\"\nDATA_KEY = \"__data__\"\nVECTOR_STORE_KEY = \"vector_store\"\nIMAGE_STORE_KEY = \"image_store\"\nGRAPH_STORE_KEY = \"graph_store\"\nINDEX_STORE_KEY = \"index_store\"\nDOC_STORE_KEY = \"doc_store\"\nPG_STORE_KEY = \"property_graph_store\"\n# llama-cloud constants\nDEFAULT_PIPELINE_NAME = \"default\"",
        "detail": "reference_code.llama-index-core.llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DATA_KEY",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.constants",
        "description": "reference_code.llama-index-core.llama_index.core.constants",
        "peekOfCode": "DATA_KEY = \"__data__\"\nVECTOR_STORE_KEY = \"vector_store\"\nIMAGE_STORE_KEY = \"image_store\"\nGRAPH_STORE_KEY = \"graph_store\"\nINDEX_STORE_KEY = \"index_store\"\nDOC_STORE_KEY = \"doc_store\"\nPG_STORE_KEY = \"property_graph_store\"\n# llama-cloud constants\nDEFAULT_PIPELINE_NAME = \"default\"\nDEFAULT_PROJECT_NAME = \"Default\"",
        "detail": "reference_code.llama-index-core.llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "VECTOR_STORE_KEY",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.constants",
        "description": "reference_code.llama-index-core.llama_index.core.constants",
        "peekOfCode": "VECTOR_STORE_KEY = \"vector_store\"\nIMAGE_STORE_KEY = \"image_store\"\nGRAPH_STORE_KEY = \"graph_store\"\nINDEX_STORE_KEY = \"index_store\"\nDOC_STORE_KEY = \"doc_store\"\nPG_STORE_KEY = \"property_graph_store\"\n# llama-cloud constants\nDEFAULT_PIPELINE_NAME = \"default\"\nDEFAULT_PROJECT_NAME = \"Default\"\nDEFAULT_BASE_URL = \"https://api.cloud.llamaindex.ai\"",
        "detail": "reference_code.llama-index-core.llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "IMAGE_STORE_KEY",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.constants",
        "description": "reference_code.llama-index-core.llama_index.core.constants",
        "peekOfCode": "IMAGE_STORE_KEY = \"image_store\"\nGRAPH_STORE_KEY = \"graph_store\"\nINDEX_STORE_KEY = \"index_store\"\nDOC_STORE_KEY = \"doc_store\"\nPG_STORE_KEY = \"property_graph_store\"\n# llama-cloud constants\nDEFAULT_PIPELINE_NAME = \"default\"\nDEFAULT_PROJECT_NAME = \"Default\"\nDEFAULT_BASE_URL = \"https://api.cloud.llamaindex.ai\"\nDEFAULT_APP_URL = \"https://cloud.llamaindex.ai\"",
        "detail": "reference_code.llama-index-core.llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "GRAPH_STORE_KEY",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.constants",
        "description": "reference_code.llama-index-core.llama_index.core.constants",
        "peekOfCode": "GRAPH_STORE_KEY = \"graph_store\"\nINDEX_STORE_KEY = \"index_store\"\nDOC_STORE_KEY = \"doc_store\"\nPG_STORE_KEY = \"property_graph_store\"\n# llama-cloud constants\nDEFAULT_PIPELINE_NAME = \"default\"\nDEFAULT_PROJECT_NAME = \"Default\"\nDEFAULT_BASE_URL = \"https://api.cloud.llamaindex.ai\"\nDEFAULT_APP_URL = \"https://cloud.llamaindex.ai\"",
        "detail": "reference_code.llama-index-core.llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "INDEX_STORE_KEY",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.constants",
        "description": "reference_code.llama-index-core.llama_index.core.constants",
        "peekOfCode": "INDEX_STORE_KEY = \"index_store\"\nDOC_STORE_KEY = \"doc_store\"\nPG_STORE_KEY = \"property_graph_store\"\n# llama-cloud constants\nDEFAULT_PIPELINE_NAME = \"default\"\nDEFAULT_PROJECT_NAME = \"Default\"\nDEFAULT_BASE_URL = \"https://api.cloud.llamaindex.ai\"\nDEFAULT_APP_URL = \"https://cloud.llamaindex.ai\"",
        "detail": "reference_code.llama-index-core.llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DOC_STORE_KEY",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.constants",
        "description": "reference_code.llama-index-core.llama_index.core.constants",
        "peekOfCode": "DOC_STORE_KEY = \"doc_store\"\nPG_STORE_KEY = \"property_graph_store\"\n# llama-cloud constants\nDEFAULT_PIPELINE_NAME = \"default\"\nDEFAULT_PROJECT_NAME = \"Default\"\nDEFAULT_BASE_URL = \"https://api.cloud.llamaindex.ai\"\nDEFAULT_APP_URL = \"https://cloud.llamaindex.ai\"",
        "detail": "reference_code.llama-index-core.llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "PG_STORE_KEY",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.constants",
        "description": "reference_code.llama-index-core.llama_index.core.constants",
        "peekOfCode": "PG_STORE_KEY = \"property_graph_store\"\n# llama-cloud constants\nDEFAULT_PIPELINE_NAME = \"default\"\nDEFAULT_PROJECT_NAME = \"Default\"\nDEFAULT_BASE_URL = \"https://api.cloud.llamaindex.ai\"\nDEFAULT_APP_URL = \"https://cloud.llamaindex.ai\"",
        "detail": "reference_code.llama-index-core.llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PIPELINE_NAME",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.constants",
        "description": "reference_code.llama-index-core.llama_index.core.constants",
        "peekOfCode": "DEFAULT_PIPELINE_NAME = \"default\"\nDEFAULT_PROJECT_NAME = \"Default\"\nDEFAULT_BASE_URL = \"https://api.cloud.llamaindex.ai\"\nDEFAULT_APP_URL = \"https://cloud.llamaindex.ai\"",
        "detail": "reference_code.llama-index-core.llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_PROJECT_NAME",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.constants",
        "description": "reference_code.llama-index-core.llama_index.core.constants",
        "peekOfCode": "DEFAULT_PROJECT_NAME = \"Default\"\nDEFAULT_BASE_URL = \"https://api.cloud.llamaindex.ai\"\nDEFAULT_APP_URL = \"https://cloud.llamaindex.ai\"",
        "detail": "reference_code.llama-index-core.llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_BASE_URL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.constants",
        "description": "reference_code.llama-index-core.llama_index.core.constants",
        "peekOfCode": "DEFAULT_BASE_URL = \"https://api.cloud.llamaindex.ai\"\nDEFAULT_APP_URL = \"https://cloud.llamaindex.ai\"",
        "detail": "reference_code.llama-index-core.llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "DEFAULT_APP_URL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.constants",
        "description": "reference_code.llama-index-core.llama_index.core.constants",
        "peekOfCode": "DEFAULT_APP_URL = \"https://cloud.llamaindex.ai\"",
        "detail": "reference_code.llama-index-core.llama_index.core.constants",
        "documentation": {}
    },
    {
        "label": "BaseImageRetriever",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.image_retriever",
        "description": "reference_code.llama-index-core.llama_index.core.image_retriever",
        "peekOfCode": "class BaseImageRetriever(PromptMixin, DispatcherSpanMixin):\n    \"\"\"Base Image Retriever Abstraction.\"\"\"\n    def text_to_image_retrieve(\n        self, str_or_query_bundle: QueryType\n    ) -> List[NodeWithScore]:\n        \"\"\"\n        Retrieve image nodes given query or single image input.\n        Args:\n            str_or_query_bundle (QueryType): a query text\n            string or a QueryBundle object.",
        "detail": "reference_code.llama-index-core.llama_index.core.image_retriever",
        "documentation": {}
    },
    {
        "label": "img_2_b64",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.img_utils",
        "description": "reference_code.llama-index-core.llama_index.core.img_utils",
        "peekOfCode": "def img_2_b64(image: ImageFile, format: str = \"JPEG\") -> str:\n    \"\"\"\n    Convert a PIL.Image to a base64 encoded image string.\n    Args:\n        image (ImageFile): The PIL Image object to be converted.\n        format (str, optional): The image format to save as. Defaults to \"JPEG\".\n    Returns:\n        str: A base64 encoded string representation of the image.\n    \"\"\"\n    buff = BytesIO()",
        "detail": "reference_code.llama-index-core.llama_index.core.img_utils",
        "documentation": {}
    },
    {
        "label": "b64_2_img",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.img_utils",
        "description": "reference_code.llama-index-core.llama_index.core.img_utils",
        "peekOfCode": "def b64_2_img(data: str) -> ImageFile:\n    \"\"\"\n    Convert base64 encoded image string to a PIL.Image.\n    Args:\n        data (str): The base64 encoded image string.\n    Returns:\n        ImageFile: A PIL Image object.\n    \"\"\"\n    buff = BytesIO(base64.b64decode(data))\n    return cast(ImageFile, Image.open(buff))",
        "detail": "reference_code.llama-index-core.llama_index.core.img_utils",
        "documentation": {}
    },
    {
        "label": "BaseComponent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "class BaseComponent(BaseModel):\n    \"\"\"Base component object to capture class names.\"\"\"\n    @classmethod\n    def __get_pydantic_json_schema__(\n        cls, core_schema: CoreSchema, handler: GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        json_schema = handler(core_schema)\n        json_schema = handler.resolve_ref_schema(json_schema)\n        # inject class name to help with serde\n        if \"properties\" in json_schema:",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TransformComponent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "class TransformComponent(BaseComponent, DispatcherSpanMixin):\n    \"\"\"Base class for transform components.\"\"\"\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    @abstractmethod\n    def __call__(self, nodes: Sequence[BaseNode], **kwargs: Any) -> Sequence[BaseNode]:\n        \"\"\"Transform nodes.\"\"\"\n    async def acall(\n        self, nodes: Sequence[BaseNode], **kwargs: Any\n    ) -> Sequence[BaseNode]:\n        \"\"\"Async transform nodes.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeRelationship",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "class NodeRelationship(str, Enum):\n    \"\"\"\n    Node relationships used in `BaseNode` class.\n    Attributes:\n        SOURCE: The node is the source document.\n        PREVIOUS: The node is the previous node in the document.\n        NEXT: The node is the next node in the document.\n        PARENT: The node is the parent node in the document.\n        CHILD: The node is a child node in the document.\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ObjectType",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "class ObjectType(str, Enum):\n    TEXT = auto()\n    IMAGE = auto()\n    INDEX = auto()\n    DOCUMENT = auto()\n    MULTIMODAL = auto()\nclass Modality(str, Enum):\n    TEXT = auto()\n    IMAGE = auto()\n    AUDIO = auto()",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Modality",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "class Modality(str, Enum):\n    TEXT = auto()\n    IMAGE = auto()\n    AUDIO = auto()\n    VIDEO = auto()\nclass MetadataMode(str, Enum):\n    ALL = \"all\"\n    EMBED = \"embed\"\n    LLM = \"llm\"\n    NONE = \"none\"",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MetadataMode",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "class MetadataMode(str, Enum):\n    ALL = \"all\"\n    EMBED = \"embed\"\n    LLM = \"llm\"\n    NONE = \"none\"\nclass RelatedNodeInfo(BaseComponent):\n    node_id: str\n    node_type: Annotated[ObjectType, EnumNameSerializer] | str | None = None\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    hash: Optional[str] = None",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "RelatedNodeInfo",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "class RelatedNodeInfo(BaseComponent):\n    node_id: str\n    node_type: Annotated[ObjectType, EnumNameSerializer] | str | None = None\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    hash: Optional[str] = None\n    @classmethod\n    def class_name(cls) -> str:\n        return \"RelatedNodeInfo\"\nRelatedNodeType = Union[RelatedNodeInfo, List[RelatedNodeInfo]]\n# Node classes for indexes",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "BaseNode",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "class BaseNode(BaseComponent):\n    \"\"\"\n    Base node Object.\n    Generic abstract interface for retrievable nodes\n    \"\"\"\n    # hash is computed on local field, during the validation process\n    model_config = ConfigDict(populate_by_name=True, validate_assignment=True)\n    id_: str = Field(\n        default_factory=lambda: str(uuid.uuid4()), description=\"Unique ID of the node.\"\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "MediaResource",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "class MediaResource(BaseModel):\n    \"\"\"\n    A container class for media content.\n    This class represents a generic media resource that can be stored and accessed\n    in multiple ways - as raw bytes, on the filesystem, or via URL. It also supports\n    storing vector embeddings for the media content.\n    Attributes:\n        embeddings: Multi-vector dict representation of this resource for embedding-based search/retrieval\n        text: Plain text representation of this resource\n        data: Raw binary data of the media content",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Node",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "class Node(BaseNode):\n    text_resource: MediaResource | None = Field(\n        default=None, description=\"Text content of the node.\"\n    )\n    image_resource: MediaResource | None = Field(\n        default=None, description=\"Image content of the node.\"\n    )\n    audio_resource: MediaResource | None = Field(\n        default=None, description=\"Audio content of the node.\"\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TextNode",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "class TextNode(BaseNode):\n    \"\"\"\n    Provided for backward compatibility.\n    Note: we keep the field with the typo \"seperator\" to maintain backward compatibility for\n    serialized objects.\n    \"\"\"\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Make TextNode forward-compatible with Node by supporting 'text_resource' in the constructor.\"\"\"\n        if \"text_resource\" in kwargs:\n            tr = kwargs.pop(\"text_resource\")",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ImageNode",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "class ImageNode(TextNode):\n    \"\"\"Node with image.\"\"\"\n    # TODO: store reference instead of actual image\n    # base64 encoded image str\n    image: Optional[str] = None\n    image_path: Optional[str] = None\n    image_url: Optional[str] = None\n    image_mimetype: Optional[str] = None\n    text_embedding: Optional[List[float]] = Field(\n        default=None,",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "IndexNode",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "class IndexNode(TextNode):\n    \"\"\"\n    Node with reference to any object.\n    This can include other indices, query engines, retrievers.\n    This can also include other nodes (though this is overlapping with `relationships`\n    on the Node class).\n    \"\"\"\n    index_id: str\n    obj: Any = None\n    def dict(self, **kwargs: Any) -> Dict[str, Any]:",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "NodeWithScore",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "class NodeWithScore(BaseComponent):\n    node: SerializeAsAny[BaseNode]\n    score: Optional[float] = None\n    def __str__(self) -> str:\n        score_str = \"None\" if self.score is None else f\"{self.score: 0.3f}\"\n        return f\"{self.node}\\nScore: {score_str}\\n\"\n    def get_score(self, raise_error: bool = False) -> float:\n        \"\"\"Get score.\"\"\"\n        if self.score is None:\n            if raise_error:",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "Document",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "class Document(Node):\n    \"\"\"\n    Generic interface for a data document.\n    This document connects to data sources.\n    \"\"\"\n    def __init__(self, **data: Any) -> None:\n        \"\"\"\n        Keeps backward compatibility with old 'Document' versions.\n        If 'text' was passed, store it in 'text_resource'.\n        If 'doc_id' was passed, store it in 'id_'.",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ImageDocument",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "class ImageDocument(Document):\n    \"\"\"Backward compatible wrapper around Document containing an image.\"\"\"\n    def __init__(self, **kwargs: Any) -> None:\n        image = kwargs.pop(\"image\", None)\n        image_path = kwargs.pop(\"image_path\", None)\n        image_url = kwargs.pop(\"image_url\", None)\n        image_mimetype = kwargs.pop(\"image_mimetype\", None)\n        text_embedding = kwargs.pop(\"text_embedding\", None)\n        if image:\n            kwargs[\"image_resource\"] = MediaResource(",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryBundle",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "class QueryBundle(DataClassJsonMixin):\n    \"\"\"\n    Query bundle.\n    This dataclass contains the original query string and associated transformations.\n    Args:\n        query_str (str): the original user-specified query string.\n            This is currently used by all non embedding-based queries.\n        custom_embedding_strs (list[str]): list of strings used for embedding the query.\n            This is currently used by all embedding-based queries.\n        embedding (list[float]): the stored embedding for the query.",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "is_image_pil",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "def is_image_pil(file_path: str) -> bool:\n    try:\n        with Image.open(file_path) as img:\n            img.verify()  # Verify it's a valid image\n        return True\n    except (IOError, SyntaxError):\n        return False\ndef is_image_url_pil(url: str) -> bool:\n    try:\n        response = requests.get(url, stream=True)",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "is_image_url_pil",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "def is_image_url_pil(url: str) -> bool:\n    try:\n        response = requests.get(url, stream=True)\n        response.raise_for_status()  # Raise an exception for bad status codes\n        # Open image from the response content\n        img = Image.open(BytesIO(response.content))\n        img.verify()\n        return True\n    except (requests.RequestException, IOError, SyntaxError):\n        return False",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "DEFAULT_TEXT_NODE_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "DEFAULT_TEXT_NODE_TMPL = \"{metadata_str}\\n\\n{content}\"\nDEFAULT_METADATA_TMPL = \"{key}: {value}\"\n# NOTE: for pretty printing\nTRUNCATE_LENGTH = 350\nWRAP_WIDTH = 70\nImageType = Union[str, BytesIO]\nlogger = logging.getLogger(__name__)\nEnumNameSerializer = PlainSerializer(\n    lambda e: e.value, return_type=\"str\", when_used=\"always\"\n)",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "DEFAULT_METADATA_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "DEFAULT_METADATA_TMPL = \"{key}: {value}\"\n# NOTE: for pretty printing\nTRUNCATE_LENGTH = 350\nWRAP_WIDTH = 70\nImageType = Union[str, BytesIO]\nlogger = logging.getLogger(__name__)\nEnumNameSerializer = PlainSerializer(\n    lambda e: e.value, return_type=\"str\", when_used=\"always\"\n)\nclass BaseComponent(BaseModel):",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "TRUNCATE_LENGTH",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "TRUNCATE_LENGTH = 350\nWRAP_WIDTH = 70\nImageType = Union[str, BytesIO]\nlogger = logging.getLogger(__name__)\nEnumNameSerializer = PlainSerializer(\n    lambda e: e.value, return_type=\"str\", when_used=\"always\"\n)\nclass BaseComponent(BaseModel):\n    \"\"\"Base component object to capture class names.\"\"\"\n    @classmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "WRAP_WIDTH",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "WRAP_WIDTH = 70\nImageType = Union[str, BytesIO]\nlogger = logging.getLogger(__name__)\nEnumNameSerializer = PlainSerializer(\n    lambda e: e.value, return_type=\"str\", when_used=\"always\"\n)\nclass BaseComponent(BaseModel):\n    \"\"\"Base component object to capture class names.\"\"\"\n    @classmethod\n    def __get_pydantic_json_schema__(",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ImageType",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "ImageType = Union[str, BytesIO]\nlogger = logging.getLogger(__name__)\nEnumNameSerializer = PlainSerializer(\n    lambda e: e.value, return_type=\"str\", when_used=\"always\"\n)\nclass BaseComponent(BaseModel):\n    \"\"\"Base component object to capture class names.\"\"\"\n    @classmethod\n    def __get_pydantic_json_schema__(\n        cls, core_schema: CoreSchema, handler: GetJsonSchemaHandler",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "logger = logging.getLogger(__name__)\nEnumNameSerializer = PlainSerializer(\n    lambda e: e.value, return_type=\"str\", when_used=\"always\"\n)\nclass BaseComponent(BaseModel):\n    \"\"\"Base component object to capture class names.\"\"\"\n    @classmethod\n    def __get_pydantic_json_schema__(\n        cls, core_schema: CoreSchema, handler: GetJsonSchemaHandler\n    ) -> JsonSchemaValue:",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "EnumNameSerializer",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "EnumNameSerializer = PlainSerializer(\n    lambda e: e.value, return_type=\"str\", when_used=\"always\"\n)\nclass BaseComponent(BaseModel):\n    \"\"\"Base component object to capture class names.\"\"\"\n    @classmethod\n    def __get_pydantic_json_schema__(\n        cls, core_schema: CoreSchema, handler: GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        json_schema = handler(core_schema)",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "RelatedNodeType",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "RelatedNodeType = Union[RelatedNodeInfo, List[RelatedNodeInfo]]\n# Node classes for indexes\nclass BaseNode(BaseComponent):\n    \"\"\"\n    Base node Object.\n    Generic abstract interface for retrievable nodes\n    \"\"\"\n    # hash is computed on local field, during the validation process\n    model_config = ConfigDict(populate_by_name=True, validate_assignment=True)\n    id_: str = Field(",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "EmbeddingKind",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "EmbeddingKind = Literal[\"sparse\", \"dense\"]\nclass MediaResource(BaseModel):\n    \"\"\"\n    A container class for media content.\n    This class represents a generic media resource that can be stored and accessed\n    in multiple ways - as raw bytes, on the filesystem, or via URL. It also supports\n    storing vector embeddings for the media content.\n    Attributes:\n        embeddings: Multi-vector dict representation of this resource for embedding-based search/retrieval\n        text: Plain text representation of this resource",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "QueryType",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.schema",
        "description": "reference_code.llama-index-core.llama_index.core.schema",
        "peekOfCode": "QueryType = Union[str, QueryBundle]",
        "detail": "reference_code.llama-index-core.llama_index.core.schema",
        "documentation": {}
    },
    {
        "label": "ServiceContext",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.service_context",
        "description": "reference_code.llama-index-core.llama_index.core.service_context",
        "peekOfCode": "class ServiceContext:\n    \"\"\"\n    Service Context container.\n    NOTE: Deprecated, use llama_index.settings.Settings instead or pass in\n    modules to local functions/methods/interfaces.\n    \"\"\"\n    def __init__(self, **kwargs: Any) -> None:\n        raise ValueError(\n            \"ServiceContext is deprecated. Use llama_index.settings.Settings instead, \"\n            \"or pass in modules to local functions/methods/interfaces.\\n\"",
        "detail": "reference_code.llama-index-core.llama_index.core.service_context",
        "documentation": {}
    },
    {
        "label": "set_global_service_context",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.service_context",
        "description": "reference_code.llama-index-core.llama_index.core.service_context",
        "peekOfCode": "def set_global_service_context(service_context: Optional[ServiceContext]) -> None:\n    \"\"\"Helper function to set the global service context.\"\"\"\n    raise ValueError(\n        \"ServiceContext is deprecated. Use llama_index.settings.Settings instead, \"\n        \"or pass in modules to local functions/methods/interfaces.\\n\"\n        \"See the docs for updated usage/migration: \\n\"\n        \"https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/\"\n    )",
        "detail": "reference_code.llama-index-core.llama_index.core.service_context",
        "documentation": {}
    },
    {
        "label": "_Settings",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.settings",
        "description": "reference_code.llama-index-core.llama_index.core.settings",
        "peekOfCode": "class _Settings:\n    \"\"\"Settings for the Llama Index, lazily initialized.\"\"\"\n    # lazy initialization\n    _llm: Optional[LLM] = None\n    _embed_model: Optional[BaseEmbedding] = None\n    _callback_manager: Optional[CallbackManager] = None\n    _tokenizer: Optional[Callable[[str], List[Any]]] = None\n    _node_parser: Optional[NodeParser] = None\n    _prompt_helper: Optional[PromptHelper] = None\n    _transformations: Optional[List[TransformComponent]] = None",
        "detail": "reference_code.llama-index-core.llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "Settings",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.settings",
        "description": "reference_code.llama-index-core.llama_index.core.settings",
        "peekOfCode": "Settings = _Settings()",
        "detail": "reference_code.llama-index-core.llama_index.core.settings",
        "documentation": {}
    },
    {
        "label": "BaseOutputParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.types",
        "description": "reference_code.llama-index-core.llama_index.core.types",
        "peekOfCode": "class BaseOutputParser(DispatcherSpanMixin, ABC):\n    \"\"\"Output parser class.\"\"\"\n    @abstractmethod\n    def parse(self, output: str) -> Any:\n        \"\"\"Parse, validate, and correct errors programmatically.\"\"\"\n    def format(self, query: str) -> str:\n        \"\"\"Format a query with structured output formatting instructions.\"\"\"\n        return query\n    def _format_message(self, message: ChatMessage) -> ChatMessage:\n        text_blocks: list[tuple[int, TextBlock]] = [",
        "detail": "reference_code.llama-index-core.llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "BasePydanticProgram",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.types",
        "description": "reference_code.llama-index-core.llama_index.core.types",
        "peekOfCode": "class BasePydanticProgram(DispatcherSpanMixin, ABC, Generic[Model]):\n    \"\"\"\n    A base class for LLM-powered function that return a pydantic model.\n    Note: this interface is not yet stable.\n    \"\"\"\n    @property\n    @abstractmethod\n    def output_cls(self) -> Type[Model]:\n        pass\n    @abstractmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "PydanticProgramMode",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.types",
        "description": "reference_code.llama-index-core.llama_index.core.types",
        "peekOfCode": "class PydanticProgramMode(str, Enum):\n    \"\"\"Pydantic program mode.\"\"\"\n    DEFAULT = \"default\"\n    OPENAI = \"openai\"\n    LLM = \"llm\"\n    FUNCTION = \"function\"\n    GUIDANCE = \"guidance\"\n    LM_FORMAT_ENFORCER = \"lm-format-enforcer\"\nclass Thread(threading.Thread):\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "Thread",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.types",
        "description": "reference_code.llama-index-core.llama_index.core.types",
        "peekOfCode": "class Thread(threading.Thread):\n    \"\"\"\n    A wrapper for threading.Thread that copies the current context and uses the copy to run the target.\n    \"\"\"\n    def __init__(\n        self,\n        group: Optional[Any] = None,\n        target: Optional[Callable[..., Any]] = None,\n        name: Optional[str] = None,\n        args: Tuple[Any, ...] = (),",
        "detail": "reference_code.llama-index-core.llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "Model",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.types",
        "description": "reference_code.llama-index-core.llama_index.core.types",
        "peekOfCode": "Model = TypeVar(\"Model\", bound=BaseModel)\nTokenGen = Generator[str, None, None]\nTokenAsyncGen = AsyncGenerator[str, None]\nRESPONSE_TEXT_TYPE = Union[BaseModel, str, TokenGen, TokenAsyncGen]\nif TYPE_CHECKING:\n    from llama_index.core.program.utils import FlexibleModel\n# TODO: move into a `core` folder\n# NOTE: this is necessary to make it compatible with pydantic\nclass BaseOutputParser(DispatcherSpanMixin, ABC):\n    \"\"\"Output parser class.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "TokenGen",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.types",
        "description": "reference_code.llama-index-core.llama_index.core.types",
        "peekOfCode": "TokenGen = Generator[str, None, None]\nTokenAsyncGen = AsyncGenerator[str, None]\nRESPONSE_TEXT_TYPE = Union[BaseModel, str, TokenGen, TokenAsyncGen]\nif TYPE_CHECKING:\n    from llama_index.core.program.utils import FlexibleModel\n# TODO: move into a `core` folder\n# NOTE: this is necessary to make it compatible with pydantic\nclass BaseOutputParser(DispatcherSpanMixin, ABC):\n    \"\"\"Output parser class.\"\"\"\n    @abstractmethod",
        "detail": "reference_code.llama-index-core.llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "TokenAsyncGen",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.types",
        "description": "reference_code.llama-index-core.llama_index.core.types",
        "peekOfCode": "TokenAsyncGen = AsyncGenerator[str, None]\nRESPONSE_TEXT_TYPE = Union[BaseModel, str, TokenGen, TokenAsyncGen]\nif TYPE_CHECKING:\n    from llama_index.core.program.utils import FlexibleModel\n# TODO: move into a `core` folder\n# NOTE: this is necessary to make it compatible with pydantic\nclass BaseOutputParser(DispatcherSpanMixin, ABC):\n    \"\"\"Output parser class.\"\"\"\n    @abstractmethod\n    def parse(self, output: str) -> Any:",
        "detail": "reference_code.llama-index-core.llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "RESPONSE_TEXT_TYPE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.types",
        "description": "reference_code.llama-index-core.llama_index.core.types",
        "peekOfCode": "RESPONSE_TEXT_TYPE = Union[BaseModel, str, TokenGen, TokenAsyncGen]\nif TYPE_CHECKING:\n    from llama_index.core.program.utils import FlexibleModel\n# TODO: move into a `core` folder\n# NOTE: this is necessary to make it compatible with pydantic\nclass BaseOutputParser(DispatcherSpanMixin, ABC):\n    \"\"\"Output parser class.\"\"\"\n    @abstractmethod\n    def parse(self, output: str) -> Any:\n        \"\"\"Parse, validate, and correct errors programmatically.\"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.types",
        "documentation": {}
    },
    {
        "label": "GlobalsHelper",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "class GlobalsHelper:\n    \"\"\"Helper to retrieve globals with asynchronous NLTK data loading.\"\"\"\n    _stopwords: Optional[List[str]] = None\n    _punkt_tokenizer: Optional[\"PunktSentenceTokenizer\"] = None\n    _nltk_data_dir: Optional[str] = None\n    def wait_for_nltk_check(self) -> None:\n        \"\"\"Initialize NLTK data download.\"\"\"\n        from nltk.data import path as nltk_path\n        # Set up NLTK data directory\n        if \"NLTK_DATA\" in os.environ:",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "Tokenizer",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "class Tokenizer(Protocol):\n    def encode(self, text: str, *args: Any, **kwargs: Any) -> List[Any]: ...\ndef set_global_tokenizer(tokenizer: Union[Tokenizer, Callable[[str], list]]) -> None:\n    import llama_index.core\n    if isinstance(tokenizer, Tokenizer):\n        llama_index.core.global_tokenizer = tokenizer.encode\n    else:\n        llama_index.core.global_tokenizer = tokenizer\ndef get_tokenizer(model_name: str = \"gpt-3.5-turbo\") -> Callable[[str], List]:\n    import llama_index.core",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "ErrorToRetry",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "class ErrorToRetry:\n    \"\"\"\n    Exception types that should be retried.\n    Args:\n        exception_cls (Type[Exception]): Class of exception.\n        check_fn (Optional[Callable[[Any]], bool]]):\n            A function that takes an exception instance as input and returns\n            whether to retry.\n    \"\"\"\n    exception_cls: Type[Exception]",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "set_global_tokenizer",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "def set_global_tokenizer(tokenizer: Union[Tokenizer, Callable[[str], list]]) -> None:\n    import llama_index.core\n    if isinstance(tokenizer, Tokenizer):\n        llama_index.core.global_tokenizer = tokenizer.encode\n    else:\n        llama_index.core.global_tokenizer = tokenizer\ndef get_tokenizer(model_name: str = \"gpt-3.5-turbo\") -> Callable[[str], List]:\n    import llama_index.core\n    if llama_index.core.global_tokenizer is None:\n        tiktoken_import_err = (",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tokenizer",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "def get_tokenizer(model_name: str = \"gpt-3.5-turbo\") -> Callable[[str], List]:\n    import llama_index.core\n    if llama_index.core.global_tokenizer is None:\n        tiktoken_import_err = (\n            \"`tiktoken` package not found, please run `pip install tiktoken`\"\n        )\n        try:\n            import tiktoken\n        except ImportError:\n            raise ImportError(tiktoken_import_err)",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_new_id",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "def get_new_id(d: Set) -> str:\n    \"\"\"Get a new ID.\"\"\"\n    while True:\n        new_id = str(uuid.uuid4())\n        if new_id not in d:\n            break\n    return new_id\ndef get_new_int_id(d: Set) -> int:\n    \"\"\"Get a new integer ID.\"\"\"\n    while True:",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_new_int_id",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "def get_new_int_id(d: Set) -> int:\n    \"\"\"Get a new integer ID.\"\"\"\n    while True:\n        new_id = random.randint(0, sys.maxsize)\n        if new_id not in d:\n            break\n    return new_id\n@contextmanager\ndef temp_set_attrs(obj: Any, **kwargs: Any) -> Generator:\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "temp_set_attrs",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "def temp_set_attrs(obj: Any, **kwargs: Any) -> Generator:\n    \"\"\"\n    Temporary setter.\n    Utility class for setting a temporary value for an attribute on a class.\n    Taken from: https://tinyurl.com/2p89xymh\n    \"\"\"\n    prev_values = {k: getattr(obj, k) for k in kwargs}\n    for k, v in kwargs.items():\n        setattr(obj, k, v)\n    try:",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "retry_on_exceptions_with_backoff",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "def retry_on_exceptions_with_backoff(\n    lambda_fn: Callable,\n    errors_to_retry: List[ErrorToRetry],\n    max_tries: int = 10,\n    min_backoff_secs: float = 0.5,\n    max_backoff_secs: float = 60.0,\n) -> Any:\n    \"\"\"\n    Execute lambda function with retries and exponential backoff.\n    Args:",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_retry_on_exceptions_with_backoff_decorator",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "def get_retry_on_exceptions_with_backoff_decorator(\n    *retry_args: Any, **retry_kwargs: Any\n) -> Callable:\n    \"\"\"Return a decorator that retries with exponential backoff on provided exceptions.\"\"\"\n    def decorator(func: Callable) -> Callable:\n        @wraps(func)\n        def wrapper(*func_args: Any, **func_kwargs: Any) -> Any:\n            return retry_on_exceptions_with_backoff(\n                lambda: func(*func_args, **func_kwargs), *retry_args, **retry_kwargs\n            )",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "truncate_text",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "def truncate_text(text: str, max_length: int) -> str:\n    \"\"\"Truncate text to a maximum length.\"\"\"\n    if len(text) <= max_length:\n        return text\n    return text[: max_length - 3] + \"...\"\ndef iter_batch(iterable: Union[Iterable, Generator], size: int) -> Iterable:\n    \"\"\"\n    Iterate over an iterable in batches.\n    >>> list(iter_batch([1,2,3,4,5], 3))\n    [[1, 2, 3], [4, 5]]",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "iter_batch",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "def iter_batch(iterable: Union[Iterable, Generator], size: int) -> Iterable:\n    \"\"\"\n    Iterate over an iterable in batches.\n    >>> list(iter_batch([1,2,3,4,5], 3))\n    [[1, 2, 3], [4, 5]]\n    \"\"\"\n    source_iter = iter(iterable)\n    while source_iter:\n        b = list(islice(source_iter, size))\n        if len(b) == 0:",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "concat_dirs",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "def concat_dirs(dirname: str, basename: str) -> str:\n    \"\"\"\n    Append basename to dirname, avoiding backslashes when running on windows.\n    os.path.join(dirname, basename) will add a backslash before dirname if\n    basename does not end with a slash, so we make sure it does.\n    \"\"\"\n    dirname += \"/\" if dirname[-1] != \"/\" else \"\"\n    return os.path.join(dirname, basename)\ndef get_tqdm_iterable(items: Iterable, show_progress: bool, desc: str) -> Iterable:\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_tqdm_iterable",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "def get_tqdm_iterable(items: Iterable, show_progress: bool, desc: str) -> Iterable:\n    \"\"\"\n    Optionally get a tqdm iterable. Ensures tqdm.auto is used.\n    \"\"\"\n    _iterator = items\n    if show_progress:\n        try:\n            from tqdm.auto import tqdm\n            return tqdm(items, desc=desc)\n        except ImportError:",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "count_tokens",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "def count_tokens(text: str) -> int:\n    tokenizer = get_tokenizer()\n    tokens = tokenizer(text)\n    return len(tokens)\ndef get_transformer_tokenizer_fn(model_name: str) -> Callable[[str], List[str]]:\n    \"\"\"\n    Args:\n        model_name(str): the model name of the tokenizer.\n                        For instance, fxmarty/tiny-llama-fast-tokenizer.\n    \"\"\"",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_transformer_tokenizer_fn",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "def get_transformer_tokenizer_fn(model_name: str) -> Callable[[str], List[str]]:\n    \"\"\"\n    Args:\n        model_name(str): the model name of the tokenizer.\n                        For instance, fxmarty/tiny-llama-fast-tokenizer.\n    \"\"\"\n    try:\n        from transformers import AutoTokenizer  # pants: no-infer-dep\n    except ImportError:\n        raise ValueError(",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_cache_dir",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "def get_cache_dir() -> str:\n    \"\"\"\n    Locate a platform-appropriate cache directory for llama_index,\n    and create it if it doesn't yet exist.\n    \"\"\"\n    # User override\n    if \"LLAMA_INDEX_CACHE_DIR\" in os.environ:\n        path = Path(os.environ[\"LLAMA_INDEX_CACHE_DIR\"])\n    else:\n        path = Path(platformdirs.user_cache_dir(\"llama_index\"))",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "add_sync_version",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "def add_sync_version(func: Any) -> Any:\n    \"\"\"\n    Decorator for adding sync version of an async function. The sync version\n    is added as a function attribute to the original function, func.\n    Args:\n        func(Any): the async function for which a sync variant will be built.\n    \"\"\"\n    assert asyncio.iscoroutinefunction(func)\n    @wraps(func)\n    def _wrapper(*args: Any, **kwds: Any) -> Any:",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "get_color_mapping",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "def get_color_mapping(\n    items: List[str], use_llama_index_colors: bool = True\n) -> Dict[str, str]:\n    \"\"\"\n    Get a mapping of items to colors.\n    Args:\n        items (List[str]): List of items to be mapped to colors.\n        use_llama_index_colors (bool, optional): Flag to indicate\n        whether to use LlamaIndex colors or ANSI colors.\n            Defaults to True.",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "print_text",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "def print_text(text: str, color: Optional[str] = None, end: str = \"\") -> None:\n    \"\"\"\n    Print the text with the specified color.\n    Args:\n        text (str): Text to be printed.\n        color (str, optional): Color to be applied to the text. Supported colors are:\n            llama_pink, llama_blue, llama_turquoise, llama_lavender,\n            red, green, yellow, blue, magenta, cyan, pink.\n        end (str, optional): String appended after the last character of the text.\n    Returns:",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "infer_torch_device",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "def infer_torch_device() -> str:\n    \"\"\"Infer the input to torch.device.\"\"\"\n    try:\n        has_cuda = torch.cuda.is_available()\n    except NameError:\n        import torch  # pants: no-infer-dep\n        has_cuda = torch.cuda.is_available()\n    if has_cuda:\n        return \"cuda\"\n    if torch.backends.mps.is_available():",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "unit_generator",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "def unit_generator(x: Any) -> Generator[Any, None, None]:\n    \"\"\"\n    A function that returns a generator of a single element.\n    Args:\n        x (Any): the element to build yield\n    Yields:\n        Any: the single element\n    \"\"\"\n    yield x\nasync def async_unit_generator(x: Any) -> AsyncGenerator[Any, None]:",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "resolve_binary",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "def resolve_binary(\n    raw_bytes: Optional[bytes] = None,\n    path: Optional[Union[str, Path]] = None,\n    url: Optional[str] = None,\n    as_base64: bool = False,\n) -> BytesIO:\n    \"\"\"\n    Resolve binary data from various sources into a BytesIO object.\n    Args:\n        raw_bytes: Raw bytes data",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "globals_helper",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "globals_helper = GlobalsHelper()\n# Global Tokenizer\n@runtime_checkable\nclass Tokenizer(Protocol):\n    def encode(self, text: str, *args: Any, **kwargs: Any) -> List[Any]: ...\ndef set_global_tokenizer(tokenizer: Union[Tokenizer, Callable[[str], list]]) -> None:\n    import llama_index.core\n    if isinstance(tokenizer, Tokenizer):\n        llama_index.core.global_tokenizer = tokenizer.encode\n    else:",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "SAMPLE_TEXT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "SAMPLE_TEXT = \"\"\"\nContext\nLLMs are a phenomenal piece of technology for knowledge generation and reasoning.\nThey are pre-trained on large amounts of publicly available data.\nHow do we best augment LLMs with our own private data?\nWe need a comprehensive toolkit to help perform this data augmentation for LLMs.\nProposed Solution\nThat's where LlamaIndex comes in. LlamaIndex is a \"data framework\" to help\nyou build LLM  apps. It provides the following tools:\nOffers data connectors to ingest your existing data sources and data formats",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "_LLAMA_INDEX_COLORS",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "_LLAMA_INDEX_COLORS = {\n    \"llama_pink\": \"38;2;237;90;200\",\n    \"llama_blue\": \"38;2;90;149;237\",\n    \"llama_turquoise\": \"38;2;11;159;203\",\n    \"llama_lavender\": \"38;2;155;135;227\",\n}\n_ANSI_COLORS = {\n    \"red\": \"31\",\n    \"green\": \"32\",\n    \"yellow\": \"33\",",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "_ANSI_COLORS",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.llama_index.core.utils",
        "description": "reference_code.llama-index-core.llama_index.core.utils",
        "peekOfCode": "_ANSI_COLORS = {\n    \"red\": \"31\",\n    \"green\": \"32\",\n    \"yellow\": \"33\",\n    \"blue\": \"34\",\n    \"magenta\": \"35\",\n    \"cyan\": \"36\",\n    \"pink\": \"38;5;200\",\n}\ndef get_color_mapping(",
        "detail": "reference_code.llama-index-core.llama_index.core.utils",
        "documentation": {}
    },
    {
        "label": "vector_memory_initial_msgs",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.memory.test_simple_composable",
        "description": "reference_code.llama-index-core.tests.agent.memory.test_simple_composable",
        "peekOfCode": "def vector_memory_initial_msgs() -> List[ChatMessage]:\n    return [\n        ChatMessage.from_str(\"Jerry likes juice.\", \"user\"),\n        ChatMessage.from_str(\"That's nice.\", \"assistant\"),\n    ]\ndef mock_get_text_embedding(text: str) -> List[float]:\n    \"\"\"Mock get text embedding.\"\"\"\n    # assume dimensions are 5\n    if text == \"Jerry likes juice.\":\n        return [1, 1, 0, 0, 0]",
        "detail": "reference_code.llama-index-core.tests.agent.memory.test_simple_composable",
        "documentation": {}
    },
    {
        "label": "mock_get_text_embedding",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.memory.test_simple_composable",
        "description": "reference_code.llama-index-core.tests.agent.memory.test_simple_composable",
        "peekOfCode": "def mock_get_text_embedding(text: str) -> List[float]:\n    \"\"\"Mock get text embedding.\"\"\"\n    # assume dimensions are 5\n    if text == \"Jerry likes juice.\":\n        return [1, 1, 0, 0, 0]\n    elif text == \"Bob likes burgers.\":\n        return [0, 1, 0, 1, 0]\n    elif text == \"Alice likes apples.\":\n        return [0, 0, 1, 0, 0]\n    elif text == \"What does Jerry like?\":",
        "detail": "reference_code.llama-index-core.tests.agent.memory.test_simple_composable",
        "documentation": {}
    },
    {
        "label": "mock_get_text_embeddings",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.memory.test_simple_composable",
        "description": "reference_code.llama-index-core.tests.agent.memory.test_simple_composable",
        "peekOfCode": "def mock_get_text_embeddings(texts: List[str]) -> List[List[float]]:\n    \"\"\"Mock get text embeddings.\"\"\"\n    return [mock_get_text_embedding(text) for text in texts]\n@patch.object(MockEmbedding, \"_get_text_embedding\", side_effect=mock_get_text_embedding)\n@patch.object(\n    MockEmbedding, \"_get_text_embeddings\", side_effect=mock_get_text_embeddings\n)\ndef test_simple_composable_memory(\n    _mock_get_text_embeddings: Any,\n    _mock_get_text_embedding: Any,",
        "detail": "reference_code.llama-index-core.tests.agent.memory.test_simple_composable",
        "documentation": {}
    },
    {
        "label": "test_simple_composable_memory",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.memory.test_simple_composable",
        "description": "reference_code.llama-index-core.tests.agent.memory.test_simple_composable",
        "peekOfCode": "def test_simple_composable_memory(\n    _mock_get_text_embeddings: Any,\n    _mock_get_text_embedding: Any,\n    vector_memory_initial_msgs: List[ChatMessage],\n) -> None:\n    \"\"\"Test vector memory.\"\"\"\n    # Arrange\n    vector_memory = VectorMemory.from_defaults(\n        vector_store=None,\n        embed_model=MockEmbedding(embed_dim=5),",
        "detail": "reference_code.llama-index-core.tests.agent.memory.test_simple_composable",
        "documentation": {}
    },
    {
        "label": "test_repeated_secondary_history",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.memory.test_simple_composable",
        "description": "reference_code.llama-index-core.tests.agent.memory.test_simple_composable",
        "peekOfCode": "def test_repeated_secondary_history(\n    _mock_get_text_embeddings: Any,\n    _mock_get_text_embedding: Any,\n    vector_memory_initial_msgs: List[ChatMessage],\n) -> None:\n    \"\"\"Test event where historical messages exist in both secondary and primary.\"\"\"\n    # Arrange\n    vector_memory = VectorMemory.from_defaults(\n        vector_store=None,\n        embed_model=MockEmbedding(embed_dim=5),",
        "detail": "reference_code.llama-index-core.tests.agent.memory.test_simple_composable",
        "documentation": {}
    },
    {
        "label": "mock_get_text_embedding",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.memory.test_vector_memory",
        "description": "reference_code.llama-index-core.tests.agent.memory.test_vector_memory",
        "peekOfCode": "def mock_get_text_embedding(text: str) -> List[float]:\n    \"\"\"Mock get text embedding.\"\"\"\n    # assume dimensions are 5\n    if text == \"Jerry likes juice.\":\n        return [1, 1, 0, 0, 0]\n    elif text == \"Bob likes burgers.\":\n        return [0, 1, 0, 1, 0]\n    elif text == \"Alice likes apples.\":\n        return [0, 0, 1, 0, 0]\n    elif text == \"What does Jerry like?\":",
        "detail": "reference_code.llama-index-core.tests.agent.memory.test_vector_memory",
        "documentation": {}
    },
    {
        "label": "mock_get_text_embeddings",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.memory.test_vector_memory",
        "description": "reference_code.llama-index-core.tests.agent.memory.test_vector_memory",
        "peekOfCode": "def mock_get_text_embeddings(texts: List[str]) -> List[List[float]]:\n    \"\"\"Mock get text embeddings.\"\"\"\n    return [mock_get_text_embedding(text) for text in texts]\n@patch.object(MockEmbedding, \"_get_text_embedding\", side_effect=mock_get_text_embedding)\n@patch.object(\n    MockEmbedding, \"_get_text_embeddings\", side_effect=mock_get_text_embeddings\n)\ndef test_vector_memory(\n    _mock_get_text_embeddings: Any, _mock_get_text_embedding: Any\n) -> None:",
        "detail": "reference_code.llama-index-core.tests.agent.memory.test_vector_memory",
        "documentation": {}
    },
    {
        "label": "test_vector_memory",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.memory.test_vector_memory",
        "description": "reference_code.llama-index-core.tests.agent.memory.test_vector_memory",
        "peekOfCode": "def test_vector_memory(\n    _mock_get_text_embeddings: Any, _mock_get_text_embedding: Any\n) -> None:\n    \"\"\"Test vector memory.\"\"\"\n    # arrange\n    embed_model = MockEmbedding(embed_dim=5)\n    vector_memory = VectorMemory.from_defaults(\n        vector_store=None,\n        embed_model=embed_model,\n        retriever_kwargs={\"similarity_top_k\": 1},",
        "detail": "reference_code.llama-index-core.tests.agent.memory.test_vector_memory",
        "documentation": {}
    },
    {
        "label": "test_partial_formatted_system_prompt",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.react.test_prompt_customization",
        "description": "reference_code.llama-index-core.tests.agent.react.test_prompt_customization",
        "peekOfCode": "def test_partial_formatted_system_prompt():\n    \"\"\"Partially formatted context should be preserved.\"\"\"\n    agent = ReActAgent()\n    prompt = PromptTemplate(\n        dedent(\n            \"\"\"\\\n            Required template variables:\n            {tool_desc}\n            {tool_names}\n            Additional variables:",
        "detail": "reference_code.llama-index-core.tests.agent.react.test_prompt_customization",
        "documentation": {}
    },
    {
        "label": "test_parse_action_reasoning_step",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "description": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "peekOfCode": "def test_parse_action_reasoning_step() -> None:\n    mock_input_text = \"\"\"\\\nThought: Gotta use a tool.\nAction: tool\nAction Input: {'pages': ['coffee'] /* comment */, 'load_kwargs': {}, 'query_str': ''}, along those lines.\n\"\"\"\n    assert parse_action_reasoning_step(mock_input_text).action_input == {\n        \"pages\": [\"coffee\"],\n        \"load_kwargs\": {},\n        \"query_str\": \"\",",
        "detail": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "documentation": {}
    },
    {
        "label": "test_extract_tool_use",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "description": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "peekOfCode": "def test_extract_tool_use() -> None:\n    mock_input_text = \"\"\"\\\nThought: I need to use a tool to help me answer the question.\nAction: add\nAction Input: {\"a\": 1, \"b\": 1}\n\"\"\"\n    thought, action, action_input = extract_tool_use(mock_input_text)\n    assert thought == \"I need to use a tool to help me answer the question.\"\n    assert action == \"add\"\n    assert action_input == '{\"a\": 1, \"b\": 1}'",
        "detail": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "documentation": {}
    },
    {
        "label": "test_extract_tool_use_no_thought",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "description": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "peekOfCode": "def test_extract_tool_use_no_thought() -> None:\n    mock_input_text = \"\"\"\\\nI need to use a tool to help me answer the question.\nAction: add\nAction Input: {\"a\": 1, \"b\": 1}\n\"\"\"\n    thought, action, action_input = extract_tool_use(mock_input_text)\n    assert thought == \"I need to use a tool to help me answer the question.\"\n    assert action == \"add\"\n    assert action_input == '{\"a\": 1, \"b\": 1}'",
        "detail": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "documentation": {}
    },
    {
        "label": "test_extract_tool_use_multiline",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "description": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "peekOfCode": "def test_extract_tool_use_multiline() -> None:\n    mock_input_text = \"\"\"\\\nThought: I need to use a tool to help me answer the question.\nAction: add\nAction Input: {\"a\": 1, \"b\": 1}\n\"\"\"\n    thought, action, action_input = extract_tool_use(mock_input_text)\n    assert thought == \"I need to use a tool to help me answer the question.\"\n    assert action == \"add\"\n    assert action_input == '{\"a\": 1, \"b\": 1}'",
        "detail": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "documentation": {}
    },
    {
        "label": "test_extract_tool_use_with_nested_dicts",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "description": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "peekOfCode": "def test_extract_tool_use_with_nested_dicts() -> None:\n    mock_input_text = \"\"\"\\\nThought: Gotta use a tool.\nAction: tool\nAction Input: {\"a\": 1, \"b\": {}}\n\"\"\"\n    thought, action, action_input = extract_tool_use(mock_input_text)\n    assert thought == \"Gotta use a tool.\"\n    assert action == \"tool\"\n    assert action_input == '{\"a\": 1, \"b\": {}}'",
        "detail": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "documentation": {}
    },
    {
        "label": "test_extract_tool_use_",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "description": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "peekOfCode": "def test_extract_tool_use_() -> None:\n    mock_input_text = \"\"\"\\\nThought: I need to use a tool to help me answer the question.\nAction: add\nAction Input: QueryEngineTool({\"a\": 1, \"b\": 1})\n\"\"\"\n    thought, action, action_input = extract_tool_use(mock_input_text)\n    assert thought == \"I need to use a tool to help me answer the question.\"\n    assert action == \"add\"\n    assert action_input == '{\"a\": 1, \"b\": 1}'",
        "detail": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "documentation": {}
    },
    {
        "label": "test_extract_tool_use_extra_action_output",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "description": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "peekOfCode": "def test_extract_tool_use_extra_action_output() -> None:\n    mock_input_text = \"\"\"\\\nThought: I need to use a tool to help me answer the question.\nAction: add (add two numbers)\nAction Input: {\"a\": 1, \"b\": 1}\n\"\"\"\n    thought, action, action_input = extract_tool_use(mock_input_text)\n    assert thought == \"I need to use a tool to help me answer the question.\"\n    assert action == \"add\"\n    assert action_input == '{\"a\": 1, \"b\": 1}'",
        "detail": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "documentation": {}
    },
    {
        "label": "test_extract_tool_number",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "description": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "peekOfCode": "def test_extract_tool_number() -> None:\n    mock_input_text = \"\"\"\\\nThought: I need to use a tool to help me answer the question.\nAction: add2\nAction Input: {\"a\": 1, \"b\": 1}\n\"\"\"\n    thought, action, action_input = extract_tool_use(mock_input_text)\n    assert thought == \"I need to use a tool to help me answer the question.\"\n    assert action == \"add2\"\n    assert action_input == '{\"a\": 1, \"b\": 1}'",
        "detail": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "documentation": {}
    },
    {
        "label": "test_extract_tool_use_multiline_action_input",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "description": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "peekOfCode": "def test_extract_tool_use_multiline_action_input() -> None:\n    mock_input_text = \"\"\"\\\nThought: I need to use a tool to help me answer the question.\nAction: add\nAction Input: {\n    \"a\": 1,\n    \"b\": 1\n}\n\"\"\"\n    thought, action, action_input = extract_tool_use(mock_input_text)",
        "detail": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "documentation": {}
    },
    {
        "label": "test_extract_tool_use_spurious_newlines",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "description": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "peekOfCode": "def test_extract_tool_use_spurious_newlines() -> None:\n    mock_input_text = \"\"\"\\\nThought: I need to use a tool to help me answer the question.\nAction: add\nAction Input: {\"a\": 1, \"b\": 1}\n\"\"\"\n    thought, action, action_input = extract_tool_use(mock_input_text)\n    assert thought == \"I need to use a tool to help me answer the question.\"\n    assert action == \"add\"\n    assert action_input == '{\"a\": 1, \"b\": 1}'",
        "detail": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "documentation": {}
    },
    {
        "label": "test_extract_tool_use_with_Chinese_characters",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "description": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "peekOfCode": "def test_extract_tool_use_with_Chinese_characters() -> None:\n    mock_input_text = \"\"\"\\\nThought: I need to use a tool to help me answer the question.\nAction: \nAction Input: {\"a\": 1, \"b\": 1}\n\"\"\"\n    thought, action, action_input = extract_tool_use(mock_input_text)\n    assert thought == \"I need to use a tool to help me answer the question.\"\n    assert action == \"\"\n    assert action_input == '{\"a\": 1, \"b\": 1}'",
        "detail": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "documentation": {}
    },
    {
        "label": "test_extract_final_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "description": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "peekOfCode": "def test_extract_final_response() -> None:\n    mock_input_text = \"\"\"\\\nThought: I have enough information to answer the question without using any more tools.\nAnswer: 2\n\"\"\"\n    expected_thought = (\n        \"I have enough information to answer the question without using any more tools.\"\n    )\n    thought, answer = extract_final_response(mock_input_text)\n    assert thought == expected_thought",
        "detail": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "documentation": {}
    },
    {
        "label": "test_extract_final_response_multiline_answer",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "description": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "peekOfCode": "def test_extract_final_response_multiline_answer() -> None:\n    mock_input_text = \"\"\"\\\nThought: I have enough information to answer the question without using any more tools.\nAnswer: Here is the answer:\nThis is the second line.\n\"\"\"\n    expected_thought = (\n        \"I have enough information to answer the question without using any more tools.\"\n    )\n    thought, answer = extract_final_response(mock_input_text)",
        "detail": "reference_code.llama-index-core.tests.agent.react.test_react_output_parser",
        "documentation": {}
    },
    {
        "label": "Structure",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "description": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "peekOfCode": "class Structure(BaseModel):\n    hello: str\n    world: int\nclass TestLLM(LLM):\n    def __init__(self, responses: List[ChatMessage], structured_response: str):\n        super().__init__()\n        self._responses = responses\n        self._structured_response = structured_response\n        self._response_index = 0\n    @property",
        "detail": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "documentation": {}
    },
    {
        "label": "TestLLM",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "description": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "peekOfCode": "class TestLLM(LLM):\n    def __init__(self, responses: List[ChatMessage], structured_response: str):\n        super().__init__()\n        self._responses = responses\n        self._structured_response = structured_response\n        self._response_index = 0\n    @property\n    def metadata(self) -> LLMMetadata:\n        return LLMMetadata(is_function_calling_model=True)\n    async def astream_chat(",
        "detail": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "documentation": {}
    },
    {
        "label": "chat_messages",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "description": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "peekOfCode": "def chat_messages() -> List[ChatMessage]:\n    return [\n        ChatMessage(role=\"user\", blocks=[TextBlock(text=\"hello\")]),\n        ChatMessage(role=\"assistant\", blocks=[TextBlock(text=\"hello back\")]),\n        ChatMessage(role=\"user\", blocks=[TextBlock(text=\"how are you?\")]),\n        ChatMessage(role=\"assistant\", blocks=[TextBlock(text=\"I am good, thank you.\")]),\n    ]\n@pytest.fixture()\ndef chat_messages_sys(chat_messages: List[ChatMessage]) -> List[ChatMessage]:\n    return [",
        "detail": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "documentation": {}
    },
    {
        "label": "chat_messages_sys",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "description": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "peekOfCode": "def chat_messages_sys(chat_messages: List[ChatMessage]) -> List[ChatMessage]:\n    return [\n        ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n        *chat_messages,\n    ]\n@pytest.fixture\ndef xml_string() -> str:\n    return \"<current_conversation>\\n\\t<user>\\n\\t\\t<message>hello</message>\\n\\t</user>\\n\\t<assistant>\\n\\t\\t<message>hello back</message>\\n\\t</assistant>\\n\\t<user>\\n\\t\\t<message>how are you?</message>\\n\\t</user>\\n\\t<assistant>\\n\\t\\t<message>I am good, thank you.</message>\\n\\t</assistant>\\n</current_conversation>\\n\\nGiven the conversation, format the output according to the provided schema.\"\n@pytest.fixture\ndef xml_string_sys() -> str:",
        "detail": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "documentation": {}
    },
    {
        "label": "xml_string",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "description": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "peekOfCode": "def xml_string() -> str:\n    return \"<current_conversation>\\n\\t<user>\\n\\t\\t<message>hello</message>\\n\\t</user>\\n\\t<assistant>\\n\\t\\t<message>hello back</message>\\n\\t</assistant>\\n\\t<user>\\n\\t\\t<message>how are you?</message>\\n\\t</user>\\n\\t<assistant>\\n\\t\\t<message>I am good, thank you.</message>\\n\\t</assistant>\\n</current_conversation>\\n\\nGiven the conversation, format the output according to the provided schema.\"\n@pytest.fixture\ndef xml_string_sys() -> str:\n    return \"<current_conversation>\\n\\t<system>\\n\\t\\t<message>You are a helpful assistant.</message>\\n\\t</system>\\n\\t<user>\\n\\t\\t<message>hello</message>\\n\\t</user>\\n\\t<assistant>\\n\\t\\t<message>hello back</message>\\n\\t</assistant>\\n\\t<user>\\n\\t\\t<message>how are you?</message>\\n\\t</user>\\n\\t<assistant>\\n\\t\\t<message>I am good, thank you.</message>\\n\\t</assistant>\\n</current_conversation>\\n\\nGiven the conversation, format the output according to the provided schema.\"\n@pytest.fixture\ndef structured_response() -> str:\n    return Structure(hello=\"test\", world=1).model_dump_json()\ndef test_messages_to_xml(chat_messages: List[ChatMessage], xml_string: str) -> None:\n    msg = messages_to_xml_format(chat_messages)",
        "detail": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "documentation": {}
    },
    {
        "label": "xml_string_sys",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "description": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "peekOfCode": "def xml_string_sys() -> str:\n    return \"<current_conversation>\\n\\t<system>\\n\\t\\t<message>You are a helpful assistant.</message>\\n\\t</system>\\n\\t<user>\\n\\t\\t<message>hello</message>\\n\\t</user>\\n\\t<assistant>\\n\\t\\t<message>hello back</message>\\n\\t</assistant>\\n\\t<user>\\n\\t\\t<message>how are you?</message>\\n\\t</user>\\n\\t<assistant>\\n\\t\\t<message>I am good, thank you.</message>\\n\\t</assistant>\\n</current_conversation>\\n\\nGiven the conversation, format the output according to the provided schema.\"\n@pytest.fixture\ndef structured_response() -> str:\n    return Structure(hello=\"test\", world=1).model_dump_json()\ndef test_messages_to_xml(chat_messages: List[ChatMessage], xml_string: str) -> None:\n    msg = messages_to_xml_format(chat_messages)\n    assert len(msg) == 1\n    assert isinstance(msg[0], ChatMessage)\n    s = \"\"",
        "detail": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "documentation": {}
    },
    {
        "label": "structured_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "description": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "peekOfCode": "def structured_response() -> str:\n    return Structure(hello=\"test\", world=1).model_dump_json()\ndef test_messages_to_xml(chat_messages: List[ChatMessage], xml_string: str) -> None:\n    msg = messages_to_xml_format(chat_messages)\n    assert len(msg) == 1\n    assert isinstance(msg[0], ChatMessage)\n    s = \"\"\n    for block in msg[0].blocks:\n        s += block.text\n    assert s == xml_string",
        "detail": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "documentation": {}
    },
    {
        "label": "test_messages_to_xml",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "description": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "peekOfCode": "def test_messages_to_xml(chat_messages: List[ChatMessage], xml_string: str) -> None:\n    msg = messages_to_xml_format(chat_messages)\n    assert len(msg) == 1\n    assert isinstance(msg[0], ChatMessage)\n    s = \"\"\n    for block in msg[0].blocks:\n        s += block.text\n    assert s == xml_string\ndef test_messages_to_xml_sys(\n    chat_messages_sys: List[ChatMessage], xml_string_sys: str",
        "detail": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "documentation": {}
    },
    {
        "label": "test_messages_to_xml_sys",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "description": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "peekOfCode": "def test_messages_to_xml_sys(\n    chat_messages_sys: List[ChatMessage], xml_string_sys: str\n) -> None:\n    msg = messages_to_xml_format(chat_messages_sys)\n    assert len(msg) == 2\n    assert isinstance(msg[0], ChatMessage)\n    assert msg[0].role == \"system\"\n    assert msg[0].content == \"You are a helpful assistant.\"\n    s = \"\"\n    for block in msg[1].blocks:",
        "detail": "reference_code.llama-index-core.tests.agent.utils.test_agent_utils",
        "documentation": {}
    },
    {
        "label": "TestLLM",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_agent_with_structured_output",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_agent_with_structured_output",
        "peekOfCode": "class TestLLM(LLM):\n    def __init__(self, responses: List[ChatMessage], structured_response: str):\n        super().__init__()\n        self._responses = responses\n        self._structured_response = structured_response\n        self._response_index = 0\n    @property\n    def metadata(self) -> LLMMetadata:\n        return LLMMetadata(is_function_calling_model=True)\n    async def astream_chat(",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_agent_with_structured_output",
        "documentation": {}
    },
    {
        "label": "Structure",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_agent_with_structured_output",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_agent_with_structured_output",
        "peekOfCode": "class Structure(BaseModel):\n    hello: str\n    world: int\n@pytest.fixture()\ndef function_agent_output_cls():\n    return FunctionAgent(\n        name=\"retriever\",\n        description=\"Manages data retrieval\",\n        system_prompt=\"You are a retrieval assistant.\",\n        llm=TestLLM(",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_agent_with_structured_output",
        "documentation": {}
    },
    {
        "label": "function_agent_output_cls",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_agent_with_structured_output",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_agent_with_structured_output",
        "peekOfCode": "def function_agent_output_cls():\n    return FunctionAgent(\n        name=\"retriever\",\n        description=\"Manages data retrieval\",\n        system_prompt=\"You are a retrieval assistant.\",\n        llm=TestLLM(\n            responses=[\n                ChatMessage(role=\"assistant\", content=\"Success with the FunctionAgent\")\n            ],\n            structured_response='{\"hello\":\"hello\",\"world\":1}',",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_agent_with_structured_output",
        "documentation": {}
    },
    {
        "label": "structured_function_fn",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_agent_with_structured_output",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_agent_with_structured_output",
        "peekOfCode": "def structured_function_fn(*args, **kwargs) -> dict:\n    return Structure(hello=\"bonjour\", world=2).model_dump()\nasync def astructured_function_fn(*args, **kwargs) -> dict:\n    return Structure(hello=\"guten tag\", world=3).model_dump()\n@pytest.fixture()\ndef function_agent_struct_fn():\n    return FunctionAgent(\n        name=\"retriever\",\n        description=\"Manages data retrieval\",\n        system_prompt=\"You are a retrieval assistant.\",",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_agent_with_structured_output",
        "documentation": {}
    },
    {
        "label": "function_agent_struct_fn",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_agent_with_structured_output",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_agent_with_structured_output",
        "peekOfCode": "def function_agent_struct_fn():\n    return FunctionAgent(\n        name=\"retriever\",\n        description=\"Manages data retrieval\",\n        system_prompt=\"You are a retrieval assistant.\",\n        llm=TestLLM(\n            responses=[\n                ChatMessage(role=\"assistant\", content=\"Success with the FunctionAgent\")\n            ],\n            structured_response='{\"hello\":\"hello\",\"world\":1}',",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_agent_with_structured_output",
        "documentation": {}
    },
    {
        "label": "function_agent_astruct_fn",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_agent_with_structured_output",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_agent_with_structured_output",
        "peekOfCode": "def function_agent_astruct_fn():\n    return FunctionAgent(\n        name=\"retriever\",\n        description=\"Manages data retrieval\",\n        system_prompt=\"You are a retrieval assistant.\",\n        llm=TestLLM(\n            responses=[\n                ChatMessage(role=\"assistant\", content=\"Success with the FunctionAgent\")\n            ],\n            structured_response='{\"hello\":\"hello\",\"world\":1}',",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_agent_with_structured_output",
        "documentation": {}
    },
    {
        "label": "skip_condition",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_agent_with_structured_output",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_agent_with_structured_output",
        "peekOfCode": "skip_condition = os.getenv(\"OPENAI_API_KEY\", None) is None\nclass TestLLM(LLM):\n    def __init__(self, responses: List[ChatMessage], structured_response: str):\n        super().__init__()\n        self._responses = responses\n        self._structured_response = structured_response\n        self._response_index = 0\n    @property\n    def metadata(self) -> LLMMetadata:\n        return LLMMetadata(is_function_calling_model=True)",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_agent_with_structured_output",
        "documentation": {}
    },
    {
        "label": "mock_llm",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_code_act_agent",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_code_act_agent",
        "peekOfCode": "def mock_llm():\n    # Create a mock that inherits from FunctionCallingLLM\n    class MockFunctionCallingLLM(FunctionCallingLLM):\n        get_tool_calls_from_response: Any = MagicMock(return_value=[])\n        def __init__(self) -> None:\n            super().__init__()\n            self._responses = []\n        @property\n        def metadata(self) -> LLMMetadata:\n            return LLMMetadata(",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_code_act_agent",
        "documentation": {}
    },
    {
        "label": "mock_code_execute_fn",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_code_act_agent",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_code_act_agent",
        "peekOfCode": "def mock_code_execute_fn():\n    return lambda code: \"Code executed\"\n@pytest.fixture()\ndef mock_memory():\n    memory = AsyncMock(spec=BaseMemory)\n    memory.aput = AsyncMock()\n    return memory\n@pytest.mark.asyncio\nasync def test_code_act_agent_basic_execution(\n    mock_llm, mock_code_execute_fn, mock_memory",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_code_act_agent",
        "documentation": {}
    },
    {
        "label": "mock_memory",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_code_act_agent",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_code_act_agent",
        "peekOfCode": "def mock_memory():\n    memory = AsyncMock(spec=BaseMemory)\n    memory.aput = AsyncMock()\n    return memory\n@pytest.mark.asyncio\nasync def test_code_act_agent_basic_execution(\n    mock_llm, mock_code_execute_fn, mock_memory\n):\n    # Setup mock response\n    mock_response = ChatResponse(",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_code_act_agent",
        "documentation": {}
    },
    {
        "label": "MathResult",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "peekOfCode": "class MathResult(BaseModel):\n    operation: str\n    result: int\nclass WrongMathResult(BaseModel):\n    operation: str\n    result: str\nclass Flavor(BaseModel):\n    flavor: str\n    extra_sugar: bool\ndef test_agent_workflow_start_event():",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "documentation": {}
    },
    {
        "label": "WrongMathResult",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "peekOfCode": "class WrongMathResult(BaseModel):\n    operation: str\n    result: str\nclass Flavor(BaseModel):\n    flavor: str\n    extra_sugar: bool\ndef test_agent_workflow_start_event():\n    event = AgentWorkflowStartEvent(\n        user_msg=\"Hello, world!\",\n        chat_history=[ChatMessage(role=\"user\", content=\"Hello, world!\")],",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "documentation": {}
    },
    {
        "label": "Flavor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "peekOfCode": "class Flavor(BaseModel):\n    flavor: str\n    extra_sugar: bool\ndef test_agent_workflow_start_event():\n    event = AgentWorkflowStartEvent(\n        user_msg=\"Hello, world!\",\n        chat_history=[ChatMessage(role=\"user\", content=\"Hello, world!\")],\n        max_iterations=10,\n    )\n    assert event.user_msg == \"Hello, world!\"",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "documentation": {}
    },
    {
        "label": "example_agent_output",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "peekOfCode": "def example_agent_output() -> dict:\n    return {\n        \"response\": ChatMessage(role=\"user\", content=\"30 times 2 is 60.\"),\n        \"tool_calls\": [\n            ToolSelection(\n                tool_id=\"1\", tool_name=\"multiply\", tool_kwargs={\"i\": 30, \"j\": 2}\n            )\n        ],\n        \"raw\": '{\"role\": \"user\", \"content\": \"30 times 2 is 60.\"}',\n        \"structured_response\": {\"operation\": \"30 times 2\", \"result\": 60},",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "documentation": {}
    },
    {
        "label": "example_agent_stream_structured_output",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "peekOfCode": "def example_agent_stream_structured_output() -> Tuple[dict, str]:\n    d = {\"output\": {\"flavor\": \"strawberry\", \"extra_sugar\": False}}\n    return d, json.dumps(d[\"output\"], indent=4)\nclass MathResult(BaseModel):\n    operation: str\n    result: int\nclass WrongMathResult(BaseModel):\n    operation: str\n    result: str\nclass Flavor(BaseModel):",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "documentation": {}
    },
    {
        "label": "test_agent_workflow_start_event",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "peekOfCode": "def test_agent_workflow_start_event():\n    event = AgentWorkflowStartEvent(\n        user_msg=\"Hello, world!\",\n        chat_history=[ChatMessage(role=\"user\", content=\"Hello, world!\")],\n        max_iterations=10,\n    )\n    assert event.user_msg == \"Hello, world!\"\n    assert event.chat_history[0].role.value == \"user\"\n    assert event.chat_history[0].content == \"Hello, world!\"\n    assert event.max_iterations == 10",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "documentation": {}
    },
    {
        "label": "test_agent_workflow_start_event_with_dict",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "peekOfCode": "def test_agent_workflow_start_event_with_dict():\n    event = AgentWorkflowStartEvent(\n        user_msg=\"Hello, world!\",\n        chat_history=[{\"role\": \"user\", \"content\": \"Hello, world!\"}],\n        max_iterations=10,\n    )\n    assert event.user_msg == \"Hello, world!\"\n    assert event.chat_history[0].role.value == \"user\"\n    assert event.chat_history[0].content == \"Hello, world!\"\n    assert event.max_iterations == 10",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "documentation": {}
    },
    {
        "label": "test_agent_workflow_start_event_to_dict",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "peekOfCode": "def test_agent_workflow_start_event_to_dict():\n    event = AgentWorkflowStartEvent(\n        user_msg=\"Hello, world!\",\n        chat_history=[ChatMessage(role=\"user\", content=\"Hello, world!\")],\n        max_iterations=10,\n        memory=Memory.from_defaults(),\n    )\n    # Memory is not included in the dump\n    dump = event.model_dump()\n    assert len(dump) == 3",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "documentation": {}
    },
    {
        "label": "test_agent_output_with_structured_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "peekOfCode": "def test_agent_output_with_structured_response(example_agent_output: dict) -> None:\n    try:\n        agent_output = AgentOutput.model_validate(example_agent_output)\n        success = True\n    except ValidationError:\n        success = False\n    assert success\n    assert agent_output.get_pydantic_model(MathResult) == MathResult.model_validate(\n        example_agent_output[\"structured_response\"]\n    )",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "documentation": {}
    },
    {
        "label": "test_agent_stream_structured_output",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "peekOfCode": "def test_agent_stream_structured_output(\n    example_agent_stream_structured_output: Tuple[dict, str],\n):\n    try:\n        ev = AgentStreamStructuredOutput.model_validate(\n            example_agent_stream_structured_output[0]\n        )\n        success = True\n    except ValidationError:\n        success = False",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_events",
        "documentation": {}
    },
    {
        "label": "TestWorkflowAgent",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_function_call",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_function_call",
        "peekOfCode": "class TestWorkflowAgent(BaseWorkflowAgent):\n    \"\"\"Test implementation of BaseWorkflowAgent for testing\"\"\"\n    async def take_step(self, ctx, llm_input, tools, memory):\n        \"\"\"Mock implementation\"\"\"\n        return AgentOutput(\n            response=ChatMessage(role=\"assistant\", content=\"test response\"),\n            tool_calls=[],\n            raw=\"test\",\n            current_agent_name=self.name,\n        )",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_function_call",
        "documentation": {}
    },
    {
        "label": "mock_context",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_function_call",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_function_call",
        "peekOfCode": "def mock_context():\n    \"\"\"Create a mock context for testing\"\"\"\n    ctx = MagicMock(spec=Context)\n    ctx.store = AsyncMock()\n    ctx.store.get = AsyncMock()\n    ctx.store.set = AsyncMock()\n    ctx.collect_events = MagicMock()\n    return ctx\n@pytest.fixture\ndef mock_memory():",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_function_call",
        "documentation": {}
    },
    {
        "label": "mock_memory",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_function_call",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_function_call",
        "peekOfCode": "def mock_memory():\n    \"\"\"Create a mock memory for testing\"\"\"\n    memory = MagicMock(spec=BaseMemory)\n    memory.aget = AsyncMock(return_value=[])\n    return memory\n@pytest.fixture\ndef test_agent():\n    \"\"\"Create a test agent instance\"\"\"\n    return TestWorkflowAgent(\n        name=\"test_agent\",",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_function_call",
        "documentation": {}
    },
    {
        "label": "test_agent",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_function_call",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_function_call",
        "peekOfCode": "def test_agent():\n    \"\"\"Create a test agent instance\"\"\"\n    return TestWorkflowAgent(\n        name=\"test_agent\",\n        description=\"Test agent for testing\",\n        tools=[],\n        llm=None,  # Will use default\n    )\n@pytest.mark.asyncio\nasync def test_aggregate_tool_results_return_direct_non_handoff_no_error_stops(",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_function_call",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_multi_agent_workflow",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_multi_agent_workflow",
        "peekOfCode": "class MockLLM(MockLLM):\n    def __init__(self, responses: List[ChatMessage]):\n        super().__init__()\n        self._responses = responses\n        self._response_index = 0\n    @property\n    def metadata(self) -> LLMMetadata:\n        return LLMMetadata(is_function_calling_model=True)\n    async def astream_chat(\n        self, messages: List[ChatMessage], **kwargs: Any",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_multi_agent_workflow",
        "documentation": {}
    },
    {
        "label": "add",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_multi_agent_workflow",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_multi_agent_workflow",
        "peekOfCode": "def add(a: int, b: int) -> int:\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\ndef subtract(a: int, b: int) -> int:\n    \"\"\"Subtract two numbers.\"\"\"\n    return a - b\n@pytest.fixture()\ndef calculator_agent():\n    return ReActAgent(\n        name=\"calculator\",",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_multi_agent_workflow",
        "documentation": {}
    },
    {
        "label": "subtract",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_multi_agent_workflow",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_multi_agent_workflow",
        "peekOfCode": "def subtract(a: int, b: int) -> int:\n    \"\"\"Subtract two numbers.\"\"\"\n    return a - b\n@pytest.fixture()\ndef calculator_agent():\n    return ReActAgent(\n        name=\"calculator\",\n        description=\"Performs basic arithmetic operations\",\n        system_prompt=\"You are a calculator assistant.\",\n        tools=[",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_multi_agent_workflow",
        "documentation": {}
    },
    {
        "label": "calculator_agent",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_multi_agent_workflow",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_multi_agent_workflow",
        "peekOfCode": "def calculator_agent():\n    return ReActAgent(\n        name=\"calculator\",\n        description=\"Performs basic arithmetic operations\",\n        system_prompt=\"You are a calculator assistant.\",\n        tools=[\n            FunctionTool.from_defaults(fn=add),\n            FunctionTool.from_defaults(fn=subtract),\n        ],\n        llm=MockLLM(",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_multi_agent_workflow",
        "documentation": {}
    },
    {
        "label": "empty_calculator_agent",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_multi_agent_workflow",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_multi_agent_workflow",
        "peekOfCode": "def empty_calculator_agent():\n    return ReActAgent(\n        name=\"calculator\",\n        description=\"Performs basic arithmetic operations\",\n        system_prompt=\"You are a calculator assistant.\",\n        tools=[\n            FunctionTool.from_defaults(fn=add),\n            FunctionTool.from_defaults(fn=subtract),\n        ],\n        llm=MockLLM(responses=[]),",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_multi_agent_workflow",
        "documentation": {}
    },
    {
        "label": "retriever_agent",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_multi_agent_workflow",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_multi_agent_workflow",
        "peekOfCode": "def retriever_agent():\n    return FunctionAgent(\n        name=\"retriever\",\n        description=\"Manages data retrieval\",\n        system_prompt=\"You are a retrieval assistant.\",\n        llm=MockLLM(\n            responses=[\n                ChatMessage(\n                    role=MessageRole.ASSISTANT,\n                    content=\"Let me help you with that calculation. I'll hand this off to the calculator.\",",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_multi_agent_workflow",
        "documentation": {}
    },
    {
        "label": "empty_retriever_agent",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_multi_agent_workflow",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_multi_agent_workflow",
        "peekOfCode": "def empty_retriever_agent():\n    return FunctionAgent(\n        name=\"retriever\",\n        description=\"Manages data retrieval\",\n        system_prompt=\"You are a retrieval assistant.\",\n        llm=MockLLM(\n            responses=[],\n        ),\n    )\n@pytest.mark.asyncio",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_multi_agent_workflow",
        "documentation": {}
    },
    {
        "label": "test_react_agent_prompts",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_react_agent",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_react_agent",
        "peekOfCode": "def test_react_agent_prompts():\n    llm = MockLLM()\n    agent = ReActAgent(\n        llm=llm,\n        tools=[],\n    )\n    prompts = agent.get_prompts()\n    assert len(prompts) == 1\n    assert isinstance(prompts[\"react_header\"], PromptTemplate)\n    new_prompt = \"New prompt\"",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_react_agent",
        "documentation": {}
    },
    {
        "label": "CI",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_return_direct_e2e",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_return_direct_e2e",
        "peekOfCode": "CI = os.getenv(\"CI\", \"\").lower() in (\"1\", \"true\", \"yes\")\n@pytest.mark.skipif(CI, reason=\"Skipping in CI environment\")\n@pytest.mark.asyncio\nasync def test_return_direct_e2e():\n    from llama_index.core.agent.workflow import FunctionAgent, ToolCallResult\n    from llama_index.core.tools import FunctionTool\n    from llama_index.core.workflow import Context\n    from llama_index.llms.openai import OpenAI\n    if not os.getenv(\"OPENAI_API_KEY\"):\n        raise KeyError(\"Please provide OPENAI_API_KEY as a Environment Variables\")",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_return_direct_e2e",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_single_agent_workflow",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_single_agent_workflow",
        "peekOfCode": "class MockLLM(MockLLM):\n    def __init__(self, responses: List[ChatMessage]):\n        super().__init__()\n        self._responses = responses\n        self._response_index = 0\n    @property\n    def metadata(self) -> LLMMetadata:\n        return LLMMetadata(is_function_calling_model=True)\n    async def astream_chat(\n        self, messages: List[ChatMessage], **kwargs: Any",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_single_agent_workflow",
        "documentation": {}
    },
    {
        "label": "function_agent",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_single_agent_workflow",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_single_agent_workflow",
        "peekOfCode": "def function_agent():\n    return FunctionAgent(\n        name=\"retriever\",\n        description=\"Manages data retrieval\",\n        system_prompt=\"You are a retrieval assistant.\",\n        llm=MockLLM(\n            responses=[\n                ChatMessage(\n                    role=MessageRole.ASSISTANT, content=\"Success with the FunctionAgent\"\n                )",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_single_agent_workflow",
        "documentation": {}
    },
    {
        "label": "add",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_single_agent_workflow",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_single_agent_workflow",
        "peekOfCode": "def add(a: int, b: int) -> int:\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\ndef subtract(a: int, b: int) -> int:\n    \"\"\"Subtract two numbers.\"\"\"\n    return a - b\n@pytest.fixture()\ndef calculator_agent():\n    return ReActAgent(\n        name=\"calculator\",",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_single_agent_workflow",
        "documentation": {}
    },
    {
        "label": "subtract",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_single_agent_workflow",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_single_agent_workflow",
        "peekOfCode": "def subtract(a: int, b: int) -> int:\n    \"\"\"Subtract two numbers.\"\"\"\n    return a - b\n@pytest.fixture()\ndef calculator_agent():\n    return ReActAgent(\n        name=\"calculator\",\n        description=\"Performs basic arithmetic operations\",\n        system_prompt=\"You are a calculator assistant.\",\n        tools=[",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_single_agent_workflow",
        "documentation": {}
    },
    {
        "label": "calculator_agent",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_single_agent_workflow",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_single_agent_workflow",
        "peekOfCode": "def calculator_agent():\n    return ReActAgent(\n        name=\"calculator\",\n        description=\"Performs basic arithmetic operations\",\n        system_prompt=\"You are a calculator assistant.\",\n        tools=[\n            FunctionTool.from_defaults(fn=add),\n            FunctionTool.from_defaults(fn=subtract),\n        ],\n        llm=MockLLM(",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_single_agent_workflow",
        "documentation": {}
    },
    {
        "label": "retry_calculator_agent",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.agent.workflow.test_single_agent_workflow",
        "description": "reference_code.llama-index-core.tests.agent.workflow.test_single_agent_workflow",
        "peekOfCode": "def retry_calculator_agent():\n    return ReActAgent(\n        name=\"calculator\",\n        description=\"Performs basic arithmetic operations\",\n        system_prompt=\"You are a calculator assistant.\",\n        tools=[\n            FunctionTool.from_defaults(fn=add),\n            FunctionTool.from_defaults(fn=subtract),\n        ],\n        llm=MockLLM(",
        "detail": "reference_code.llama-index-core.tests.agent.workflow.test_single_agent_workflow",
        "documentation": {}
    },
    {
        "label": "empty_bytes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def empty_bytes() -> bytes:\n    return b\"\"\n@pytest.fixture()\ndef png_1px_b64() -> bytes:\n    return b\"iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8z8BQDwAEhQGAhKmMIQAAAABJRU5ErkJggg==\"\n@pytest.fixture()\ndef png_1px(png_1px_b64) -> bytes:\n    return base64.b64decode(png_1px_b64)\n@pytest.fixture()\ndef pdf_url() -> str:",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "png_1px_b64",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def png_1px_b64() -> bytes:\n    return b\"iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8z8BQDwAEhQGAhKmMIQAAAABJRU5ErkJggg==\"\n@pytest.fixture()\ndef png_1px(png_1px_b64) -> bytes:\n    return base64.b64decode(png_1px_b64)\n@pytest.fixture()\ndef pdf_url() -> str:\n    return \"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\"\n@pytest.fixture()\ndef mock_pdf_bytes(pdf_url) -> bytes:",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "png_1px",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def png_1px(png_1px_b64) -> bytes:\n    return base64.b64decode(png_1px_b64)\n@pytest.fixture()\ndef pdf_url() -> str:\n    return \"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\"\n@pytest.fixture()\ndef mock_pdf_bytes(pdf_url) -> bytes:\n    \"\"\"\n    Returns a byte string representing a very simple, minimal PDF file.\n    \"\"\"",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "pdf_url",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def pdf_url() -> str:\n    return \"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\"\n@pytest.fixture()\ndef mock_pdf_bytes(pdf_url) -> bytes:\n    \"\"\"\n    Returns a byte string representing a very simple, minimal PDF file.\n    \"\"\"\n    return httpx.get(pdf_url).content\n@pytest.fixture()\ndef pdf_base64(mock_pdf_bytes) -> bytes:",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "mock_pdf_bytes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def mock_pdf_bytes(pdf_url) -> bytes:\n    \"\"\"\n    Returns a byte string representing a very simple, minimal PDF file.\n    \"\"\"\n    return httpx.get(pdf_url).content\n@pytest.fixture()\ndef pdf_base64(mock_pdf_bytes) -> bytes:\n    return base64.b64encode(mock_pdf_bytes)\ndef test_chat_message_from_str():\n    m = ChatMessage.from_str(content=\"test content\")",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "pdf_base64",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def pdf_base64(mock_pdf_bytes) -> bytes:\n    return base64.b64encode(mock_pdf_bytes)\ndef test_chat_message_from_str():\n    m = ChatMessage.from_str(content=\"test content\")\n    assert m.content == \"test content\"\n    assert len(m.blocks) == 1\n    assert type(m.blocks[0]) is TextBlock\n    assert m.blocks[0].text == \"test content\"\ndef test_chat_message_content_legacy_get():\n    m = ChatMessage(content=\"test content\")",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "test_chat_message_from_str",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def test_chat_message_from_str():\n    m = ChatMessage.from_str(content=\"test content\")\n    assert m.content == \"test content\"\n    assert len(m.blocks) == 1\n    assert type(m.blocks[0]) is TextBlock\n    assert m.blocks[0].text == \"test content\"\ndef test_chat_message_content_legacy_get():\n    m = ChatMessage(content=\"test content\")\n    assert m.content == \"test content\"\n    assert len(m.blocks) == 1",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "test_chat_message_content_legacy_get",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def test_chat_message_content_legacy_get():\n    m = ChatMessage(content=\"test content\")\n    assert m.content == \"test content\"\n    assert len(m.blocks) == 1\n    assert type(m.blocks[0]) is TextBlock\n    assert m.blocks[0].text == \"test content\"\n    m = ChatMessage(role=\"user\", content=\"test content\")\n    assert m.role == \"user\"\n    assert m.content == \"test content\"\n    assert len(m.blocks) == 1",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "test_chat_message_content_legacy_set",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def test_chat_message_content_legacy_set():\n    m = ChatMessage()\n    m.content = \"test content\"\n    assert len(m.blocks) == 1\n    assert type(m.blocks[0]) is TextBlock\n    assert m.blocks[0].text == \"test content\"\n    m = ChatMessage(content=\"some original content\")\n    m.content = \"test content\"\n    assert len(m.blocks) == 1\n    assert type(m.blocks[0]) is TextBlock",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "test_chat_message_content_returns_empty_string",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def test_chat_message_content_returns_empty_string():\n    m = ChatMessage(content=[TextBlock(text=\"test content\"), ImageBlock()])\n    assert m.content == \"test content\"\n    m = ChatMessage()\n    assert m.content is None\ndef test_chat_message__str__():\n    assert str(ChatMessage(content=\"test content\")) == \"user: test content\"\ndef test_chat_message_serializer():\n    class SimpleModel(BaseModel):\n        some_field: str = \"\"",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "test_chat_message__str__",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def test_chat_message__str__():\n    assert str(ChatMessage(content=\"test content\")) == \"user: test content\"\ndef test_chat_message_serializer():\n    class SimpleModel(BaseModel):\n        some_field: str = \"\"\n    m = ChatMessage(\n        content=\"test content\",\n        additional_kwargs={\"some_list\": [\"a\", \"b\", \"c\"], \"some_object\": SimpleModel()},\n    )\n    assert m.model_dump() == {",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "test_chat_message_serializer",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def test_chat_message_serializer():\n    class SimpleModel(BaseModel):\n        some_field: str = \"\"\n    m = ChatMessage(\n        content=\"test content\",\n        additional_kwargs={\"some_list\": [\"a\", \"b\", \"c\"], \"some_object\": SimpleModel()},\n    )\n    assert m.model_dump() == {\n        \"role\": MessageRole.USER,\n        \"additional_kwargs\": {",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "test_chat_message_legacy_roundtrip",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def test_chat_message_legacy_roundtrip():\n    legacy_message = {\n        \"role\": MessageRole.USER,\n        \"content\": \"foo\",\n        \"additional_kwargs\": {},\n    }\n    m = ChatMessage(**legacy_message)\n    assert m.model_dump() == {\n        \"additional_kwargs\": {},\n        \"blocks\": [{\"block_type\": \"text\", \"text\": \"foo\"}],",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "test_image_block_resolve_image",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def test_image_block_resolve_image(png_1px: bytes, png_1px_b64: bytes):\n    b = ImageBlock(image=png_1px)\n    img = b.resolve_image()\n    assert isinstance(img, BytesIO)\n    assert img.read() == png_1px\n    img = b.resolve_image(as_base64=True)\n    assert isinstance(img, BytesIO)\n    assert img.read() == png_1px_b64\ndef test_image_block_resolve_image_path(\n    tmp_path: Path, png_1px_b64: bytes, png_1px: bytes",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "test_image_block_resolve_image_path",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def test_image_block_resolve_image_path(\n    tmp_path: Path, png_1px_b64: bytes, png_1px: bytes\n):\n    png_path = tmp_path / \"test.png\"\n    png_path.write_bytes(png_1px)\n    b = ImageBlock(path=png_path)\n    img = b.resolve_image()\n    assert isinstance(img, BytesIO)\n    assert img.read() == png_1px\n    img = b.resolve_image(as_base64=True)",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "test_image_block_resolve_image_url",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def test_image_block_resolve_image_url(png_1px_b64: bytes, png_1px: bytes):\n    with mock.patch(\"llama_index.core.utils.requests\") as mocked_req:\n        url_str = \"http://example.com\"\n        mocked_req.get.return_value = mock.MagicMock(content=png_1px)\n        b = ImageBlock(url=AnyUrl(url=url_str))\n        img = b.resolve_image()\n        assert isinstance(img, BytesIO)\n        assert img.read() == png_1px\n        img = b.resolve_image(as_base64=True)\n        assert isinstance(img, BytesIO)",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "test_image_block_resolve_error",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def test_image_block_resolve_error():\n    with pytest.raises(\n        ValueError, match=\"No valid source provided to resolve binary data!\"\n    ):\n        b = ImageBlock()\n        b.resolve_image()\ndef test_image_block_store_as_anyurl():\n    url_str = \"http://example.com\"\n    b = ImageBlock(url=url_str)\n    assert b.url == AnyUrl(url=url_str)",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "test_image_block_store_as_anyurl",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def test_image_block_store_as_anyurl():\n    url_str = \"http://example.com\"\n    b = ImageBlock(url=url_str)\n    assert b.url == AnyUrl(url=url_str)\ndef test_image_block_store_as_base64(png_1px_b64: bytes, png_1px: bytes):\n    # Store regular bytes\n    assert ImageBlock(image=png_1px).image == png_1px_b64\n    # Store already encoded data\n    assert ImageBlock(image=png_1px_b64).image == png_1px_b64\ndef test_legacy_image_additional_kwargs(png_1px_b64: bytes):",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "test_image_block_store_as_base64",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def test_image_block_store_as_base64(png_1px_b64: bytes, png_1px: bytes):\n    # Store regular bytes\n    assert ImageBlock(image=png_1px).image == png_1px_b64\n    # Store already encoded data\n    assert ImageBlock(image=png_1px_b64).image == png_1px_b64\ndef test_legacy_image_additional_kwargs(png_1px_b64: bytes):\n    image_doc = ImageDocument(image=png_1px_b64)\n    msg = ChatMessage(additional_kwargs={\"images\": [image_doc]})\n    assert len(msg.blocks) == 1\n    assert msg.blocks[0].image == png_1px_b64",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "test_legacy_image_additional_kwargs",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def test_legacy_image_additional_kwargs(png_1px_b64: bytes):\n    image_doc = ImageDocument(image=png_1px_b64)\n    msg = ChatMessage(additional_kwargs={\"images\": [image_doc]})\n    assert len(msg.blocks) == 1\n    assert msg.blocks[0].image == png_1px_b64\ndef test_chat_response():\n    message = ChatMessage(\"some content\")\n    cr = ChatResponse(message=message)\n    assert str(cr) == str(message)\ndef test_completion_response():",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "test_chat_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def test_chat_response():\n    message = ChatMessage(\"some content\")\n    cr = ChatResponse(message=message)\n    assert str(cr) == str(message)\ndef test_completion_response():\n    cr = CompletionResponse(text=\"some text\")\n    assert str(cr) == \"some text\"\ndef test_document_block_from_bytes(mock_pdf_bytes: bytes, pdf_base64: bytes):\n    document = DocumentBlock(data=mock_pdf_bytes, document_mimetype=\"application/pdf\")\n    assert document.title == \"input_document\"",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "test_completion_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def test_completion_response():\n    cr = CompletionResponse(text=\"some text\")\n    assert str(cr) == \"some text\"\ndef test_document_block_from_bytes(mock_pdf_bytes: bytes, pdf_base64: bytes):\n    document = DocumentBlock(data=mock_pdf_bytes, document_mimetype=\"application/pdf\")\n    assert document.title == \"input_document\"\n    assert document.document_mimetype == \"application/pdf\"\n    assert pdf_base64 == document.data\ndef test_document_block_from_b64(pdf_base64: bytes):\n    document = DocumentBlock(data=pdf_base64)",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "test_document_block_from_bytes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def test_document_block_from_bytes(mock_pdf_bytes: bytes, pdf_base64: bytes):\n    document = DocumentBlock(data=mock_pdf_bytes, document_mimetype=\"application/pdf\")\n    assert document.title == \"input_document\"\n    assert document.document_mimetype == \"application/pdf\"\n    assert pdf_base64 == document.data\ndef test_document_block_from_b64(pdf_base64: bytes):\n    document = DocumentBlock(data=pdf_base64)\n    assert document.title == \"input_document\"\n    assert pdf_base64 == document.data\ndef test_document_block_from_path(tmp_path: Path, pdf_url: str):",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "test_document_block_from_b64",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def test_document_block_from_b64(pdf_base64: bytes):\n    document = DocumentBlock(data=pdf_base64)\n    assert document.title == \"input_document\"\n    assert pdf_base64 == document.data\ndef test_document_block_from_path(tmp_path: Path, pdf_url: str):\n    pdf_path = tmp_path / \"test.pdf\"\n    pdf_content = httpx.get(pdf_url).content\n    pdf_path.write_bytes(pdf_content)\n    document = DocumentBlock(path=pdf_path.__str__())\n    file_buffer = document.resolve_document()",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "test_document_block_from_path",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def test_document_block_from_path(tmp_path: Path, pdf_url: str):\n    pdf_path = tmp_path / \"test.pdf\"\n    pdf_content = httpx.get(pdf_url).content\n    pdf_path.write_bytes(pdf_content)\n    document = DocumentBlock(path=pdf_path.__str__())\n    file_buffer = document.resolve_document()\n    assert isinstance(file_buffer, BytesIO)\n    file_bytes = file_buffer.read()\n    document._guess_mimetype()\n    assert document.document_mimetype == \"application/pdf\"",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "test_document_block_from_url",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def test_document_block_from_url(pdf_url: str):\n    document = DocumentBlock(url=pdf_url, title=\"dummy_pdf\")\n    file_buffer = document.resolve_document()\n    assert isinstance(file_buffer, BytesIO)\n    file_bytes = file_buffer.read()\n    document._guess_mimetype()\n    assert document.document_mimetype == \"application/pdf\"\n    fm = document.guess_format()\n    assert fm == \"pdf\"\n    b64_string = document._get_b64_string(file_buffer)",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "test_empty_bytes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def test_empty_bytes(empty_bytes: bytes, png_1px: bytes):\n    errors = []\n    try:\n        DocumentBlock(data=empty_bytes).resolve_document()\n        errors.append(0)\n    except ValueError:\n        errors.append(1)\n    try:\n        AudioBlock(audio=empty_bytes).resolve_audio()\n        errors.append(0)",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "test_cache_control",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.base.llms.test_types",
        "description": "reference_code.llama-index-core.tests.base.llms.test_types",
        "peekOfCode": "def test_cache_control() -> None:\n    cp = CachePoint(cache_control=CacheControl(type=\"ephemeral\"))\n    assert isinstance(cp.model_dump()[\"cache_control\"], dict)\n    assert cp.model_dump()[\"cache_control\"][\"type\"] == \"ephemeral\"\n    with pytest.raises(ValidationError):\n        CachePoint.model_validate({\"cache_control\": \"default\"})",
        "detail": "reference_code.llama-index-core.tests.base.llms.test_types",
        "documentation": {}
    },
    {
        "label": "test_on_event_start",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.callbacks.test_llama_debug",
        "description": "reference_code.llama-index-core.tests.callbacks.test_llama_debug",
        "peekOfCode": "def test_on_event_start() -> None:\n    \"\"\"Test event start.\"\"\"\n    handler = LlamaDebugHandler()\n    event_id = handler.on_event_start(\n        CBEventType.LLM, payload=TEST_PAYLOAD, event_id=TEST_ID\n    )\n    assert event_id == TEST_ID\n    assert len(handler.event_pairs_by_type) == 1\n    assert len(handler.sequential_events) == 1\n    events = handler.event_pairs_by_type.get(CBEventType.LLM)",
        "detail": "reference_code.llama-index-core.tests.callbacks.test_llama_debug",
        "documentation": {}
    },
    {
        "label": "test_on_event_end",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.callbacks.test_llama_debug",
        "description": "reference_code.llama-index-core.tests.callbacks.test_llama_debug",
        "peekOfCode": "def test_on_event_end() -> None:\n    \"\"\"Test event end.\"\"\"\n    handler = LlamaDebugHandler()\n    handler.on_event_end(CBEventType.EMBEDDING, payload=TEST_PAYLOAD, event_id=TEST_ID)\n    assert len(handler.event_pairs_by_type) == 1\n    assert len(handler.sequential_events) == 1\n    events = handler.event_pairs_by_type.get(CBEventType.EMBEDDING)\n    assert isinstance(events, list)\n    assert events[0].payload == TEST_PAYLOAD\n    assert events[0].id_ == TEST_ID",
        "detail": "reference_code.llama-index-core.tests.callbacks.test_llama_debug",
        "documentation": {}
    },
    {
        "label": "test_get_event_stats",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.callbacks.test_llama_debug",
        "description": "reference_code.llama-index-core.tests.callbacks.test_llama_debug",
        "peekOfCode": "def test_get_event_stats() -> None:\n    \"\"\"Test get event stats.\"\"\"\n    handler = LlamaDebugHandler()\n    event_id = handler.on_event_start(CBEventType.CHUNKING, payload=TEST_PAYLOAD)\n    handler.on_event_end(CBEventType.CHUNKING, event_id=event_id)\n    assert len(handler.event_pairs_by_type[CBEventType.CHUNKING]) == 2\n    event_stats = handler.get_event_time_info(CBEventType.CHUNKING)\n    assert event_stats.total_count == 1\n    assert event_stats.total_secs > 0.0\ndef test_flush_events() -> None:",
        "detail": "reference_code.llama-index-core.tests.callbacks.test_llama_debug",
        "documentation": {}
    },
    {
        "label": "test_flush_events",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.callbacks.test_llama_debug",
        "description": "reference_code.llama-index-core.tests.callbacks.test_llama_debug",
        "peekOfCode": "def test_flush_events() -> None:\n    \"\"\"Test flush events.\"\"\"\n    handler = LlamaDebugHandler()\n    event_id = handler.on_event_start(CBEventType.CHUNKING, payload=TEST_PAYLOAD)\n    handler.on_event_end(CBEventType.CHUNKING, event_id=event_id)\n    event_id = handler.on_event_start(CBEventType.CHUNKING, payload=TEST_PAYLOAD)\n    handler.on_event_end(CBEventType.CHUNKING, event_id=event_id)\n    assert len(handler.event_pairs_by_type[CBEventType.CHUNKING]) == 4\n    handler.flush_event_logs()\n    assert len(handler.event_pairs_by_type) == 0",
        "detail": "reference_code.llama-index-core.tests.callbacks.test_llama_debug",
        "documentation": {}
    },
    {
        "label": "test_ignore_events",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.callbacks.test_llama_debug",
        "description": "reference_code.llama-index-core.tests.callbacks.test_llama_debug",
        "peekOfCode": "def test_ignore_events() -> None:\n    \"\"\"Test ignore event starts and ends.\"\"\"\n    handler = LlamaDebugHandler(\n        event_starts_to_ignore=[CBEventType.CHUNKING],\n        event_ends_to_ignore=[CBEventType.LLM],\n    )\n    manager = CallbackManager([handler])\n    event_id = manager.on_event_start(CBEventType.CHUNKING, payload=TEST_PAYLOAD)\n    manager.on_event_end(CBEventType.CHUNKING, event_id=event_id)\n    event_id = manager.on_event_start(CBEventType.LLM, payload=TEST_PAYLOAD)",
        "detail": "reference_code.llama-index-core.tests.callbacks.test_llama_debug",
        "documentation": {}
    },
    {
        "label": "TEST_PAYLOAD",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.callbacks.test_llama_debug",
        "description": "reference_code.llama-index-core.tests.callbacks.test_llama_debug",
        "peekOfCode": "TEST_PAYLOAD = {\"one\": 1, \"two\": 2}\nTEST_ID = \"my id\"\ndef test_on_event_start() -> None:\n    \"\"\"Test event start.\"\"\"\n    handler = LlamaDebugHandler()\n    event_id = handler.on_event_start(\n        CBEventType.LLM, payload=TEST_PAYLOAD, event_id=TEST_ID\n    )\n    assert event_id == TEST_ID\n    assert len(handler.event_pairs_by_type) == 1",
        "detail": "reference_code.llama-index-core.tests.callbacks.test_llama_debug",
        "documentation": {}
    },
    {
        "label": "TEST_ID",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.callbacks.test_llama_debug",
        "description": "reference_code.llama-index-core.tests.callbacks.test_llama_debug",
        "peekOfCode": "TEST_ID = \"my id\"\ndef test_on_event_start() -> None:\n    \"\"\"Test event start.\"\"\"\n    handler = LlamaDebugHandler()\n    event_id = handler.on_event_start(\n        CBEventType.LLM, payload=TEST_PAYLOAD, event_id=TEST_ID\n    )\n    assert event_id == TEST_ID\n    assert len(handler.event_pairs_by_type) == 1\n    assert len(handler.sequential_events) == 1",
        "detail": "reference_code.llama-index-core.tests.callbacks.test_llama_debug",
        "documentation": {}
    },
    {
        "label": "test_on_event_start",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.callbacks.test_token_counter",
        "description": "reference_code.llama-index-core.tests.callbacks.test_token_counter",
        "peekOfCode": "def test_on_event_start() -> None:\n    \"\"\"Test event start.\"\"\"\n    handler = TokenCountingHandler()\n    event_id = handler.on_event_start(\n        CBEventType.LLM, payload=TEST_PAYLOAD, event_id=TEST_ID\n    )\n    assert event_id == TEST_ID\n    event_id = handler.on_event_start(\n        CBEventType.EMBEDDING, payload=TEST_PAYLOAD, event_id=TEST_ID\n    )",
        "detail": "reference_code.llama-index-core.tests.callbacks.test_token_counter",
        "documentation": {}
    },
    {
        "label": "test_on_event_end",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.callbacks.test_token_counter",
        "description": "reference_code.llama-index-core.tests.callbacks.test_token_counter",
        "peekOfCode": "def test_on_event_end() -> None:\n    \"\"\"Test event end.\"\"\"\n    handler = TokenCountingHandler()\n    handler.on_event_end(CBEventType.LLM, payload=TEST_PAYLOAD, event_id=TEST_ID)\n    assert len(handler.llm_token_counts) == 1\n    assert len(handler.embedding_token_counts) == 0\n    handler.on_event_end(CBEventType.EMBEDDING, payload=TEST_PAYLOAD, event_id=TEST_ID)\n    assert len(handler.llm_token_counts) == 1\n    assert len(handler.embedding_token_counts) == 1\n    assert handler.embedding_token_counts[0].total_token_count == 1",
        "detail": "reference_code.llama-index-core.tests.callbacks.test_token_counter",
        "documentation": {}
    },
    {
        "label": "TEST_PAYLOAD",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.callbacks.test_token_counter",
        "description": "reference_code.llama-index-core.tests.callbacks.test_token_counter",
        "peekOfCode": "TEST_PAYLOAD = {\"chunks\": [\"one\"], \"formatted_prompt\": \"two\", \"response\": \"three\"}\nTEST_ID = \"my id\"\ndef test_on_event_start() -> None:\n    \"\"\"Test event start.\"\"\"\n    handler = TokenCountingHandler()\n    event_id = handler.on_event_start(\n        CBEventType.LLM, payload=TEST_PAYLOAD, event_id=TEST_ID\n    )\n    assert event_id == TEST_ID\n    event_id = handler.on_event_start(",
        "detail": "reference_code.llama-index-core.tests.callbacks.test_token_counter",
        "documentation": {}
    },
    {
        "label": "TEST_ID",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.callbacks.test_token_counter",
        "description": "reference_code.llama-index-core.tests.callbacks.test_token_counter",
        "peekOfCode": "TEST_ID = \"my id\"\ndef test_on_event_start() -> None:\n    \"\"\"Test event start.\"\"\"\n    handler = TokenCountingHandler()\n    event_id = handler.on_event_start(\n        CBEventType.LLM, payload=TEST_PAYLOAD, event_id=TEST_ID\n    )\n    assert event_id == TEST_ID\n    event_id = handler.on_event_start(\n        CBEventType.EMBEDDING, payload=TEST_PAYLOAD, event_id=TEST_ID",
        "detail": "reference_code.llama-index-core.tests.callbacks.test_token_counter",
        "documentation": {}
    },
    {
        "label": "chat_engine",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.chat_engine.test_condense_plus_context",
        "description": "reference_code.llama-index-core.tests.chat_engine.test_condense_plus_context",
        "peekOfCode": "def chat_engine() -> CondensePlusContextChatEngine:\n    index = VectorStoreIndex.from_documents(\n        [Document.example()], embed_model=MockEmbedding(embed_dim=3)\n    )\n    retriever = index.as_retriever()\n    return CondensePlusContextChatEngine.from_defaults(\n        retriever, llm=MockLLM(), system_prompt=SYSTEM_PROMPT\n    )\ndef test_chat(chat_engine: CondensePlusContextChatEngine):\n    response = chat_engine.chat(\"Hello World!\")",
        "detail": "reference_code.llama-index-core.tests.chat_engine.test_condense_plus_context",
        "documentation": {}
    },
    {
        "label": "test_chat",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.chat_engine.test_condense_plus_context",
        "description": "reference_code.llama-index-core.tests.chat_engine.test_condense_plus_context",
        "peekOfCode": "def test_chat(chat_engine: CondensePlusContextChatEngine):\n    response = chat_engine.chat(\"Hello World!\")\n    assert SYSTEM_PROMPT in str(response)\n    assert \"Hello World!\" in str(response)\n    assert len(chat_engine.chat_history) == 2\n    response = chat_engine.chat(\"What is the capital of the moon?\")\n    assert SYSTEM_PROMPT in str(response)\n    assert \"Hello World!\" in str(response)\n    assert \"What is the capital of the moon?\" in str(response)\n    assert len(chat_engine.chat_history) == 4",
        "detail": "reference_code.llama-index-core.tests.chat_engine.test_condense_plus_context",
        "documentation": {}
    },
    {
        "label": "test_chat_stream",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.chat_engine.test_condense_plus_context",
        "description": "reference_code.llama-index-core.tests.chat_engine.test_condense_plus_context",
        "peekOfCode": "def test_chat_stream(chat_engine: CondensePlusContextChatEngine):\n    response = chat_engine.stream_chat(\"Hello World!\")\n    num_iters = 0\n    for _ in response.response_gen:\n        num_iters += 1\n    assert num_iters > 10\n    assert SYSTEM_PROMPT in str(response)\n    assert \"Hello World!\" in str(response)\n    assert len(chat_engine.chat_history) == 2\n    response = chat_engine.stream_chat(\"What is the capital of the moon?\")",
        "detail": "reference_code.llama-index-core.tests.chat_engine.test_condense_plus_context",
        "documentation": {}
    },
    {
        "label": "SYSTEM_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.chat_engine.test_condense_plus_context",
        "description": "reference_code.llama-index-core.tests.chat_engine.test_condense_plus_context",
        "peekOfCode": "SYSTEM_PROMPT = \"Talk like a pirate.\"\n@pytest.fixture()\ndef chat_engine() -> CondensePlusContextChatEngine:\n    index = VectorStoreIndex.from_documents(\n        [Document.example()], embed_model=MockEmbedding(embed_dim=3)\n    )\n    retriever = index.as_retriever()\n    return CondensePlusContextChatEngine.from_defaults(\n        retriever, llm=MockLLM(), system_prompt=SYSTEM_PROMPT\n    )",
        "detail": "reference_code.llama-index-core.tests.chat_engine.test_condense_plus_context",
        "documentation": {}
    },
    {
        "label": "test_condense_question_chat_engine",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.chat_engine.test_condense_question",
        "description": "reference_code.llama-index-core.tests.chat_engine.test_condense_question",
        "peekOfCode": "def test_condense_question_chat_engine(patch_llm_predictor) -> None:\n    query_engine = Mock(spec=BaseQueryEngine)\n    query_engine.query.side_effect = lambda x: Response(response=x)\n    engine = CondenseQuestionChatEngine.from_defaults(query_engine=query_engine)\n    engine.reset()\n    response = engine.chat(\"Test message 1\")\n    assert str(response) == \"Test message 1\"\n    response = engine.chat(\"Test message 2\")\n    assert str(response) == (\n        \"{\"",
        "detail": "reference_code.llama-index-core.tests.chat_engine.test_condense_question",
        "documentation": {}
    },
    {
        "label": "test_condense_question_chat_engine_with_init_history",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.chat_engine.test_condense_question",
        "description": "reference_code.llama-index-core.tests.chat_engine.test_condense_question",
        "peekOfCode": "def test_condense_question_chat_engine_with_init_history(patch_llm_predictor) -> None:\n    query_engine = Mock(spec=BaseQueryEngine)\n    query_engine.query.side_effect = lambda x: Response(response=x)\n    engine = CondenseQuestionChatEngine.from_defaults(\n        query_engine=query_engine,\n        chat_history=[\n            ChatMessage(role=MessageRole.USER, content=\"test human message\"),\n            ChatMessage(role=MessageRole.ASSISTANT, content=\"test ai message\"),\n        ],\n    )",
        "detail": "reference_code.llama-index-core.tests.chat_engine.test_condense_question",
        "documentation": {}
    },
    {
        "label": "chat_engine",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.chat_engine.test_context",
        "description": "reference_code.llama-index-core.tests.chat_engine.test_context",
        "peekOfCode": "def chat_engine() -> ContextChatEngine:\n    index = VectorStoreIndex.from_documents(\n        [Document.example()], embed_model=MockEmbedding(embed_dim=3)\n    )\n    retriever = index.as_retriever()\n    return ContextChatEngine.from_defaults(\n        retriever, llm=MockLLM(), system_prompt=SYSTEM_PROMPT\n    )\ndef test_chat(chat_engine: ContextChatEngine):\n    response = chat_engine.chat(\"Hello World!\")",
        "detail": "reference_code.llama-index-core.tests.chat_engine.test_context",
        "documentation": {}
    },
    {
        "label": "test_chat",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.chat_engine.test_context",
        "description": "reference_code.llama-index-core.tests.chat_engine.test_context",
        "peekOfCode": "def test_chat(chat_engine: ContextChatEngine):\n    response = chat_engine.chat(\"Hello World!\")\n    assert SYSTEM_PROMPT in str(response)\n    assert \"Hello World!\" in str(response)\n    assert len(chat_engine.chat_history) == 2\n    response = chat_engine.chat(\"What is the capital of the moon?\")\n    assert SYSTEM_PROMPT in str(response)\n    assert \"Hello World!\" in str(response)\n    assert \"What is the capital of the moon?\" in str(response)\n    assert len(chat_engine.chat_history) == 4",
        "detail": "reference_code.llama-index-core.tests.chat_engine.test_context",
        "documentation": {}
    },
    {
        "label": "test_chat_stream",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.chat_engine.test_context",
        "description": "reference_code.llama-index-core.tests.chat_engine.test_context",
        "peekOfCode": "def test_chat_stream(chat_engine: ContextChatEngine):\n    response = chat_engine.stream_chat(\"Hello World!\")\n    num_iters = 0\n    for _ in response.response_gen:\n        num_iters += 1\n    assert num_iters > 10\n    assert SYSTEM_PROMPT in str(response)\n    assert \"Hello World!\" in str(response)\n    assert len(chat_engine.chat_history) == 2\n    response = chat_engine.stream_chat(\"What is the capital of the moon?\")",
        "detail": "reference_code.llama-index-core.tests.chat_engine.test_context",
        "documentation": {}
    },
    {
        "label": "SYSTEM_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.chat_engine.test_context",
        "description": "reference_code.llama-index-core.tests.chat_engine.test_context",
        "peekOfCode": "SYSTEM_PROMPT = \"Talk like a pirate.\"\n@pytest.fixture()\ndef chat_engine() -> ContextChatEngine:\n    index = VectorStoreIndex.from_documents(\n        [Document.example()], embed_model=MockEmbedding(embed_dim=3)\n    )\n    retriever = index.as_retriever()\n    return ContextChatEngine.from_defaults(\n        retriever, llm=MockLLM(), system_prompt=SYSTEM_PROMPT\n    )",
        "detail": "reference_code.llama-index-core.tests.chat_engine.test_context",
        "documentation": {}
    },
    {
        "label": "test_simple_chat_engine",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.chat_engine.test_simple",
        "description": "reference_code.llama-index-core.tests.chat_engine.test_simple",
        "peekOfCode": "def test_simple_chat_engine() -> None:\n    engine = SimpleChatEngine.from_defaults()\n    engine.reset()\n    response = engine.chat(\"Test message 1\")\n    assert str(response) == \"user: Test message 1\\nassistant: \"\n    response = engine.chat(\"Test message 2\")\n    assert (\n        str(response)\n        == \"user: Test message 1\\nassistant: user: Test message 1\\nassistant: \\n\"\n        \"user: Test message 2\\nassistant: \"",
        "detail": "reference_code.llama-index-core.tests.chat_engine.test_simple",
        "documentation": {}
    },
    {
        "label": "test_simple_chat_engine_with_init_history",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.chat_engine.test_simple",
        "description": "reference_code.llama-index-core.tests.chat_engine.test_simple",
        "peekOfCode": "def test_simple_chat_engine_with_init_history() -> None:\n    engine = SimpleChatEngine.from_defaults(\n        chat_history=[\n            ChatMessage(role=MessageRole.USER, content=\"test human message\"),\n            ChatMessage(role=MessageRole.ASSISTANT, content=\"test ai message\"),\n        ],\n    )\n    response = engine.chat(\"new human message\")\n    assert (\n        str(response) == \"user: test human message\\nassistant: test ai message\\n\"",
        "detail": "reference_code.llama-index-core.tests.chat_engine.test_simple",
        "documentation": {}
    },
    {
        "label": "test_simple_chat_engine_astream_exception_handling",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.chat_engine.test_simple",
        "description": "reference_code.llama-index-core.tests.chat_engine.test_simple",
        "peekOfCode": "def test_simple_chat_engine_astream_exception_handling():\n    \"\"\"\n    Test that an exception thrown while retrieving the streamed LLM response gets bubbled up to the user.\n    Also tests that the non-retrieved exception does not remain in an task that was not awaited leading to\n    a 'Task exception was never retrieved' message during garbage collection.\n    \"\"\"\n    class ExceptionThrownInTest(Exception):\n        pass\n    class ExceptionMockLLM(MockLLM):\n        \"\"\"Raises an exception while streaming back the mocked LLM response.\"\"\"",
        "detail": "reference_code.llama-index-core.tests.chat_engine.test_simple",
        "documentation": {}
    },
    {
        "label": "mock_get_text_embedding",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.embeddings.test_base",
        "description": "reference_code.llama-index-core.tests.embeddings.test_base",
        "peekOfCode": "def mock_get_text_embedding(text: str) -> List[float]:\n    \"\"\"Mock get text embedding.\"\"\"\n    # assume dimensions are 5\n    if text == \"Hello world.\":\n        return [1, 0, 0, 0, 0]\n    elif text == \"This is a test.\":\n        return [0, 1, 0, 0, 0]\n    elif text == \"This is another test.\":\n        return [0, 0, 1, 0, 0]\n    elif text == \"This is a test v2.\":",
        "detail": "reference_code.llama-index-core.tests.embeddings.test_base",
        "documentation": {}
    },
    {
        "label": "mock_get_text_embeddings",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.embeddings.test_base",
        "description": "reference_code.llama-index-core.tests.embeddings.test_base",
        "peekOfCode": "def mock_get_text_embeddings(texts: List[str]) -> List[List[float]]:\n    \"\"\"Mock get text embeddings.\"\"\"\n    return [mock_get_text_embedding(text) for text in texts]\n@patch.object(MockEmbedding, \"_get_text_embedding\", side_effect=mock_get_text_embedding)\n@patch.object(\n    MockEmbedding, \"_get_text_embeddings\", side_effect=mock_get_text_embeddings\n)\ndef test_get_text_embeddings(\n    _mock_get_text_embeddings: Any, _mock_get_text_embedding: Any\n) -> None:",
        "detail": "reference_code.llama-index-core.tests.embeddings.test_base",
        "documentation": {}
    },
    {
        "label": "test_get_text_embeddings",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.embeddings.test_base",
        "description": "reference_code.llama-index-core.tests.embeddings.test_base",
        "peekOfCode": "def test_get_text_embeddings(\n    _mock_get_text_embeddings: Any, _mock_get_text_embedding: Any\n) -> None:\n    \"\"\"Test get queued text embeddings.\"\"\"\n    embed_model = MockEmbedding(embed_dim=8)\n    texts_to_embed = []\n    for i in range(8):\n        texts_to_embed.append(\"Hello world.\")\n    for i in range(8):\n        texts_to_embed.append(\"This is a test.\")",
        "detail": "reference_code.llama-index-core.tests.embeddings.test_base",
        "documentation": {}
    },
    {
        "label": "test_embedding_similarity",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.embeddings.test_base",
        "description": "reference_code.llama-index-core.tests.embeddings.test_base",
        "peekOfCode": "def test_embedding_similarity() -> None:\n    \"\"\"Test embedding similarity.\"\"\"\n    embed_model = MockEmbedding(embed_dim=3)\n    text_embedding = [3.0, 4.0, 0.0]\n    query_embedding = [0.0, 1.0, 0.0]\n    cosine = embed_model.similarity(query_embedding, text_embedding)\n    assert cosine == 0.8\ndef test_embedding_similarity_euclidean() -> None:\n    embed_model = MockEmbedding(embed_dim=2)\n    query_embedding = [1.0, 0.0]",
        "detail": "reference_code.llama-index-core.tests.embeddings.test_base",
        "documentation": {}
    },
    {
        "label": "test_embedding_similarity_euclidean",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.embeddings.test_base",
        "description": "reference_code.llama-index-core.tests.embeddings.test_base",
        "peekOfCode": "def test_embedding_similarity_euclidean() -> None:\n    embed_model = MockEmbedding(embed_dim=2)\n    query_embedding = [1.0, 0.0]\n    text1_embedding = [0.0, 1.0]  # further from query_embedding distance=1.414\n    text2_embedding = [1.0, 1.0]  # closer to query_embedding distance=1.0\n    euclidean_similarity1 = embed_model.similarity(\n        query_embedding, text1_embedding, mode=SimilarityMode.EUCLIDEAN\n    )\n    euclidean_similarity2 = embed_model.similarity(\n        query_embedding, text2_embedding, mode=SimilarityMode.EUCLIDEAN",
        "detail": "reference_code.llama-index-core.tests.embeddings.test_base",
        "documentation": {}
    },
    {
        "label": "test_mean_agg",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.embeddings.test_base",
        "description": "reference_code.llama-index-core.tests.embeddings.test_base",
        "peekOfCode": "def test_mean_agg() -> None:\n    \"\"\"Test mean aggregation for embeddings.\"\"\"\n    embedding_0 = [3.0, 4.0, 0.0]\n    embedding_1 = [0.0, 1.0, 0.0]\n    output = mean_agg([embedding_0, embedding_1])\n    assert output == [1.5, 2.5, 0.0]",
        "detail": "reference_code.llama-index-core.tests.embeddings.test_base",
        "documentation": {}
    },
    {
        "label": "test_resolve_embed_model",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.embeddings.test_utils",
        "description": "reference_code.llama-index-core.tests.embeddings.test_utils",
        "peekOfCode": "def test_resolve_embed_model(monkeypatch: MonkeyPatch) -> None:\n    # Test None\n    embed_model = resolve_embed_model(None)\n    assert isinstance(embed_model, MockEmbedding)",
        "detail": "reference_code.llama-index-core.tests.embeddings.test_utils",
        "documentation": {}
    },
    {
        "label": "custom_embeddings",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.embeddings.test_with_cache",
        "description": "reference_code.llama-index-core.tests.embeddings.test_with_cache",
        "peekOfCode": "def custom_embeddings(texts):\n    return [[float(ord(c)) for c in text[-4:]] for text in texts]\ndef test_sync_get_with_cache():\n    embeddings_cache = SimpleKVStore()\n    embed_model = MockEmbedding(embed_dim=4, embeddings_cache=embeddings_cache)\n    text = \"Hello\"\n    text_embedding = embed_model.get_text_embedding(text)\n    assert text_embedding == expected_embedding\n    assert embeddings_cache.get(key=\"Hello\", collection=\"embeddings\") is not None\n    embd_dict = embeddings_cache.get(key=\"Hello\", collection=\"embeddings\")",
        "detail": "reference_code.llama-index-core.tests.embeddings.test_with_cache",
        "documentation": {}
    },
    {
        "label": "test_sync_get_with_cache",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.embeddings.test_with_cache",
        "description": "reference_code.llama-index-core.tests.embeddings.test_with_cache",
        "peekOfCode": "def test_sync_get_with_cache():\n    embeddings_cache = SimpleKVStore()\n    embed_model = MockEmbedding(embed_dim=4, embeddings_cache=embeddings_cache)\n    text = \"Hello\"\n    text_embedding = embed_model.get_text_embedding(text)\n    assert text_embedding == expected_embedding\n    assert embeddings_cache.get(key=\"Hello\", collection=\"embeddings\") is not None\n    embd_dict = embeddings_cache.get(key=\"Hello\", collection=\"embeddings\")\n    first_key = next(iter(embd_dict.keys()))\n    assert embd_dict[first_key] == expected_embedding",
        "detail": "reference_code.llama-index-core.tests.embeddings.test_with_cache",
        "documentation": {}
    },
    {
        "label": "test_sync_get_batch_with_cache",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.embeddings.test_with_cache",
        "description": "reference_code.llama-index-core.tests.embeddings.test_with_cache",
        "peekOfCode": "def test_sync_get_batch_with_cache():\n    \"\"\"Test mixed scenario with some cached and some new inputs.\"\"\"\n    embeddings_cache = SimpleKVStore()\n    embed_model = MockEmbedding(embed_dim=4, embeddings_cache=embeddings_cache)\n    texts = [\"Cached1\", \"Miss1\", \"Cached2\", \"Miss2\"]\n    # Pre-cache\n    embed_model.embeddings_cache.put(\n        key=\"Cached1\",\n        val={\"uuid1\": [104.0, 101.0, 100.0, 49.0]},\n        collection=\"embeddings\",",
        "detail": "reference_code.llama-index-core.tests.embeddings.test_with_cache",
        "documentation": {}
    },
    {
        "label": "expected_embedding",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.embeddings.test_with_cache",
        "description": "reference_code.llama-index-core.tests.embeddings.test_with_cache",
        "peekOfCode": "expected_embedding = [0.5, 0.5, 0.5, 0.5]\n# Create unique embeddings for each text to verify order\ndef custom_embeddings(texts):\n    return [[float(ord(c)) for c in text[-4:]] for text in texts]\ndef test_sync_get_with_cache():\n    embeddings_cache = SimpleKVStore()\n    embed_model = MockEmbedding(embed_dim=4, embeddings_cache=embeddings_cache)\n    text = \"Hello\"\n    text_embedding = embed_model.get_text_embedding(text)\n    assert text_embedding == expected_embedding",
        "detail": "reference_code.llama-index-core.tests.embeddings.test_with_cache",
        "documentation": {}
    },
    {
        "label": "mock_hf_embeddings",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.embeddings.todo_hf_test_utils",
        "description": "reference_code.llama-index-core.tests.embeddings.todo_hf_test_utils",
        "peekOfCode": "def mock_hf_embeddings(self: Any, *args: Any, **kwargs: Dict[str, Any]) -> Any:\n    \"\"\"Mock HuggingFaceEmbeddings.\"\"\"\n    super(HuggingFaceEmbedding, self).__init__(\n        model_name=\"fake\",\n        tokenizer_name=\"fake\",\n        model=\"fake\",\n        tokenizer=\"fake\",\n    )\n    return\ndef mock_openai_embeddings(self: Any, *args: Any, **kwargs: Dict[str, Any]) -> Any:",
        "detail": "reference_code.llama-index-core.tests.embeddings.todo_hf_test_utils",
        "documentation": {}
    },
    {
        "label": "mock_openai_embeddings",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.embeddings.todo_hf_test_utils",
        "description": "reference_code.llama-index-core.tests.embeddings.todo_hf_test_utils",
        "peekOfCode": "def mock_openai_embeddings(self: Any, *args: Any, **kwargs: Dict[str, Any]) -> Any:\n    \"\"\"Mock OpenAIEmbedding.\"\"\"\n    super(OpenAIEmbedding, self).__init__(\n        api_key=\"fake\", api_base=\"fake\", api_version=\"fake\"\n    )\n    return\ndef test_resolve_embed_model(monkeypatch: MonkeyPatch) -> None:\n    monkeypatch.setattr(\n        \"llama_index.embeddings.huggingface.HuggingFaceEmbedding.__init__\",\n        mock_hf_embeddings,",
        "detail": "reference_code.llama-index-core.tests.embeddings.todo_hf_test_utils",
        "documentation": {}
    },
    {
        "label": "test_resolve_embed_model",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.embeddings.todo_hf_test_utils",
        "description": "reference_code.llama-index-core.tests.embeddings.todo_hf_test_utils",
        "peekOfCode": "def test_resolve_embed_model(monkeypatch: MonkeyPatch) -> None:\n    monkeypatch.setattr(\n        \"llama_index.embeddings.huggingface.HuggingFaceEmbedding.__init__\",\n        mock_hf_embeddings,\n    )\n    monkeypatch.setattr(\n        \"llama_index.embeddings.openai.OpenAIEmbedding.__init__\",\n        mock_openai_embeddings,\n    )\n    # Test None",
        "detail": "reference_code.llama-index-core.tests.embeddings.todo_hf_test_utils",
        "documentation": {}
    },
    {
        "label": "MockEvaluator",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.evaluation.test_base",
        "description": "reference_code.llama-index-core.tests.evaluation.test_base",
        "peekOfCode": "class MockEvaluator(BaseEvaluator):\n    def __init__(\n        self,\n        mock_score: float = 1.0,\n        mock_passing: bool = True,\n        mock_feedback: str = \"test feedback\",\n    ) -> None:\n        self._mock_score = mock_score\n        self._mock_passing = mock_passing\n        self._mock_feedback = mock_feedback",
        "detail": "reference_code.llama-index-core.tests.evaluation.test_base",
        "documentation": {}
    },
    {
        "label": "test_evaluator_basic",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.evaluation.test_base",
        "description": "reference_code.llama-index-core.tests.evaluation.test_base",
        "peekOfCode": "def test_evaluator_basic() -> None:\n    test_evaluator = MockEvaluator()\n    eval_result_0 = test_evaluator.evaluate(\n        query=\"test query\",\n        response=\"test response\",\n        contexts=[\"test context 1\", \"test context 2\"],\n    )\n    eval_result_1 = test_evaluator.evaluate_response(\n        query=\"test query\",\n        response=Response(",
        "detail": "reference_code.llama-index-core.tests.evaluation.test_base",
        "documentation": {}
    },
    {
        "label": "MockEvaluator",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.evaluation.test_batch_runner",
        "description": "reference_code.llama-index-core.tests.evaluation.test_batch_runner",
        "peekOfCode": "class MockEvaluator(BaseEvaluator):\n    def __init__(\n        self,\n        mock_score: float = 1.0,\n        mock_passing: bool = True,\n        mock_feedback: str = \"test feedback\",\n    ) -> None:\n        self._mock_score = mock_score\n        self._mock_passing = mock_passing\n        self._mock_feedback = mock_feedback",
        "detail": "reference_code.llama-index-core.tests.evaluation.test_batch_runner",
        "documentation": {}
    },
    {
        "label": "get_eval_results",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.evaluation.test_batch_runner",
        "description": "reference_code.llama-index-core.tests.evaluation.test_batch_runner",
        "peekOfCode": "def get_eval_results(key, eval_results):\n    results = eval_results[key]\n    correct = 0\n    for result in results:\n        if result.passing:\n            correct += 1\n    return correct / len(results)\ndef test_batch_runner() -> None:\n    # single evaluator\n    runner = BatchEvalRunner(",
        "detail": "reference_code.llama-index-core.tests.evaluation.test_batch_runner",
        "documentation": {}
    },
    {
        "label": "test_batch_runner",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.evaluation.test_batch_runner",
        "description": "reference_code.llama-index-core.tests.evaluation.test_batch_runner",
        "peekOfCode": "def test_batch_runner() -> None:\n    # single evaluator\n    runner = BatchEvalRunner(\n        evaluators={\n            \"evaluator1\": MockEvaluator(),\n            \"no_kwarg_evaluator\": MockEvaluator(),\n        }\n    )\n    exp_queries = [\"query1\", \"query2\"]\n    exp_response_strs = [\"response1\", \"response2\"]",
        "detail": "reference_code.llama-index-core.tests.evaluation.test_batch_runner",
        "documentation": {}
    },
    {
        "label": "test_dataset_generation",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.evaluation.test_dataset_generation",
        "description": "reference_code.llama-index-core.tests.evaluation.test_dataset_generation",
        "peekOfCode": "def test_dataset_generation(patch_llm_predictor) -> None:\n    \"\"\"Test dataset generation.\"\"\"\n    test_nodes = [TextNode(text=\"hello_world\"), TextNode(text=\"foo_bar\")]\n    question_gen_prompt = PromptTemplate(\n        \"\"\"\\\nContext information is below.\n---------------------\n{context_str}\n---------------------\nGiven the context information and not prior knowledge.",
        "detail": "reference_code.llama-index-core.tests.evaluation.test_dataset_generation",
        "documentation": {}
    },
    {
        "label": "test_hit_rate",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.evaluation.test_metrics",
        "description": "reference_code.llama-index-core.tests.evaluation.test_metrics",
        "peekOfCode": "def test_hit_rate(expected_ids, retrieved_ids, use_granular, expected_result):\n    hr = HitRate()\n    hr.use_granular_hit_rate = use_granular\n    result = hr.compute(expected_ids=expected_ids, retrieved_ids=retrieved_ids)\n    assert result.score == pytest.approx(expected_result)\n# Test cases for the updated MRR class using instance attribute\n@pytest.mark.parametrize(\n    (\"expected_ids\", \"retrieved_ids\", \"use_granular\", \"expected_result\"),\n    [\n        ([\"id1\", \"id2\", \"id3\"], [\"id3\", \"id1\", \"id2\", \"id4\"], False, 1 / 1),",
        "detail": "reference_code.llama-index-core.tests.evaluation.test_metrics",
        "documentation": {}
    },
    {
        "label": "test_mrr",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.evaluation.test_metrics",
        "description": "reference_code.llama-index-core.tests.evaluation.test_metrics",
        "peekOfCode": "def test_mrr(expected_ids, retrieved_ids, use_granular, expected_result):\n    mrr = MRR()\n    mrr.use_granular_mrr = use_granular\n    result = mrr.compute(expected_ids=expected_ids, retrieved_ids=retrieved_ids)\n    assert result.score == pytest.approx(expected_result)\n@pytest.mark.parametrize(\n    (\"expected_ids\", \"retrieved_ids\", \"expected_result\"),\n    [\n        ([\"id1\", \"id2\", \"id3\"], [\"id3\", \"id1\", \"id2\", \"id4\"], 3 / 4),\n        ([\"id1\", \"id2\", \"id3\", \"id4\"], [\"id5\", \"id1\"], 1 / 2),",
        "detail": "reference_code.llama-index-core.tests.evaluation.test_metrics",
        "documentation": {}
    },
    {
        "label": "test_precision",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.evaluation.test_metrics",
        "description": "reference_code.llama-index-core.tests.evaluation.test_metrics",
        "peekOfCode": "def test_precision(expected_ids, retrieved_ids, expected_result):\n    prec = Precision()\n    result = prec.compute(expected_ids=expected_ids, retrieved_ids=retrieved_ids)\n    assert result.score == pytest.approx(expected_result)\n@pytest.mark.parametrize(\n    (\"expected_ids\", \"retrieved_ids\", \"expected_result\"),\n    [\n        ([\"id1\", \"id2\", \"id3\"], [\"id3\", \"id1\", \"id2\", \"id4\"], 3 / 3),\n        ([\"id1\", \"id2\", \"id3\", \"id4\"], [\"id5\", \"id1\"], 1 / 4),\n        ([\"id1\", \"id2\"], [\"id3\", \"id4\"], 0 / 2),",
        "detail": "reference_code.llama-index-core.tests.evaluation.test_metrics",
        "documentation": {}
    },
    {
        "label": "test_recall",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.evaluation.test_metrics",
        "description": "reference_code.llama-index-core.tests.evaluation.test_metrics",
        "peekOfCode": "def test_recall(expected_ids, retrieved_ids, expected_result):\n    recall = Recall()\n    result = recall.compute(expected_ids=expected_ids, retrieved_ids=retrieved_ids)\n    assert result.score == pytest.approx(expected_result)\n@pytest.mark.parametrize(\n    (\"expected_ids\", \"retrieved_ids\", \"expected_result\"),\n    [\n        (\n            [\"id1\", \"id2\", \"id3\"],\n            [\"id3\", \"id1\", \"id2\", \"id4\"],",
        "detail": "reference_code.llama-index-core.tests.evaluation.test_metrics",
        "documentation": {}
    },
    {
        "label": "test_ap",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.evaluation.test_metrics",
        "description": "reference_code.llama-index-core.tests.evaluation.test_metrics",
        "peekOfCode": "def test_ap(expected_ids, retrieved_ids, expected_result):\n    ap = AveragePrecision()\n    result = ap.compute(expected_ids=expected_ids, retrieved_ids=retrieved_ids)\n    assert result.score == pytest.approx(expected_result)\n@pytest.mark.parametrize(\n    (\"expected_ids\", \"retrieved_ids\", \"mode\", \"expected_result\"),\n    [\n        (\n            [\"id1\", \"id2\", \"id3\"],\n            [\"id3\", \"id1\", \"id2\", \"id4\"],",
        "detail": "reference_code.llama-index-core.tests.evaluation.test_metrics",
        "documentation": {}
    },
    {
        "label": "test_ndcg",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.evaluation.test_metrics",
        "description": "reference_code.llama-index-core.tests.evaluation.test_metrics",
        "peekOfCode": "def test_ndcg(expected_ids, retrieved_ids, mode, expected_result):\n    ndcg = NDCG()\n    ndcg.mode = mode\n    result = ndcg.compute(expected_ids=expected_ids, retrieved_ids=retrieved_ids)\n    assert result.score == pytest.approx(expected_result)\n# Test cases for exceptions handling for both HitRate and MRR\n@pytest.mark.parametrize(\n    (\"expected_ids\", \"retrieved_ids\", \"use_granular\"),\n    [\n        (",
        "detail": "reference_code.llama-index-core.tests.evaluation.test_metrics",
        "documentation": {}
    },
    {
        "label": "test_exceptions",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.evaluation.test_metrics",
        "description": "reference_code.llama-index-core.tests.evaluation.test_metrics",
        "peekOfCode": "def test_exceptions(expected_ids, retrieved_ids, use_granular):\n    with pytest.raises(ValueError):\n        hr = HitRate()\n        hr.use_granular_hit_rate = use_granular\n        hr.compute(expected_ids=expected_ids, retrieved_ids=retrieved_ids)\n    with pytest.raises(ValueError):\n        mrr = MRR()\n        mrr.use_granular_mrr = use_granular\n        mrr.compute(expected_ids=expected_ids, retrieved_ids=retrieved_ids)\n    with pytest.raises(ValueError):",
        "detail": "reference_code.llama-index-core.tests.evaluation.test_metrics",
        "documentation": {}
    },
    {
        "label": "test_upload_eval_dataset",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.evaluation.test_platform_eval",
        "description": "reference_code.llama-index-core.tests.evaluation.test_platform_eval",
        "peekOfCode": "def test_upload_eval_dataset() -> None:\n    from llama_cloud.client import LlamaCloud\n    eval_dataset_id = upload_eval_dataset(\n        \"test_dataset\" + python_version,  # avoid CI test clashes\n        project_name=\"test_project\" + python_version,\n        questions=[\"foo\", \"bar\"],\n        overwrite=True,\n    )\n    client = LlamaCloud(base_url=base_url, token=api_key)\n    eval_dataset = client.evals.get_dataset(dataset_id=eval_dataset_id)",
        "detail": "reference_code.llama-index-core.tests.evaluation.test_platform_eval",
        "documentation": {}
    },
    {
        "label": "base_url",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.evaluation.test_platform_eval",
        "description": "reference_code.llama-index-core.tests.evaluation.test_platform_eval",
        "peekOfCode": "base_url = os.environ.get(\"LLAMA_CLOUD_BASE_URL\", None)\napi_key = os.environ.get(\"LLAMA_CLOUD_API_KEY\", None)\npython_version = sys.version\n@pytest.mark.skipif(\n    not base_url or not api_key, reason=\"No platform base url or api keyset\"\n)\n@pytest.mark.integration\ndef test_upload_eval_dataset() -> None:\n    from llama_cloud.client import LlamaCloud\n    eval_dataset_id = upload_eval_dataset(",
        "detail": "reference_code.llama-index-core.tests.evaluation.test_platform_eval",
        "documentation": {}
    },
    {
        "label": "api_key",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.evaluation.test_platform_eval",
        "description": "reference_code.llama-index-core.tests.evaluation.test_platform_eval",
        "peekOfCode": "api_key = os.environ.get(\"LLAMA_CLOUD_API_KEY\", None)\npython_version = sys.version\n@pytest.mark.skipif(\n    not base_url or not api_key, reason=\"No platform base url or api keyset\"\n)\n@pytest.mark.integration\ndef test_upload_eval_dataset() -> None:\n    from llama_cloud.client import LlamaCloud\n    eval_dataset_id = upload_eval_dataset(\n        \"test_dataset\" + python_version,  # avoid CI test clashes",
        "detail": "reference_code.llama-index-core.tests.evaluation.test_platform_eval",
        "documentation": {}
    },
    {
        "label": "python_version",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.evaluation.test_platform_eval",
        "description": "reference_code.llama-index-core.tests.evaluation.test_platform_eval",
        "peekOfCode": "python_version = sys.version\n@pytest.mark.skipif(\n    not base_url or not api_key, reason=\"No platform base url or api keyset\"\n)\n@pytest.mark.integration\ndef test_upload_eval_dataset() -> None:\n    from llama_cloud.client import LlamaCloud\n    eval_dataset_id = upload_eval_dataset(\n        \"test_dataset\" + python_version,  # avoid CI test clashes\n        project_name=\"test_project\" + python_version,",
        "detail": "reference_code.llama-index-core.tests.evaluation.test_platform_eval",
        "documentation": {}
    },
    {
        "label": "mock_llm",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.extractors.test_document_context_extractor",
        "description": "reference_code.llama-index-core.tests.extractors.test_document_context_extractor",
        "peekOfCode": "def mock_llm():\n    class CustomMockLLM(MockLLM):\n        def chat(self, messages, **kwargs):\n            return ChatResponse(\n                message=ChatMessage(\n                    role=\"assistant\",\n                    blocks=[\n                        {\n                            \"text\": f\"Context for the provided chunk\",\n                            \"block_type\": \"text\",",
        "detail": "reference_code.llama-index-core.tests.extractors.test_document_context_extractor",
        "documentation": {}
    },
    {
        "label": "sample_documents",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.extractors.test_document_context_extractor",
        "description": "reference_code.llama-index-core.tests.extractors.test_document_context_extractor",
        "peekOfCode": "def sample_documents():\n    return [\n        Document(\n            text=\"This is chapter 1. It contains important information. This is a test document.\",\n            metadata={\"title\": \"Doc 1\"},\n        ),\n        Document(\n            text=\"Chapter 2 builds on previous concepts. It introduces new ideas. More test content here.\",\n            metadata={\"title\": \"Doc 2\"},\n        ),",
        "detail": "reference_code.llama-index-core.tests.extractors.test_document_context_extractor",
        "documentation": {}
    },
    {
        "label": "create_text_nodes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.extractors.test_document_context_extractor",
        "description": "reference_code.llama-index-core.tests.extractors.test_document_context_extractor",
        "peekOfCode": "def create_text_nodes():\n    def _create_nodes(document, texts):\n        doc_info = document.as_related_node_info()\n        return [\n            TextNode(\n                text=text,\n                metadata={},\n                relationships={NodeRelationship.SOURCE: doc_info},\n            )\n            for text in texts",
        "detail": "reference_code.llama-index-core.tests.extractors.test_document_context_extractor",
        "documentation": {}
    },
    {
        "label": "docstore",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.extractors.test_document_context_extractor",
        "description": "reference_code.llama-index-core.tests.extractors.test_document_context_extractor",
        "peekOfCode": "def docstore(sample_documents):\n    docstore = SimpleDocumentStore()\n    for doc in sample_documents:\n        docstore.add_documents([doc])\n    return docstore\n@pytest.fixture()\ndef context_extractor(docstore, mock_llm):\n    return DocumentContextExtractor(\n        docstore=docstore,\n        llm=mock_llm,",
        "detail": "reference_code.llama-index-core.tests.extractors.test_document_context_extractor",
        "documentation": {}
    },
    {
        "label": "context_extractor",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.extractors.test_document_context_extractor",
        "description": "reference_code.llama-index-core.tests.extractors.test_document_context_extractor",
        "peekOfCode": "def context_extractor(docstore, mock_llm):\n    return DocumentContextExtractor(\n        docstore=docstore,\n        llm=mock_llm,\n        max_context_length=1000,\n        max_output_tokens=100,\n        oversized_document_strategy=\"error\",\n    )\n@pytest.mark.asyncio\nasync def test_context_extraction_basic(",
        "detail": "reference_code.llama-index-core.tests.extractors.test_document_context_extractor",
        "documentation": {}
    },
    {
        "label": "test_invalid_oversized_strategy",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.extractors.test_document_context_extractor",
        "description": "reference_code.llama-index-core.tests.extractors.test_document_context_extractor",
        "peekOfCode": "def test_invalid_oversized_strategy():\n    with pytest.raises(ValueError):\n        DocumentContextExtractor(\n            docstore=SimpleDocumentStore(),\n            llm=MockLLM(),\n            max_context_length=1000,\n            max_output_tokens=100,\n            oversized_document_strategy=\"invalid_strategy\",\n        )\n@pytest.mark.asyncio",
        "detail": "reference_code.llama-index-core.tests.extractors.test_document_context_extractor",
        "documentation": {}
    },
    {
        "label": "test_add",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.graph_stores.test_simple_lpg",
        "description": "reference_code.llama-index-core.tests.graph_stores.test_simple_lpg",
        "peekOfCode": "def test_add() -> None:\n    g = SimplePropertyGraphStore()\n    e1 = EntityNode(name=\"e1\")\n    e2 = EntityNode(name=\"e2\")\n    r = Relation(label=\"r\", source_id=e1.id, target_id=e2.id)\n    g.upsert_nodes([e1, e2])\n    g.upsert_relations([r])\n    assert len(g.graph.get_triplets()) == 1\ndef test_delete() -> None:\n    g = SimplePropertyGraphStore()",
        "detail": "reference_code.llama-index-core.tests.graph_stores.test_simple_lpg",
        "documentation": {}
    },
    {
        "label": "test_delete",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.graph_stores.test_simple_lpg",
        "description": "reference_code.llama-index-core.tests.graph_stores.test_simple_lpg",
        "peekOfCode": "def test_delete() -> None:\n    g = SimplePropertyGraphStore()\n    e1 = EntityNode(name=\"e1\")\n    e2 = EntityNode(name=\"e2\")\n    r = Relation(label=\"r\", source_id=e1.id, target_id=e2.id)\n    g.upsert_nodes([e1, e2])\n    g.upsert_relations([r])\n    g.delete(ids=[e1.id])\n    assert len(g.graph.get_triplets()) == 0\ndef test_get() -> None:",
        "detail": "reference_code.llama-index-core.tests.graph_stores.test_simple_lpg",
        "documentation": {}
    },
    {
        "label": "test_get",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.graph_stores.test_simple_lpg",
        "description": "reference_code.llama-index-core.tests.graph_stores.test_simple_lpg",
        "peekOfCode": "def test_get() -> None:\n    g = SimplePropertyGraphStore()\n    e1 = EntityNode(name=\"e1\")\n    e2 = EntityNode(name=\"e2\", properties={\"key\": \"value\"})\n    r = Relation(label=\"r\", source_id=e1.id, target_id=e2.id)\n    g.upsert_nodes([e1, e2])\n    g.upsert_relations([r])\n    assert g.get_triplets() == []\n    assert g.get_triplets(entity_names=[\"e1\"]) == [(e1, r, e2)]\n    assert g.get_triplets(entity_names=[\"e2\"]) == [(e1, r, e2)]",
        "detail": "reference_code.llama-index-core.tests.graph_stores.test_simple_lpg",
        "documentation": {}
    },
    {
        "label": "test_add_node",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.graph_stores.test_simple_lpg",
        "description": "reference_code.llama-index-core.tests.graph_stores.test_simple_lpg",
        "peekOfCode": "def test_add_node() -> None:\n    g = SimplePropertyGraphStore()\n    n1 = TextNode(id_=\"n1\", text=\"n1\")\n    n2 = TextNode(id_=\"n2\", text=\"n2\")\n    g.upsert_llama_nodes([n1, n2])\n    assert len(g.graph.get_all_nodes()) == 2\ndef test_delete_node_by_node_ids() -> None:\n    g = SimplePropertyGraphStore()\n    n1 = TextNode(id_=\"n1\", text=\"n1\")\n    n2 = TextNode(id_=\"n2\", text=\"n2\")",
        "detail": "reference_code.llama-index-core.tests.graph_stores.test_simple_lpg",
        "documentation": {}
    },
    {
        "label": "test_delete_node_by_node_ids",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.graph_stores.test_simple_lpg",
        "description": "reference_code.llama-index-core.tests.graph_stores.test_simple_lpg",
        "peekOfCode": "def test_delete_node_by_node_ids() -> None:\n    g = SimplePropertyGraphStore()\n    n1 = TextNode(id_=\"n1\", text=\"n1\")\n    n2 = TextNode(id_=\"n2\", text=\"n2\")\n    g.upsert_llama_nodes([n1, n2])\n    g.delete_llama_nodes(node_ids=[\"n1\"])\n    assert len(g.graph.get_all_nodes()) == 1\ndef test_delete_node_by_ref_doc_ids() -> None:\n    g = SimplePropertyGraphStore()\n    n1 = TextNode(",
        "detail": "reference_code.llama-index-core.tests.graph_stores.test_simple_lpg",
        "documentation": {}
    },
    {
        "label": "test_delete_node_by_ref_doc_ids",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.graph_stores.test_simple_lpg",
        "description": "reference_code.llama-index-core.tests.graph_stores.test_simple_lpg",
        "peekOfCode": "def test_delete_node_by_ref_doc_ids() -> None:\n    g = SimplePropertyGraphStore()\n    n1 = TextNode(\n        id_=\"n1\",\n        text=\"n1\",\n        relationships={NodeRelationship.SOURCE: RelatedNodeInfo(node_id=\"n2\")},\n    )\n    n2 = TextNode(id_=\"n2\", text=\"n2\")\n    g.upsert_llama_nodes([n1, n2])\n    g.delete_llama_nodes(ref_doc_ids=[\"n2\"])",
        "detail": "reference_code.llama-index-core.tests.graph_stores.test_simple_lpg",
        "documentation": {}
    },
    {
        "label": "test_get_nodes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.graph_stores.test_simple_lpg",
        "description": "reference_code.llama-index-core.tests.graph_stores.test_simple_lpg",
        "peekOfCode": "def test_get_nodes() -> None:\n    g = SimplePropertyGraphStore()\n    n1 = TextNode(id_=\"n1\", text=\"n1\")\n    n2 = TextNode(id_=\"n2\", text=\"n2\")\n    g.upsert_llama_nodes([n1, n2])\n    assert g.get_llama_nodes([\"n1\", \"n2\"]) == [n1, n2]",
        "detail": "reference_code.llama-index-core.tests.graph_stores.test_simple_lpg",
        "documentation": {}
    },
    {
        "label": "docs",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.document_summary.conftest",
        "description": "reference_code.llama-index-core.tests.indices.document_summary.conftest",
        "peekOfCode": "def docs() -> List[Document]:\n    return [\n        Document(text=\"This is a test v2.\", id_=\"doc_1\"),\n        Document(text=\"This is another test.\", id_=\"doc_2\"),\n        Document(text=\"This is a test.\", id_=\"doc_3\"),\n        Document(text=\"Hello world.\", id_=\"doc_4\"),\n    ]\n@pytest.fixture()\ndef index(\n    docs: List[Document], patch_llm_predictor, mock_embed_model",
        "detail": "reference_code.llama-index-core.tests.indices.document_summary.conftest",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.document_summary.conftest",
        "description": "reference_code.llama-index-core.tests.indices.document_summary.conftest",
        "peekOfCode": "def index(\n    docs: List[Document], patch_llm_predictor, mock_embed_model\n) -> DocumentSummaryIndex:\n    response_synthesizer = get_response_synthesizer(\n        text_qa_template=MOCK_TEXT_QA_PROMPT,\n        refine_template=MOCK_REFINE_PROMPT,\n    )\n    return DocumentSummaryIndex.from_documents(\n        docs,\n        response_synthesizer=response_synthesizer,",
        "detail": "reference_code.llama-index-core.tests.indices.document_summary.conftest",
        "documentation": {}
    },
    {
        "label": "test_build_index",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.document_summary.test_index",
        "description": "reference_code.llama-index-core.tests.indices.document_summary.test_index",
        "peekOfCode": "def test_build_index(\n    docs: List[Document],\n    index: DocumentSummaryIndex,\n) -> None:\n    \"\"\"Test build tree.\"\"\"\n    test = index.get_document_summary(\"doc_1\")\n    assert test == \"summary_query:This is a test v2.\"\n    test4 = index.get_document_summary(\"doc_4\")\n    assert test4 == \"summary_query:Hello world.\"\n    all_ref_doc_info = index.ref_doc_info",
        "detail": "reference_code.llama-index-core.tests.indices.document_summary.test_index",
        "documentation": {}
    },
    {
        "label": "test_delete_ref_doc",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.document_summary.test_index",
        "description": "reference_code.llama-index-core.tests.indices.document_summary.test_index",
        "peekOfCode": "def test_delete_ref_doc(\n    docs: List[Document],\n    index: DocumentSummaryIndex,\n) -> None:\n    \"\"\"Test delete node.\"\"\"\n    index.delete_ref_doc(\"doc_1\")\n    # assert that error is raised for doc_1\n    with pytest.raises(ValueError):\n        index.get_document_summary(\"doc_1\")\n    assert index.get_document_summary(\"doc_2\") == \"summary_query:This is another test.\"",
        "detail": "reference_code.llama-index-core.tests.indices.document_summary.test_index",
        "documentation": {}
    },
    {
        "label": "test_delete_nodes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.document_summary.test_index",
        "description": "reference_code.llama-index-core.tests.indices.document_summary.test_index",
        "peekOfCode": "def test_delete_nodes(\n    docs: List[Document],\n    index: DocumentSummaryIndex,\n) -> None:\n    \"\"\"Test delete node.\"\"\"\n    nodes = list(index.index_struct.node_id_to_summary_id.keys())\n    index.delete_nodes([nodes[0], nodes[1]])\n    assert len(index.ref_doc_info) == 2\n    assert len(index.index_struct.doc_id_to_summary_id) == 2\n    assert len(index.index_struct.node_id_to_summary_id) == 2",
        "detail": "reference_code.llama-index-core.tests.indices.document_summary.test_index",
        "documentation": {}
    },
    {
        "label": "test_embedding_retriever",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.document_summary.test_retrievers",
        "description": "reference_code.llama-index-core.tests.indices.document_summary.test_retrievers",
        "peekOfCode": "def test_embedding_retriever(index: DocumentSummaryIndex) -> None:\n    retriever = index.as_retriever()\n    assert isinstance(retriever, DocumentSummaryIndexEmbeddingRetriever)\n    results = retriever.retrieve(\"Test query\")\n    assert len(results) == 1\n    assert results[0].node.ref_doc_id == \"doc_4\"\n    retriever = index.as_retriever(similarity_top_k=2)\n    assert isinstance(retriever, DocumentSummaryIndexEmbeddingRetriever)\n    results = retriever.retrieve(\"Test query\")\n    assert len(results) == 2",
        "detail": "reference_code.llama-index-core.tests.indices.document_summary.test_retrievers",
        "documentation": {}
    },
    {
        "label": "test_llm_retriever",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.document_summary.test_retrievers",
        "description": "reference_code.llama-index-core.tests.indices.document_summary.test_retrievers",
        "peekOfCode": "def test_llm_retriever(\n    index: DocumentSummaryIndex,\n) -> None:\n    retriever = index.as_retriever(retriever_mode=DocumentSummaryRetrieverMode.LLM)\n    assert isinstance(retriever, DocumentSummaryIndexLLMRetriever)\n    results = retriever.retrieve(\"Test query\")\n    assert len(results) == 1",
        "detail": "reference_code.llama-index-core.tests.indices.document_summary.test_retrievers",
        "documentation": {}
    },
    {
        "label": "test_empty",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.empty.test_base",
        "description": "reference_code.llama-index-core.tests.indices.empty.test_base",
        "peekOfCode": "def test_empty() -> None:\n    \"\"\"Test build list.\"\"\"\n    empty_index = EmptyIndex()\n    assert isinstance(empty_index.index_struct, EmptyIndexStruct)\n    retriever = empty_index.as_retriever()\n    nodes = retriever.retrieve(\"What is?\")\n    assert len(nodes) == 0",
        "detail": "reference_code.llama-index-core.tests.indices.empty.test_base",
        "documentation": {}
    },
    {
        "label": "documents",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.keyword_table.test_base",
        "description": "reference_code.llama-index-core.tests.indices.keyword_table.test_base",
        "peekOfCode": "def documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    doc_text = (\n        \"Hello world.\\nThis is a test.\\nThis is another test.\\nThis is a test v2.\"\n    )\n    return [Document(text=doc_text)]\n@patch(\n    \"llama_index.core.indices.keyword_table.simple_base.simple_extract_keywords\",\n    mock_extract_keywords,",
        "detail": "reference_code.llama-index-core.tests.indices.keyword_table.test_base",
        "documentation": {}
    },
    {
        "label": "test_build_table",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.keyword_table.test_base",
        "description": "reference_code.llama-index-core.tests.indices.keyword_table.test_base",
        "peekOfCode": "def test_build_table(documents: List[Document], patch_token_text_splitter) -> None:\n    \"\"\"Test build table.\"\"\"\n    # test simple keyword table\n    # NOTE: here the keyword extraction isn't mocked because we're using\n    # the regex-based keyword extractor, not GPT\n    table = SimpleKeywordTableIndex.from_documents(documents)\n    nodes = table.docstore.get_nodes(list(table.index_struct.node_ids))\n    table_chunks = {n.get_content() for n in nodes}\n    assert len(table_chunks) == 4\n    assert \"Hello world.\" in table_chunks",
        "detail": "reference_code.llama-index-core.tests.indices.keyword_table.test_base",
        "documentation": {}
    },
    {
        "label": "test_build_table_async",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.keyword_table.test_base",
        "description": "reference_code.llama-index-core.tests.indices.keyword_table.test_base",
        "peekOfCode": "def test_build_table_async(\n    allow_networking: Any, documents: List[Document], patch_token_text_splitter\n) -> None:\n    \"\"\"Test build table.\"\"\"\n    # test simple keyword table\n    # NOTE: here the keyword extraction isn't mocked because we're using\n    # the regex-based keyword extractor, not GPT\n    table = SimpleKeywordTableIndex.from_documents(documents, use_async=True)\n    nodes = table.docstore.get_nodes(list(table.index_struct.node_ids))\n    table_chunks = {n.get_content() for n in nodes}",
        "detail": "reference_code.llama-index-core.tests.indices.keyword_table.test_base",
        "documentation": {}
    },
    {
        "label": "test_insert",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.keyword_table.test_base",
        "description": "reference_code.llama-index-core.tests.indices.keyword_table.test_base",
        "peekOfCode": "def test_insert(documents: List[Document], patch_token_text_splitter) -> None:\n    \"\"\"Test insert.\"\"\"\n    table = SimpleKeywordTableIndex([])\n    assert len(table.index_struct.table.keys()) == 0\n    table.insert(documents[0])\n    nodes = table.docstore.get_nodes(list(table.index_struct.node_ids))\n    table_chunks = {n.get_content() for n in nodes}\n    assert \"Hello world.\" in table_chunks\n    assert \"This is a test.\" in table_chunks\n    assert \"This is another test.\" in table_chunks",
        "detail": "reference_code.llama-index-core.tests.indices.keyword_table.test_base",
        "documentation": {}
    },
    {
        "label": "test_delete",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.keyword_table.test_base",
        "description": "reference_code.llama-index-core.tests.indices.keyword_table.test_base",
        "peekOfCode": "def test_delete(patch_token_text_splitter) -> None:\n    \"\"\"Test insert.\"\"\"\n    new_documents = [\n        Document(text=\"Hello world.\\nThis is a test.\", id_=\"test_id_1\"),\n        Document(text=\"This is another test.\", id_=\"test_id_2\"),\n        Document(text=\"This is a test v2.\", id_=\"test_id_3\"),\n    ]\n    # test delete\n    table = SimpleKeywordTableIndex.from_documents(new_documents)\n    # test delete",
        "detail": "reference_code.llama-index-core.tests.indices.keyword_table.test_base",
        "documentation": {}
    },
    {
        "label": "test_retrieve",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.keyword_table.test_retrievers",
        "description": "reference_code.llama-index-core.tests.indices.keyword_table.test_retrievers",
        "peekOfCode": "def test_retrieve(\n    documents: List[Document], mock_embed_model, patch_token_text_splitter\n) -> None:\n    \"\"\"Test query.\"\"\"\n    # test simple keyword table\n    # NOTE: here the keyword extraction isn't mocked because we're using\n    # the regex-based keyword extractor, not GPT\n    table = SimpleKeywordTableIndex.from_documents(documents)\n    retriever = table.as_retriever(\n        retriever_mode=\"simple\", embed_model=mock_embed_model",
        "detail": "reference_code.llama-index-core.tests.indices.keyword_table.test_retrievers",
        "documentation": {}
    },
    {
        "label": "test_expand_tokens_with_subtokens",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.keyword_table.test_utils",
        "description": "reference_code.llama-index-core.tests.indices.keyword_table.test_utils",
        "peekOfCode": "def test_expand_tokens_with_subtokens() -> None:\n    \"\"\"Test extract keywords given response.\"\"\"\n    response = \"foo bar, baz, Hello hello wOrld bye\"\n    keywords = extract_keywords_given_response(response)\n    assert keywords == {\n        \"foo bar\",\n        \"foo\",\n        \"bar\",\n        \"baz\",\n        \"hello hello world bye\",",
        "detail": "reference_code.llama-index-core.tests.indices.keyword_table.test_utils",
        "documentation": {}
    },
    {
        "label": "test_extract_keywords_with_start_delimiter",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.keyword_table.test_utils",
        "description": "reference_code.llama-index-core.tests.indices.keyword_table.test_utils",
        "peekOfCode": "def test_extract_keywords_with_start_delimiter() -> None:\n    \"\"\"Test extract keywords with start delimiter.\"\"\"\n    response = \"KEYWORDS: foo, bar, foobar\"\n    keywords = extract_keywords_given_response(response, start_token=\"KEYWORDS:\")\n    assert keywords == {\n        \"foo\",\n        \"bar\",\n        \"foobar\",\n    }\n    response = \"TOKENS: foo, bar, foobar\"",
        "detail": "reference_code.llama-index-core.tests.indices.keyword_table.test_utils",
        "documentation": {}
    },
    {
        "label": "documents",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.knowledge_graph.conftest",
        "description": "reference_code.llama-index-core.tests.indices.knowledge_graph.conftest",
        "peekOfCode": "def documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    # NOTE: in this unit test, document text == triplets\n    doc_text = \"(foo, is, bar)\\n(hello, is not, world)\\n(Jane, is mother of, Bob)\"\n    return [Document(text=doc_text)]\n@pytest.fixture()\ndef doc_triplets_with_text_around() -> List[str]:\n    \"\"\"Get triplets returned from LLM with text around triplet.\"\"\"\n    # NOTE: the first two triplets below are returned by LLM 'solar'.",
        "detail": "reference_code.llama-index-core.tests.indices.knowledge_graph.conftest",
        "documentation": {}
    },
    {
        "label": "doc_triplets_with_text_around",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.knowledge_graph.conftest",
        "description": "reference_code.llama-index-core.tests.indices.knowledge_graph.conftest",
        "peekOfCode": "def doc_triplets_with_text_around() -> List[str]:\n    \"\"\"Get triplets returned from LLM with text around triplet.\"\"\"\n    # NOTE: the first two triplets below are returned by LLM 'solar'.\n    # NOTE: in general it's good to be more relaxed when parsing triplet response. illustrated by the third triplet.\n    # NOTE: one document for now\n    # NOTE: in this unit test, document text == triplets\n    doc_text = (\n        \"1. (foo, is, bar)\\n\"\n        \"2. (hello, is not, world)\\n\"\n        \"Third triplet is (Jane, is mother of, Bob) according to your query\"",
        "detail": "reference_code.llama-index-core.tests.indices.knowledge_graph.conftest",
        "documentation": {}
    },
    {
        "label": "MockEmbedding",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_base",
        "description": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_base",
        "peekOfCode": "class MockEmbedding(BaseEmbedding):\n    @classmethod\n    def class_name(cls) -> str:\n        return \"MockEmbedding\"\n    async def _aget_query_embedding(self, query: str) -> List[float]:\n        del query\n        return [0, 0, 1, 0, 0]\n    async def _aget_text_embedding(self, text: str) -> List[float]:\n        # assume dimensions are 4\n        if text == \"('foo', 'is', 'bar')\":",
        "detail": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_base",
        "documentation": {}
    },
    {
        "label": "struct_kwargs",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_base",
        "description": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_base",
        "peekOfCode": "def struct_kwargs() -> Tuple[Dict, Dict]:\n    \"\"\"Index kwargs.\"\"\"\n    index_kwargs = {\n        \"kg_triplet_extract_template\": MOCK_KG_TRIPLET_EXTRACT_PROMPT,\n    }\n    query_kwargs = {\n        \"query_keyword_extract_template\": MOCK_QUERY_KEYWORD_EXTRACT_PROMPT,\n    }\n    return index_kwargs, query_kwargs\ndef mock_extract_triplets(text: str) -> List[Tuple[str, str, str]]:",
        "detail": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_base",
        "documentation": {}
    },
    {
        "label": "mock_extract_triplets",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_base",
        "description": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_base",
        "peekOfCode": "def mock_extract_triplets(text: str) -> List[Tuple[str, str, str]]:\n    \"\"\"Mock extract triplets.\"\"\"\n    lines = text.split(\"\\n\")\n    triplets: List[Tuple[str, str, str]] = []\n    for line in lines:\n        tokens = line[1:-1].split(\",\")\n        tokens = [t.strip() for t in tokens]\n        subj, pred, obj = tokens\n        triplets.append((subj, pred, obj))\n    return triplets",
        "detail": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_base",
        "documentation": {}
    },
    {
        "label": "test_build_kg_manual",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_base",
        "description": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_base",
        "peekOfCode": "def test_build_kg_manual(_patch_extract_triplets: Any) -> None:\n    \"\"\"Test build knowledge graph.\"\"\"\n    index = KnowledgeGraphIndex([])\n    tuples = [\n        (\"foo\", \"is\", \"bar\"),\n        (\"hello\", \"is not\", \"world\"),\n        (\"Jane\", \"is mother of\", \"Bob\"),\n    ]\n    nodes = [TextNode(text=str(tup)) for tup in tuples]\n    for tup, node in zip(tuples, nodes):",
        "detail": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_base",
        "documentation": {}
    },
    {
        "label": "test_build_kg_similarity",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_base",
        "description": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_base",
        "peekOfCode": "def test_build_kg_similarity(\n    _patch_extract_triplets: Any, documents: List[Document]\n) -> None:\n    \"\"\"Test build knowledge graph.\"\"\"\n    index = KnowledgeGraphIndex.from_documents(\n        documents, include_embeddings=True, embed_model=MockEmbedding()\n    )\n    # get embedding dict from KG index struct\n    rel_text_embeddings = index.index_struct.embedding_dict\n    # check that all rel_texts were embedded",
        "detail": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_base",
        "documentation": {}
    },
    {
        "label": "test_build_kg",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_base",
        "description": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_base",
        "peekOfCode": "def test_build_kg(\n    _patch_extract_triplets: Any, documents: List[Document], patch_token_text_splitter\n) -> None:\n    \"\"\"Test build knowledge graph.\"\"\"\n    index = KnowledgeGraphIndex.from_documents(documents)\n    # NOTE: in these unit tests, document text == triplets\n    nodes = index.docstore.get_nodes(list(index.index_struct.node_ids))\n    table_chunks = {n.get_content() for n in nodes}\n    assert len(table_chunks) == 3\n    assert \"(foo, is, bar)\" in table_chunks",
        "detail": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_base",
        "documentation": {}
    },
    {
        "label": "test__parse_triplet_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_base",
        "description": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_base",
        "peekOfCode": "def test__parse_triplet_response(doc_triplets_with_text_around: List[Document]) -> None:\n    \"\"\"Test build knowledge graph with triplet response in other format.\"\"\"\n    parsed_triplets = []\n    for doc_triplet in doc_triplets_with_text_around:\n        parsed_triplets.append(\n            KnowledgeGraphIndex._parse_triplet_response(doc_triplet.text)\n        )\n    assert len(parsed_triplets) == 1\n    assert len(parsed_triplets[0]) == 3\n    # Expecting Capitalized triplet Outputs",
        "detail": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_base",
        "documentation": {}
    },
    {
        "label": "MockEmbedding",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_retrievers",
        "description": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_retrievers",
        "peekOfCode": "class MockEmbedding(BaseEmbedding):\n    @classmethod\n    def class_name(cls) -> str:\n        return \"MockEmbedding\"\n    async def _aget_query_embedding(self, query: str) -> List[float]:\n        del query\n        return [0, 0, 1, 0, 0]\n    async def _aget_text_embedding(self, text: str) -> List[float]:\n        # assume dimensions are 4\n        if text == \"('foo', 'is', 'bar')\":",
        "detail": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_retrievers",
        "documentation": {}
    },
    {
        "label": "mock_extract_triplets",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_retrievers",
        "description": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_retrievers",
        "peekOfCode": "def mock_extract_triplets(text: str) -> List[Tuple[str, str, str]]:\n    \"\"\"Mock extract triplets.\"\"\"\n    lines = text.split(\"\\n\")\n    triplets: List[Tuple[str, str, str]] = []\n    for line in lines:\n        tokens = line[1:-1].split(\",\")\n        tokens = [t.strip() for t in tokens]\n        subj, pred, obj = tokens\n        triplets.append((subj, pred, obj))\n    return triplets",
        "detail": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_retrievers",
        "documentation": {}
    },
    {
        "label": "test_as_retriever",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_retrievers",
        "description": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_retrievers",
        "peekOfCode": "def test_as_retriever(_patch_extract_triplets: Any, documents: List[Document]) -> None:\n    \"\"\"Test query.\"\"\"\n    graph_store = SimpleGraphStore()\n    storage_context = StorageContext.from_defaults(graph_store=graph_store)\n    index = KnowledgeGraphIndex.from_documents(\n        documents, storage_context=storage_context\n    )\n    retriever: KGTableRetriever = index.as_retriever()  # type: ignore\n    nodes = retriever.retrieve(QueryBundle(\"foo\"))\n    # when include_text is True, the first node is the raw text",
        "detail": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_retrievers",
        "documentation": {}
    },
    {
        "label": "test_retrievers",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_retrievers",
        "description": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_retrievers",
        "peekOfCode": "def test_retrievers(_patch_extract_triplets: Any, documents: List[Document]) -> None:\n    # test specific retriever class\n    graph_store = SimpleGraphStore()\n    storage_context = StorageContext.from_defaults(graph_store=graph_store)\n    index = KnowledgeGraphIndex.from_documents(\n        documents, storage_context=storage_context\n    )\n    retriever = KGTableRetriever(\n        index,\n        query_keyword_extract_template=MOCK_QUERY_KEYWORD_EXTRACT_PROMPT,",
        "detail": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_retrievers",
        "documentation": {}
    },
    {
        "label": "test_retriever_no_text",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_retrievers",
        "description": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_retrievers",
        "peekOfCode": "def test_retriever_no_text(\n    _patch_extract_triplets: Any, documents: List[Document]\n) -> None:\n    # test specific retriever class\n    graph_store = SimpleGraphStore()\n    storage_context = StorageContext.from_defaults(graph_store=graph_store)\n    index = KnowledgeGraphIndex.from_documents(\n        documents, storage_context=storage_context\n    )\n    retriever = KGTableRetriever(",
        "detail": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_retrievers",
        "documentation": {}
    },
    {
        "label": "test_retrieve_similarity",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_retrievers",
        "description": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_retrievers",
        "peekOfCode": "def test_retrieve_similarity(\n    _patch_extract_triplets: Any, documents: List[Document]\n) -> None:\n    \"\"\"Test query.\"\"\"\n    graph_store = SimpleGraphStore()\n    storage_context = StorageContext.from_defaults(graph_store=graph_store)\n    index = KnowledgeGraphIndex.from_documents(\n        documents,\n        include_embeddings=True,\n        storage_context=storage_context,",
        "detail": "reference_code.llama-index-core.tests.indices.knowledge_graph.test_retrievers",
        "documentation": {}
    },
    {
        "label": "test_build_list",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.list.test_index",
        "description": "reference_code.llama-index-core.tests.indices.list.test_index",
        "peekOfCode": "def test_build_list(documents: List[Document], patch_token_text_splitter) -> None:\n    \"\"\"Test build list.\"\"\"\n    summary_index = SummaryIndex.from_documents(documents)\n    assert len(summary_index.index_struct.nodes) == 4\n    # check contents of nodes\n    node_ids = summary_index.index_struct.nodes\n    nodes = summary_index.docstore.get_nodes(node_ids)\n    assert nodes[0].get_content() == \"Hello world.\"\n    assert nodes[1].get_content() == \"This is a test.\"\n    assert nodes[2].get_content() == \"This is another test.\"",
        "detail": "reference_code.llama-index-core.tests.indices.list.test_index",
        "documentation": {}
    },
    {
        "label": "test_refresh_list",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.list.test_index",
        "description": "reference_code.llama-index-core.tests.indices.list.test_index",
        "peekOfCode": "def test_refresh_list(documents: List[Document]) -> None:\n    \"\"\"Test build list.\"\"\"\n    # add extra document\n    more_documents = [*documents, Document(text=\"Test document 2\")]\n    # ensure documents have doc_id\n    for i in range(len(more_documents)):\n        more_documents[i].doc_id = str(i)  # type: ignore[misc]\n    # create index\n    summary_index = SummaryIndex.from_documents(more_documents)\n    # check that no documents are refreshed",
        "detail": "reference_code.llama-index-core.tests.indices.list.test_index",
        "documentation": {}
    },
    {
        "label": "test_build_list_multiple",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.list.test_index",
        "description": "reference_code.llama-index-core.tests.indices.list.test_index",
        "peekOfCode": "def test_build_list_multiple(patch_token_text_splitter) -> None:\n    \"\"\"Test build list multiple.\"\"\"\n    documents = [\n        Document(text=\"Hello world.\\nThis is a test.\"),\n        Document(text=\"This is another test.\\nThis is a test v2.\"),\n    ]\n    summary_index = SummaryIndex.from_documents(documents)\n    assert len(summary_index.index_struct.nodes) == 4\n    nodes = summary_index.docstore.get_nodes(summary_index.index_struct.nodes)\n    # check contents of nodes",
        "detail": "reference_code.llama-index-core.tests.indices.list.test_index",
        "documentation": {}
    },
    {
        "label": "test_list_insert",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.list.test_index",
        "description": "reference_code.llama-index-core.tests.indices.list.test_index",
        "peekOfCode": "def test_list_insert(documents: List[Document], patch_token_text_splitter) -> None:\n    \"\"\"Test insert to list.\"\"\"\n    summary_index = SummaryIndex([])\n    assert len(summary_index.index_struct.nodes) == 0\n    summary_index.insert(documents[0])\n    nodes = summary_index.docstore.get_nodes(summary_index.index_struct.nodes)\n    # check contents of nodes\n    assert nodes[0].get_content() == \"Hello world.\"\n    assert nodes[1].get_content() == \"This is a test.\"\n    assert nodes[2].get_content() == \"This is another test.\"",
        "detail": "reference_code.llama-index-core.tests.indices.list.test_index",
        "documentation": {}
    },
    {
        "label": "test_list_delete",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.list.test_index",
        "description": "reference_code.llama-index-core.tests.indices.list.test_index",
        "peekOfCode": "def test_list_delete(documents: List[Document], patch_token_text_splitter) -> None:\n    \"\"\"Test insert to list and then delete.\"\"\"\n    new_documents = [\n        Document(text=\"Hello world.\\nThis is a test.\", id_=\"test_id_1\"),\n        Document(text=\"This is another test.\", id_=\"test_id_2\"),\n        Document(text=\"This is a test v2.\", id_=\"test_id_3\"),\n    ]\n    summary_index = SummaryIndex.from_documents(new_documents)\n    # test ref doc info for three docs\n    all_ref_doc_info = summary_index.ref_doc_info",
        "detail": "reference_code.llama-index-core.tests.indices.list.test_index",
        "documentation": {}
    },
    {
        "label": "test_as_retriever",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.list.test_index",
        "description": "reference_code.llama-index-core.tests.indices.list.test_index",
        "peekOfCode": "def test_as_retriever(documents: List[Document]) -> None:\n    summary_index = SummaryIndex.from_documents(documents)\n    default_retriever = summary_index.as_retriever(\n        retriever_mode=ListRetrieverMode.DEFAULT\n    )\n    assert isinstance(default_retriever, BaseRetriever)\n    embedding_retriever = summary_index.as_retriever(\n        retriever_mode=ListRetrieverMode.EMBEDDING\n    )\n    assert isinstance(embedding_retriever, BaseRetriever)",
        "detail": "reference_code.llama-index-core.tests.indices.list.test_index",
        "documentation": {}
    },
    {
        "label": "test_retrieve_default",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.list.test_retrievers",
        "description": "reference_code.llama-index-core.tests.indices.list.test_retrievers",
        "peekOfCode": "def test_retrieve_default(documents: List[Document], patch_token_text_splitter) -> None:\n    \"\"\"Test list query.\"\"\"\n    index = SummaryIndex.from_documents(documents)\n    query_str = \"What is?\"\n    retriever = index.as_retriever(retriever_mode=\"default\")\n    nodes = retriever.retrieve(query_str)\n    for node_with_score, line in zip(nodes, documents[0].get_content().split(\"\\n\")):\n        assert node_with_score.node.get_content() == line\n@patch.object(\n    SummaryIndexEmbeddingRetriever,",
        "detail": "reference_code.llama-index-core.tests.indices.list.test_retrievers",
        "documentation": {}
    },
    {
        "label": "test_embedding_query",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.list.test_retrievers",
        "description": "reference_code.llama-index-core.tests.indices.list.test_retrievers",
        "peekOfCode": "def test_embedding_query(\n    _patch_get_embeddings: Any, documents: List[Document], patch_token_text_splitter\n) -> None:\n    \"\"\"Test embedding query.\"\"\"\n    index = SummaryIndex.from_documents(documents)\n    # test embedding query\n    query_str = \"What is?\"\n    retriever = index.as_retriever(retriever_mode=\"embedding\", similarity_top_k=1)\n    nodes = retriever.retrieve(query_str)\n    assert len(nodes) == 1",
        "detail": "reference_code.llama-index-core.tests.indices.list.test_retrievers",
        "documentation": {}
    },
    {
        "label": "mock_llmpredictor_predict",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.list.test_retrievers",
        "description": "reference_code.llama-index-core.tests.indices.list.test_retrievers",
        "peekOfCode": "def mock_llmpredictor_predict(\n    self: Any, prompt: BasePromptTemplate, **prompt_args: Any\n) -> str:\n    \"\"\"Patch llm predictor predict.\"\"\"\n    return \"Doc: 2, Relevance: 5\"\n@patch.object(\n    MockLLM,\n    \"predict\",\n    mock_llmpredictor_predict,\n)",
        "detail": "reference_code.llama-index-core.tests.indices.list.test_retrievers",
        "documentation": {}
    },
    {
        "label": "test_llm_query",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.list.test_retrievers",
        "description": "reference_code.llama-index-core.tests.indices.list.test_retrievers",
        "peekOfCode": "def test_llm_query(documents: List[Document], patch_token_text_splitter) -> None:\n    \"\"\"Test llm query.\"\"\"\n    index = SummaryIndex.from_documents(documents)\n    # test llm query (batch size 10)\n    query_str = \"What is?\"\n    retriever = index.as_retriever(retriever_mode=\"llm\")\n    nodes = retriever.retrieve(query_str)\n    assert len(nodes) == 1\n    assert nodes[0].node.get_content() == \"This is a test.\"\n    # test llm query (batch size 2)",
        "detail": "reference_code.llama-index-core.tests.indices.list.test_retrievers",
        "documentation": {}
    },
    {
        "label": "MockKGExtractor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.indices.property_graph.test_property_graph",
        "description": "reference_code.llama-index-core.tests.indices.property_graph.test_property_graph",
        "peekOfCode": "class MockKGExtractor(TransformComponent):\n    \"\"\"A mock knowledge graph extractor that extracts a simple relation from a text.\"\"\"\n    def __call__(self, nodes: List[BaseNode], **kwargs: Any) -> List[BaseNode]:\n        entity1 = EntityNode(name=\"Logan\", label=\"PERSON\")\n        entity2 = EntityNode(name=\"Canada\", label=\"LOCATION\")\n        relation = Relation(label=\"BORN_IN\", source_id=entity1.id, target_id=entity2.id)\n        return [\n            TextNode(\n                id_=\"test\",\n                text=\"Logan was born in Canada\",",
        "detail": "reference_code.llama-index-core.tests.indices.property_graph.test_property_graph",
        "documentation": {}
    },
    {
        "label": "test_construction",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.property_graph.test_property_graph",
        "description": "reference_code.llama-index-core.tests.indices.property_graph.test_property_graph",
        "peekOfCode": "def test_construction() -> None:\n    graph_store = SimplePropertyGraphStore()\n    vector_store = SimpleVectorStore()\n    kg_extractor = MockKGExtractor()\n    # test construction\n    index = PropertyGraphIndex.from_documents(\n        [Document.example()],\n        property_graph_store=graph_store,\n        vector_store=vector_store,\n        llm=MockLLM(),",
        "detail": "reference_code.llama-index-core.tests.indices.property_graph.test_property_graph",
        "documentation": {}
    },
    {
        "label": "MOCK_DECOMPOSE_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.indices.query.query_transform.mock_utils",
        "description": "reference_code.llama-index-core.tests.indices.query.query_transform.mock_utils",
        "peekOfCode": "MOCK_DECOMPOSE_TMPL = \"{context_str}\\n{query_str}\"\nMOCK_DECOMPOSE_PROMPT = DecomposeQueryTransformPrompt(\n    MOCK_DECOMPOSE_TMPL, prompt_type=PromptType.DECOMPOSE\n)",
        "detail": "reference_code.llama-index-core.tests.indices.query.query_transform.mock_utils",
        "documentation": {}
    },
    {
        "label": "MOCK_DECOMPOSE_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.indices.query.query_transform.mock_utils",
        "description": "reference_code.llama-index-core.tests.indices.query.query_transform.mock_utils",
        "peekOfCode": "MOCK_DECOMPOSE_PROMPT = DecomposeQueryTransformPrompt(\n    MOCK_DECOMPOSE_TMPL, prompt_type=PromptType.DECOMPOSE\n)",
        "detail": "reference_code.llama-index-core.tests.indices.query.query_transform.mock_utils",
        "documentation": {}
    },
    {
        "label": "test_decompose_query_transform",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.query.query_transform.test_base",
        "description": "reference_code.llama-index-core.tests.indices.query.query_transform.test_base",
        "peekOfCode": "def test_decompose_query_transform(patch_llm_predictor) -> None:\n    \"\"\"Test decompose query transform.\"\"\"\n    query_transform = DecomposeQueryTransform(\n        decompose_query_prompt=MOCK_DECOMPOSE_PROMPT\n    )\n    query_str = \"What is?\"\n    new_query_bundle = query_transform.run(query_str, {\"index_summary\": \"Foo bar\"})\n    assert new_query_bundle.query_str == \"What is?:Foo bar\"\n    assert new_query_bundle.embedding_strs == [\"What is?:Foo bar\"]",
        "detail": "reference_code.llama-index-core.tests.indices.query.query_transform.test_base",
        "documentation": {}
    },
    {
        "label": "index_kwargs",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.query.conftest",
        "description": "reference_code.llama-index-core.tests.indices.query.conftest",
        "peekOfCode": "def index_kwargs() -> Dict:\n    \"\"\"Index kwargs.\"\"\"\n    return {\n        \"tree\": {\n            \"summary_template\": MOCK_SUMMARY_PROMPT,\n            \"insert_prompt\": MOCK_INSERT_PROMPT,\n            \"num_children\": 2,\n        },\n        \"list\": {},\n        \"table\": {",
        "detail": "reference_code.llama-index-core.tests.indices.query.conftest",
        "documentation": {}
    },
    {
        "label": "retriever_kwargs",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.query.conftest",
        "description": "reference_code.llama-index-core.tests.indices.query.conftest",
        "peekOfCode": "def retriever_kwargs() -> Dict:\n    return {\n        IndexStructType.TREE: {\n            \"query_template\": MOCK_QUERY_PROMPT,\n            \"text_qa_template\": MOCK_TEXT_QA_PROMPT,\n            \"refine_template\": MOCK_REFINE_PROMPT,\n        },\n        IndexStructType.LIST: {},\n        IndexStructType.KEYWORD_TABLE: {\n            \"query_keyword_extract_template\": MOCK_QUERY_KEYWORD_EXTRACT_PROMPT,",
        "detail": "reference_code.llama-index-core.tests.indices.query.conftest",
        "documentation": {}
    },
    {
        "label": "documents",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.query.conftest",
        "description": "reference_code.llama-index-core.tests.indices.query.conftest",
        "peekOfCode": "def documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    return [\n        Document(text=\"This is a test v2.\"),\n        Document(text=\"This is another test.\"),\n        Document(text=\"This is a test.\"),\n        Document(text=\"Hello world.\"),\n        Document(text=\"Hello world.\"),\n        Document(text=\"This is a test.\"),\n        Document(text=\"This is another test.\"),",
        "detail": "reference_code.llama-index-core.tests.indices.query.conftest",
        "documentation": {}
    },
    {
        "label": "test_recursive_query_list_tree",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.query.test_compose",
        "description": "reference_code.llama-index-core.tests.indices.query.test_compose",
        "peekOfCode": "def test_recursive_query_list_tree(\n    documents: List[Document],\n    index_kwargs: Dict,\n    patch_token_text_splitter,\n    patch_llm_predictor,\n) -> None:\n    \"\"\"Test query.\"\"\"\n    list_kwargs = index_kwargs[\"list\"]\n    tree_kwargs = index_kwargs[\"tree\"]\n    # try building a list for every two, then a tree",
        "detail": "reference_code.llama-index-core.tests.indices.query.test_compose",
        "documentation": {}
    },
    {
        "label": "test_recursive_query_tree_list",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.query.test_compose",
        "description": "reference_code.llama-index-core.tests.indices.query.test_compose",
        "peekOfCode": "def test_recursive_query_tree_list(\n    documents: List[Document],\n    patch_llm_predictor,\n    patch_token_text_splitter,\n    index_kwargs: Dict,\n) -> None:\n    \"\"\"Test query.\"\"\"\n    list_kwargs = index_kwargs[\"list\"]\n    tree_kwargs = index_kwargs[\"tree\"]\n    # try building a tree for a group of 4, then a list",
        "detail": "reference_code.llama-index-core.tests.indices.query.test_compose",
        "documentation": {}
    },
    {
        "label": "test_recursive_query_table_list",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.query.test_compose",
        "description": "reference_code.llama-index-core.tests.indices.query.test_compose",
        "peekOfCode": "def test_recursive_query_table_list(\n    documents: List[Document],\n    patch_llm_predictor,\n    patch_token_text_splitter,\n    index_kwargs: Dict,\n) -> None:\n    \"\"\"Test query.\"\"\"\n    list_kwargs = index_kwargs[\"list\"]\n    table_kwargs = index_kwargs[\"table\"]\n    # try building a tree for a group of 4, then a list",
        "detail": "reference_code.llama-index-core.tests.indices.query.test_compose",
        "documentation": {}
    },
    {
        "label": "test_recursive_query_list_table",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.query.test_compose",
        "description": "reference_code.llama-index-core.tests.indices.query.test_compose",
        "peekOfCode": "def test_recursive_query_list_table(\n    documents: List[Document],\n    patch_llm_predictor,\n    patch_token_text_splitter,\n    index_kwargs: Dict,\n) -> None:\n    \"\"\"Test query.\"\"\"\n    list_kwargs = index_kwargs[\"list\"]\n    table_kwargs = index_kwargs[\"table\"]\n    # try building a tree for a group of 4, then a list",
        "detail": "reference_code.llama-index-core.tests.indices.query.test_compose",
        "documentation": {}
    },
    {
        "label": "MockEmbedding",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.indices.query.test_compose_vector",
        "description": "reference_code.llama-index-core.tests.indices.query.test_compose_vector",
        "peekOfCode": "class MockEmbedding(BaseEmbedding):\n    @classmethod\n    def class_name(cls) -> str:\n        return \"MockEmbedding\"\n    async def _aget_query_embedding(self, query: str) -> List[float]:\n        if query == \"Foo?\":\n            return [0, 0, 1, 0, 0]\n        elif query == \"Orange?\":\n            return [0, 1, 0, 0, 0]\n        elif query == \"Cat?\":",
        "detail": "reference_code.llama-index-core.tests.indices.query.test_compose_vector",
        "documentation": {}
    },
    {
        "label": "test_recursive_query_vector_table",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.query.test_compose_vector",
        "description": "reference_code.llama-index-core.tests.indices.query.test_compose_vector",
        "peekOfCode": "def test_recursive_query_vector_table(\n    documents: List[Document],\n    index_kwargs: Dict,\n    patch_token_text_splitter,\n    patch_llm_predictor,\n) -> None:\n    \"\"\"Test query.\"\"\"\n    vector_kwargs = index_kwargs[\"vector\"]\n    table_kwargs = index_kwargs[\"table\"]\n    # try building a tree for a group of 4, then a list",
        "detail": "reference_code.llama-index-core.tests.indices.query.test_compose_vector",
        "documentation": {}
    },
    {
        "label": "test_recursive_query_vector_table_query_configs",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.query.test_compose_vector",
        "description": "reference_code.llama-index-core.tests.indices.query.test_compose_vector",
        "peekOfCode": "def test_recursive_query_vector_table_query_configs(\n    documents: List[Document],\n    index_kwargs: Dict,\n    patch_llm_predictor,\n    patch_token_text_splitter,\n) -> None:\n    \"\"\"\n    Test query.\n    Difference with above test is we specify query config params and\n    assert that they're passed in.",
        "detail": "reference_code.llama-index-core.tests.indices.query.test_compose_vector",
        "documentation": {}
    },
    {
        "label": "test_recursive_query_vector_table_async",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.query.test_compose_vector",
        "description": "reference_code.llama-index-core.tests.indices.query.test_compose_vector",
        "peekOfCode": "def test_recursive_query_vector_table_async(\n    allow_networking: Any,\n    documents: List[Document],\n    patch_llm_predictor,\n    patch_token_text_splitter,\n    index_kwargs: Dict,\n) -> None:\n    \"\"\"Test async query of table index over vector indices.\"\"\"\n    vector_kwargs = index_kwargs[\"vector\"]\n    table_kwargs = index_kwargs[\"table\"]",
        "detail": "reference_code.llama-index-core.tests.indices.query.test_compose_vector",
        "documentation": {}
    },
    {
        "label": "test_recursive_query_vector_vector",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.query.test_compose_vector",
        "description": "reference_code.llama-index-core.tests.indices.query.test_compose_vector",
        "peekOfCode": "def test_recursive_query_vector_vector(\n    documents: List[Document],\n    patch_llm_predictor,\n    patch_token_text_splitter,\n    index_kwargs: Dict,\n) -> None:\n    \"\"\"Test query.\"\"\"\n    vector_kwargs = index_kwargs[\"vector\"]\n    # try building a tree for a group of 4, then a list\n    # use a diff set of documents",
        "detail": "reference_code.llama-index-core.tests.indices.query.test_compose_vector",
        "documentation": {}
    },
    {
        "label": "test_get_top_k_mmr_embeddings",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.query.test_embedding_utils",
        "description": "reference_code.llama-index-core.tests.indices.query.test_embedding_utils",
        "peekOfCode": "def test_get_top_k_mmr_embeddings() -> None:\n    \"\"\"Test Maximum Marginal Relevance.\"\"\"\n    # Results score should follow from the mmr algorithm\n    query_embedding = [5.0, 0.0, 0.0]\n    embeddings = [[4.0, 3.0, 0.0], [3.0, 4.0, 0.0], [-4.0, 3.0, 0.0]]\n    result_similarities, result_ids = get_top_k_mmr_embeddings(\n        query_embedding, embeddings, mmr_threshold=0.8\n    )\n    assert np.isclose(0.8 * 4 / 5, result_similarities[0], atol=0.00001)\n    assert np.isclose(",
        "detail": "reference_code.llama-index-core.tests.indices.query.test_embedding_utils",
        "documentation": {}
    },
    {
        "label": "MockEmbedding",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.indices.query.test_query_bundle",
        "description": "reference_code.llama-index-core.tests.indices.query.test_query_bundle",
        "peekOfCode": "class MockEmbedding(BaseEmbedding):\n    @classmethod\n    def class_name(cls) -> str:\n        return \"MockEmbedding\"\n    async def _aget_query_embedding(self, query: str) -> List[float]:\n        text_embed_map: Dict[str, List[float]] = {\n            \"It is what it is.\": [1.0, 0.0, 0.0, 0.0, 0.0],\n            \"The meaning of life\": [0.0, 1.0, 0.0, 0.0, 0.0],\n        }\n        return text_embed_map[query]",
        "detail": "reference_code.llama-index-core.tests.indices.query.test_query_bundle",
        "documentation": {}
    },
    {
        "label": "documents",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.query.test_query_bundle",
        "description": "reference_code.llama-index-core.tests.indices.query.test_query_bundle",
        "peekOfCode": "def documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    doc_text = (\n        \"Correct.\\n\"\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )",
        "detail": "reference_code.llama-index-core.tests.indices.query.test_query_bundle",
        "documentation": {}
    },
    {
        "label": "test_embedding_query",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.query.test_query_bundle",
        "description": "reference_code.llama-index-core.tests.indices.query.test_query_bundle",
        "peekOfCode": "def test_embedding_query(\n    documents: List[Document], patch_llm_predictor, patch_token_text_splitter\n) -> None:\n    \"\"\"Test embedding query.\"\"\"\n    index = SummaryIndex.from_documents(documents)\n    # test embedding query\n    query_bundle = QueryBundle(\n        query_str=\"What is?\",\n        custom_embedding_strs=[\n            \"It is what it is.\",",
        "detail": "reference_code.llama-index-core.tests.indices.query.test_query_bundle",
        "documentation": {}
    },
    {
        "label": "test_give_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.response.test_response_builder",
        "description": "reference_code.llama-index-core.tests.indices.response.test_response_builder",
        "peekOfCode": "def test_give_response(\n    documents: List[Document], patch_llm_predictor, patch_token_text_splitter\n) -> None:\n    \"\"\"Test give response.\"\"\"\n    prompt_helper = PromptHelper(\n        context_window=DEFAULT_CONTEXT_WINDOW, num_output=DEFAULT_NUM_OUTPUTS\n    )\n    query_str = \"What is?\"\n    # test single line\n    builder = get_response_synthesizer(",
        "detail": "reference_code.llama-index-core.tests.indices.response.test_response_builder",
        "documentation": {}
    },
    {
        "label": "test_compact_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.response.test_response_builder",
        "description": "reference_code.llama-index-core.tests.indices.response.test_response_builder",
        "peekOfCode": "def test_compact_response(patch_llm_predictor, patch_token_text_splitter) -> None:\n    \"\"\"Test give response.\"\"\"\n    # test response with ResponseMode.COMPACT\n    # NOTE: here we want to guarantee that prompts have 0 extra tokens\n    mock_refine_prompt_tmpl = \"{query_str}{existing_answer}{context_msg}\"\n    mock_refine_prompt = PromptTemplate(\n        mock_refine_prompt_tmpl, prompt_type=PromptType.REFINE\n    )\n    mock_qa_prompt_tmpl = \"{context_str}{query_str}\"\n    mock_qa_prompt = PromptTemplate(",
        "detail": "reference_code.llama-index-core.tests.indices.response.test_response_builder",
        "documentation": {}
    },
    {
        "label": "test_accumulate_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.response.test_response_builder",
        "description": "reference_code.llama-index-core.tests.indices.response.test_response_builder",
        "peekOfCode": "def test_accumulate_response(patch_llm_predictor, patch_token_text_splitter) -> None:\n    \"\"\"Test accumulate response.\"\"\"\n    # test response with ResponseMode.ACCUMULATE\n    # NOTE: here we want to guarantee that prompts have 0 extra tokens\n    mock_qa_prompt_tmpl = \"{context_str}{query_str}\"\n    mock_qa_prompt = PromptTemplate(\n        mock_qa_prompt_tmpl, prompt_type=PromptType.QUESTION_ANSWER\n    )\n    # max input size is 11, prompt is two tokens (the query) --> 9 tokens\n    # --> padding is 1 --> 8 tokens",
        "detail": "reference_code.llama-index-core.tests.indices.response.test_response_builder",
        "documentation": {}
    },
    {
        "label": "test_accumulate_response_async",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.response.test_response_builder",
        "description": "reference_code.llama-index-core.tests.indices.response.test_response_builder",
        "peekOfCode": "def test_accumulate_response_async(\n    patch_llm_predictor, patch_token_text_splitter\n) -> None:\n    \"\"\"Test accumulate response.\"\"\"\n    # test response with ResponseMode.ACCUMULATE\n    # NOTE: here we want to guarantee that prompts have 0 extra tokens\n    mock_qa_prompt_tmpl = \"{context_str}{query_str}\"\n    mock_qa_prompt = PromptTemplate(\n        mock_qa_prompt_tmpl, prompt_type=PromptType.QUESTION_ANSWER\n    )",
        "detail": "reference_code.llama-index-core.tests.indices.response.test_response_builder",
        "documentation": {}
    },
    {
        "label": "test_accumulate_response_aget",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.response.test_response_builder",
        "description": "reference_code.llama-index-core.tests.indices.response.test_response_builder",
        "peekOfCode": "def test_accumulate_response_aget(\n    patch_llm_predictor, patch_token_text_splitter\n) -> None:\n    \"\"\"Test accumulate response.\"\"\"\n    # test response with ResponseMode.ACCUMULATE\n    # NOTE: here we want to guarantee that prompts have 0 extra tokens\n    mock_qa_prompt_tmpl = \"{context_str}{query_str}\"\n    mock_qa_prompt = PromptTemplate(\n        mock_qa_prompt_tmpl, prompt_type=PromptType.QUESTION_ANSWER\n    )",
        "detail": "reference_code.llama-index-core.tests.indices.response.test_response_builder",
        "documentation": {}
    },
    {
        "label": "test_accumulate_compact_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.response.test_response_builder",
        "description": "reference_code.llama-index-core.tests.indices.response.test_response_builder",
        "peekOfCode": "def test_accumulate_compact_response(patch_llm_predictor):\n    \"\"\"Test accumulate response.\"\"\"\n    # test response with ResponseMode.ACCUMULATE\n    # NOTE: here we want to guarantee that prompts have 0 extra tokens\n    mock_qa_prompt_tmpl = \"{context_str}{query_str}\"\n    mock_qa_prompt = PromptTemplate(\n        mock_qa_prompt_tmpl, prompt_type=PromptType.QUESTION_ANSWER\n    )\n    # max input size is 11, prompt is two tokens (the query) --> 9 tokens\n    # --> padding is 1 --> 8 tokens",
        "detail": "reference_code.llama-index-core.tests.indices.response.test_response_builder",
        "documentation": {}
    },
    {
        "label": "TestModel",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.indices.response.test_tree_summarize",
        "description": "reference_code.llama-index-core.tests.indices.response.test_tree_summarize",
        "peekOfCode": "class TestModel(BaseModel):\n    hello: str\ndef mock_return_class(*args: Any, **kwargs: Any) -> TestModel:\n    return TestModel(hello=\"Test Chunk 5\")\n@patch.object(MockLLM, \"structured_predict\", mock_return_class)\ndef test_tree_summarize_output_cls(mock_prompt_helper) -> None:\n    mock_summary_prompt_tmpl = \"{context_str}{query_str}\"\n    mock_summary_prompt = PromptTemplate(\n        mock_summary_prompt_tmpl, prompt_type=PromptType.SUMMARY\n    )",
        "detail": "reference_code.llama-index-core.tests.indices.response.test_tree_summarize",
        "documentation": {}
    },
    {
        "label": "mock_prompt_helper",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.response.test_tree_summarize",
        "description": "reference_code.llama-index-core.tests.indices.response.test_tree_summarize",
        "peekOfCode": "def mock_prompt_helper(patch_llm_predictor, patch_token_text_splitter):\n    def mock_repack(\n        prompt_template: PromptTemplate,\n        text_chunks: Sequence[str],\n        llm: Optional[Any] = None,\n        tools: Optional[Any] = None,\n    ) -> List[str]:\n        merged_chunks = []\n        for chunks in zip(*[iter(text_chunks)] * 2):\n            merged_chunks.append(\"\\n\".join(chunks))",
        "detail": "reference_code.llama-index-core.tests.indices.response.test_tree_summarize",
        "documentation": {}
    },
    {
        "label": "test_tree_summarize",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.response.test_tree_summarize",
        "description": "reference_code.llama-index-core.tests.indices.response.test_tree_summarize",
        "peekOfCode": "def test_tree_summarize(mock_prompt_helper) -> None:\n    mock_summary_prompt_tmpl = \"{context_str}{query_str}\"\n    mock_summary_prompt = PromptTemplate(\n        mock_summary_prompt_tmpl, prompt_type=PromptType.SUMMARY\n    )\n    query_str = \"What is?\"\n    texts = [\n        \"Text chunk 1\",\n        \"Text chunk 2\",\n        \"Text chunk 3\",",
        "detail": "reference_code.llama-index-core.tests.indices.response.test_tree_summarize",
        "documentation": {}
    },
    {
        "label": "mock_return_class",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.response.test_tree_summarize",
        "description": "reference_code.llama-index-core.tests.indices.response.test_tree_summarize",
        "peekOfCode": "def mock_return_class(*args: Any, **kwargs: Any) -> TestModel:\n    return TestModel(hello=\"Test Chunk 5\")\n@patch.object(MockLLM, \"structured_predict\", mock_return_class)\ndef test_tree_summarize_output_cls(mock_prompt_helper) -> None:\n    mock_summary_prompt_tmpl = \"{context_str}{query_str}\"\n    mock_summary_prompt = PromptTemplate(\n        mock_summary_prompt_tmpl, prompt_type=PromptType.SUMMARY\n    )\n    query_str = \"What is?\"\n    texts = [",
        "detail": "reference_code.llama-index-core.tests.indices.response.test_tree_summarize",
        "documentation": {}
    },
    {
        "label": "test_tree_summarize_output_cls",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.response.test_tree_summarize",
        "description": "reference_code.llama-index-core.tests.indices.response.test_tree_summarize",
        "peekOfCode": "def test_tree_summarize_output_cls(mock_prompt_helper) -> None:\n    mock_summary_prompt_tmpl = \"{context_str}{query_str}\"\n    mock_summary_prompt = PromptTemplate(\n        mock_summary_prompt_tmpl, prompt_type=PromptType.SUMMARY\n    )\n    query_str = \"What is?\"\n    texts = [\n        '{\"hello\":\"Test Chunk 1\"}',\n        '{\"hello\":\"Test Chunk 2\"}',\n        '{\"hello\":\"Test Chunk 3\"}',",
        "detail": "reference_code.llama-index-core.tests.indices.response.test_tree_summarize",
        "documentation": {}
    },
    {
        "label": "test_tree_summarize_use_async",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.response.test_tree_summarize",
        "description": "reference_code.llama-index-core.tests.indices.response.test_tree_summarize",
        "peekOfCode": "def test_tree_summarize_use_async(mock_prompt_helper) -> None:\n    mock_summary_prompt_tmpl = \"{context_str}{query_str}\"\n    mock_summary_prompt = PromptTemplate(\n        mock_summary_prompt_tmpl, prompt_type=PromptType.SUMMARY\n    )\n    query_str = \"What is?\"\n    texts = [\n        \"Text chunk 1\",\n        \"Text chunk 2\",\n        \"Text chunk 3\",",
        "detail": "reference_code.llama-index-core.tests.indices.response.test_tree_summarize",
        "documentation": {}
    },
    {
        "label": "struct_kwargs",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.struct_store.conftest",
        "description": "reference_code.llama-index-core.tests.indices.struct_store.conftest",
        "peekOfCode": "def struct_kwargs() -> Tuple[Dict, Dict]:\n    \"\"\"Index kwargs.\"\"\"\n    # NOTE: QuestionAnswer and Refine templates aren't technically used\n    index_kwargs = {\n        \"schema_extract_prompt\": MOCK_SCHEMA_EXTRACT_PROMPT,\n        \"output_parser\": _mock_output_parser,\n    }\n    query_kwargs = {\n        \"text_qa_template\": MOCK_TEXT_QA_PROMPT,\n        \"refine_template\": MOCK_REFINE_PROMPT,",
        "detail": "reference_code.llama-index-core.tests.indices.struct_store.conftest",
        "documentation": {}
    },
    {
        "label": "test_sql_index",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.struct_store.test_base",
        "description": "reference_code.llama-index-core.tests.indices.struct_store.test_base",
        "peekOfCode": "def test_sql_index(\n    struct_kwargs: Tuple[Dict, Dict], patch_llm_predictor, patch_token_text_splitter\n) -> None:\n    \"\"\"Test SQLStructStoreIndex.\"\"\"\n    engine = create_engine(\"sqlite:///:memory:\")\n    metadata_obj = MetaData()\n    table_name = \"test_table\"\n    test_table = Table(\n        table_name,\n        metadata_obj,",
        "detail": "reference_code.llama-index-core.tests.indices.struct_store.test_base",
        "documentation": {}
    },
    {
        "label": "test_sql_index_nodes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.struct_store.test_base",
        "description": "reference_code.llama-index-core.tests.indices.struct_store.test_base",
        "peekOfCode": "def test_sql_index_nodes(\n    patch_llm_predictor,\n    patch_token_text_splitter,\n    struct_kwargs: Tuple[Dict, Dict],\n) -> None:\n    \"\"\"Test SQLStructStoreIndex with nodes.\"\"\"\n    engine = create_engine(\"sqlite:///:memory:\")\n    metadata_obj = MetaData()\n    table_name = \"test_table\"\n    test_table = Table(",
        "detail": "reference_code.llama-index-core.tests.indices.struct_store.test_base",
        "documentation": {}
    },
    {
        "label": "test_sql_index_with_context",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.struct_store.test_base",
        "description": "reference_code.llama-index-core.tests.indices.struct_store.test_base",
        "peekOfCode": "def test_sql_index_with_context(\n    patch_llm_predictor,\n    patch_token_text_splitter,\n    struct_kwargs: Tuple[Dict, Dict],\n) -> None:\n    \"\"\"Test SQLStructStoreIndex.\"\"\"\n    # test setting table_context_dict\n    engine = create_engine(\"sqlite:///:memory:\")\n    metadata_obj = MetaData()\n    table_name = \"test_table\"",
        "detail": "reference_code.llama-index-core.tests.indices.struct_store.test_base",
        "documentation": {}
    },
    {
        "label": "test_sql_index_with_derive_index",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.struct_store.test_base",
        "description": "reference_code.llama-index-core.tests.indices.struct_store.test_base",
        "peekOfCode": "def test_sql_index_with_derive_index(\n    patch_llm_predictor, patch_token_text_splitter\n) -> None:\n    \"\"\"Test derive index.\"\"\"\n    # test setting table_context_dict\n    engine = create_engine(\"sqlite:///:memory:\")\n    metadata_obj = MetaData()\n    table_name = \"test_table\"\n    Table(\n        table_name,",
        "detail": "reference_code.llama-index-core.tests.indices.struct_store.test_base",
        "documentation": {}
    },
    {
        "label": "test_sql_index_with_index_context",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.struct_store.test_base",
        "description": "reference_code.llama-index-core.tests.indices.struct_store.test_base",
        "peekOfCode": "def test_sql_index_with_index_context(\n    patch_llm_predictor,\n    patch_token_text_splitter,\n    struct_kwargs: Tuple[Dict, Dict],\n) -> None:\n    \"\"\"Test SQLStructStoreIndex.\"\"\"\n    # test setting table_context_dict\n    engine = create_engine(\"sqlite:///:memory:\")\n    metadata_obj = MetaData()\n    table_name = \"test_table\"",
        "detail": "reference_code.llama-index-core.tests.indices.struct_store.test_base",
        "documentation": {}
    },
    {
        "label": "mock_predict",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.struct_store.test_json_query",
        "description": "reference_code.llama-index-core.tests.indices.struct_store.test_json_query",
        "peekOfCode": "def mock_predict(self: Any, prompt: BasePromptTemplate, **prompt_args: Any) -> str:\n    return TEST_LLM_OUTPUT\nasync def amock_predict(\n    self: Any, prompt: BasePromptTemplate, **prompt_args: Any\n) -> str:\n    return TEST_LLM_OUTPUT\n@pytest.mark.parametrize((\"synthesize_response\", \"call_apredict\"), TEST_PARAMS)\n@patch.object(\n    MockLLM,\n    \"predict\",",
        "detail": "reference_code.llama-index-core.tests.indices.struct_store.test_json_query",
        "documentation": {}
    },
    {
        "label": "test_json_query_engine",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.struct_store.test_json_query",
        "description": "reference_code.llama-index-core.tests.indices.struct_store.test_json_query",
        "peekOfCode": "def test_json_query_engine(\n    synthesize_response: bool,\n    call_apredict: bool,\n    patch_llm_predictor,\n    patch_token_text_splitter,\n) -> None:\n    \"\"\"Test GPTNLJSONQueryEngine.\"\"\"\n    # Test on some sample data\n    json_val = cast(JSONType, {})\n    json_schema = cast(JSONType, {})",
        "detail": "reference_code.llama-index-core.tests.indices.struct_store.test_json_query",
        "documentation": {}
    },
    {
        "label": "TEST_PARAMS",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.indices.struct_store.test_json_query",
        "description": "reference_code.llama-index-core.tests.indices.struct_store.test_json_query",
        "peekOfCode": "TEST_PARAMS = [\n    # synthesize_response, call_apredict\n    (True, True),\n    (True, False),\n    (False, True),\n    (False, False),\n]\nTEST_LLM_OUTPUT = \"test_llm_output\"\ndef mock_predict(self: Any, prompt: BasePromptTemplate, **prompt_args: Any) -> str:\n    return TEST_LLM_OUTPUT",
        "detail": "reference_code.llama-index-core.tests.indices.struct_store.test_json_query",
        "documentation": {}
    },
    {
        "label": "TEST_LLM_OUTPUT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.indices.struct_store.test_json_query",
        "description": "reference_code.llama-index-core.tests.indices.struct_store.test_json_query",
        "peekOfCode": "TEST_LLM_OUTPUT = \"test_llm_output\"\ndef mock_predict(self: Any, prompt: BasePromptTemplate, **prompt_args: Any) -> str:\n    return TEST_LLM_OUTPUT\nasync def amock_predict(\n    self: Any, prompt: BasePromptTemplate, **prompt_args: Any\n) -> str:\n    return TEST_LLM_OUTPUT\n@pytest.mark.parametrize((\"synthesize_response\", \"call_apredict\"), TEST_PARAMS)\n@patch.object(\n    MockLLM,",
        "detail": "reference_code.llama-index-core.tests.indices.struct_store.test_json_query",
        "documentation": {}
    },
    {
        "label": "test_sql_index_query",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.struct_store.test_sql_query",
        "description": "reference_code.llama-index-core.tests.indices.struct_store.test_sql_query",
        "peekOfCode": "def test_sql_index_query(\n    patch_llm_predictor,\n    patch_token_text_splitter,\n    struct_kwargs: Tuple[Dict, Dict],\n) -> None:\n    \"\"\"Test SQLStructStoreIndex.\"\"\"\n    index_kwargs, query_kwargs = struct_kwargs\n    docs = [Document(text=\"user_id:2,foo:bar\"), Document(text=\"user_id:8,foo:hello\")]\n    engine = create_engine(\"sqlite:///:memory:\")\n    metadata_obj = MetaData()",
        "detail": "reference_code.llama-index-core.tests.indices.struct_store.test_sql_query",
        "documentation": {}
    },
    {
        "label": "test_sql_index_async_query",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.struct_store.test_sql_query",
        "description": "reference_code.llama-index-core.tests.indices.struct_store.test_sql_query",
        "peekOfCode": "def test_sql_index_async_query(\n    allow_networking: Any,\n    patch_llm_predictor,\n    patch_token_text_splitter,\n    struct_kwargs: Tuple[Dict, Dict],\n) -> None:\n    \"\"\"Test SQLStructStoreIndex.\"\"\"\n    index_kwargs, query_kwargs = struct_kwargs\n    docs = [Document(text=\"user_id:2,foo:bar\"), Document(text=\"user_id:8,foo:hello\")]\n    engine = create_engine(\"sqlite:///:memory:\")",
        "detail": "reference_code.llama-index-core.tests.indices.struct_store.test_sql_query",
        "documentation": {}
    },
    {
        "label": "test_default_output_parser",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.struct_store.test_sql_query",
        "description": "reference_code.llama-index-core.tests.indices.struct_store.test_sql_query",
        "peekOfCode": "def test_default_output_parser() -> None:\n    \"\"\"Test default output parser.\"\"\"\n    test_str = \"user_id:2\\nfoo:bar\\n,,testing:testing2..\\nnumber:123,456,789\\n\"\n    fields = default_output_parser(test_str)\n    assert fields == {\n        \"user_id\": \"2\",\n        \"foo\": \"bar\",\n        \"testing\": \"testing2\",\n        \"number\": \"123456789\",\n    }",
        "detail": "reference_code.llama-index-core.tests.indices.struct_store.test_sql_query",
        "documentation": {}
    },
    {
        "label": "test_nl_query_engine_parser",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.struct_store.test_sql_query",
        "description": "reference_code.llama-index-core.tests.indices.struct_store.test_sql_query",
        "peekOfCode": "def test_nl_query_engine_parser(\n    patch_llm_predictor,\n    patch_token_text_splitter,\n    struct_kwargs: Tuple[Dict, Dict],\n) -> None:\n    \"\"\"Test the sql response parser.\"\"\"\n    index_kwargs, _ = struct_kwargs\n    docs = [Document(text=\"user_id:2,foo:bar\"), Document(text=\"user_id:8,foo:hello\")]\n    engine = create_engine(\"sqlite:///:memory:\")\n    metadata_obj = MetaData()",
        "detail": "reference_code.llama-index-core.tests.indices.struct_store.test_sql_query",
        "documentation": {}
    },
    {
        "label": "test_sql_table_retriever_query_engine_with_rows_retriever",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.struct_store.test_sql_query",
        "description": "reference_code.llama-index-core.tests.indices.struct_store.test_sql_query",
        "peekOfCode": "def test_sql_table_retriever_query_engine_with_rows_retriever(\n    patch_llm_predictor,\n    patch_token_text_splitter,\n    struct_kwargs: Tuple[Dict, Dict],\n) -> None:\n    \"\"\"Test SQLTableRetrieverQueryEngine.\"\"\"\n    index_kwargs, query_kwargs = struct_kwargs\n    sql_to_test = \"SELECT user_id, foo FROM test_table\"\n    engine = create_engine(\"sqlite:///:memory:\")\n    metadata_obj = MetaData()",
        "detail": "reference_code.llama-index-core.tests.indices.struct_store.test_sql_query",
        "documentation": {}
    },
    {
        "label": "test_sql_table_retriever_query_engine_with_cols_retriever",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.struct_store.test_sql_query",
        "description": "reference_code.llama-index-core.tests.indices.struct_store.test_sql_query",
        "peekOfCode": "def test_sql_table_retriever_query_engine_with_cols_retriever(\n    patch_llm_predictor,\n    patch_token_text_splitter,\n    struct_kwargs: Tuple[Dict, Dict],\n) -> None:\n    \"\"\"Test SQLTableRetrieverQueryEngine.\"\"\"\n    index_kwargs, query_kwargs = struct_kwargs\n    sql_to_test = \"SELECT user_id, foo FROM test_table\"\n    engine = create_engine(\"sqlite:///:memory:\")\n    metadata_obj = MetaData()",
        "detail": "reference_code.llama-index-core.tests.indices.struct_store.test_sql_query",
        "documentation": {}
    },
    {
        "label": "documents",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.tree.conftest",
        "description": "reference_code.llama-index-core.tests.indices.tree.conftest",
        "peekOfCode": "def documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    doc_text = (\n        \"Hello world.\\nThis is a test.\\nThis is another test.\\nThis is a test v2.\"\n    )\n    return [Document(text=doc_text)]\n@pytest.fixture()\ndef struct_kwargs() -> Tuple[Dict, Dict]:\n    \"\"\"Index kwargs.\"\"\"",
        "detail": "reference_code.llama-index-core.tests.indices.tree.conftest",
        "documentation": {}
    },
    {
        "label": "struct_kwargs",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.tree.conftest",
        "description": "reference_code.llama-index-core.tests.indices.tree.conftest",
        "peekOfCode": "def struct_kwargs() -> Tuple[Dict, Dict]:\n    \"\"\"Index kwargs.\"\"\"\n    index_kwargs = {\n        \"summary_template\": MOCK_SUMMARY_PROMPT,\n        \"insert_prompt\": MOCK_INSERT_PROMPT,\n        \"num_children\": 2,\n    }\n    query_kwargs = {\n        \"query_template\": MOCK_QUERY_PROMPT,\n        \"text_qa_template\": MOCK_TEXT_QA_PROMPT,",
        "detail": "reference_code.llama-index-core.tests.indices.tree.conftest",
        "documentation": {}
    },
    {
        "label": "index_kwargs",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.tree.test_embedding_retriever",
        "description": "reference_code.llama-index-core.tests.indices.tree.test_embedding_retriever",
        "peekOfCode": "def index_kwargs() -> dict:\n    \"\"\"Index kwargs.\"\"\"\n    return {\n        \"summary_template\": MOCK_SUMMARY_PROMPT,\n        \"insert_prompt\": MOCK_INSERT_PROMPT,\n        \"num_children\": 2,\n    }\n@pytest.fixture()\ndef documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"",
        "detail": "reference_code.llama-index-core.tests.indices.tree.test_embedding_retriever",
        "documentation": {}
    },
    {
        "label": "documents",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.tree.test_embedding_retriever",
        "description": "reference_code.llama-index-core.tests.indices.tree.test_embedding_retriever",
        "peekOfCode": "def documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    doc_text = (\n        \"Hello world.\\nThis is a test.\\nThis is another test.\\nThis is a test v2.\"\n    )\n    return [Document(text=doc_text)]\ndef _get_node_text_embedding_similarities(\n    query_embedding: List[float], nodes: List[BaseNode]\n) -> List[float]:",
        "detail": "reference_code.llama-index-core.tests.indices.tree.test_embedding_retriever",
        "documentation": {}
    },
    {
        "label": "test_embedding_query",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.tree.test_embedding_retriever",
        "description": "reference_code.llama-index-core.tests.indices.tree.test_embedding_retriever",
        "peekOfCode": "def test_embedding_query(\n    _patch_similarity: Any,\n    index_kwargs: Dict,\n    documents: List[Document],\n    patch_llm_predictor,\n    patch_token_text_splitter,\n) -> None:\n    \"\"\"Test embedding query.\"\"\"\n    tree = TreeIndex.from_documents(documents, **index_kwargs)\n    # test embedding query",
        "detail": "reference_code.llama-index-core.tests.indices.tree.test_embedding_retriever",
        "documentation": {}
    },
    {
        "label": "test_build_tree",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.tree.test_index",
        "description": "reference_code.llama-index-core.tests.indices.tree.test_index",
        "peekOfCode": "def test_build_tree(\n    documents: List[Document],\n    patch_llm_predictor,\n    patch_token_text_splitter,\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test build tree.\"\"\"\n    index_kwargs, _ = struct_kwargs\n    tree = TreeIndex.from_documents(documents, **index_kwargs)\n    assert len(tree.index_struct.all_nodes) == 6",
        "detail": "reference_code.llama-index-core.tests.indices.tree.test_index",
        "documentation": {}
    },
    {
        "label": "test_build_tree_with_embed",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.tree.test_index",
        "description": "reference_code.llama-index-core.tests.indices.tree.test_index",
        "peekOfCode": "def test_build_tree_with_embed(\n    documents: List[Document],\n    struct_kwargs: Dict,\n    patch_llm_predictor,\n    patch_token_text_splitter,\n) -> None:\n    \"\"\"Test build tree.\"\"\"\n    index_kwargs, _ = struct_kwargs\n    doc_text = (\n        \"Hello world.\\nThis is a test.\\nThis is another test.\\nThis is a test v2.\"",
        "detail": "reference_code.llama-index-core.tests.indices.tree.test_index",
        "documentation": {}
    },
    {
        "label": "test_build_tree_async",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.tree.test_index",
        "description": "reference_code.llama-index-core.tests.indices.tree.test_index",
        "peekOfCode": "def test_build_tree_async(\n    _mock_run_async_tasks: Any,\n    documents: List[Document],\n    patch_llm_predictor,\n    patch_token_text_splitter,\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test build tree with use_async.\"\"\"\n    index_kwargs, _ = struct_kwargs\n    tree = TreeIndex.from_documents(documents, use_async=True, **index_kwargs)",
        "detail": "reference_code.llama-index-core.tests.indices.tree.test_index",
        "documentation": {}
    },
    {
        "label": "test_build_tree_multiple",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.tree.test_index",
        "description": "reference_code.llama-index-core.tests.indices.tree.test_index",
        "peekOfCode": "def test_build_tree_multiple(\n    patch_llm_predictor,\n    patch_token_text_splitter,\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test build tree.\"\"\"\n    new_docs = [\n        Document(text=\"Hello world.\\nThis is a test.\"),\n        Document(text=\"This is another test.\\nThis is a test v2.\"),\n    ]",
        "detail": "reference_code.llama-index-core.tests.indices.tree.test_index",
        "documentation": {}
    },
    {
        "label": "test_insert",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.tree.test_index",
        "description": "reference_code.llama-index-core.tests.indices.tree.test_index",
        "peekOfCode": "def test_insert(\n    documents: List[Document],\n    patch_llm_predictor,\n    patch_token_text_splitter,\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test insert.\"\"\"\n    index_kwargs, _ = struct_kwargs\n    tree = TreeIndex.from_documents(documents, **index_kwargs)\n    # test insert",
        "detail": "reference_code.llama-index-core.tests.indices.tree.test_index",
        "documentation": {}
    },
    {
        "label": "test_twice_insert_empty",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.tree.test_index",
        "description": "reference_code.llama-index-core.tests.indices.tree.test_index",
        "peekOfCode": "def test_twice_insert_empty(patch_llm_predictor, patch_token_text_splitter) -> None:\n    \"\"\"# test twice insert from empty (with_id).\"\"\"\n    tree = TreeIndex.from_documents([])\n    # test first insert\n    new_doc = Document(text=\"This is a new doc.\", id_=\"new_doc\")\n    tree.insert(new_doc)\n    # test second insert\n    new_doc_second = Document(text=\"This is a new doc2.\", id_=\"new_doc_2\")\n    tree.insert(new_doc_second)\n    assert len(tree.index_struct.all_nodes) == 2",
        "detail": "reference_code.llama-index-core.tests.indices.tree.test_index",
        "documentation": {}
    },
    {
        "label": "OUTPUTS",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.indices.tree.test_index",
        "description": "reference_code.llama-index-core.tests.indices.tree.test_index",
        "peekOfCode": "OUTPUTS = [\n    (\"Hello world.\\nThis is a test.\", \"\"),\n    (\"This is another test.\\nThis is a test v2.\", \"\"),\n]\n@patch(\n    \"llama_index.core.indices.common_tree.base.run_async_tasks\",\n    side_effect=[OUTPUTS],\n)\ndef test_build_tree_async(\n    _mock_run_async_tasks: Any,",
        "detail": "reference_code.llama-index-core.tests.indices.tree.test_index",
        "documentation": {}
    },
    {
        "label": "test_query",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.tree.test_retrievers",
        "description": "reference_code.llama-index-core.tests.indices.tree.test_retrievers",
        "peekOfCode": "def test_query(\n    documents: List[Document],\n    patch_llm_predictor,\n    patch_token_text_splitter,\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test query.\"\"\"\n    index_kwargs, query_kwargs = struct_kwargs\n    tree = TreeIndex.from_documents(documents, **index_kwargs)\n    # test default query",
        "detail": "reference_code.llama-index-core.tests.indices.tree.test_retrievers",
        "documentation": {}
    },
    {
        "label": "test_summarize_query",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.tree.test_retrievers",
        "description": "reference_code.llama-index-core.tests.indices.tree.test_retrievers",
        "peekOfCode": "def test_summarize_query(\n    documents: List[Document],\n    patch_llm_predictor,\n    patch_token_text_splitter,\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test summarize query.\"\"\"\n    # create tree index without building tree\n    index_kwargs, orig_query_kwargs = struct_kwargs\n    index_kwargs = index_kwargs.copy()",
        "detail": "reference_code.llama-index-core.tests.indices.tree.test_retrievers",
        "documentation": {}
    },
    {
        "label": "test_output_parser",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.vector_store.auto_retriever.test_output_parser",
        "description": "reference_code.llama-index-core.tests.indices.vector_store.auto_retriever.test_output_parser",
        "peekOfCode": "def test_output_parser() -> None:\n    output_str = \"\"\"\\\n    ```json\n    {\n        \"query\": \"test query str\",\n        \"filters\": [\n            {\n                \"key\": \"director\",\n                \"value\": \"Nolan\"\n            },",
        "detail": "reference_code.llama-index-core.tests.indices.vector_store.auto_retriever.test_output_parser",
        "documentation": {}
    },
    {
        "label": "MockEmbedding",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.indices.vector_store.mock_services",
        "description": "reference_code.llama-index-core.tests.indices.vector_store.mock_services",
        "peekOfCode": "class MockEmbedding(BaseEmbedding):\n    @classmethod\n    def class_name(cls) -> str:\n        return \"MockEmbedding\"\n    async def _aget_query_embedding(self, query: str) -> List[float]:\n        del query\n        return [0, 0, 1, 0, 0]\n    async def _aget_text_embedding(self, text: str) -> List[float]:\n        # assume dimensions are 5\n        if text == \"Hello world.\":",
        "detail": "reference_code.llama-index-core.tests.indices.vector_store.mock_services",
        "documentation": {}
    },
    {
        "label": "test_simple_query",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.vector_store.test_retrievers",
        "description": "reference_code.llama-index-core.tests.indices.vector_store.test_retrievers",
        "peekOfCode": "def test_simple_query(\n    documents: List[Document],\n    patch_llm_predictor,\n    patch_token_text_splitter,\n    mock_embed_model,\n) -> None:\n    \"\"\"Test embedding query.\"\"\"\n    index = VectorStoreIndex.from_documents(documents, embed_model=mock_embed_model)\n    # test embedding query\n    query_str = \"What is?\"",
        "detail": "reference_code.llama-index-core.tests.indices.vector_store.test_retrievers",
        "documentation": {}
    },
    {
        "label": "test_query_and_similarity_scores",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.vector_store.test_retrievers",
        "description": "reference_code.llama-index-core.tests.indices.vector_store.test_retrievers",
        "peekOfCode": "def test_query_and_similarity_scores(\n    patch_llm_predictor,\n    patch_token_text_splitter,\n) -> None:\n    \"\"\"Test that sources nodes have similarity scores.\"\"\"\n    doc_text = (\n        \"Hello world.\\nThis is a test.\\nThis is another test.\\nThis is a test v2.\"\n    )\n    document = Document(text=doc_text)\n    index = VectorStoreIndex.from_documents([document])",
        "detail": "reference_code.llama-index-core.tests.indices.vector_store.test_retrievers",
        "documentation": {}
    },
    {
        "label": "test_simple_check_ids",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.vector_store.test_retrievers",
        "description": "reference_code.llama-index-core.tests.indices.vector_store.test_retrievers",
        "peekOfCode": "def test_simple_check_ids(\n    patch_llm_predictor,\n    patch_token_text_splitter,\n) -> None:\n    \"\"\"Test build VectorStoreIndex.\"\"\"\n    ref_doc_id = \"ref_doc_id_test\"\n    source_rel = {NodeRelationship.SOURCE: RelatedNodeInfo(node_id=ref_doc_id)}\n    all_nodes = [\n        TextNode(text=\"Hello world.\", id_=\"node1\", relationships=source_rel),\n        TextNode(text=\"This is a test.\", id_=\"node2\", relationships=source_rel),",
        "detail": "reference_code.llama-index-core.tests.indices.vector_store.test_retrievers",
        "documentation": {}
    },
    {
        "label": "test_query",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.vector_store.test_retrievers",
        "description": "reference_code.llama-index-core.tests.indices.vector_store.test_retrievers",
        "peekOfCode": "def test_query(\n    patch_llm_predictor,\n    patch_token_text_splitter,\n) -> None:\n    \"\"\"Test embedding query.\"\"\"\n    doc_text = (\n        \"Hello world.\\nThis is a test.\\nThis is another test.\\nThis is a test v2.\"\n    )\n    document = Document(text=doc_text)\n    index = VectorStoreIndex.from_documents([document])",
        "detail": "reference_code.llama-index-core.tests.indices.vector_store.test_retrievers",
        "documentation": {}
    },
    {
        "label": "test_query_image_node",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.vector_store.test_retrievers",
        "description": "reference_code.llama-index-core.tests.indices.vector_store.test_retrievers",
        "peekOfCode": "def test_query_image_node() -> None:\n    \"\"\"Test embedding query.\"\"\"\n    image_node = ImageNode(\n        image=\"potato\", embeddings=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n    )\n    text_node = TextNode(\n        text=\"potato\", embeddings=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n    )\n    index = VectorStoreIndex.from_documents([])\n    index.insert_nodes([image_node, text_node])",
        "detail": "reference_code.llama-index-core.tests.indices.vector_store.test_retrievers",
        "documentation": {}
    },
    {
        "label": "test_insert_fetched_nodes_handles_all_branches",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.vector_store.test_retrievers",
        "description": "reference_code.llama-index-core.tests.indices.vector_store.test_retrievers",
        "peekOfCode": "def test_insert_fetched_nodes_handles_all_branches():\n    \"\"\"Test _insert_fetched_nodes_into_query_result for full branch coverage.\"\"\"\n    fetched_nodes = [\n        TextNode(id_=\"0\", text=\"doc 0\"),\n        TextNode(id_=\"1\", text=\"doc 1\"),\n        TextNode(id_=\"two\", text=\"doc two\"),\n    ]\n    query_result = VectorStoreQueryResult(\n        ids=[0, \"1\", \"unknown\"], similarities=[0.9, 0.8, 0.7], nodes=None\n    )",
        "detail": "reference_code.llama-index-core.tests.indices.vector_store.test_retrievers",
        "documentation": {}
    },
    {
        "label": "test_insert_fetched_nodes_with_nodes_present",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.vector_store.test_retrievers",
        "description": "reference_code.llama-index-core.tests.indices.vector_store.test_retrievers",
        "peekOfCode": "def test_insert_fetched_nodes_with_nodes_present():\n    \"\"\"Test _insert_fetched_nodes_into_query_result with `nodes` present instead of `ids`.\"\"\"\n    fetched_nodes = [TextNode(id_=\"abc\", text=\"Updated text\")]\n    # This simulates query_result.nodes populated with old version of the same node\n    old_node = TextNode(id_=\"abc\", text=\"Old text\")\n    query_result = VectorStoreQueryResult(nodes=[old_node], similarities=[0.9])\n    dummy_index = VectorStoreIndex([])\n    retriever = VectorIndexRetriever(\n        index=dummy_index, vector_store=None, docstore=None, embed_model=None\n    )",
        "detail": "reference_code.llama-index-core.tests.indices.vector_store.test_retrievers",
        "documentation": {}
    },
    {
        "label": "test_build_simple",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.vector_store.test_simple",
        "description": "reference_code.llama-index-core.tests.indices.vector_store.test_simple",
        "peekOfCode": "def test_build_simple(\n    patch_llm_predictor,\n    patch_token_text_splitter,\n    mock_embed_model,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test build VectorStoreIndex.\"\"\"\n    index = VectorStoreIndex.from_documents(\n        documents=documents, embed_model=mock_embed_model\n    )",
        "detail": "reference_code.llama-index-core.tests.indices.vector_store.test_simple",
        "documentation": {}
    },
    {
        "label": "test_simple_insert",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.vector_store.test_simple",
        "description": "reference_code.llama-index-core.tests.indices.vector_store.test_simple",
        "peekOfCode": "def test_simple_insert(\n    documents: List[Document],\n    patch_llm_predictor,\n    patch_token_text_splitter,\n    mock_embed_model,\n) -> None:\n    \"\"\"Test insert VectorStoreIndex.\"\"\"\n    index = VectorStoreIndex.from_documents(\n        documents=documents, embed_model=mock_embed_model\n    )",
        "detail": "reference_code.llama-index-core.tests.indices.vector_store.test_simple",
        "documentation": {}
    },
    {
        "label": "test_simple_delete",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.vector_store.test_simple",
        "description": "reference_code.llama-index-core.tests.indices.vector_store.test_simple",
        "peekOfCode": "def test_simple_delete(\n    patch_llm_predictor, patch_token_text_splitter, mock_embed_model\n) -> None:\n    \"\"\"Test delete VectorStoreIndex.\"\"\"\n    new_documents = [\n        Document(text=\"Hello world.\", id_=\"test_id_0\"),\n        Document(text=\"This is a test.\", id_=\"test_id_1\"),\n        Document(text=\"This is another test.\", id_=\"test_id_2\"),\n        Document(text=\"This is a test v2.\", id_=\"test_id_3\"),\n    ]",
        "detail": "reference_code.llama-index-core.tests.indices.vector_store.test_simple",
        "documentation": {}
    },
    {
        "label": "test_simple_delete_ref_node_from_docstore",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.vector_store.test_simple",
        "description": "reference_code.llama-index-core.tests.indices.vector_store.test_simple",
        "peekOfCode": "def test_simple_delete_ref_node_from_docstore(\n    patch_llm_predictor, patch_token_text_splitter, mock_embed_model\n) -> None:\n    \"\"\"Test delete VectorStoreIndex.\"\"\"\n    new_documents = [\n        Document(text=\"This is a test.\", id_=\"test_id_1\"),\n        Document(text=\"This is another test.\", id_=\"test_id_2\"),\n    ]\n    index = VectorStoreIndex.from_documents(\n        documents=new_documents, embed_model=mock_embed_model",
        "detail": "reference_code.llama-index-core.tests.indices.vector_store.test_simple",
        "documentation": {}
    },
    {
        "label": "test_simple_async",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.vector_store.test_simple",
        "description": "reference_code.llama-index-core.tests.indices.vector_store.test_simple",
        "peekOfCode": "def test_simple_async(\n    allow_networking: Any,\n    documents: List[Document],\n    patch_llm_predictor,\n    patch_token_text_splitter,\n    mock_embed_model,\n) -> None:\n    \"\"\"Test simple vector index with use_async.\"\"\"\n    index = VectorStoreIndex.from_documents(\n        documents=documents, use_async=True, embed_model=mock_embed_model",
        "detail": "reference_code.llama-index-core.tests.indices.vector_store.test_simple",
        "documentation": {}
    },
    {
        "label": "test_simple_insert_save",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.vector_store.test_simple",
        "description": "reference_code.llama-index-core.tests.indices.vector_store.test_simple",
        "peekOfCode": "def test_simple_insert_save(\n    documents: List[Document],\n    patch_llm_predictor,\n    patch_token_text_splitter,\n    mock_embed_model,\n) -> None:\n    storage_context = StorageContext.from_defaults()\n    index = VectorStoreIndex.from_documents(\n        documents=documents,\n        embed_model=mock_embed_model,",
        "detail": "reference_code.llama-index-core.tests.indices.vector_store.test_simple",
        "documentation": {}
    },
    {
        "label": "test_simple_pickle",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.vector_store.test_simple",
        "description": "reference_code.llama-index-core.tests.indices.vector_store.test_simple",
        "peekOfCode": "def test_simple_pickle(\n    patch_llm_predictor,\n    patch_token_text_splitter,\n    mock_embed_model,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test build VectorStoreIndex.\"\"\"\n    index = VectorStoreIndex.from_documents(\n        documents=documents, embed_model=mock_embed_model\n    )",
        "detail": "reference_code.llama-index-core.tests.indices.vector_store.test_simple",
        "documentation": {}
    },
    {
        "label": "documents",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.conftest",
        "description": "reference_code.llama-index-core.tests.indices.conftest",
        "peekOfCode": "def documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    doc_text = (\n        \"Hello world.\\nThis is a test.\\nThis is another test.\\nThis is a test v2.\"\n    )\n    return [Document(text=doc_text)]\n@pytest.fixture()\ndef nodes() -> List[TextNode]:\n    \"\"\"Get documents.\"\"\"",
        "detail": "reference_code.llama-index-core.tests.indices.conftest",
        "documentation": {}
    },
    {
        "label": "nodes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.conftest",
        "description": "reference_code.llama-index-core.tests.indices.conftest",
        "peekOfCode": "def nodes() -> List[TextNode]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    return [\n        TextNode(\n            text=\"Hello world.\",\n            relationships={\n                NodeRelationship.SOURCE: RelatedNodeInfo(node_id=\"test doc\")\n            },\n        ),",
        "detail": "reference_code.llama-index-core.tests.indices.conftest",
        "documentation": {}
    },
    {
        "label": "test_load_index_from_storage_simple",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.test_loading",
        "description": "reference_code.llama-index-core.tests.indices.test_loading",
        "peekOfCode": "def test_load_index_from_storage_simple(\n    documents: List[Document], tmp_path: Path\n) -> None:\n    # construct simple (i.e. in memory) storage context\n    storage_context = StorageContext.from_defaults()\n    # construct index\n    index = VectorStoreIndex.from_documents(\n        documents=documents,\n        storage_context=storage_context,\n    )",
        "detail": "reference_code.llama-index-core.tests.indices.test_loading",
        "documentation": {}
    },
    {
        "label": "test_load_index_from_storage_multiple",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.test_loading",
        "description": "reference_code.llama-index-core.tests.indices.test_loading",
        "peekOfCode": "def test_load_index_from_storage_multiple(\n    nodes: List[BaseNode], tmp_path: Path\n) -> None:\n    # construct simple (i.e. in memory) storage context\n    storage_context = StorageContext.from_defaults()\n    # add nodes to docstore\n    storage_context.docstore.add_documents(nodes)\n    # construct multiple indices\n    vector_index = VectorStoreIndex(nodes=nodes, storage_context=storage_context)\n    vector_id = vector_index.index_id",
        "detail": "reference_code.llama-index-core.tests.indices.test_loading",
        "documentation": {}
    },
    {
        "label": "test_load_index_from_storage_retrieval_result_identical",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.test_loading",
        "description": "reference_code.llama-index-core.tests.indices.test_loading",
        "peekOfCode": "def test_load_index_from_storage_retrieval_result_identical(\n    documents: List[Document], tmp_path: Path\n) -> None:\n    # construct simple (i.e. in memory) storage context\n    storage_context = StorageContext.from_defaults()\n    # construct index\n    index = VectorStoreIndex.from_documents(\n        documents=documents, storage_context=storage_context\n    )\n    nodes = index.as_retriever().retrieve(\"test query str\")",
        "detail": "reference_code.llama-index-core.tests.indices.test_loading",
        "documentation": {}
    },
    {
        "label": "test_load_graph_from_storage_simple",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.test_loading_graph",
        "description": "reference_code.llama-index-core.tests.indices.test_loading_graph",
        "peekOfCode": "def test_load_graph_from_storage_simple(\n    documents: List[Document], tmp_path: Path\n) -> None:\n    # construct simple (i.e. in memory) storage context\n    storage_context = StorageContext.from_defaults()\n    # construct index\n    vector_index_1 = VectorStoreIndex.from_documents(\n        documents=documents,\n        storage_context=storage_context,\n    )",
        "detail": "reference_code.llama-index-core.tests.indices.test_loading_graph",
        "documentation": {}
    },
    {
        "label": "test_get_chunk_size",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.test_prompt_helper",
        "description": "reference_code.llama-index-core.tests.indices.test_prompt_helper",
        "peekOfCode": "def test_get_chunk_size(\n    prompt: str,\n    chunk_size_limit: Optional[int],\n    num_chunks: int,\n    padding: int,\n    expected: Union[int, Type[Exception]],\n) -> None:\n    \"\"\"Test get chunk size given prompt.\"\"\"\n    prompt_helper = PromptHelper(\n        context_window=11,",
        "detail": "reference_code.llama-index-core.tests.indices.test_prompt_helper",
        "documentation": {}
    },
    {
        "label": "test_get_text_splitter",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.test_prompt_helper",
        "description": "reference_code.llama-index-core.tests.indices.test_prompt_helper",
        "peekOfCode": "def test_get_text_splitter() -> None:\n    \"\"\"Test get text splitter.\"\"\"\n    test_prompt_text = \"This is the prompt{text}\"\n    test_prompt = PromptTemplate(test_prompt_text)\n    prompt_helper = PromptHelper(\n        context_window=11, num_output=1, chunk_overlap_ratio=0, tokenizer=mock_tokenizer\n    )\n    text_splitter = prompt_helper.get_text_splitter_given_prompt(\n        test_prompt, 2, padding=1\n    )",
        "detail": "reference_code.llama-index-core.tests.indices.test_prompt_helper",
        "documentation": {}
    },
    {
        "label": "test_get_text_splitter_partial",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.test_prompt_helper",
        "description": "reference_code.llama-index-core.tests.indices.test_prompt_helper",
        "peekOfCode": "def test_get_text_splitter_partial() -> None:\n    \"\"\"Test get text splitter with a partially formatted prompt.\"\"\"\n    # test without partially formatting\n    test_prompt_text = \"This is the {foo} prompt{text}\"\n    test_prompt = PromptTemplate(test_prompt_text)\n    prompt_helper = PromptHelper(\n        context_window=11, num_output=1, chunk_overlap_ratio=0, tokenizer=mock_tokenizer\n    )\n    text_splitter = prompt_helper.get_text_splitter_given_prompt(\n        test_prompt, 2, padding=1",
        "detail": "reference_code.llama-index-core.tests.indices.test_prompt_helper",
        "documentation": {}
    },
    {
        "label": "test_truncate",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.test_prompt_helper",
        "description": "reference_code.llama-index-core.tests.indices.test_prompt_helper",
        "peekOfCode": "def test_truncate() -> None:\n    \"\"\"Test truncate.\"\"\"\n    # test prompt uses up one token\n    test_prompt_txt = \"test{text}\"\n    test_prompt = PromptTemplate(test_prompt_txt)\n    # set context_window=19\n    # For each text chunk, there's 4 tokens for text + 5 for the padding\n    prompt_helper = PromptHelper(\n        context_window=19, num_output=0, chunk_overlap_ratio=0, tokenizer=mock_tokenizer\n    )",
        "detail": "reference_code.llama-index-core.tests.indices.test_prompt_helper",
        "documentation": {}
    },
    {
        "label": "test_get_numbered_text_from_nodes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.test_prompt_helper",
        "description": "reference_code.llama-index-core.tests.indices.test_prompt_helper",
        "peekOfCode": "def test_get_numbered_text_from_nodes() -> None:\n    \"\"\"Test get_text_from_nodes.\"\"\"\n    # test prompt uses up one token\n    test_prompt_txt = \"test{text}\"\n    test_prompt = PromptTemplate(test_prompt_txt)\n    # set context_window=17\n    # For each text chunk, there's 3 for text, 5 for padding (including number)\n    prompt_helper = PromptHelper(\n        context_window=17, num_output=0, chunk_overlap_ratio=0, tokenizer=mock_tokenizer\n    )",
        "detail": "reference_code.llama-index-core.tests.indices.test_prompt_helper",
        "documentation": {}
    },
    {
        "label": "test_repack",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.test_prompt_helper",
        "description": "reference_code.llama-index-core.tests.indices.test_prompt_helper",
        "peekOfCode": "def test_repack() -> None:\n    \"\"\"Test repack.\"\"\"\n    test_prompt_text = \"This is the prompt{text}\"\n    test_prompt = PromptTemplate(test_prompt_text)\n    prompt_helper = PromptHelper(\n        context_window=13,\n        num_output=1,\n        chunk_overlap_ratio=0,\n        tokenizer=mock_tokenizer,\n        separator=\"\\n\\n\",",
        "detail": "reference_code.llama-index-core.tests.indices.test_prompt_helper",
        "documentation": {}
    },
    {
        "label": "test_get_biggest_prompt",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.test_prompt_helper",
        "description": "reference_code.llama-index-core.tests.indices.test_prompt_helper",
        "peekOfCode": "def test_get_biggest_prompt() -> None:\n    \"\"\"Test get_biggest_prompt from PromptHelper.\"\"\"\n    prompt1 = PromptTemplate(\"This is the prompt{text}\")\n    prompt2 = PromptTemplate(\"This is the longer prompt{text}\")\n    prompt3 = PromptTemplate(\"This is the {text}\")\n    biggest_prompt = get_biggest_prompt([prompt1, prompt2, prompt3])\n    assert biggest_prompt == prompt2\ndef test_json_in_prompt() -> None:\n    \"\"\"Test that a JSON object in the prompt doesn't cause an error.\"\"\"\n    # test with normal prompt",
        "detail": "reference_code.llama-index-core.tests.indices.test_prompt_helper",
        "documentation": {}
    },
    {
        "label": "test_json_in_prompt",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.test_prompt_helper",
        "description": "reference_code.llama-index-core.tests.indices.test_prompt_helper",
        "peekOfCode": "def test_json_in_prompt() -> None:\n    \"\"\"Test that a JSON object in the prompt doesn't cause an error.\"\"\"\n    # test with normal prompt\n    prompt = PromptTemplate(\n        'This is the prompt {text} but it also has {\"json\": \"in it\"}'\n    )\n    prompt.partial_format(text=\"hello_world\")\n    prompt_helper = PromptHelper()\n    texts = prompt_helper.repack(prompt, [\"hello_world\"])\n    assert len(texts) == 1",
        "detail": "reference_code.llama-index-core.tests.indices.test_prompt_helper",
        "documentation": {}
    },
    {
        "label": "test_expand_tokens_with_subtokens",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.test_utils",
        "description": "reference_code.llama-index-core.tests.indices.test_utils",
        "peekOfCode": "def test_expand_tokens_with_subtokens() -> None:\n    \"\"\"Test expand tokens.\"\"\"\n    tokens = {\"foo bar\", \"baz\", \"hello hello world bye\"}\n    keywords = expand_tokens_with_subtokens(tokens)\n    assert keywords == {\n        \"foo bar\",\n        \"foo\",\n        \"bar\",\n        \"baz\",\n        \"hello hello world bye\",",
        "detail": "reference_code.llama-index-core.tests.indices.test_utils",
        "documentation": {}
    },
    {
        "label": "test_default_parse_choice_select_answer_fn",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.indices.test_utils",
        "description": "reference_code.llama-index-core.tests.indices.test_utils",
        "peekOfCode": "def test_default_parse_choice_select_answer_fn(answer):\n    from llama_index.core.indices.utils import default_parse_choice_select_answer_fn\n    answer_nums, answer_relevances = default_parse_choice_select_answer_fn(answer, 5)\n    assert answer_nums == [2, 4]\n    assert answer_relevances == [8, 6]",
        "detail": "reference_code.llama-index-core.tests.indices.test_utils",
        "documentation": {}
    },
    {
        "label": "parse_choice_test_lines",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.indices.test_utils",
        "description": "reference_code.llama-index-core.tests.indices.test_utils",
        "peekOfCode": "parse_choice_test_lines = [\n    \"\"\" Doc: 2, Relevance: 8 (The document mentions taking a \"tasty turn around Barcelona\\'s Santa Caterina market\" and listening to an episode about Barcelona.)\\nDoc: 4, Relevance: 6 (The document mentions Ferramenta in Barcelona and recommends cocktails and pasta dishes that can be tried there.)\"\"\",\n    \"Doc: 2, Relevance: 8\\nDoc: 4, Relevance: 6\",\n    \"answer_num: 2, answer_relevance:8\\nanswer_num: 4, answer_relevance:6\",\n]\n@pytest.mark.parametrize(\"answer\", parse_choice_test_lines)\ndef test_default_parse_choice_select_answer_fn(answer):\n    from llama_index.core.indices.utils import default_parse_choice_select_answer_fn\n    answer_nums, answer_relevances = default_parse_choice_select_answer_fn(answer, 5)\n    assert answer_nums == [2, 4]",
        "detail": "reference_code.llama-index-core.tests.indices.test_utils",
        "documentation": {}
    },
    {
        "label": "DummyTransform",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.ingestion.test_cache",
        "description": "reference_code.llama-index-core.tests.ingestion.test_cache",
        "peekOfCode": "class DummyTransform(TransformComponent):\n    def __call__(self, nodes: List[BaseNode], **kwargs: Any) -> List[BaseNode]:\n        for node in nodes:\n            node.set_content(node.get_content() + \"\\nTESTTEST\")\n        return nodes\ndef test_cache() -> None:\n    cache = IngestionCache()\n    transformation = DummyTransform()\n    node = TextNode(text=\"dummy\")\n    hash = get_transformation_hash([node], transformation)",
        "detail": "reference_code.llama-index-core.tests.ingestion.test_cache",
        "documentation": {}
    },
    {
        "label": "test_cache",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.ingestion.test_cache",
        "description": "reference_code.llama-index-core.tests.ingestion.test_cache",
        "peekOfCode": "def test_cache() -> None:\n    cache = IngestionCache()\n    transformation = DummyTransform()\n    node = TextNode(text=\"dummy\")\n    hash = get_transformation_hash([node], transformation)\n    new_nodes = transformation([node])\n    cache.put(hash, new_nodes)\n    cache_hit = cache.get(hash)\n    assert cache_hit is not None\n    assert cache_hit[0].get_content() == new_nodes[0].get_content()",
        "detail": "reference_code.llama-index-core.tests.ingestion.test_cache",
        "documentation": {}
    },
    {
        "label": "test_cache_clear",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.ingestion.test_cache",
        "description": "reference_code.llama-index-core.tests.ingestion.test_cache",
        "peekOfCode": "def test_cache_clear() -> None:\n    cache = IngestionCache()\n    transformation = DummyTransform()\n    node = TextNode(text=\"dummy\")\n    hash = get_transformation_hash([node], transformation)\n    new_nodes = transformation([node])\n    cache.put(hash, new_nodes)\n    cache_hit = cache.get(hash)\n    assert cache_hit is not None\n    cache.clear()",
        "detail": "reference_code.llama-index-core.tests.ingestion.test_cache",
        "documentation": {}
    },
    {
        "label": "test_can_generate_schema_for_data_sink_component_type",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.ingestion.test_data_sinks",
        "description": "reference_code.llama-index-core.tests.ingestion.test_data_sinks",
        "peekOfCode": "def test_can_generate_schema_for_data_sink_component_type(\n    configurable_data_sink_type: ConfigurableDataSinks,\n) -> None:\n    schema = configurable_data_sink_type.value.model_json_schema()  # type: ignore\n    assert schema is not None\n    assert len(schema) > 0\n    # also check that we can generate schemas for\n    # ConfiguredDataSink[component_type]\n    component_type = configurable_data_sink_type.value.component_type\n    configured_schema = ConfiguredDataSink[component_type].model_json_schema()  # type: ignore",
        "detail": "reference_code.llama-index-core.tests.ingestion.test_data_sinks",
        "documentation": {}
    },
    {
        "label": "test_can_build_configured_data_sink_from_component",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.ingestion.test_data_sinks",
        "description": "reference_code.llama-index-core.tests.ingestion.test_data_sinks",
        "peekOfCode": "def test_can_build_configured_data_sink_from_component() -> None:\n    sys.modules[\"weaviate\"] = MagicMock()\n    weaviate_client = MagicMock()\n    batch_context_manager = MagicMock()\n    weaviate_client.batch.__enter__.return_value = batch_context_manager\n    vector_store = WeaviateVectorStore(weaviate_client=weaviate_client)\n    configured_data_sink = ConfiguredDataSink.from_component(vector_store)\n    assert isinstance(\n        configured_data_sink,\n        ConfiguredDataSink[WeaviateVectorStore],  # type: ignore",
        "detail": "reference_code.llama-index-core.tests.ingestion.test_data_sinks",
        "documentation": {}
    },
    {
        "label": "test_build_configured_data_sink",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.ingestion.test_data_sinks",
        "description": "reference_code.llama-index-core.tests.ingestion.test_data_sinks",
        "peekOfCode": "def test_build_configured_data_sink() -> None:\n    sys.modules[\"weaviate\"] = MagicMock()\n    weaviate_client = MagicMock()\n    batch_context_manager = MagicMock()\n    weaviate_client.batch.__enter__.return_value = batch_context_manager\n    vector_store = WeaviateVectorStore(weaviate_client=weaviate_client)\n    configured_data_sink = ConfigurableDataSinks.WEAVIATE.build_configured_data_sink(\n        vector_store\n    )\n    assert isinstance(",
        "detail": "reference_code.llama-index-core.tests.ingestion.test_data_sinks",
        "documentation": {}
    },
    {
        "label": "test_unique_configurable_data_sink_names",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.ingestion.test_data_sinks",
        "description": "reference_code.llama-index-core.tests.ingestion.test_data_sinks",
        "peekOfCode": "def test_unique_configurable_data_sink_names() -> None:\n    names = set()\n    for configurable_data_sink_type in ConfigurableDataSinks:\n        assert configurable_data_sink_type.value.name not in names\n        names.add(configurable_data_sink_type.value.name)\n    assert len(names) == len(ConfigurableDataSinks)",
        "detail": "reference_code.llama-index-core.tests.ingestion.test_data_sinks",
        "documentation": {}
    },
    {
        "label": "test_can_generate_schema_for_data_source_component_type",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.ingestion.test_data_sources",
        "description": "reference_code.llama-index-core.tests.ingestion.test_data_sources",
        "peekOfCode": "def test_can_generate_schema_for_data_source_component_type(\n    configurable_data_source_type: ConfigurableDataSources,\n) -> None:\n    schema = configurable_data_source_type.value.model_json_schema()  # type: ignore\n    assert schema is not None\n    assert len(schema) > 0\n    # also check that we can generate schemas for\n    # ConfiguredDataSource[component_type]\n    component_type = configurable_data_source_type.value.component_type\n    configured_schema = ConfiguredDataSource[component_type].model_json_schema()  # type: ignore",
        "detail": "reference_code.llama-index-core.tests.ingestion.test_data_sources",
        "documentation": {}
    },
    {
        "label": "test_can_build_configured_data_source_from_component",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.ingestion.test_data_sources",
        "description": "reference_code.llama-index-core.tests.ingestion.test_data_sources",
        "peekOfCode": "def test_can_build_configured_data_source_from_component() -> None:\n    document = Document.example()\n    configured_data_source = ConfiguredDataSource.from_component(document)\n    assert isinstance(\n        configured_data_source,\n        ConfiguredDataSource[Document],  # type: ignore\n    )\n    assert (\n        configured_data_source.configurable_data_source_type.value.component_type\n        == Document",
        "detail": "reference_code.llama-index-core.tests.ingestion.test_data_sources",
        "documentation": {}
    },
    {
        "label": "test_build_configured_data_source",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.ingestion.test_data_sources",
        "description": "reference_code.llama-index-core.tests.ingestion.test_data_sources",
        "peekOfCode": "def test_build_configured_data_source() -> None:\n    document = Document.example()\n    configured_data_source = (\n        ConfigurableDataSources.DOCUMENT.build_configured_data_source(document)\n    )\n    assert isinstance(\n        configured_data_source,\n        ConfiguredDataSource[Document],  # type: ignore\n    )\ndef test_unique_configurable_data_source_names() -> None:",
        "detail": "reference_code.llama-index-core.tests.ingestion.test_data_sources",
        "documentation": {}
    },
    {
        "label": "test_unique_configurable_data_source_names",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.ingestion.test_data_sources",
        "description": "reference_code.llama-index-core.tests.ingestion.test_data_sources",
        "peekOfCode": "def test_unique_configurable_data_source_names() -> None:\n    names = set()\n    for configurable_data_source_type in ConfigurableDataSources:\n        assert configurable_data_source_type.value.name not in names\n        names.add(configurable_data_source_type.value.name)\n    assert len(names) > 0\n    assert len(names) == len(ConfigurableDataSources)",
        "detail": "reference_code.llama-index-core.tests.ingestion.test_data_sources",
        "documentation": {}
    },
    {
        "label": "teardown_function",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "description": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "peekOfCode": "def teardown_function() -> None:\n    import shutil\n    shutil.rmtree(\"./test_pipeline\", ignore_errors=True)\ndef test_build_pipeline() -> None:\n    pipeline = IngestionPipeline(\n        readers=[\n            ReaderConfig(\n                reader=StringIterableReader(),\n                reader_kwargs={\"texts\": [\"This is a test.\"]},\n            )",
        "detail": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "documentation": {}
    },
    {
        "label": "test_build_pipeline",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "description": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "peekOfCode": "def test_build_pipeline() -> None:\n    pipeline = IngestionPipeline(\n        readers=[\n            ReaderConfig(\n                reader=StringIterableReader(),\n                reader_kwargs={\"texts\": [\"This is a test.\"]},\n            )\n        ],\n        documents=[Document.example()],\n        transformations=[",
        "detail": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "documentation": {}
    },
    {
        "label": "test_run_pipeline",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "description": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "peekOfCode": "def test_run_pipeline() -> None:\n    pipeline = IngestionPipeline(\n        readers=[\n            ReaderConfig(\n                reader=StringIterableReader(),\n                reader_kwargs={\"texts\": [\"This is a test.\"]},\n            )\n        ],\n        documents=[Document.example()],\n        transformations=[",
        "detail": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "documentation": {}
    },
    {
        "label": "test_run_pipeline_with_ref_doc_id",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "description": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "peekOfCode": "def test_run_pipeline_with_ref_doc_id():\n    documents = [\n        Document(text=\"one\", doc_id=\"1\"),\n    ]\n    pipeline = IngestionPipeline(\n        documents=documents,\n        transformations=[\n            MarkdownElementNodeParser(),\n            SentenceSplitter(),\n            MockEmbedding(embed_dim=8),",
        "detail": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "documentation": {}
    },
    {
        "label": "test_save_load_pipeline",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "description": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "peekOfCode": "def test_save_load_pipeline() -> None:\n    documents = [\n        Document(text=\"one\", doc_id=\"1\"),\n        Document(text=\"two\", doc_id=\"2\"),\n        Document(text=\"one\", doc_id=\"1\"),\n    ]\n    pipeline = IngestionPipeline(\n        transformations=[\n            SentenceSplitter(chunk_size=25, chunk_overlap=0),\n        ],",
        "detail": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "documentation": {}
    },
    {
        "label": "test_save_load_pipeline_without_docstore",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "description": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "peekOfCode": "def test_save_load_pipeline_without_docstore() -> None:\n    documents = [\n        Document(text=\"one\", doc_id=\"1\"),\n        Document(text=\"two\", doc_id=\"2\"),\n        Document(text=\"one\", doc_id=\"1\"),\n    ]\n    pipeline = IngestionPipeline(\n        transformations=[\n            SentenceSplitter(chunk_size=25, chunk_overlap=0),\n        ],",
        "detail": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "documentation": {}
    },
    {
        "label": "test_pipeline_update_text_content",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "description": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "peekOfCode": "def test_pipeline_update_text_content() -> None:\n    document1 = Document.example()\n    document1.id_ = \"1\"\n    pipeline = IngestionPipeline(\n        transformations=[\n            SentenceSplitter(chunk_size=25, chunk_overlap=0),\n        ],\n        docstore=SimpleDocumentStore(),\n    )\n    nodes = pipeline.run(documents=[document1])",
        "detail": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "documentation": {}
    },
    {
        "label": "test_pipeline_update_metadata",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "description": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "peekOfCode": "def test_pipeline_update_metadata() -> None:\n    \"\"\"Test that IngestionPipeline updates document metadata, if it changed.\"\"\"\n    old_metadata = {\"filename\": \"README.md\", \"category\": \"codebase\"}\n    document1 = Document.example()\n    document1.metadata = old_metadata\n    document1.id_ = \"1\"\n    pipeline = IngestionPipeline(\n        transformations=[\n            SentenceSplitter(chunk_size=25, chunk_overlap=0),\n        ],",
        "detail": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "documentation": {}
    },
    {
        "label": "test_pipeline_dedup_duplicates_only",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "description": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "peekOfCode": "def test_pipeline_dedup_duplicates_only() -> None:\n    documents = [\n        Document(text=\"one\", doc_id=\"1\"),\n        Document(text=\"two\", doc_id=\"2\"),\n        Document(text=\"three\", doc_id=\"3\"),\n    ]\n    pipeline = IngestionPipeline(\n        transformations=[\n            SentenceSplitter(chunk_size=25, chunk_overlap=0),\n        ],",
        "detail": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "documentation": {}
    },
    {
        "label": "test_pipeline_parallel",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "description": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "peekOfCode": "def test_pipeline_parallel() -> None:\n    document1 = Document.example()\n    document1.id_ = \"1\"\n    document2 = Document(text=\"One\\n\\n\\nTwo\\n\\n\\nThree.\", doc_id=\"2\")\n    pipeline = IngestionPipeline(\n        transformations=[\n            SentenceSplitter(chunk_size=25, chunk_overlap=0),\n        ],\n        docstore=SimpleDocumentStore(),\n    )",
        "detail": "reference_code.llama-index-core.tests.ingestion.test_pipeline",
        "documentation": {}
    },
    {
        "label": "test_can_generate_schema_for_transformation_component_type",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.ingestion.test_transformations",
        "description": "reference_code.llama-index-core.tests.ingestion.test_transformations",
        "peekOfCode": "def test_can_generate_schema_for_transformation_component_type(\n    configurable_transformation_type: ConfigurableTransformations,\n) -> None:\n    schema = configurable_transformation_type.value.model_json_schema()  # type: ignore\n    assert schema is not None\n    assert len(schema) > 0\n    # also check that we can generate schemas for\n    # ConfiguredTransformation[component_type]\n    component_type = configurable_transformation_type.value.component_type\n    configured_schema = ConfiguredTransformation[",
        "detail": "reference_code.llama-index-core.tests.ingestion.test_transformations",
        "documentation": {}
    },
    {
        "label": "test_can_build_configured_transform_from_component",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.ingestion.test_transformations",
        "description": "reference_code.llama-index-core.tests.ingestion.test_transformations",
        "peekOfCode": "def test_can_build_configured_transform_from_component() -> None:\n    parser = SentenceSplitter()\n    configured_transformation = ConfiguredTransformation.from_component(parser)\n    assert isinstance(\n        configured_transformation,\n        ConfiguredTransformation[SentenceSplitter],  # type: ignore\n    )\n    assert not isinstance(\n        configured_transformation,\n        ConfiguredTransformation[TokenTextSplitter],  # type: ignore",
        "detail": "reference_code.llama-index-core.tests.ingestion.test_transformations",
        "documentation": {}
    },
    {
        "label": "test_build_configured_transformation",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.ingestion.test_transformations",
        "description": "reference_code.llama-index-core.tests.ingestion.test_transformations",
        "peekOfCode": "def test_build_configured_transformation() -> None:\n    parser = SentenceSplitter()\n    configured_transformation = ConfigurableTransformations.SENTENCE_AWARE_NODE_PARSER.build_configured_transformation(\n        parser\n    )\n    assert isinstance(\n        configured_transformation,\n        ConfiguredTransformation[SentenceSplitter],  # type: ignore\n    )\n    with pytest.raises(ValueError):",
        "detail": "reference_code.llama-index-core.tests.ingestion.test_transformations",
        "documentation": {}
    },
    {
        "label": "test_unique_configurable_transformations_names",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.ingestion.test_transformations",
        "description": "reference_code.llama-index-core.tests.ingestion.test_transformations",
        "peekOfCode": "def test_unique_configurable_transformations_names() -> None:\n    names = set()\n    for configurable_transformation_type in ConfigurableTransformations:\n        assert configurable_transformation_type.value.name not in names\n        names.add(configurable_transformation_type.value.name)\n    assert len(names) > 0\n    assert len(names) == len(ConfigurableTransformations)",
        "detail": "reference_code.llama-index-core.tests.ingestion.test_transformations",
        "documentation": {}
    },
    {
        "label": "nonyielding_llm",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "description": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "peekOfCode": "def nonyielding_llm() -> LLM:\n    return MockLLMWithNonyieldingChatStream()\n@pytest.fixture()\ndef llm() -> LLM:\n    return MockLLM()\n@pytest.fixture()\ndef prompt() -> str:\n    return \"test prompt\"\ndef test_llm_stream_chat_handles_nonyielding_stream(\n    nonyielding_llm: LLM, prompt: str",
        "detail": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "description": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "peekOfCode": "def llm() -> LLM:\n    return MockLLM()\n@pytest.fixture()\ndef prompt() -> str:\n    return \"test prompt\"\ndef test_llm_stream_chat_handles_nonyielding_stream(\n    nonyielding_llm: LLM, prompt: str\n) -> None:\n    response = nonyielding_llm.stream_chat([ChatMessage(role=\"user\", content=prompt)])\n    for _ in response:",
        "detail": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "description": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "peekOfCode": "def prompt() -> str:\n    return \"test prompt\"\ndef test_llm_stream_chat_handles_nonyielding_stream(\n    nonyielding_llm: LLM, prompt: str\n) -> None:\n    response = nonyielding_llm.stream_chat([ChatMessage(role=\"user\", content=prompt)])\n    for _ in response:\n        pass\n@pytest.mark.asyncio\nasync def test_llm_astream_chat_handles_nonyielding_stream(",
        "detail": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "documentation": {}
    },
    {
        "label": "test_llm_stream_chat_handles_nonyielding_stream",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "description": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "peekOfCode": "def test_llm_stream_chat_handles_nonyielding_stream(\n    nonyielding_llm: LLM, prompt: str\n) -> None:\n    response = nonyielding_llm.stream_chat([ChatMessage(role=\"user\", content=prompt)])\n    for _ in response:\n        pass\n@pytest.mark.asyncio\nasync def test_llm_astream_chat_handles_nonyielding_stream(\n    nonyielding_llm: LLM, prompt: str\n) -> None:",
        "detail": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "documentation": {}
    },
    {
        "label": "test_llm_complete_prompt_arg",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "description": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "peekOfCode": "def test_llm_complete_prompt_arg(llm: LLM, prompt: str) -> None:\n    res = llm.complete(prompt)\n    expected_res_text = prompt\n    assert res.text == expected_res_text\ndef test_llm_complete_prompt_kwarg(llm: LLM, prompt: str) -> None:\n    res = llm.complete(prompt=prompt)\n    expected_res_text = prompt\n    assert res.text == expected_res_text\ndef test_llm_complete_throws_if_duplicate_prompt(llm: LLM, prompt: str) -> None:\n    with pytest.raises(TypeError):",
        "detail": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "documentation": {}
    },
    {
        "label": "test_llm_complete_prompt_kwarg",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "description": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "peekOfCode": "def test_llm_complete_prompt_kwarg(llm: LLM, prompt: str) -> None:\n    res = llm.complete(prompt=prompt)\n    expected_res_text = prompt\n    assert res.text == expected_res_text\ndef test_llm_complete_throws_if_duplicate_prompt(llm: LLM, prompt: str) -> None:\n    with pytest.raises(TypeError):\n        llm.complete(prompt, prompt=prompt)\ndef test_llm_complete_throws_if_no_prompt(llm: LLM) -> None:\n    with pytest.raises(ValueError):\n        llm.complete()",
        "detail": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "documentation": {}
    },
    {
        "label": "test_llm_complete_throws_if_duplicate_prompt",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "description": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "peekOfCode": "def test_llm_complete_throws_if_duplicate_prompt(llm: LLM, prompt: str) -> None:\n    with pytest.raises(TypeError):\n        llm.complete(prompt, prompt=prompt)\ndef test_llm_complete_throws_if_no_prompt(llm: LLM) -> None:\n    with pytest.raises(ValueError):\n        llm.complete()\ndef test_llm_stream_complete_prompt_arg(llm: LLM, prompt: str) -> None:\n    res_text = \"\".join(r.delta for r in llm.stream_complete(prompt))\n    expected_res_text = prompt\n    assert res_text == expected_res_text",
        "detail": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "documentation": {}
    },
    {
        "label": "test_llm_complete_throws_if_no_prompt",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "description": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "peekOfCode": "def test_llm_complete_throws_if_no_prompt(llm: LLM) -> None:\n    with pytest.raises(ValueError):\n        llm.complete()\ndef test_llm_stream_complete_prompt_arg(llm: LLM, prompt: str) -> None:\n    res_text = \"\".join(r.delta for r in llm.stream_complete(prompt))\n    expected_res_text = prompt\n    assert res_text == expected_res_text\ndef test_llm_stream_complete_prompt_kwarg(llm: LLM, prompt: str) -> None:\n    res_text = \"\".join(r.delta for r in llm.stream_complete(prompt=prompt))\n    expected_res_text = prompt",
        "detail": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "documentation": {}
    },
    {
        "label": "test_llm_stream_complete_prompt_arg",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "description": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "peekOfCode": "def test_llm_stream_complete_prompt_arg(llm: LLM, prompt: str) -> None:\n    res_text = \"\".join(r.delta for r in llm.stream_complete(prompt))\n    expected_res_text = prompt\n    assert res_text == expected_res_text\ndef test_llm_stream_complete_prompt_kwarg(llm: LLM, prompt: str) -> None:\n    res_text = \"\".join(r.delta for r in llm.stream_complete(prompt=prompt))\n    expected_res_text = prompt\n    assert res_text == expected_res_text\ndef test_llm_stream_complete_throws_if_duplicate_prompt(llm: LLM, prompt: str) -> None:\n    with pytest.raises(TypeError):",
        "detail": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "documentation": {}
    },
    {
        "label": "test_llm_stream_complete_prompt_kwarg",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "description": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "peekOfCode": "def test_llm_stream_complete_prompt_kwarg(llm: LLM, prompt: str) -> None:\n    res_text = \"\".join(r.delta for r in llm.stream_complete(prompt=prompt))\n    expected_res_text = prompt\n    assert res_text == expected_res_text\ndef test_llm_stream_complete_throws_if_duplicate_prompt(llm: LLM, prompt: str) -> None:\n    with pytest.raises(TypeError):\n        llm.stream_complete(prompt, prompt=prompt)\ndef test_llm_stream_complete_throws_if_no_prompt(llm: LLM) -> None:\n    with pytest.raises(ValueError):\n        llm.stream_complete()",
        "detail": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "documentation": {}
    },
    {
        "label": "test_llm_stream_complete_throws_if_duplicate_prompt",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "description": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "peekOfCode": "def test_llm_stream_complete_throws_if_duplicate_prompt(llm: LLM, prompt: str) -> None:\n    with pytest.raises(TypeError):\n        llm.stream_complete(prompt, prompt=prompt)\ndef test_llm_stream_complete_throws_if_no_prompt(llm: LLM) -> None:\n    with pytest.raises(ValueError):\n        llm.stream_complete()",
        "detail": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "documentation": {}
    },
    {
        "label": "test_llm_stream_complete_throws_if_no_prompt",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "description": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "peekOfCode": "def test_llm_stream_complete_throws_if_no_prompt(llm: LLM) -> None:\n    with pytest.raises(ValueError):\n        llm.stream_complete()",
        "detail": "reference_code.llama-index-core.tests.llms.test_callbacks",
        "documentation": {}
    },
    {
        "label": "TestLLM",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.llms.test_custom",
        "description": "reference_code.llama-index-core.tests.llms.test_custom",
        "peekOfCode": "class TestLLM(CustomLLM):\n    __test__ = False\n    def __init__(self) -> None:\n        super().__init__(callback_manager=None)\n    @property\n    def metadata(self) -> LLMMetadata:\n        return LLMMetadata()\n    def complete(\n        self, prompt: str, formatted: bool = False, **kwargs: Any\n    ) -> CompletionResponse:",
        "detail": "reference_code.llama-index-core.tests.llms.test_custom",
        "documentation": {}
    },
    {
        "label": "test_basic",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.llms.test_custom",
        "description": "reference_code.llama-index-core.tests.llms.test_custom",
        "peekOfCode": "def test_basic() -> None:\n    llm = TestLLM()\n    prompt = \"test prompt\"\n    message = ChatMessage(role=\"user\", content=\"test message\")\n    llm.complete(prompt)\n    llm.chat([message])\ndef test_streaming() -> None:\n    llm = TestLLM()\n    prompt = \"test prompt\"\n    message = ChatMessage(role=\"user\", content=\"test message\")",
        "detail": "reference_code.llama-index-core.tests.llms.test_custom",
        "documentation": {}
    },
    {
        "label": "test_streaming",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.llms.test_custom",
        "description": "reference_code.llama-index-core.tests.llms.test_custom",
        "peekOfCode": "def test_streaming() -> None:\n    llm = TestLLM()\n    prompt = \"test prompt\"\n    message = ChatMessage(role=\"user\", content=\"test message\")\n    llm.stream_complete(prompt)\n    llm.stream_chat([message])",
        "detail": "reference_code.llama-index-core.tests.llms.test_custom",
        "documentation": {}
    },
    {
        "label": "MockFunctionCallingLLM",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "description": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "peekOfCode": "class MockFunctionCallingLLM(FunctionCallingLLM):\n    def __init__(self, tool_selection: List[ToolSelection]):\n        super().__init__()\n        self._tool_selection = tool_selection\n    async def achat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) -> Coroutine[Any, Any, ChatResponse]:\n        return ChatResponse(message=ChatMessage(role=\"user\", content=\"\"))\n    def acomplete(\n        self, prompt: str, formatted: bool = False, **kwargs: Any",
        "detail": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "documentation": {}
    },
    {
        "label": "MockFunctionCallingLLMWithoutToolRequired",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "description": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "peekOfCode": "class MockFunctionCallingLLMWithoutToolRequired(MockFunctionCallingLLM):\n    def _prepare_chat_with_tools(\n        self,\n        tools: Sequence[\"BaseTool\"],\n        user_msg: Optional[Union[str, ChatMessage]] = None,\n        chat_history: Optional[List[ChatMessage]] = None,\n        verbose: bool = False,\n        allow_parallel_tool_calls: bool = False,\n        **kwargs: Any,\n    ) -> Dict[str, Any]:",
        "detail": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "documentation": {}
    },
    {
        "label": "Person",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "description": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "peekOfCode": "class Person(BaseModel):\n    name: str = Field(description=\"Person name\")\n@pytest.fixture()\ndef person_tool() -> FunctionTool:\n    return get_function_tool(Person)\n@pytest.fixture()\ndef person_tool_selection(person_tool: FunctionTool) -> ToolSelection:\n    return ToolSelection(\n        tool_id=\"\",\n        tool_name=person_tool.metadata.name,",
        "detail": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "documentation": {}
    },
    {
        "label": "person_tool",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "description": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "peekOfCode": "def person_tool() -> FunctionTool:\n    return get_function_tool(Person)\n@pytest.fixture()\ndef person_tool_selection(person_tool: FunctionTool) -> ToolSelection:\n    return ToolSelection(\n        tool_id=\"\",\n        tool_name=person_tool.metadata.name,\n        tool_kwargs={},\n    )\ndef test_predict_and_call(",
        "detail": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "documentation": {}
    },
    {
        "label": "person_tool_selection",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "description": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "peekOfCode": "def person_tool_selection(person_tool: FunctionTool) -> ToolSelection:\n    return ToolSelection(\n        tool_id=\"\",\n        tool_name=person_tool.metadata.name,\n        tool_kwargs={},\n    )\ndef test_predict_and_call(\n    person_tool: FunctionTool, person_tool_selection: ToolSelection\n) -> None:\n    \"\"\"Test predict_and_call will return ToolOutput with error rather than raising one.\"\"\"",
        "detail": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "documentation": {}
    },
    {
        "label": "test_predict_and_call",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "description": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "peekOfCode": "def test_predict_and_call(\n    person_tool: FunctionTool, person_tool_selection: ToolSelection\n) -> None:\n    \"\"\"Test predict_and_call will return ToolOutput with error rather than raising one.\"\"\"\n    llm = MockFunctionCallingLLM([person_tool_selection])\n    response = llm.predict_and_call(tools=[person_tool])\n    assert all(tool_output.is_error for tool_output in response.sources)\ndef test_predict_and_call_throws_if_error_on_tool(\n    person_tool: FunctionTool, person_tool_selection: ToolSelection\n) -> None:",
        "detail": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "documentation": {}
    },
    {
        "label": "test_predict_and_call_throws_if_error_on_tool",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "description": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "peekOfCode": "def test_predict_and_call_throws_if_error_on_tool(\n    person_tool: FunctionTool, person_tool_selection: ToolSelection\n) -> None:\n    \"\"\"Test predict_and_call will raise an error.\"\"\"\n    llm = MockFunctionCallingLLM([person_tool_selection])\n    with pytest.raises(ValueError):\n        llm.predict_and_call(tools=[person_tool], error_on_tool_error=True)\n@pytest.mark.asyncio\nasync def test_apredict_and_call(\n    person_tool: FunctionTool, person_tool_selection: ToolSelection",
        "detail": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "documentation": {}
    },
    {
        "label": "test_tool_required_compatibility_without_support",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "description": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "peekOfCode": "def test_tool_required_compatibility_without_support(\n    person_tool: FunctionTool, person_tool_selection: ToolSelection\n) -> None:\n    \"\"\"Test that tool_required parameter is not passed to LLMs that don't support it.\"\"\"\n    llm = MockFunctionCallingLLMWithoutToolRequired([person_tool_selection])\n    # Mock the _prepare_chat_with_tools method to capture what arguments it receives\n    with patch.object(\n        llm, \"_prepare_chat_with_tools\", wraps=llm._prepare_chat_with_tools\n    ) as mock_prepare:\n        llm.chat_with_tools(tools=[person_tool], tool_required=True)",
        "detail": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "documentation": {}
    },
    {
        "label": "test_tool_required_compatibility_with_support",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "description": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "peekOfCode": "def test_tool_required_compatibility_with_support(\n    person_tool: FunctionTool, person_tool_selection: ToolSelection\n) -> None:\n    \"\"\"Test that tool_required parameter is passed to LLMs that support it.\"\"\"\n    llm = MockFunctionCallingLLM([person_tool_selection])\n    # Mock the _prepare_chat_with_tools method to capture what arguments it receives\n    with patch.object(\n        llm, \"_prepare_chat_with_tools\", wraps=llm._prepare_chat_with_tools\n    ) as mock_prepare:\n        llm.chat_with_tools(tools=[person_tool], tool_required=True)",
        "detail": "reference_code.llama-index-core.tests.llms.test_function_calling",
        "documentation": {}
    },
    {
        "label": "test_mock_llm_stream_complete_empty_prompt_no_max_tokens",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.llms.test_mock",
        "description": "reference_code.llama-index-core.tests.llms.test_mock",
        "peekOfCode": "def test_mock_llm_stream_complete_empty_prompt_no_max_tokens() -> None:\n    \"\"\"\n    Test that MockLLM.stream_complete with an empty prompt and max_tokens=None\n    does not raise a validation error.\n    This test case is based on issue #19353.\n    \"\"\"\n    llm = MockLLM(max_tokens=None)\n    response_gen = llm.stream_complete(\"\")\n    # Consume the generator to trigger the potential error\n    responses = list(response_gen)",
        "detail": "reference_code.llama-index-core.tests.llms.test_mock",
        "documentation": {}
    },
    {
        "label": "MyMockLLM",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.memory.blocks.test_fact",
        "description": "reference_code.llama-index-core.tests.memory.blocks.test_fact",
        "peekOfCode": "class MyMockLLM(MockLLM):\n    \"\"\"Test-specific subclass of MockLLM with mocked achat method.\"\"\"\n    def __init__(self, *args, responses: List[ChatResponse], **kwargs):\n        super().__init__(*args, **kwargs)\n        self._responses = responses\n        self._index = 0\n    async def achat(self, messages: List[ChatMessage], **kwargs) -> ChatResponse:\n        response = self._responses[self._index]\n        self._index += 1\n        return response",
        "detail": "reference_code.llama-index-core.tests.memory.blocks.test_fact",
        "documentation": {}
    },
    {
        "label": "mock_extraction_llm",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.blocks.test_fact",
        "description": "reference_code.llama-index-core.tests.memory.blocks.test_fact",
        "peekOfCode": "def mock_extraction_llm():\n    \"\"\"Create a mock LLM with extraction responses.\"\"\"\n    return MyMockLLM(\n        responses=[\n            ChatResponse(\n                message=ChatMessage(\n                    content=\"<facts><fact>John lives in New York</fact><fact>John is a software engineer</fact></facts>\"\n                )\n            ),\n        ]",
        "detail": "reference_code.llama-index-core.tests.memory.blocks.test_fact",
        "documentation": {}
    },
    {
        "label": "sample_messages",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.blocks.test_fact",
        "description": "reference_code.llama-index-core.tests.memory.blocks.test_fact",
        "peekOfCode": "def sample_messages():\n    \"\"\"Create sample chat messages.\"\"\"\n    return [\n        ChatMessage(\n            role=MessageRole.USER, content=\"My name is John and I live in New York.\"\n        ),\n        ChatMessage(role=MessageRole.ASSISTANT, content=\"Nice to meet you John!\"),\n        ChatMessage(\n            role=MessageRole.USER,\n            content=\"I work as a software engineer and I'm allergic to peanuts.\",",
        "detail": "reference_code.llama-index-core.tests.memory.blocks.test_fact",
        "documentation": {}
    },
    {
        "label": "sample_messages",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.blocks.test_static",
        "description": "reference_code.llama-index-core.tests.memory.blocks.test_static",
        "peekOfCode": "def sample_messages():\n    \"\"\"Create sample chat messages.\"\"\"\n    return [\n        ChatMessage(role=MessageRole.USER, content=\"Hello, how are you?\"),\n        ChatMessage(role=MessageRole.ASSISTANT, content=\"I'm doing well, thanks for asking!\"),\n        ChatMessage(role=MessageRole.USER, content=\"What's the weather like today?\"),\n    ]\n@pytest.mark.asyncio\nasync def test_initialization_with_string():\n    \"\"\"Test initialization of StaticMemoryBlock with a string.\"\"\"",
        "detail": "reference_code.llama-index-core.tests.memory.blocks.test_static",
        "documentation": {}
    },
    {
        "label": "MockVectorStore",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.memory.blocks.test_vector",
        "description": "reference_code.llama-index-core.tests.memory.blocks.test_vector",
        "peekOfCode": "class MockVectorStore(BasePydanticVectorStore):\n    \"\"\"Mock vector store for testing.\"\"\"\n    stores_text: bool = True\n    is_embedding_query: bool = True\n    def __init__(self):\n        super().__init__()\n        self._nodes = {}\n    @property\n    def client(self) -> Any:\n        return self",
        "detail": "reference_code.llama-index-core.tests.memory.blocks.test_vector",
        "documentation": {}
    },
    {
        "label": "MockNodePostprocessor",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.memory.blocks.test_vector",
        "description": "reference_code.llama-index-core.tests.memory.blocks.test_vector",
        "peekOfCode": "class MockNodePostprocessor(BaseNodePostprocessor):\n    \"\"\"Mock node postprocessor for testing.\"\"\"\n    def _postprocess_nodes(\n        self, nodes: List[NodeWithScore], query: Any = None\n    ) -> List[NodeWithScore]:\n        \"\"\"Add a prefix to each node's text.\"\"\"\n        for node in nodes:\n            if isinstance(node.node, TextNode):\n                node.node.text = f\"PROCESSED: {node.node.text}\"\n        return nodes",
        "detail": "reference_code.llama-index-core.tests.memory.blocks.test_vector",
        "documentation": {}
    },
    {
        "label": "mock_embedding",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.blocks.test_vector",
        "description": "reference_code.llama-index-core.tests.memory.blocks.test_vector",
        "peekOfCode": "def mock_embedding():\n    \"\"\"Create a mock embedding model.\"\"\"\n    return MockEmbedding(embed_dim=10)\n@pytest.fixture\ndef mock_vector_store():\n    \"\"\"Create a mock vector store.\"\"\"\n    return MockVectorStore()\n@pytest.fixture\ndef vector_memory_block(\n    mock_vector_store: MockVectorStore, mock_embedding: MockEmbedding",
        "detail": "reference_code.llama-index-core.tests.memory.blocks.test_vector",
        "documentation": {}
    },
    {
        "label": "mock_vector_store",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.blocks.test_vector",
        "description": "reference_code.llama-index-core.tests.memory.blocks.test_vector",
        "peekOfCode": "def mock_vector_store():\n    \"\"\"Create a mock vector store.\"\"\"\n    return MockVectorStore()\n@pytest.fixture\ndef vector_memory_block(\n    mock_vector_store: MockVectorStore, mock_embedding: MockEmbedding\n):\n    \"\"\"Create a vector memory block.\"\"\"\n    return VectorMemoryBlock(\n        vector_store=mock_vector_store,",
        "detail": "reference_code.llama-index-core.tests.memory.blocks.test_vector",
        "documentation": {}
    },
    {
        "label": "vector_memory_block",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.blocks.test_vector",
        "description": "reference_code.llama-index-core.tests.memory.blocks.test_vector",
        "peekOfCode": "def vector_memory_block(\n    mock_vector_store: MockVectorStore, mock_embedding: MockEmbedding\n):\n    \"\"\"Create a vector memory block.\"\"\"\n    return VectorMemoryBlock(\n        vector_store=mock_vector_store,\n        embed_model=mock_embedding,\n        similarity_top_k=2,\n    )\n@pytest.mark.asyncio",
        "detail": "reference_code.llama-index-core.tests.memory.blocks.test_vector",
        "documentation": {}
    },
    {
        "label": "test_put_get",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "peekOfCode": "def test_put_get() -> None:\n    # Given one message in the memory without limit\n    memory = ChatMemoryBuffer.from_defaults(chat_history=[USER_CHAT_MESSAGE])\n    # When I get the chat history from the memory\n    history = memory.get()\n    # Then the history should contain the message\n    assert len(history) == 1\n    assert history[0].content == USER_CHAT_MESSAGE.content\ndef test_get_when_initial_tokens_less_than_limit_returns_history() -> None:\n    # Given some initial tokens much smaller than token_limit and message tokens",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "documentation": {}
    },
    {
        "label": "test_get_when_initial_tokens_less_than_limit_returns_history",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "peekOfCode": "def test_get_when_initial_tokens_less_than_limit_returns_history() -> None:\n    # Given some initial tokens much smaller than token_limit and message tokens\n    initial_tokens = 5\n    # Given a user message\n    memory = ChatMemoryBuffer.from_defaults(\n        token_limit=1000, chat_history=[USER_CHAT_MESSAGE]\n    )\n    # When I get the chat history from the memory\n    history = memory.get(initial_tokens)\n    # Then the history should contain the message",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "documentation": {}
    },
    {
        "label": "test_get_when_initial_tokens_exceed_limit_raises_value_error",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "peekOfCode": "def test_get_when_initial_tokens_exceed_limit_raises_value_error() -> None:\n    # Given some initial tokens exceeding token_limit\n    initial_tokens = 50\n    memory = ChatMemoryBuffer.from_defaults(token_limit=initial_tokens - 1)\n    # When I get the chat history from the memory\n    with pytest.raises(ValueError) as error:\n        memory.get(initial_token_count=initial_tokens)\n    # Then a value error should be raised\n    assert str(error.value) == \"Initial token count exceeds token limit\"\ndef test_get_when_initial_tokens_same_as_limit_removes_message() -> None:",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "documentation": {}
    },
    {
        "label": "test_get_when_initial_tokens_same_as_limit_removes_message",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "peekOfCode": "def test_get_when_initial_tokens_same_as_limit_removes_message() -> None:\n    # Given some initial tokens equal to the token_limit\n    initial_tokens = 5\n    # Given a user message\n    memory = ChatMemoryBuffer.from_defaults(\n        token_limit=initial_tokens, chat_history=[USER_CHAT_MESSAGE]\n    )\n    # When I get the chat history from the memory\n    history = memory.get(initial_token_count=initial_tokens)\n    # Then the history should be empty",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "documentation": {}
    },
    {
        "label": "test_get_when_space_for_assistant_message_removes_assistant_message_at_start_of_history",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "peekOfCode": "def test_get_when_space_for_assistant_message_removes_assistant_message_at_start_of_history() -> (\n    None\n):\n    # Given some initial tokens equal to the token_limit minus the user message\n    token_limit = 5\n    initial_tokens = token_limit - USER_CHAT_MESSAGE_TOKENS\n    # Given a user message and an assistant answer\n    memory = ChatMemoryBuffer.from_defaults(\n        token_limit=token_limit,\n        chat_history=[USER_CHAT_MESSAGE, ASSISTANT_CHAT_MESSAGE],",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "documentation": {}
    },
    {
        "label": "test_get_when_space_for_second_message_and_answer_removes_only_first_message_and_answer",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "peekOfCode": "def test_get_when_space_for_second_message_and_answer_removes_only_first_message_and_answer() -> (\n    None\n):\n    # Given some initial tokens equal to the token_limit minus one message and one answer\n    token_limit = 5\n    initial_tokens = (\n        token_limit - USER_CHAT_MESSAGE_TOKENS - ASSISTANT_CHAT_MESSAGE_TOKENS\n    )\n    # Given two user messages and two assistant answers\n    memory = ChatMemoryBuffer.from_defaults(",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "documentation": {}
    },
    {
        "label": "test_get_when_space_for_all_but_first_message_removes_first_message_and_answer",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "peekOfCode": "def test_get_when_space_for_all_but_first_message_removes_first_message_and_answer() -> (\n    None\n):\n    # Given some initial tokens equal to the token_limit minus one message and one answer\n    token_limit = 10\n    history_tokens = (\n        ASSISTANT_CHAT_MESSAGE_TOKENS\n        + USER_CHAT_MESSAGE_TOKENS\n        + SECOND_ASSISTANT_CHAT_MESSAGE_TOKENS\n    )",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "documentation": {}
    },
    {
        "label": "test_set",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "peekOfCode": "def test_set() -> None:\n    memory = ChatMemoryBuffer.from_defaults(chat_history=[USER_CHAT_MESSAGE])\n    memory.put(USER_CHAT_MESSAGE)\n    assert len(memory.get()) == 2\n    memory.set([USER_CHAT_MESSAGE])\n    assert len(memory.get()) == 1\ndef test_max_tokens() -> None:\n    memory = ChatMemoryBuffer.from_defaults(\n        chat_history=[USER_CHAT_MESSAGE], token_limit=5\n    )",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "documentation": {}
    },
    {
        "label": "test_max_tokens",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "peekOfCode": "def test_max_tokens() -> None:\n    memory = ChatMemoryBuffer.from_defaults(\n        chat_history=[USER_CHAT_MESSAGE], token_limit=5\n    )\n    memory.put(USER_CHAT_MESSAGE)\n    assert len(memory.get()) == 2\n    # do we limit properly\n    memory.put(USER_CHAT_MESSAGE)\n    memory.put(USER_CHAT_MESSAGE)\n    assert len(memory.get()) == 2",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "documentation": {}
    },
    {
        "label": "test_string_save_load",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "peekOfCode": "def test_string_save_load() -> None:\n    memory = ChatMemoryBuffer.from_defaults(\n        chat_history=[USER_CHAT_MESSAGE], token_limit=5\n    )\n    json_str = memory.to_string()\n    new_memory = ChatMemoryBuffer.from_string(json_str)\n    assert len(new_memory.get()) == 1\n    assert new_memory.token_limit == 5\ndef test_dict_save_load() -> None:\n    memory = ChatMemoryBuffer.from_defaults(",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "documentation": {}
    },
    {
        "label": "test_dict_save_load",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "peekOfCode": "def test_dict_save_load() -> None:\n    memory = ChatMemoryBuffer.from_defaults(\n        chat_history=[USER_CHAT_MESSAGE], token_limit=5\n    )\n    json_dict = memory.to_dict()\n    new_memory = ChatMemoryBuffer.from_dict(json_dict)\n    assert len(new_memory.get()) == 1\n    assert new_memory.token_limit == 5\ndef test_pickle() -> None:\n    \"\"\"Unpickleable tiktoken tokenizer should be circumvented when pickling.\"\"\"",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "documentation": {}
    },
    {
        "label": "test_pickle",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "peekOfCode": "def test_pickle() -> None:\n    \"\"\"Unpickleable tiktoken tokenizer should be circumvented when pickling.\"\"\"\n    memory = ChatMemoryBuffer.from_defaults()\n    bytes_ = pickle.dumps(memory)\n    assert isinstance(pickle.loads(bytes_), ChatMemoryBuffer)",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "peekOfCode": "tokenizer = get_tokenizer()\nUSER_CHAT_MESSAGE = ChatMessage(role=MessageRole.USER, content=\"first message\")\nUSER_CHAT_MESSAGE_TOKENS = len(tokenizer(str(USER_CHAT_MESSAGE.content)))\nSECOND_USER_CHAT_MESSAGE = ChatMessage(role=MessageRole.USER, content=\"second message\")\nSECOND_USER_CHAT_MESSAGE_TOKENS = len(tokenizer(str(SECOND_USER_CHAT_MESSAGE.content)))\nASSISTANT_CHAT_MESSAGE = ChatMessage(role=MessageRole.ASSISTANT, content=\"first answer\")\nASSISTANT_CHAT_MESSAGE_TOKENS = len(tokenizer(str(ASSISTANT_CHAT_MESSAGE.content)))\nSECOND_ASSISTANT_CHAT_MESSAGE = ChatMessage(\n    role=MessageRole.USER, content=\"second answer\"\n)",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "documentation": {}
    },
    {
        "label": "USER_CHAT_MESSAGE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "peekOfCode": "USER_CHAT_MESSAGE = ChatMessage(role=MessageRole.USER, content=\"first message\")\nUSER_CHAT_MESSAGE_TOKENS = len(tokenizer(str(USER_CHAT_MESSAGE.content)))\nSECOND_USER_CHAT_MESSAGE = ChatMessage(role=MessageRole.USER, content=\"second message\")\nSECOND_USER_CHAT_MESSAGE_TOKENS = len(tokenizer(str(SECOND_USER_CHAT_MESSAGE.content)))\nASSISTANT_CHAT_MESSAGE = ChatMessage(role=MessageRole.ASSISTANT, content=\"first answer\")\nASSISTANT_CHAT_MESSAGE_TOKENS = len(tokenizer(str(ASSISTANT_CHAT_MESSAGE.content)))\nSECOND_ASSISTANT_CHAT_MESSAGE = ChatMessage(\n    role=MessageRole.USER, content=\"second answer\"\n)\nSECOND_ASSISTANT_CHAT_MESSAGE_TOKENS = len(",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "documentation": {}
    },
    {
        "label": "USER_CHAT_MESSAGE_TOKENS",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "peekOfCode": "USER_CHAT_MESSAGE_TOKENS = len(tokenizer(str(USER_CHAT_MESSAGE.content)))\nSECOND_USER_CHAT_MESSAGE = ChatMessage(role=MessageRole.USER, content=\"second message\")\nSECOND_USER_CHAT_MESSAGE_TOKENS = len(tokenizer(str(SECOND_USER_CHAT_MESSAGE.content)))\nASSISTANT_CHAT_MESSAGE = ChatMessage(role=MessageRole.ASSISTANT, content=\"first answer\")\nASSISTANT_CHAT_MESSAGE_TOKENS = len(tokenizer(str(ASSISTANT_CHAT_MESSAGE.content)))\nSECOND_ASSISTANT_CHAT_MESSAGE = ChatMessage(\n    role=MessageRole.USER, content=\"second answer\"\n)\nSECOND_ASSISTANT_CHAT_MESSAGE_TOKENS = len(\n    tokenizer(str(SECOND_ASSISTANT_CHAT_MESSAGE.content))",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "documentation": {}
    },
    {
        "label": "SECOND_USER_CHAT_MESSAGE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "peekOfCode": "SECOND_USER_CHAT_MESSAGE = ChatMessage(role=MessageRole.USER, content=\"second message\")\nSECOND_USER_CHAT_MESSAGE_TOKENS = len(tokenizer(str(SECOND_USER_CHAT_MESSAGE.content)))\nASSISTANT_CHAT_MESSAGE = ChatMessage(role=MessageRole.ASSISTANT, content=\"first answer\")\nASSISTANT_CHAT_MESSAGE_TOKENS = len(tokenizer(str(ASSISTANT_CHAT_MESSAGE.content)))\nSECOND_ASSISTANT_CHAT_MESSAGE = ChatMessage(\n    role=MessageRole.USER, content=\"second answer\"\n)\nSECOND_ASSISTANT_CHAT_MESSAGE_TOKENS = len(\n    tokenizer(str(SECOND_ASSISTANT_CHAT_MESSAGE.content))\n)",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "documentation": {}
    },
    {
        "label": "SECOND_USER_CHAT_MESSAGE_TOKENS",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "peekOfCode": "SECOND_USER_CHAT_MESSAGE_TOKENS = len(tokenizer(str(SECOND_USER_CHAT_MESSAGE.content)))\nASSISTANT_CHAT_MESSAGE = ChatMessage(role=MessageRole.ASSISTANT, content=\"first answer\")\nASSISTANT_CHAT_MESSAGE_TOKENS = len(tokenizer(str(ASSISTANT_CHAT_MESSAGE.content)))\nSECOND_ASSISTANT_CHAT_MESSAGE = ChatMessage(\n    role=MessageRole.USER, content=\"second answer\"\n)\nSECOND_ASSISTANT_CHAT_MESSAGE_TOKENS = len(\n    tokenizer(str(SECOND_ASSISTANT_CHAT_MESSAGE.content))\n)\ndef test_put_get() -> None:",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "documentation": {}
    },
    {
        "label": "ASSISTANT_CHAT_MESSAGE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "peekOfCode": "ASSISTANT_CHAT_MESSAGE = ChatMessage(role=MessageRole.ASSISTANT, content=\"first answer\")\nASSISTANT_CHAT_MESSAGE_TOKENS = len(tokenizer(str(ASSISTANT_CHAT_MESSAGE.content)))\nSECOND_ASSISTANT_CHAT_MESSAGE = ChatMessage(\n    role=MessageRole.USER, content=\"second answer\"\n)\nSECOND_ASSISTANT_CHAT_MESSAGE_TOKENS = len(\n    tokenizer(str(SECOND_ASSISTANT_CHAT_MESSAGE.content))\n)\ndef test_put_get() -> None:\n    # Given one message in the memory without limit",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "documentation": {}
    },
    {
        "label": "ASSISTANT_CHAT_MESSAGE_TOKENS",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "peekOfCode": "ASSISTANT_CHAT_MESSAGE_TOKENS = len(tokenizer(str(ASSISTANT_CHAT_MESSAGE.content)))\nSECOND_ASSISTANT_CHAT_MESSAGE = ChatMessage(\n    role=MessageRole.USER, content=\"second answer\"\n)\nSECOND_ASSISTANT_CHAT_MESSAGE_TOKENS = len(\n    tokenizer(str(SECOND_ASSISTANT_CHAT_MESSAGE.content))\n)\ndef test_put_get() -> None:\n    # Given one message in the memory without limit\n    memory = ChatMemoryBuffer.from_defaults(chat_history=[USER_CHAT_MESSAGE])",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "documentation": {}
    },
    {
        "label": "SECOND_ASSISTANT_CHAT_MESSAGE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "peekOfCode": "SECOND_ASSISTANT_CHAT_MESSAGE = ChatMessage(\n    role=MessageRole.USER, content=\"second answer\"\n)\nSECOND_ASSISTANT_CHAT_MESSAGE_TOKENS = len(\n    tokenizer(str(SECOND_ASSISTANT_CHAT_MESSAGE.content))\n)\ndef test_put_get() -> None:\n    # Given one message in the memory without limit\n    memory = ChatMemoryBuffer.from_defaults(chat_history=[USER_CHAT_MESSAGE])\n    # When I get the chat history from the memory",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "documentation": {}
    },
    {
        "label": "SECOND_ASSISTANT_CHAT_MESSAGE_TOKENS",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "peekOfCode": "SECOND_ASSISTANT_CHAT_MESSAGE_TOKENS = len(\n    tokenizer(str(SECOND_ASSISTANT_CHAT_MESSAGE.content))\n)\ndef test_put_get() -> None:\n    # Given one message in the memory without limit\n    memory = ChatMemoryBuffer.from_defaults(chat_history=[USER_CHAT_MESSAGE])\n    # When I get the chat history from the memory\n    history = memory.get()\n    # Then the history should contain the message\n    assert len(history) == 1",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_memory_buffer",
        "documentation": {}
    },
    {
        "label": "MockSummarizerLLM",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "peekOfCode": "class MockSummarizerLLM(MockLLM):\n    _i: int = PrivateAttr()\n    _responses: List[ChatMessage] = PrivateAttr()\n    _role_counts: dict = PrivateAttr()\n    def __init__(self, responses: List[ChatMessage], max_tokens: int = 512) -> None:\n        super().__init__(max_tokens=max_tokens)\n        self._i = 0  # call counter, determines which response to return\n        self._responses = responses  # list of responses to return\n        self._role_counts: dict = dict.fromkeys(MessageRole, 0)\n    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "documentation": {}
    },
    {
        "label": "summarizer_llm",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "peekOfCode": "def summarizer_llm():\n    return MockSummarizerLLM(\n        responses=[\n            ChatMessage(\n                content=FIRST_SUMMARY_RESPONSE,\n                role=MessageRole.ASSISTANT,\n            ),\n            ChatMessage(\n                content=SECOND_SUMMARY_RESPONSE,\n                role=MessageRole.ASSISTANT,",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "documentation": {}
    },
    {
        "label": "test_put_get",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "peekOfCode": "def test_put_get(summarizer_llm) -> None:\n    # Given one message with fewer tokens than token_limit\n    memory = ChatSummaryMemoryBuffer.from_defaults(\n        chat_history=[USER_CHAT_MESSAGE], llm=summarizer_llm\n    )\n    # When I get the chat history from the memory\n    history = memory.get()\n    # Then the history should contain the full message\n    assert len(history) == 1\n    assert history[0].content == USER_CHAT_MESSAGE.content",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "documentation": {}
    },
    {
        "label": "test_put_get_summarize_long_message",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "peekOfCode": "def test_put_get_summarize_long_message(summarizer_llm) -> None:\n    # Given one message with more tokens than token_limit\n    memory = ChatSummaryMemoryBuffer.from_defaults(\n        chat_history=[LONG_USER_CHAT_MESSAGE],\n        token_limit=2,\n        llm=summarizer_llm,\n    )\n    # When I get the chat history from the memory\n    history = memory.get()\n    # Then the history should contain the summarized message",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "documentation": {}
    },
    {
        "label": "test_put_get_summarize_message_with_tool_call",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "peekOfCode": "def test_put_get_summarize_message_with_tool_call(summarizer_llm) -> None:\n    # Given one message with more tokens than token_limit and tool calls\n    # This case test 2 things:\n    #   1. It can summarize the ASSISTANT_TOOL_CALLING_MESSAGE with content=None (Issue #14014).\n    #   2. In `_handle_assistant_and_tool_messages`, when chat_history_full_text only\n    #      contains tool calls or assistant messages, it could add them all into\n    #      `chat_history_to_be_summarized`, without triggering the IndexError.\n    memory = ChatSummaryMemoryBuffer.from_defaults(\n        chat_history=[\n            LONG_USER_CHAT_MESSAGE,",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "documentation": {}
    },
    {
        "label": "test_put_get_summarize_part_of_conversation",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "peekOfCode": "def test_put_get_summarize_part_of_conversation(summarizer_llm) -> None:\n    # Given a chat history where only 2 responses fit in the token_limit\n    tokens_most_recent_messages = sum(\n        [\n            len(tokenizer(str(LONG_RUNNING_CONVERSATION[-i].content)))\n            for i in range(1, 3)\n        ]\n    )\n    memory = ChatSummaryMemoryBuffer.from_defaults(\n        chat_history=LONG_RUNNING_CONVERSATION.copy(),",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "documentation": {}
    },
    {
        "label": "test_get_when_initial_tokens_less_than_limit_returns_history",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "peekOfCode": "def test_get_when_initial_tokens_less_than_limit_returns_history() -> None:\n    # Given some initial tokens much smaller than token_limit and message tokens\n    initial_tokens = 5\n    # Given a user message\n    memory = ChatSummaryMemoryBuffer.from_defaults(\n        token_limit=1000, chat_history=[USER_CHAT_MESSAGE]\n    )\n    # When I get the chat history from the memory\n    history = memory.get(initial_tokens)\n    # Then the history should contain the message",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "documentation": {}
    },
    {
        "label": "test_get_when_initial_tokens_exceed_limit_raises_value_error",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "peekOfCode": "def test_get_when_initial_tokens_exceed_limit_raises_value_error() -> None:\n    # Given some initial tokens exceeding token_limit\n    initial_tokens = 50\n    memory = ChatSummaryMemoryBuffer.from_defaults(\n        chat_history=[USER_CHAT_MESSAGE],\n        token_limit=initial_tokens - 1,\n        count_initial_tokens=True,\n    )\n    # When I get the chat history from the memory\n    with pytest.raises(ValueError) as error:",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "documentation": {}
    },
    {
        "label": "test_set",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "peekOfCode": "def test_set() -> None:\n    memory = ChatSummaryMemoryBuffer.from_defaults(chat_history=[USER_CHAT_MESSAGE])\n    memory.put(USER_CHAT_MESSAGE)\n    assert len(memory.get()) == 2\n    memory.set([USER_CHAT_MESSAGE])\n    assert len(memory.get()) == 1\n@pytest.mark.skipif(not openai_installed, reason=\"OpenAI not installed\")\ndef test_max_tokens_without_summarizer() -> None:\n    memory = ChatSummaryMemoryBuffer.from_defaults(\n        chat_history=[USER_CHAT_MESSAGE], token_limit=5",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "documentation": {}
    },
    {
        "label": "test_max_tokens_without_summarizer",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "peekOfCode": "def test_max_tokens_without_summarizer() -> None:\n    memory = ChatSummaryMemoryBuffer.from_defaults(\n        chat_history=[USER_CHAT_MESSAGE], token_limit=5\n    )\n    memory.put(USER_CHAT_MESSAGE)\n    assert len(memory.get()) == 2\n    # do we limit properly\n    memory.put(USER_CHAT_MESSAGE)\n    memory.put(USER_CHAT_MESSAGE)\n    assert len(memory.get()) == 2",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "documentation": {}
    },
    {
        "label": "test_max_tokens_with_summarizer",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "peekOfCode": "def test_max_tokens_with_summarizer(summarizer_llm) -> None:\n    max_tokens = 1\n    summarizer_llm.set_max_tokens(max_tokens)\n    memory = ChatSummaryMemoryBuffer.from_defaults(\n        llm=summarizer_llm,\n        chat_history=[USER_CHAT_MESSAGE],\n        token_limit=5,\n    )\n    # do we limit properly\n    memory.put(USER_CHAT_MESSAGE)",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "documentation": {}
    },
    {
        "label": "test_assistant_never_first_message",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "peekOfCode": "def test_assistant_never_first_message(summarizer_llm) -> None:\n    chat_history = [\n        USER_CHAT_MESSAGE,\n        ASSISTANT_CHAT_MESSAGE,\n        USER_CHAT_MESSAGE,\n        ASSISTANT_CHAT_MESSAGE,\n    ]\n    tokens_last_3_messages = sum(\n        [len(tokenizer(str(chat_history[-i].content))) for i in range(1, 4)]\n    )",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "documentation": {}
    },
    {
        "label": "test_assistant_tool_pairs",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "peekOfCode": "def test_assistant_tool_pairs(summarizer_llm) -> None:\n    chat_history = [\n        USER_CHAT_MESSAGE,\n        ASSISTANT_CHAT_MESSAGE,\n        TOOL_CHAT_MESSAGE,\n        USER_CHAT_MESSAGE,\n        ASSISTANT_CHAT_MESSAGE,\n    ]\n    tokens_last_3_messages = sum(\n        [len(tokenizer(str(chat_history[-i].content))) for i in range(1, 4)]",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "documentation": {}
    },
    {
        "label": "test_string_save_load",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "peekOfCode": "def test_string_save_load(summarizer_llm) -> None:\n    memory = ChatSummaryMemoryBuffer.from_defaults(\n        llm=summarizer_llm,\n        chat_history=[USER_CHAT_MESSAGE],\n        token_limit=5,\n        summarize_prompt=\"Mock summary\",\n        count_initial_tokens=True,\n    )\n    json_str = memory.to_string()\n    new_memory = ChatSummaryMemoryBuffer.from_string(json_str)",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "documentation": {}
    },
    {
        "label": "test_dict_save_load",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "peekOfCode": "def test_dict_save_load(summarizer_llm) -> None:\n    memory = ChatSummaryMemoryBuffer.from_defaults(\n        llm=summarizer_llm,\n        chat_history=[USER_CHAT_MESSAGE],\n        token_limit=5,\n        summarize_prompt=\"Mock summary\",\n        count_initial_tokens=True,\n    )\n    json_dict = memory.to_dict()\n    new_memory = ChatSummaryMemoryBuffer.from_dict(json_dict)",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "documentation": {}
    },
    {
        "label": "test_pickle",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "peekOfCode": "def test_pickle() -> None:\n    \"\"\"Unpickleable tiktoken tokenizer should be circumvented when pickling.\"\"\"\n    memory = ChatSummaryMemoryBuffer.from_defaults()\n    bytes_ = pickle.dumps(memory)\n    assert isinstance(pickle.loads(bytes_), ChatSummaryMemoryBuffer)",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "peekOfCode": "tokenizer = get_tokenizer()\ndef _get_role_alternating_order(i: int):\n    if i % 2 == 0:\n        return MessageRole.USER\n    return MessageRole.ASSISTANT\ntry:\n    from openai.types.chat.chat_completion_message_tool_call import (\n        ChatCompletionMessageToolCall,\n        Function,\n    )",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "documentation": {}
    },
    {
        "label": "FIRST_SUMMARY_RESPONSE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "peekOfCode": "FIRST_SUMMARY_RESPONSE = \"First, the user asked what an LLM was, and the assistant explained the basic ideas.\"\nSECOND_SUMMARY_RESPONSE = (\n    \"The conversation started about LLMs. It then continued about LlamaIndex.\"\n)\n@pytest.fixture()\ndef summarizer_llm():\n    return MockSummarizerLLM(\n        responses=[\n            ChatMessage(\n                content=FIRST_SUMMARY_RESPONSE,",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "documentation": {}
    },
    {
        "label": "SECOND_SUMMARY_RESPONSE",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "description": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "peekOfCode": "SECOND_SUMMARY_RESPONSE = (\n    \"The conversation started about LLMs. It then continued about LlamaIndex.\"\n)\n@pytest.fixture()\ndef summarizer_llm():\n    return MockSummarizerLLM(\n        responses=[\n            ChatMessage(\n                content=FIRST_SUMMARY_RESPONSE,\n                role=MessageRole.ASSISTANT,",
        "detail": "reference_code.llama-index-core.tests.memory.test_chat_summary_memory_buffer",
        "documentation": {}
    },
    {
        "label": "memory",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_memory_base",
        "description": "reference_code.llama-index-core.tests.memory.test_memory_base",
        "peekOfCode": "def memory():\n    \"\"\"Create a basic memory instance for testing.\"\"\"\n    return Memory(\n        token_limit=1000,\n        token_flush_size=700,\n        chat_history_token_ratio=0.9,\n        session_id=\"test_user\",\n    )\n@pytest.mark.asyncio\nasync def test_initialization(memory):",
        "detail": "reference_code.llama-index-core.tests.memory.test_memory_base",
        "documentation": {}
    },
    {
        "label": "TextMemoryBlock",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.memory.test_memory_blocks_base",
        "description": "reference_code.llama-index-core.tests.memory.test_memory_blocks_base",
        "peekOfCode": "class TextMemoryBlock(BaseMemoryBlock[str]):\n    \"\"\"Memory block that returns text content.\"\"\"\n    async def _aget(self, messages: List[ChatMessage], **kwargs: Any) -> str:\n        return \"Simple text content from TextMemoryBlock\"\n    async def _aput(self, messages: List[ChatMessage]) -> None:\n        # Just a no-op for testing\n        pass\nclass ContentBlocksMemoryBlock(BaseMemoryBlock[List[ContentBlock]]):\n    \"\"\"Memory block that returns content blocks.\"\"\"\n    async def _aget(",
        "detail": "reference_code.llama-index-core.tests.memory.test_memory_blocks_base",
        "documentation": {}
    },
    {
        "label": "ContentBlocksMemoryBlock",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.memory.test_memory_blocks_base",
        "description": "reference_code.llama-index-core.tests.memory.test_memory_blocks_base",
        "peekOfCode": "class ContentBlocksMemoryBlock(BaseMemoryBlock[List[ContentBlock]]):\n    \"\"\"Memory block that returns content blocks.\"\"\"\n    async def _aget(\n        self, messages: List[ChatMessage], **kwargs: Any\n    ) -> List[ContentBlock]:\n        return [\n            TextBlock(text=\"Text block 1\"),\n            TextBlock(text=\"Text block 2\"),\n        ]\n    async def _aput(self, messages: List[ChatMessage]) -> None:",
        "detail": "reference_code.llama-index-core.tests.memory.test_memory_blocks_base",
        "documentation": {}
    },
    {
        "label": "ChatMessagesMemoryBlock",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.memory.test_memory_blocks_base",
        "description": "reference_code.llama-index-core.tests.memory.test_memory_blocks_base",
        "peekOfCode": "class ChatMessagesMemoryBlock(BaseMemoryBlock[List[ChatMessage]]):\n    \"\"\"Memory block that returns chat messages.\"\"\"\n    async def _aget(\n        self, messages: List[ChatMessage], **kwargs: Any\n    ) -> List[ChatMessage]:\n        return [\n            ChatMessage(role=\"user\", content=\"Historical user message\"),\n            ChatMessage(role=\"assistant\", content=\"Historical assistant response\"),\n        ]\n    async def _aput(self, messages: List[ChatMessage]) -> None:",
        "detail": "reference_code.llama-index-core.tests.memory.test_memory_blocks_base",
        "documentation": {}
    },
    {
        "label": "ComplexMemoryBlock",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.memory.test_memory_blocks_base",
        "description": "reference_code.llama-index-core.tests.memory.test_memory_blocks_base",
        "peekOfCode": "class ComplexMemoryBlock(BaseMemoryBlock[Union[str, List[ContentBlock]]]):\n    \"\"\"Memory block that can return different types based on input.\"\"\"\n    mode: str = \"text\"  # Can be \"text\" or \"blocks\"\n    async def _aget(\n        self, messages: List[ChatMessage], **kwargs: Any\n    ) -> Union[str, List[ContentBlock]]:\n        if self.mode == \"text\":\n            return \"Text content from ComplexMemoryBlock\"\n        else:\n            return [",
        "detail": "reference_code.llama-index-core.tests.memory.test_memory_blocks_base",
        "documentation": {}
    },
    {
        "label": "ParameterizedMemoryBlock",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.memory.test_memory_blocks_base",
        "description": "reference_code.llama-index-core.tests.memory.test_memory_blocks_base",
        "peekOfCode": "class ParameterizedMemoryBlock(BaseMemoryBlock[str]):\n    \"\"\"Memory block that uses parameters passed to aget.\"\"\"\n    async def _aget(self, messages: List[ChatMessage], **kwargs: Any) -> str:\n        # Use parameters passed to aget\n        parameter = kwargs.get(\"test_parameter\", \"default\")\n        return f\"Parameter value: {parameter}\"\n    async def _aput(self, messages: List[ChatMessage]) -> None:\n        # Just a no-op for testing\n        pass\n@pytest.fixture()",
        "detail": "reference_code.llama-index-core.tests.memory.test_memory_blocks_base",
        "documentation": {}
    },
    {
        "label": "memory_with_blocks",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.memory.test_memory_blocks_base",
        "description": "reference_code.llama-index-core.tests.memory.test_memory_blocks_base",
        "peekOfCode": "def memory_with_blocks():\n    \"\"\"Set up memory with different types of memory blocks.\"\"\"\n    return Memory(\n        token_limit=1000,\n        token_flush_size=700,\n        chat_history_token_ratio=0.9,\n        session_id=\"test_blocks\",\n        memory_blocks=[\n            TextMemoryBlock(name=\"text_block\", priority=1),\n            ContentBlocksMemoryBlock(name=\"content_blocks\", priority=2),",
        "detail": "reference_code.llama-index-core.tests.memory.test_memory_blocks_base",
        "documentation": {}
    },
    {
        "label": "mock_llmpredictor_predict",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_predict",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_predict",
        "peekOfCode": "def mock_llmpredictor_predict(prompt: BasePromptTemplate, **prompt_args: Any) -> str:\n    \"\"\"\n    Mock predict method of LLMPredictor.\n    Depending on the prompt, return response.\n    \"\"\"\n    full_prompt_args = {\n        **prompt.kwargs,\n        **prompt_args,\n    }\n    prompt_type = prompt.metadata[\"prompt_type\"]",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_predict",
        "documentation": {}
    },
    {
        "label": "patch_llmpredictor_predict",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_predict",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_predict",
        "peekOfCode": "def patch_llmpredictor_predict(\n    self: Any, prompt: BasePromptTemplate, **prompt_args: Any\n) -> str:\n    \"\"\"\n    Mock predict method of LLMPredictor.\n    Depending on the prompt, return response.\n    \"\"\"\n    return mock_llmpredictor_predict(prompt, **prompt_args)\nasync def patch_llmpredictor_apredict(\n    self: Any, prompt: BasePromptTemplate, **prompt_args: Any",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_predict",
        "documentation": {}
    },
    {
        "label": "MOCK_SUMMARY_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_SUMMARY_PROMPT_TMPL = \"{context_str}\\n\"\nMOCK_SUMMARY_PROMPT = PromptTemplate(\n    MOCK_SUMMARY_PROMPT_TMPL, prompt_type=PromptType.SUMMARY\n)\nMOCK_INSERT_PROMPT_TMPL = \"{num_chunks}\\n{context_list}{new_chunk_text}\\n\"\nMOCK_INSERT_PROMPT = PromptTemplate(\n    MOCK_INSERT_PROMPT_TMPL, prompt_type=PromptType.TREE_INSERT\n)\n# # single choice\nMOCK_QUERY_PROMPT_TMPL = \"{num_chunks}\\n{context_list}\\n{query_str}'\\n\"",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_SUMMARY_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_SUMMARY_PROMPT = PromptTemplate(\n    MOCK_SUMMARY_PROMPT_TMPL, prompt_type=PromptType.SUMMARY\n)\nMOCK_INSERT_PROMPT_TMPL = \"{num_chunks}\\n{context_list}{new_chunk_text}\\n\"\nMOCK_INSERT_PROMPT = PromptTemplate(\n    MOCK_INSERT_PROMPT_TMPL, prompt_type=PromptType.TREE_INSERT\n)\n# # single choice\nMOCK_QUERY_PROMPT_TMPL = \"{num_chunks}\\n{context_list}\\n{query_str}'\\n\"\nMOCK_QUERY_PROMPT = PromptTemplate(",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_INSERT_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_INSERT_PROMPT_TMPL = \"{num_chunks}\\n{context_list}{new_chunk_text}\\n\"\nMOCK_INSERT_PROMPT = PromptTemplate(\n    MOCK_INSERT_PROMPT_TMPL, prompt_type=PromptType.TREE_INSERT\n)\n# # single choice\nMOCK_QUERY_PROMPT_TMPL = \"{num_chunks}\\n{context_list}\\n{query_str}'\\n\"\nMOCK_QUERY_PROMPT = PromptTemplate(\n    MOCK_QUERY_PROMPT_TMPL, prompt_type=PromptType.TREE_SELECT\n)\nMOCK_REFINE_PROMPT_TMPL = \"{query_str}\\n{existing_answer}\\n{context_msg}\\n\"",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_INSERT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_INSERT_PROMPT = PromptTemplate(\n    MOCK_INSERT_PROMPT_TMPL, prompt_type=PromptType.TREE_INSERT\n)\n# # single choice\nMOCK_QUERY_PROMPT_TMPL = \"{num_chunks}\\n{context_list}\\n{query_str}'\\n\"\nMOCK_QUERY_PROMPT = PromptTemplate(\n    MOCK_QUERY_PROMPT_TMPL, prompt_type=PromptType.TREE_SELECT\n)\nMOCK_REFINE_PROMPT_TMPL = \"{query_str}\\n{existing_answer}\\n{context_msg}\\n\"\nMOCK_REFINE_PROMPT = PromptTemplate(",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_QUERY_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_QUERY_PROMPT_TMPL = \"{num_chunks}\\n{context_list}\\n{query_str}'\\n\"\nMOCK_QUERY_PROMPT = PromptTemplate(\n    MOCK_QUERY_PROMPT_TMPL, prompt_type=PromptType.TREE_SELECT\n)\nMOCK_REFINE_PROMPT_TMPL = \"{query_str}\\n{existing_answer}\\n{context_msg}\\n\"\nMOCK_REFINE_PROMPT = PromptTemplate(\n    MOCK_REFINE_PROMPT_TMPL, prompt_type=PromptType.REFINE\n)\nMOCK_TEXT_QA_PROMPT_TMPL = \"{context_str}\\n{query_str}\\n\"\nMOCK_TEXT_QA_PROMPT = PromptTemplate(",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_QUERY_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_QUERY_PROMPT = PromptTemplate(\n    MOCK_QUERY_PROMPT_TMPL, prompt_type=PromptType.TREE_SELECT\n)\nMOCK_REFINE_PROMPT_TMPL = \"{query_str}\\n{existing_answer}\\n{context_msg}\\n\"\nMOCK_REFINE_PROMPT = PromptTemplate(\n    MOCK_REFINE_PROMPT_TMPL, prompt_type=PromptType.REFINE\n)\nMOCK_TEXT_QA_PROMPT_TMPL = \"{context_str}\\n{query_str}\\n\"\nMOCK_TEXT_QA_PROMPT = PromptTemplate(\n    MOCK_TEXT_QA_PROMPT_TMPL, prompt_type=PromptType.QUESTION_ANSWER",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_REFINE_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_REFINE_PROMPT_TMPL = \"{query_str}\\n{existing_answer}\\n{context_msg}\\n\"\nMOCK_REFINE_PROMPT = PromptTemplate(\n    MOCK_REFINE_PROMPT_TMPL, prompt_type=PromptType.REFINE\n)\nMOCK_TEXT_QA_PROMPT_TMPL = \"{context_str}\\n{query_str}\\n\"\nMOCK_TEXT_QA_PROMPT = PromptTemplate(\n    MOCK_TEXT_QA_PROMPT_TMPL, prompt_type=PromptType.QUESTION_ANSWER\n)\nMOCK_KEYWORD_EXTRACT_PROMPT_TMPL = \"{max_keywords}\\n{text}\\n\"\nMOCK_KEYWORD_EXTRACT_PROMPT = PromptTemplate(",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_REFINE_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_REFINE_PROMPT = PromptTemplate(\n    MOCK_REFINE_PROMPT_TMPL, prompt_type=PromptType.REFINE\n)\nMOCK_TEXT_QA_PROMPT_TMPL = \"{context_str}\\n{query_str}\\n\"\nMOCK_TEXT_QA_PROMPT = PromptTemplate(\n    MOCK_TEXT_QA_PROMPT_TMPL, prompt_type=PromptType.QUESTION_ANSWER\n)\nMOCK_KEYWORD_EXTRACT_PROMPT_TMPL = \"{max_keywords}\\n{text}\\n\"\nMOCK_KEYWORD_EXTRACT_PROMPT = PromptTemplate(\n    MOCK_KEYWORD_EXTRACT_PROMPT_TMPL, prompt_type=PromptType.KEYWORD_EXTRACT",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_TEXT_QA_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_TEXT_QA_PROMPT_TMPL = \"{context_str}\\n{query_str}\\n\"\nMOCK_TEXT_QA_PROMPT = PromptTemplate(\n    MOCK_TEXT_QA_PROMPT_TMPL, prompt_type=PromptType.QUESTION_ANSWER\n)\nMOCK_KEYWORD_EXTRACT_PROMPT_TMPL = \"{max_keywords}\\n{text}\\n\"\nMOCK_KEYWORD_EXTRACT_PROMPT = PromptTemplate(\n    MOCK_KEYWORD_EXTRACT_PROMPT_TMPL, prompt_type=PromptType.KEYWORD_EXTRACT\n)\n# TODO: consolidate with keyword extract\nMOCK_QUERY_KEYWORD_EXTRACT_PROMPT_TMPL = \"{max_keywords}\\n{question}\\n\"",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_TEXT_QA_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_TEXT_QA_PROMPT = PromptTemplate(\n    MOCK_TEXT_QA_PROMPT_TMPL, prompt_type=PromptType.QUESTION_ANSWER\n)\nMOCK_KEYWORD_EXTRACT_PROMPT_TMPL = \"{max_keywords}\\n{text}\\n\"\nMOCK_KEYWORD_EXTRACT_PROMPT = PromptTemplate(\n    MOCK_KEYWORD_EXTRACT_PROMPT_TMPL, prompt_type=PromptType.KEYWORD_EXTRACT\n)\n# TODO: consolidate with keyword extract\nMOCK_QUERY_KEYWORD_EXTRACT_PROMPT_TMPL = \"{max_keywords}\\n{question}\\n\"\nMOCK_QUERY_KEYWORD_EXTRACT_PROMPT = PromptTemplate(",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_KEYWORD_EXTRACT_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_KEYWORD_EXTRACT_PROMPT_TMPL = \"{max_keywords}\\n{text}\\n\"\nMOCK_KEYWORD_EXTRACT_PROMPT = PromptTemplate(\n    MOCK_KEYWORD_EXTRACT_PROMPT_TMPL, prompt_type=PromptType.KEYWORD_EXTRACT\n)\n# TODO: consolidate with keyword extract\nMOCK_QUERY_KEYWORD_EXTRACT_PROMPT_TMPL = \"{max_keywords}\\n{question}\\n\"\nMOCK_QUERY_KEYWORD_EXTRACT_PROMPT = PromptTemplate(\n    MOCK_QUERY_KEYWORD_EXTRACT_PROMPT_TMPL, prompt_type=PromptType.QUERY_KEYWORD_EXTRACT\n)\nMOCK_SCHEMA_EXTRACT_PROMPT_TMPL = \"{text}\\n{schema}\"",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_KEYWORD_EXTRACT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_KEYWORD_EXTRACT_PROMPT = PromptTemplate(\n    MOCK_KEYWORD_EXTRACT_PROMPT_TMPL, prompt_type=PromptType.KEYWORD_EXTRACT\n)\n# TODO: consolidate with keyword extract\nMOCK_QUERY_KEYWORD_EXTRACT_PROMPT_TMPL = \"{max_keywords}\\n{question}\\n\"\nMOCK_QUERY_KEYWORD_EXTRACT_PROMPT = PromptTemplate(\n    MOCK_QUERY_KEYWORD_EXTRACT_PROMPT_TMPL, prompt_type=PromptType.QUERY_KEYWORD_EXTRACT\n)\nMOCK_SCHEMA_EXTRACT_PROMPT_TMPL = \"{text}\\n{schema}\"\nMOCK_SCHEMA_EXTRACT_PROMPT = PromptTemplate(",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_QUERY_KEYWORD_EXTRACT_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_QUERY_KEYWORD_EXTRACT_PROMPT_TMPL = \"{max_keywords}\\n{question}\\n\"\nMOCK_QUERY_KEYWORD_EXTRACT_PROMPT = PromptTemplate(\n    MOCK_QUERY_KEYWORD_EXTRACT_PROMPT_TMPL, prompt_type=PromptType.QUERY_KEYWORD_EXTRACT\n)\nMOCK_SCHEMA_EXTRACT_PROMPT_TMPL = \"{text}\\n{schema}\"\nMOCK_SCHEMA_EXTRACT_PROMPT = PromptTemplate(\n    MOCK_SCHEMA_EXTRACT_PROMPT_TMPL, prompt_type=PromptType.SCHEMA_EXTRACT\n)\nMOCK_TEXT_TO_SQL_PROMPT_TMPL = \"{dialect}\\n{schema}\\n{query_str}\"\nMOCK_TEXT_TO_SQL_PROMPT = PromptTemplate(",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_QUERY_KEYWORD_EXTRACT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_QUERY_KEYWORD_EXTRACT_PROMPT = PromptTemplate(\n    MOCK_QUERY_KEYWORD_EXTRACT_PROMPT_TMPL, prompt_type=PromptType.QUERY_KEYWORD_EXTRACT\n)\nMOCK_SCHEMA_EXTRACT_PROMPT_TMPL = \"{text}\\n{schema}\"\nMOCK_SCHEMA_EXTRACT_PROMPT = PromptTemplate(\n    MOCK_SCHEMA_EXTRACT_PROMPT_TMPL, prompt_type=PromptType.SCHEMA_EXTRACT\n)\nMOCK_TEXT_TO_SQL_PROMPT_TMPL = \"{dialect}\\n{schema}\\n{query_str}\"\nMOCK_TEXT_TO_SQL_PROMPT = PromptTemplate(\n    MOCK_TEXT_TO_SQL_PROMPT_TMPL, prompt_type=PromptType.TEXT_TO_SQL",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_SCHEMA_EXTRACT_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_SCHEMA_EXTRACT_PROMPT_TMPL = \"{text}\\n{schema}\"\nMOCK_SCHEMA_EXTRACT_PROMPT = PromptTemplate(\n    MOCK_SCHEMA_EXTRACT_PROMPT_TMPL, prompt_type=PromptType.SCHEMA_EXTRACT\n)\nMOCK_TEXT_TO_SQL_PROMPT_TMPL = \"{dialect}\\n{schema}\\n{query_str}\"\nMOCK_TEXT_TO_SQL_PROMPT = PromptTemplate(\n    MOCK_TEXT_TO_SQL_PROMPT_TMPL, prompt_type=PromptType.TEXT_TO_SQL\n)\nMOCK_TABLE_CONTEXT_PROMPT_TMPL = \"{schema}\\n{context_str}\\n{query_str}\"\nMOCK_TABLE_CONTEXT_PROMPT = PromptTemplate(",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_SCHEMA_EXTRACT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_SCHEMA_EXTRACT_PROMPT = PromptTemplate(\n    MOCK_SCHEMA_EXTRACT_PROMPT_TMPL, prompt_type=PromptType.SCHEMA_EXTRACT\n)\nMOCK_TEXT_TO_SQL_PROMPT_TMPL = \"{dialect}\\n{schema}\\n{query_str}\"\nMOCK_TEXT_TO_SQL_PROMPT = PromptTemplate(\n    MOCK_TEXT_TO_SQL_PROMPT_TMPL, prompt_type=PromptType.TEXT_TO_SQL\n)\nMOCK_TABLE_CONTEXT_PROMPT_TMPL = \"{schema}\\n{context_str}\\n{query_str}\"\nMOCK_TABLE_CONTEXT_PROMPT = PromptTemplate(\n    MOCK_TABLE_CONTEXT_PROMPT_TMPL, prompt_type=PromptType.TABLE_CONTEXT",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_TEXT_TO_SQL_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_TEXT_TO_SQL_PROMPT_TMPL = \"{dialect}\\n{schema}\\n{query_str}\"\nMOCK_TEXT_TO_SQL_PROMPT = PromptTemplate(\n    MOCK_TEXT_TO_SQL_PROMPT_TMPL, prompt_type=PromptType.TEXT_TO_SQL\n)\nMOCK_TABLE_CONTEXT_PROMPT_TMPL = \"{schema}\\n{context_str}\\n{query_str}\"\nMOCK_TABLE_CONTEXT_PROMPT = PromptTemplate(\n    MOCK_TABLE_CONTEXT_PROMPT_TMPL, prompt_type=PromptType.TABLE_CONTEXT\n)\nMOCK_KG_TRIPLET_EXTRACT_PROMPT_TMPL = \"{max_knowledge_triplets}\\n{text}\"\nMOCK_KG_TRIPLET_EXTRACT_PROMPT = PromptTemplate(",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_TEXT_TO_SQL_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_TEXT_TO_SQL_PROMPT = PromptTemplate(\n    MOCK_TEXT_TO_SQL_PROMPT_TMPL, prompt_type=PromptType.TEXT_TO_SQL\n)\nMOCK_TABLE_CONTEXT_PROMPT_TMPL = \"{schema}\\n{context_str}\\n{query_str}\"\nMOCK_TABLE_CONTEXT_PROMPT = PromptTemplate(\n    MOCK_TABLE_CONTEXT_PROMPT_TMPL, prompt_type=PromptType.TABLE_CONTEXT\n)\nMOCK_KG_TRIPLET_EXTRACT_PROMPT_TMPL = \"{max_knowledge_triplets}\\n{text}\"\nMOCK_KG_TRIPLET_EXTRACT_PROMPT = PromptTemplate(\n    MOCK_KG_TRIPLET_EXTRACT_PROMPT_TMPL,",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_TABLE_CONTEXT_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_TABLE_CONTEXT_PROMPT_TMPL = \"{schema}\\n{context_str}\\n{query_str}\"\nMOCK_TABLE_CONTEXT_PROMPT = PromptTemplate(\n    MOCK_TABLE_CONTEXT_PROMPT_TMPL, prompt_type=PromptType.TABLE_CONTEXT\n)\nMOCK_KG_TRIPLET_EXTRACT_PROMPT_TMPL = \"{max_knowledge_triplets}\\n{text}\"\nMOCK_KG_TRIPLET_EXTRACT_PROMPT = PromptTemplate(\n    MOCK_KG_TRIPLET_EXTRACT_PROMPT_TMPL,\n    prompt_type=PromptType.KNOWLEDGE_TRIPLET_EXTRACT,\n)\nMOCK_INPUT_PROMPT_TMPL = \"{query_str}\"",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_TABLE_CONTEXT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_TABLE_CONTEXT_PROMPT = PromptTemplate(\n    MOCK_TABLE_CONTEXT_PROMPT_TMPL, prompt_type=PromptType.TABLE_CONTEXT\n)\nMOCK_KG_TRIPLET_EXTRACT_PROMPT_TMPL = \"{max_knowledge_triplets}\\n{text}\"\nMOCK_KG_TRIPLET_EXTRACT_PROMPT = PromptTemplate(\n    MOCK_KG_TRIPLET_EXTRACT_PROMPT_TMPL,\n    prompt_type=PromptType.KNOWLEDGE_TRIPLET_EXTRACT,\n)\nMOCK_INPUT_PROMPT_TMPL = \"{query_str}\"\nMOCK_INPUT_PROMPT = PromptTemplate(",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_KG_TRIPLET_EXTRACT_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_KG_TRIPLET_EXTRACT_PROMPT_TMPL = \"{max_knowledge_triplets}\\n{text}\"\nMOCK_KG_TRIPLET_EXTRACT_PROMPT = PromptTemplate(\n    MOCK_KG_TRIPLET_EXTRACT_PROMPT_TMPL,\n    prompt_type=PromptType.KNOWLEDGE_TRIPLET_EXTRACT,\n)\nMOCK_INPUT_PROMPT_TMPL = \"{query_str}\"\nMOCK_INPUT_PROMPT = PromptTemplate(\n    MOCK_INPUT_PROMPT_TMPL, prompt_type=PromptType.SIMPLE_INPUT\n)\nMOCK_PANDAS_PROMPT_TMPL = \"{query_str}\\n{df_str}\\n{instruction_str}\"",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_KG_TRIPLET_EXTRACT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_KG_TRIPLET_EXTRACT_PROMPT = PromptTemplate(\n    MOCK_KG_TRIPLET_EXTRACT_PROMPT_TMPL,\n    prompt_type=PromptType.KNOWLEDGE_TRIPLET_EXTRACT,\n)\nMOCK_INPUT_PROMPT_TMPL = \"{query_str}\"\nMOCK_INPUT_PROMPT = PromptTemplate(\n    MOCK_INPUT_PROMPT_TMPL, prompt_type=PromptType.SIMPLE_INPUT\n)\nMOCK_PANDAS_PROMPT_TMPL = \"{query_str}\\n{df_str}\\n{instruction_str}\"\nMOCK_PANDAS_PROMPT = PromptTemplate(",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_INPUT_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_INPUT_PROMPT_TMPL = \"{query_str}\"\nMOCK_INPUT_PROMPT = PromptTemplate(\n    MOCK_INPUT_PROMPT_TMPL, prompt_type=PromptType.SIMPLE_INPUT\n)\nMOCK_PANDAS_PROMPT_TMPL = \"{query_str}\\n{df_str}\\n{instruction_str}\"\nMOCK_PANDAS_PROMPT = PromptTemplate(\n    MOCK_PANDAS_PROMPT_TMPL, prompt_type=PromptType.PANDAS\n)",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_INPUT_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_INPUT_PROMPT = PromptTemplate(\n    MOCK_INPUT_PROMPT_TMPL, prompt_type=PromptType.SIMPLE_INPUT\n)\nMOCK_PANDAS_PROMPT_TMPL = \"{query_str}\\n{df_str}\\n{instruction_str}\"\nMOCK_PANDAS_PROMPT = PromptTemplate(\n    MOCK_PANDAS_PROMPT_TMPL, prompt_type=PromptType.PANDAS\n)",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_PANDAS_PROMPT_TMPL",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_PANDAS_PROMPT_TMPL = \"{query_str}\\n{df_str}\\n{instruction_str}\"\nMOCK_PANDAS_PROMPT = PromptTemplate(\n    MOCK_PANDAS_PROMPT_TMPL, prompt_type=PromptType.PANDAS\n)",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "MOCK_PANDAS_PROMPT",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "peekOfCode": "MOCK_PANDAS_PROMPT = PromptTemplate(\n    MOCK_PANDAS_PROMPT_TMPL, prompt_type=PromptType.PANDAS\n)",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_prompts",
        "documentation": {}
    },
    {
        "label": "patch_token_splitter_newline",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_text_splitter",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_text_splitter",
        "peekOfCode": "def patch_token_splitter_newline(\n    self: Any, text: str, metadata_str: Optional[str] = None\n) -> List[str]:\n    \"\"\"Mock token splitter by newline.\"\"\"\n    if text == \"\":\n        return []\n    return text.split(\"\\n\")\ndef mock_token_splitter_newline(\n    text: str, metadata_str: Optional[str] = None\n) -> List[str]:",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_text_splitter",
        "documentation": {}
    },
    {
        "label": "mock_token_splitter_newline",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_text_splitter",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_text_splitter",
        "peekOfCode": "def mock_token_splitter_newline(\n    text: str, metadata_str: Optional[str] = None\n) -> List[str]:\n    \"\"\"Mock token splitter by newline.\"\"\"\n    if text == \"\":\n        return []\n    return text.split(\"\\n\")",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_text_splitter",
        "documentation": {}
    },
    {
        "label": "mock_tokenizer",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_utils",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_utils",
        "peekOfCode": "def mock_tokenizer(text: str) -> List[str]:\n    \"\"\"Mock tokenizer.\"\"\"\n    tokens = re.split(r\"[ \\n]\", text)  # split by space or newline\n    result = []\n    for token in tokens:\n        if token.strip() == \"\":\n            continue\n        result.append(token.strip())\n    return result\ndef mock_extract_keywords(",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_utils",
        "documentation": {}
    },
    {
        "label": "mock_extract_keywords",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_utils",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_utils",
        "peekOfCode": "def mock_extract_keywords(\n    text_chunk: str, max_keywords: Optional[int] = None, filter_stopwords: bool = True\n) -> Set[str]:\n    \"\"\"\n    Extract keywords (mock).\n    Same as simple_extract_keywords but without filtering stopwords.\n    \"\"\"\n    return simple_extract_keywords(\n        text_chunk, max_keywords=max_keywords, filter_stopwords=False\n    )",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_utils",
        "documentation": {}
    },
    {
        "label": "mock_extract_keywords_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_utils",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_utils",
        "peekOfCode": "def mock_extract_keywords_response(\n    text_chunk: str, max_keywords: Optional[int] = None, filter_stopwords: bool = True\n) -> str:\n    \"\"\"\n    Extract keywords mock response.\n    Same as simple_extract_keywords but without filtering stopwords.\n    \"\"\"\n    return \",\".join(\n        simple_extract_keywords(\n            text_chunk, max_keywords=max_keywords, filter_stopwords=False",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_utils",
        "documentation": {}
    },
    {
        "label": "mock_extract_kg_triplets_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.mock_utils.mock_utils",
        "description": "reference_code.llama-index-core.tests.mock_utils.mock_utils",
        "peekOfCode": "def mock_extract_kg_triplets_response(\n    text_chunk: str, max_triplets: Optional[int] = None\n) -> str:\n    \"\"\"Generate 1 or more fake triplets.\"\"\"\n    response = \"\"\n    if max_triplets is not None:\n        for i in range(max_triplets):\n            response += \"(This is, a mock, triplet)\\n\"\n    else:\n        response += \"(This is, a mock, triplet)\\n\"",
        "detail": "reference_code.llama-index-core.tests.mock_utils.mock_utils",
        "documentation": {}
    },
    {
        "label": "TestMultiModalLLMMetadata",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.multi_modal_llms.test_base_multi_modal_llm_metadata",
        "description": "reference_code.llama-index-core.tests.multi_modal_llms.test_base_multi_modal_llm_metadata",
        "peekOfCode": "class TestMultiModalLLMMetadata:\n    def test_default_values(self):\n        metadata = MultiModalLLMMetadata()\n        assert metadata.model_name == \"unknown\"\n        assert metadata.is_chat_model is False\n        assert metadata.is_function_calling_model is False\n        assert metadata.context_window is not None\n        assert metadata.num_output is not None\n        assert metadata.num_input_files is not None\n    def test_custom_values(self):",
        "detail": "reference_code.llama-index-core.tests.multi_modal_llms.test_base_multi_modal_llm_metadata",
        "documentation": {}
    },
    {
        "label": "mock_successful_response",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "description": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "peekOfCode": "def mock_successful_response():\n    mock_response = MagicMock()\n    mock_response.content = EXP_BINARY\n    return mock_response\ndef test_load_image_urls():\n    \"\"\"Test loading image URLs into ImageDocument objects.\"\"\"\n    result = load_image_urls(EXP_IMAGE_URLS)\n    assert len(result) == len(EXP_IMAGE_URLS)\n    assert all(isinstance(doc, ImageDocument) for doc in result)\n    assert all(doc.image_url == url for doc, url in zip(result, EXP_IMAGE_URLS))",
        "detail": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "documentation": {}
    },
    {
        "label": "test_load_image_urls",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "description": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "peekOfCode": "def test_load_image_urls():\n    \"\"\"Test loading image URLs into ImageDocument objects.\"\"\"\n    result = load_image_urls(EXP_IMAGE_URLS)\n    assert len(result) == len(EXP_IMAGE_URLS)\n    assert all(isinstance(doc, ImageDocument) for doc in result)\n    assert all(doc.image_url == url for doc, url in zip(result, EXP_IMAGE_URLS))\ndef test_load_image_urls_with_empty_list():\n    \"\"\"Test loading an empty list of URLs.\"\"\"\n    result = load_image_urls([])\n    assert result == []",
        "detail": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "documentation": {}
    },
    {
        "label": "test_load_image_urls_with_empty_list",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "description": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "peekOfCode": "def test_load_image_urls_with_empty_list():\n    \"\"\"Test loading an empty list of URLs.\"\"\"\n    result = load_image_urls([])\n    assert result == []\ndef test_encode_image():\n    \"\"\"Test successful image encoding.\"\"\"\n    with patch(\"builtins.open\", mock_open(read_data=EXP_BINARY)):\n        result = encode_image(\"fake_image.jpg\")\n    assert result == EXP_BASE64\ndef test_image_documents_to_base64_multiple_sources(tmp_path: Path):",
        "detail": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "documentation": {}
    },
    {
        "label": "test_encode_image",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "description": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "peekOfCode": "def test_encode_image():\n    \"\"\"Test successful image encoding.\"\"\"\n    with patch(\"builtins.open\", mock_open(read_data=EXP_BINARY)):\n        result = encode_image(\"fake_image.jpg\")\n    assert result == EXP_BASE64\ndef test_image_documents_to_base64_multiple_sources(tmp_path: Path):\n    \"\"\"Test converting multiple ImageDocuments with different source types.\"\"\"\n    content = httpx.get(EXP_IMAGE_URLS[0]).content\n    expected_b64 = base64.b64encode(content).decode(\"utf-8\")\n    fl_path = tmp_path / \"test_image.png\"",
        "detail": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "documentation": {}
    },
    {
        "label": "test_image_documents_to_base64_multiple_sources",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "description": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "peekOfCode": "def test_image_documents_to_base64_multiple_sources(tmp_path: Path):\n    \"\"\"Test converting multiple ImageDocuments with different source types.\"\"\"\n    content = httpx.get(EXP_IMAGE_URLS[0]).content\n    expected_b64 = base64.b64encode(content).decode(\"utf-8\")\n    fl_path = tmp_path / \"test_image.png\"\n    fl_path.write_bytes(content)\n    documents = [\n        ImageDocument(image=expected_b64),\n        ImageDocument(image_path=fl_path),\n        ImageDocument(metadata={\"file_path\": \"test.jpg\"}),",
        "detail": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "documentation": {}
    },
    {
        "label": "test_image_documents_to_base64_failed_url",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "description": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "peekOfCode": "def test_image_documents_to_base64_failed_url():\n    \"\"\"Test handling of failed URL requests.\"\"\"\n    document = ImageDocument(image_url=EXP_IMAGE_URLS[0])\n    with patch(\"requests.get\"):\n        result = image_documents_to_base64([document])\n    assert result == []\ndef test_image_documents_to_base64_empty_sequence():\n    \"\"\"Test handling of empty sequence of documents.\"\"\"\n    result = image_documents_to_base64([])\n    assert result == []",
        "detail": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "documentation": {}
    },
    {
        "label": "test_image_documents_to_base64_empty_sequence",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "description": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "peekOfCode": "def test_image_documents_to_base64_empty_sequence():\n    \"\"\"Test handling of empty sequence of documents.\"\"\"\n    result = image_documents_to_base64([])\n    assert result == []\ndef test_image_documents_to_base64_invalid_metadata():\n    \"\"\"Test handling of document with invalid metadata path.\"\"\"\n    document = ImageDocument(metadata={\"file_path\": \"\"})\n    result = image_documents_to_base64([document])\n    assert result == []\ndef test_complete_workflow():",
        "detail": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "documentation": {}
    },
    {
        "label": "test_image_documents_to_base64_invalid_metadata",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "description": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "peekOfCode": "def test_image_documents_to_base64_invalid_metadata():\n    \"\"\"Test handling of document with invalid metadata path.\"\"\"\n    document = ImageDocument(metadata={\"file_path\": \"\"})\n    result = image_documents_to_base64([document])\n    assert result == []\ndef test_complete_workflow():\n    \"\"\"Test the complete workflow from URL to base64 encoding.\"\"\"\n    documents = load_image_urls(EXP_IMAGE_URLS)\n    with patch(\"requests.get\") as mock_get:\n        mock_get.return_value.content = EXP_BINARY",
        "detail": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "documentation": {}
    },
    {
        "label": "test_complete_workflow",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "description": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "peekOfCode": "def test_complete_workflow():\n    \"\"\"Test the complete workflow from URL to base64 encoding.\"\"\"\n    documents = load_image_urls(EXP_IMAGE_URLS)\n    with patch(\"requests.get\") as mock_get:\n        mock_get.return_value.content = EXP_BINARY\n        result = image_documents_to_base64(documents)\n    assert len(result) == len(EXP_IMAGE_URLS)\n    assert isinstance(result[0], str)\n    assert base64.b64decode(result[0]) == EXP_BINARY\ndef test_infer_image_mimetype_from_base64():",
        "detail": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "documentation": {}
    },
    {
        "label": "test_infer_image_mimetype_from_base64",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "description": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "peekOfCode": "def test_infer_image_mimetype_from_base64():\n    \"\"\"Test inferring image mimetype from base64-encoded data.\"\"\"\n    # Create a minimal valid PNG in base64\n    base64_png = \"iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAACklEQVR4nGMAAQAABQABDQottAAAAABJRU5ErkJggg==\"\n    result = infer_image_mimetype_from_base64(base64_png)\n    assert result == \"image/png\"\n    # Valid, meaningless base64\n    result = infer_image_mimetype_from_base64(EXP_BASE64)\n    assert result is None\ndef test_infer_image_mimetype_from_file_path():",
        "detail": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "documentation": {}
    },
    {
        "label": "test_infer_image_mimetype_from_file_path",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "description": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "peekOfCode": "def test_infer_image_mimetype_from_file_path():\n    \"\"\"Test inferring image mimetype from file extensions.\"\"\"\n    # JPG/JPEG\n    assert infer_image_mimetype_from_file_path(\"image.jpg\") == \"image/jpeg\"\n    assert infer_image_mimetype_from_file_path(\"image.jpeg\") == \"image/jpeg\"\n    # PNG\n    assert infer_image_mimetype_from_file_path(\"image.png\") == \"image/png\"\n    # GIF\n    assert infer_image_mimetype_from_file_path(\"image.gif\") == \"image/gif\"\n    # WEBP",
        "detail": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "documentation": {}
    },
    {
        "label": "test_set_base64_and_mimetype_for_image_docs",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "description": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "peekOfCode": "def test_set_base64_and_mimetype_for_image_docs(tmp_path: Path):\n    \"\"\"Test setting base64 and mimetype fields for ImageDocument objects.\"\"\"\n    content = httpx.get(EXP_IMAGE_URLS[0]).content\n    expected_b64 = base64.b64encode(content).decode(\"utf-8\")\n    fl_path = tmp_path / \"test_image.png\"\n    fl_path.write_bytes(content)\n    image_docs = [\n        ImageDocument(image=expected_b64),\n        ImageDocument(image_path=fl_path.__str__()),\n    ]",
        "detail": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "documentation": {}
    },
    {
        "label": "EXP_IMAGE_URLS",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "description": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "peekOfCode": "EXP_IMAGE_URLS = [\n    \"https://astrabert.github.io/hophop-science/images/whale_doing_science.png\"\n]\nEXP_BASE64 = \"SGVsbG8gV29ybGQ=\"  # \"Hello World\" in base64\nEXP_BINARY = b\"Hello World\"\n@pytest.fixture()\ndef mock_successful_response():\n    mock_response = MagicMock()\n    mock_response.content = EXP_BINARY\n    return mock_response",
        "detail": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "documentation": {}
    },
    {
        "label": "EXP_BASE64",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "description": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "peekOfCode": "EXP_BASE64 = \"SGVsbG8gV29ybGQ=\"  # \"Hello World\" in base64\nEXP_BINARY = b\"Hello World\"\n@pytest.fixture()\ndef mock_successful_response():\n    mock_response = MagicMock()\n    mock_response.content = EXP_BINARY\n    return mock_response\ndef test_load_image_urls():\n    \"\"\"Test loading image URLs into ImageDocument objects.\"\"\"\n    result = load_image_urls(EXP_IMAGE_URLS)",
        "detail": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "documentation": {}
    },
    {
        "label": "EXP_BINARY",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "description": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "peekOfCode": "EXP_BINARY = b\"Hello World\"\n@pytest.fixture()\ndef mock_successful_response():\n    mock_response = MagicMock()\n    mock_response.content = EXP_BINARY\n    return mock_response\ndef test_load_image_urls():\n    \"\"\"Test loading image URLs into ImageDocument objects.\"\"\"\n    result = load_image_urls(EXP_IMAGE_URLS)\n    assert len(result) == len(EXP_IMAGE_URLS)",
        "detail": "reference_code.llama-index-core.tests.multi_modal_llms.test_generic_utils",
        "documentation": {}
    },
    {
        "label": "test_metadata_extractor",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.metadata_extractor",
        "description": "reference_code.llama-index-core.tests.node_parser.metadata_extractor",
        "peekOfCode": "def test_metadata_extractor() -> None:\n    extractors: List[TransformComponent] = [\n        TitleExtractor(nodes=5),\n        QuestionsAnsweredExtractor(questions=3),\n        SummaryExtractor(summaries=[\"prev\", \"self\"]),\n        KeywordExtractor(keywords=10),\n    ]\n    node_parser: TransformComponent = SentenceSplitter()\n    document = Document(\n        text=\"sample text\",",
        "detail": "reference_code.llama-index-core.tests.node_parser.metadata_extractor",
        "documentation": {}
    },
    {
        "label": "test_split_and_window",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.sentence_window",
        "description": "reference_code.llama-index-core.tests.node_parser.sentence_window",
        "peekOfCode": "def test_split_and_window() -> None:\n    document = Document(text=\"This is a test 1. This is a test 2. This is a test 3.\")\n    node_parser = SentenceWindowNodeParser.from_defaults()\n    nodes = node_parser.get_nodes_from_documents([document])\n    assert len(nodes) == 3\n    assert nodes[0].get_content() == \"This is a test 1. \"\n    assert nodes[1].get_content() == \"This is a test 2. \"\n    assert nodes[2].get_content() == \"This is a test 3.\"\n    assert (\n        \"\".join(nodes[0].metadata[\"window\"])",
        "detail": "reference_code.llama-index-core.tests.node_parser.sentence_window",
        "documentation": {}
    },
    {
        "label": "test_unsupported_extension",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_file",
        "description": "reference_code.llama-index-core.tests.node_parser.test_file",
        "peekOfCode": "def test_unsupported_extension() -> None:\n    simple_file_node_parser = SimpleFileNodeParser()\n    nodes = simple_file_node_parser._parse_nodes(\n        [\n            Document(\n                text=\"\"\"def evenOdd(n):\n  # if n&1 == 0, then num is even\n  if n & 1:\n    return False\n  # if n&1 == 1, then num is odd",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_file",
        "documentation": {}
    },
    {
        "label": "nodes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "description": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "peekOfCode": "def nodes() -> list:\n    node_parser = HierarchicalNodeParser.from_defaults(\n        chunk_sizes=[512, 128, 64],\n        chunk_overlap=10,\n    )\n    return node_parser.get_nodes_from_documents([Document.example()])\ndef test_get_root_nodes(nodes: list) -> None:\n    root_nodes = get_root_nodes(nodes)\n    assert len(root_nodes) == ROOT_NODES_LEN\ndef test_get_root_nodes_empty(nodes: list) -> None:",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "documentation": {}
    },
    {
        "label": "test_get_root_nodes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "description": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "peekOfCode": "def test_get_root_nodes(nodes: list) -> None:\n    root_nodes = get_root_nodes(nodes)\n    assert len(root_nodes) == ROOT_NODES_LEN\ndef test_get_root_nodes_empty(nodes: list) -> None:\n    root_nodes = get_root_nodes(get_leaf_nodes(nodes))\n    assert root_nodes == []\ndef test_get_leaf_nodes(nodes: list) -> None:\n    leaf_nodes = get_leaf_nodes(nodes)\n    assert len(leaf_nodes) == GRAND_CHILDREN_NODES_LEN\ndef test_get_child_nodes(nodes: list) -> None:",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "documentation": {}
    },
    {
        "label": "test_get_root_nodes_empty",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "description": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "peekOfCode": "def test_get_root_nodes_empty(nodes: list) -> None:\n    root_nodes = get_root_nodes(get_leaf_nodes(nodes))\n    assert root_nodes == []\ndef test_get_leaf_nodes(nodes: list) -> None:\n    leaf_nodes = get_leaf_nodes(nodes)\n    assert len(leaf_nodes) == GRAND_CHILDREN_NODES_LEN\ndef test_get_child_nodes(nodes: list) -> None:\n    child_nodes = get_child_nodes(get_root_nodes(nodes), all_nodes=nodes)\n    assert len(child_nodes) == CHILDREN_NODES_LEN\ndef test_get_deeper_nodes(nodes: list) -> None:",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "documentation": {}
    },
    {
        "label": "test_get_leaf_nodes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "description": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "peekOfCode": "def test_get_leaf_nodes(nodes: list) -> None:\n    leaf_nodes = get_leaf_nodes(nodes)\n    assert len(leaf_nodes) == GRAND_CHILDREN_NODES_LEN\ndef test_get_child_nodes(nodes: list) -> None:\n    child_nodes = get_child_nodes(get_root_nodes(nodes), all_nodes=nodes)\n    assert len(child_nodes) == CHILDREN_NODES_LEN\ndef test_get_deeper_nodes(nodes: list) -> None:\n    deep_nodes = get_deeper_nodes(nodes, depth=0)\n    assert deep_nodes == get_root_nodes(nodes)\n    deep_nodes = get_deeper_nodes(nodes, depth=1)",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "documentation": {}
    },
    {
        "label": "test_get_child_nodes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "description": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "peekOfCode": "def test_get_child_nodes(nodes: list) -> None:\n    child_nodes = get_child_nodes(get_root_nodes(nodes), all_nodes=nodes)\n    assert len(child_nodes) == CHILDREN_NODES_LEN\ndef test_get_deeper_nodes(nodes: list) -> None:\n    deep_nodes = get_deeper_nodes(nodes, depth=0)\n    assert deep_nodes == get_root_nodes(nodes)\n    deep_nodes = get_deeper_nodes(nodes, depth=1)\n    assert deep_nodes == get_child_nodes(get_root_nodes(nodes), nodes)\n    deep_nodes = get_deeper_nodes(nodes, depth=2)\n    assert deep_nodes == get_leaf_nodes(nodes)",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "documentation": {}
    },
    {
        "label": "test_get_deeper_nodes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "description": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "peekOfCode": "def test_get_deeper_nodes(nodes: list) -> None:\n    deep_nodes = get_deeper_nodes(nodes, depth=0)\n    assert deep_nodes == get_root_nodes(nodes)\n    deep_nodes = get_deeper_nodes(nodes, depth=1)\n    assert deep_nodes == get_child_nodes(get_root_nodes(nodes), nodes)\n    deep_nodes = get_deeper_nodes(nodes, depth=2)\n    assert deep_nodes == get_leaf_nodes(nodes)\n    deep_nodes = get_deeper_nodes(nodes, depth=2)\n    assert deep_nodes == get_child_nodes(\n        get_child_nodes(get_root_nodes(nodes), nodes), nodes",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "documentation": {}
    },
    {
        "label": "test_get_deeper_nodes_with_no_root_nodes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "description": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "peekOfCode": "def test_get_deeper_nodes_with_no_root_nodes(nodes: list) -> None:\n    with pytest.raises(ValueError, match=\"There is no*\"):\n        get_deeper_nodes(get_leaf_nodes(nodes))\ndef test_get_deeper_nodes_with_negative_depth(nodes: list) -> None:\n    with pytest.raises(ValueError, match=\"Depth cannot be*\"):\n        get_deeper_nodes(nodes, -1)",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "documentation": {}
    },
    {
        "label": "test_get_deeper_nodes_with_negative_depth",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "description": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "peekOfCode": "def test_get_deeper_nodes_with_negative_depth(nodes: list) -> None:\n    with pytest.raises(ValueError, match=\"Depth cannot be*\"):\n        get_deeper_nodes(nodes, -1)",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "documentation": {}
    },
    {
        "label": "ROOT_NODES_LEN",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "description": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "peekOfCode": "ROOT_NODES_LEN = 1\nCHILDREN_NODES_LEN = 3\nGRAND_CHILDREN_NODES_LEN = 7\n@pytest.fixture(scope=\"module\")\ndef nodes() -> list:\n    node_parser = HierarchicalNodeParser.from_defaults(\n        chunk_sizes=[512, 128, 64],\n        chunk_overlap=10,\n    )\n    return node_parser.get_nodes_from_documents([Document.example()])",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "documentation": {}
    },
    {
        "label": "CHILDREN_NODES_LEN",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "description": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "peekOfCode": "CHILDREN_NODES_LEN = 3\nGRAND_CHILDREN_NODES_LEN = 7\n@pytest.fixture(scope=\"module\")\ndef nodes() -> list:\n    node_parser = HierarchicalNodeParser.from_defaults(\n        chunk_sizes=[512, 128, 64],\n        chunk_overlap=10,\n    )\n    return node_parser.get_nodes_from_documents([Document.example()])\ndef test_get_root_nodes(nodes: list) -> None:",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "documentation": {}
    },
    {
        "label": "GRAND_CHILDREN_NODES_LEN",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "description": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "peekOfCode": "GRAND_CHILDREN_NODES_LEN = 7\n@pytest.fixture(scope=\"module\")\ndef nodes() -> list:\n    node_parser = HierarchicalNodeParser.from_defaults(\n        chunk_sizes=[512, 128, 64],\n        chunk_overlap=10,\n    )\n    return node_parser.get_nodes_from_documents([Document.example()])\ndef test_get_root_nodes(nodes: list) -> None:\n    root_nodes = get_root_nodes(nodes)",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_hierarchical",
        "documentation": {}
    },
    {
        "label": "test_no_splits",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_html",
        "description": "reference_code.llama-index-core.tests.node_parser.test_html",
        "peekOfCode": "def test_no_splits() -> None:\n    html_parser = HTMLNodeParser(tags=[\"h2\"])\n    splits = html_parser.get_nodes_from_documents(\n        [\n            Document(\n                text=\"\"\"\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Test Page</title>",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_html",
        "documentation": {}
    },
    {
        "label": "test_single_splits",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_html",
        "description": "reference_code.llama-index-core.tests.node_parser.test_html",
        "peekOfCode": "def test_single_splits() -> None:\n    html_parser = HTMLNodeParser(tags=[\"h1\"])\n    splits = html_parser.get_nodes_from_documents(\n        [\n            Document(\n                text=\"\"\"\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Test Page</title>",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_html",
        "documentation": {}
    },
    {
        "label": "test_multiple_tags_splits",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_html",
        "description": "reference_code.llama-index-core.tests.node_parser.test_html",
        "peekOfCode": "def test_multiple_tags_splits() -> None:\n    html_parser = HTMLNodeParser(tags=[\"h2\", \"p\"])\n    splits = html_parser.get_nodes_from_documents(\n        [\n            Document(\n                text=\"\"\"\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Test Page</title>",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_html",
        "documentation": {}
    },
    {
        "label": "test_nesting_tags_splits",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_html",
        "description": "reference_code.llama-index-core.tests.node_parser.test_html",
        "peekOfCode": "def test_nesting_tags_splits() -> None:\n    html_parser = HTMLNodeParser(tags=[\"h2\", \"b\"])\n    splits = html_parser.get_nodes_from_documents(\n        [\n            Document(\n                text=\"\"\"\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Test Page</title>",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_html",
        "documentation": {}
    },
    {
        "label": "test_neighbor_tags_splits",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_html",
        "description": "reference_code.llama-index-core.tests.node_parser.test_html",
        "peekOfCode": "def test_neighbor_tags_splits() -> None:\n    html_parser = HTMLNodeParser(tags=[\"p\"])\n    splits = html_parser.get_nodes_from_documents(\n        [\n            Document(\n                text=\"\"\"\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Test Page</title>",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_html",
        "documentation": {}
    },
    {
        "label": "test_split_empty_text",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_json",
        "description": "reference_code.llama-index-core.tests.node_parser.test_json",
        "peekOfCode": "def test_split_empty_text() -> None:\n    json_splitter = JSONNodeParser()\n    input_text = Document(text=\"\")\n    result = json_splitter.get_nodes_from_documents([input_text])\n    assert result == []\ndef test_split_valid_json() -> None:\n    json_splitter = JSONNodeParser()\n    input_text = Document(\n        text='[{\"name\": \"John\", \"age\": 30}, {\"name\": \"Alice\", \"age\": 25}]'\n    )",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_json",
        "documentation": {}
    },
    {
        "label": "test_split_valid_json",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_json",
        "description": "reference_code.llama-index-core.tests.node_parser.test_json",
        "peekOfCode": "def test_split_valid_json() -> None:\n    json_splitter = JSONNodeParser()\n    input_text = Document(\n        text='[{\"name\": \"John\", \"age\": 30}, {\"name\": \"Alice\", \"age\": 25}]'\n    )\n    result = json_splitter.get_nodes_from_documents([input_text])\n    assert len(result) == 2\n    assert result[0].text == \"name John\\nage 30\"\n    assert result[1].text == \"name Alice\\nage 25\"\ndef test_split_valid_json_defaults() -> None:",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_json",
        "documentation": {}
    },
    {
        "label": "test_split_valid_json_defaults",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_json",
        "description": "reference_code.llama-index-core.tests.node_parser.test_json",
        "peekOfCode": "def test_split_valid_json_defaults() -> None:\n    json_splitter = JSONNodeParser()\n    input_text = Document(text='[{\"name\": \"John\", \"age\": 30}]')\n    result = json_splitter.get_nodes_from_documents([input_text])\n    assert len(result) == 1\n    assert result[0].text == \"name John\\nage 30\"\ndef test_split_valid_dict_json() -> None:\n    json_splitter = JSONNodeParser()\n    input_text = Document(text='{\"name\": \"John\", \"age\": 30}')\n    result = json_splitter.get_nodes_from_documents([input_text])",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_json",
        "documentation": {}
    },
    {
        "label": "test_split_valid_dict_json",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_json",
        "description": "reference_code.llama-index-core.tests.node_parser.test_json",
        "peekOfCode": "def test_split_valid_dict_json() -> None:\n    json_splitter = JSONNodeParser()\n    input_text = Document(text='{\"name\": \"John\", \"age\": 30}')\n    result = json_splitter.get_nodes_from_documents([input_text])\n    assert len(result) == 1\n    assert result[0].text == \"name John\\nage 30\"\ndef test_split_invalid_json() -> None:\n    json_splitter = JSONNodeParser()\n    input_text = Document(text='{\"name\": \"John\", \"age\": 30,}')\n    result = json_splitter.get_nodes_from_documents([input_text])",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_json",
        "documentation": {}
    },
    {
        "label": "test_split_invalid_json",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_json",
        "description": "reference_code.llama-index-core.tests.node_parser.test_json",
        "peekOfCode": "def test_split_invalid_json() -> None:\n    json_splitter = JSONNodeParser()\n    input_text = Document(text='{\"name\": \"John\", \"age\": 30,}')\n    result = json_splitter.get_nodes_from_documents([input_text])\n    assert result == []",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_json",
        "documentation": {}
    },
    {
        "label": "test_header_splits",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_markdown",
        "description": "reference_code.llama-index-core.tests.node_parser.test_markdown",
        "peekOfCode": "def test_header_splits() -> None:\n    markdown_parser = MarkdownNodeParser()\n    splits = markdown_parser.get_nodes_from_documents(\n        [\n            Document(\n                text=\"\"\"# Main Header\nHeader 1 content\n# Header 2\nHeader 2 content\n    \"\"\"",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_markdown",
        "documentation": {}
    },
    {
        "label": "test_header_splits_with_forwardslash",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_markdown",
        "description": "reference_code.llama-index-core.tests.node_parser.test_markdown",
        "peekOfCode": "def test_header_splits_with_forwardslash() -> None:\n    markdown_parser = MarkdownNodeParser(\n        header_path_separator=\"\\u203a\"\n    )  # Unicode for \"\", infrequently used char\n    splits = markdown_parser.get_nodes_from_documents(\n        [\n            Document(\n                text=\"\"\"# Main Header\nHeader 1 content\n## FAQ",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_markdown",
        "documentation": {}
    },
    {
        "label": "test_header_splits_with_indented_code_blocks",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_markdown",
        "description": "reference_code.llama-index-core.tests.node_parser.test_markdown",
        "peekOfCode": "def test_header_splits_with_indented_code_blocks() -> None:\n    markdown_parser = MarkdownNodeParser()\n    splits = markdown_parser.get_nodes_from_documents(\n        [\n            Document(\n                text=\"\"\"Some text\n# Header 1\n## Header 2\n### Header 3\n```txt",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_markdown",
        "documentation": {}
    },
    {
        "label": "test_non_header_splits",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_markdown",
        "description": "reference_code.llama-index-core.tests.node_parser.test_markdown",
        "peekOfCode": "def test_non_header_splits() -> None:\n    markdown_parser = MarkdownNodeParser()\n    splits = markdown_parser.get_nodes_from_documents(\n        [\n            Document(\n                text=\"\"\"# Header 1\n#Not a header\nAlso # not a header\n    # Still not a header\n    \"\"\"",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_markdown",
        "documentation": {}
    },
    {
        "label": "test_pre_header_content",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_markdown",
        "description": "reference_code.llama-index-core.tests.node_parser.test_markdown",
        "peekOfCode": "def test_pre_header_content() -> None:\n    markdown_parser = MarkdownNodeParser()\n    splits = markdown_parser.get_nodes_from_documents(\n        [\n            Document(\n                text=\"\"\"\npre-header content\n# Header 1\nContent\n## Sub-header",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_markdown",
        "documentation": {}
    },
    {
        "label": "test_header_metadata",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_markdown",
        "description": "reference_code.llama-index-core.tests.node_parser.test_markdown",
        "peekOfCode": "def test_header_metadata() -> None:\n    markdown_parser = MarkdownNodeParser()\n    splits = markdown_parser.get_nodes_from_documents(\n        [\n            Document(\n                text=\"\"\"# Main Header\nContent\n## Sub-header\nContent\n### Sub-sub header",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_markdown",
        "documentation": {}
    },
    {
        "label": "test_header_metadata_with_level_jump",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_markdown",
        "description": "reference_code.llama-index-core.tests.node_parser.test_markdown",
        "peekOfCode": "def test_header_metadata_with_level_jump() -> None:\n    markdown_parser = MarkdownNodeParser()\n    splits = markdown_parser.get_nodes_from_documents(\n        [\n            Document(\n                text=\"\"\"# Main Header\nContent\n### Sub-header\nContent\n### Sub-sub header",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_markdown",
        "documentation": {}
    },
    {
        "label": "test_md_table_extraction",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "description": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "peekOfCode": "def test_md_table_extraction() -> None:\n    test_data = Document(\n        text=\"\"\"\n# This is a test\n| Year | Benefits |\n| ---- | -------- |\n| 2020 | 12,000   |\n| 2021 | 10,000   |\n| 2022 | 130,000  |\n# This is another test",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "documentation": {}
    },
    {
        "label": "test_md_table_extraction_broken_table",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "description": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "peekOfCode": "def test_md_table_extraction_broken_table() -> None:\n    test_data = Document(\n        text=\"\"\"\n# This is a test\n| Year | Benefits |\n| ---- | -------- |\n| 2020 | 12,000   | not a table |\n| 2021 | 10,000   |\n| 2022 | 130,000  |\n# This is another test",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "documentation": {}
    },
    {
        "label": "test_complex_md",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "description": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "peekOfCode": "def test_complex_md() -> None:\n    test_data = Document(\n        text=\"\"\"\n# Using LLMs\n## Concept\nPicking the proper Large Language Model (LLM) is one of the first steps you need to consider when building any LLM application over your data.\nLLMs are a core component of LlamaIndex. They can be used as standalone modules or plugged into other core LlamaIndex modules (indices, retrievers, query engines). They are always used during the response synthesis step (e.g. after retrieval). Depending on the type of index being used, LLMs may also be used during index construction, insertion, and query traversal.\nLlamaIndex provides a unified interface for defining LLM modules, whether it's from OpenAI, Hugging Face, or LangChain, so that you\ndon't have to write the boilerplate code of defining the LLM interface yourself. This interface consists of the following (more details below):\n- Support for **text completion** and **chat** endpoints (details below)",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "documentation": {}
    },
    {
        "label": "test_llama2_bad_md",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "description": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "peekOfCode": "def test_llama2_bad_md() -> None:\n    test_data = Document(\n        text=\"\"\"\n# Llama 2: Open Foundation and Fine-Tuned Chat Models\nHugo Touvron       Louis Martin    Kevin Stone\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "documentation": {}
    },
    {
        "label": "test_extract_ref_doc_id",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "description": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "peekOfCode": "def test_extract_ref_doc_id():\n    test_document = Document(\n        text=\"\"\"\n# Introduction\nHello world!\n\"\"\",\n    )\n    node_parser = MarkdownElementNodeParser(llm=MockLLM())\n    nodes = node_parser.get_nodes_from_documents([test_document])\n    assert len(nodes) == 1",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "documentation": {}
    },
    {
        "label": "test_start_end_char_idx",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "description": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "peekOfCode": "def test_start_end_char_idx():\n    test_document = Document(\n        text=\"\"\"\n# This is a test\n| Year | Benefits |\n| ---- | -------- |\n| 2020 | 12,000   |\n| 2021 | 10,000   |\n| 2022 | 130,000  |\n\"\"\",",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "documentation": {}
    },
    {
        "label": "test_extract_html_table",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "description": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "peekOfCode": "def test_extract_html_table():\n    test_document = Document(\n        text=\"\"\"\n<table>\n  <tr>\n    <th>Month</th>\n    <th>Savings</th>\n  </tr>\n  <tr>\n    <td>January</td>",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "documentation": {}
    },
    {
        "label": "resp",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "description": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "peekOfCode": "resp = OpenAI().complete(\"Paul Graham is \")\nprint(resp)\n```\n```{toctree}\n---\nmaxdepth: 1\n---\nllms/usage_standalone.md\nllms/usage_custom.md\n```",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "documentation": {}
    },
    {
        "label": "Lranking",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "description": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "peekOfCode": "Lranking = log((r(x, yc)  r(x, yr))) (1)\nwhere r(x, y) is the scalar score output for prompt x and completion y with model weights . yc is the preferred response that annotators choose and yr is the rejected counterpart.\nBuilt on top of this binary ranking loss, we further modify it separately for better helpfulness and safety reward models as follows. Given that our preference ratings is decomposed as a scale of four points (e.g., significantly better), as presented in Section 3.2.1, it can be useful to leverage this information to explicitly teach the reward model to assign more discrepant scores to the generations that have more differences. To do so, we further add a margin component in the loss:\nLranking = log((r(x, yc)  r(x, yr)  m(r)) (2)\nwhere the margin m(r) is a discrete function of the preference rating. Naturally, we use a large margin for pairs with distinct responses, and a smaller one for those with similar responses (shown in Table 27). We found this margin component can improve Helpfulness reward model accuracy especially on samples where two responses are more separable. More detailed ablation and analysis can be found in Table 28 in Appendix A.3.3.\nData Composition. We combine our newly collected data with existing open-source preference datasets to form a larger training dataset. Initially, open-source datasets were used to bootstrap our reward models while we were in the process of collecting preference annotation data. We note that in the context of RLHF in this study, the role of reward signals is to learn human preference for Llama 2-Chat outputs rather than any model outputs. However, in our experiments, we do not observe negative transfer from the open-source preference datasets. Thus, we have decided to keep them in our data mixture, as they could enable better generalization for the reward model and prevent reward hacking, i.e. Llama 2-Chat taking advantage of some weaknesses of our reward, and so artificially inflating the score despite performing less well.\nWith training data available from different sources, we experimented with different mixing recipes for both Helpfulness and Safety reward models to ascertain the best settings. After extensive experimentation, the 11\n# Helpfulness and Safety Reward Model Training Details\nThe helpfulness reward model is eventually trained on all Meta Helpfulness data, combined with an equal parts of the remaining data uniformly sampled from Meta Safety and from the open-source datasets. The Meta Safety reward model is trained on all Meta Safety and Anthropic Harmless data, mixed with Meta Helpfulness and open-source helpfulness data in a 90/10 proportion. We found that the setting with 10% helpfulness data is especially beneficial for the accuracy on samples where both the chosen and rejected responses were deemed safe.\n**Training Details:**",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "documentation": {}
    },
    {
        "label": "Lranking",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "description": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "peekOfCode": "Lranking = log((r(x, yc)  r(x, yr)  m(r)) (2)\nwhere the margin m(r) is a discrete function of the preference rating. Naturally, we use a large margin for pairs with distinct responses, and a smaller one for those with similar responses (shown in Table 27). We found this margin component can improve Helpfulness reward model accuracy especially on samples where two responses are more separable. More detailed ablation and analysis can be found in Table 28 in Appendix A.3.3.\nData Composition. We combine our newly collected data with existing open-source preference datasets to form a larger training dataset. Initially, open-source datasets were used to bootstrap our reward models while we were in the process of collecting preference annotation data. We note that in the context of RLHF in this study, the role of reward signals is to learn human preference for Llama 2-Chat outputs rather than any model outputs. However, in our experiments, we do not observe negative transfer from the open-source preference datasets. Thus, we have decided to keep them in our data mixture, as they could enable better generalization for the reward model and prevent reward hacking, i.e. Llama 2-Chat taking advantage of some weaknesses of our reward, and so artificially inflating the score despite performing less well.\nWith training data available from different sources, we experimented with different mixing recipes for both Helpfulness and Safety reward models to ascertain the best settings. After extensive experimentation, the 11\n# Helpfulness and Safety Reward Model Training Details\nThe helpfulness reward model is eventually trained on all Meta Helpfulness data, combined with an equal parts of the remaining data uniformly sampled from Meta Safety and from the open-source datasets. The Meta Safety reward model is trained on all Meta Safety and Anthropic Harmless data, mixed with Meta Helpfulness and open-source helpfulness data in a 90/10 proportion. We found that the setting with 10% helpfulness data is especially beneficial for the accuracy on samples where both the chosen and rejected responses were deemed safe.\n**Training Details:**\nWe train for one epoch over the training data. In earlier experiments, we found that training longer can lead to over-fitting. We use the same optimizer parameters as for the base model. The maximum learning rate is 5  106 for the 70B parameter Llama 2-Chat and 1  105 for the rest. The learning rate is decreased on a cosine learning rate schedule, down to 10% of the maximum learning rate. We use a warm-up of 3% of the total number of steps, with a minimum of 5. The effective batch size is kept fixed at 512 pairs, or 1024 rows per batch.\n| Model          | Meta Helpful. | Meta Safety | Anthropic Helpful | Anthropic Harmless | OpenAI Summ. | Stanford SHP | Avg |\n|----------------|---------------|-------------|-------------------|---------------------|--------------|--------------|-----|",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_markdown_element",
        "documentation": {}
    },
    {
        "label": "_TestNodeParser",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_node_parser",
        "description": "reference_code.llama-index-core.tests.node_parser.test_node_parser",
        "peekOfCode": "class _TestNodeParser(NodeParser):\n    def _parse_nodes(\n        self, nodes: Sequence[BaseNode], show_progress: bool = False, **kwargs: Any\n    ) -> List[BaseNode]:\n        return super()._parse_nodes(nodes, show_progress, **kwargs)\ndef test__postprocess_parsed_nodes_include_metadata():\n    np = _TestNodeParser()\n    nodes = []\n    for i in range(3):\n        node = TextNode(text=f\"I am Node number {i}\")",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_node_parser",
        "documentation": {}
    },
    {
        "label": "test__postprocess_parsed_nodes_include_metadata",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_node_parser",
        "description": "reference_code.llama-index-core.tests.node_parser.test_node_parser",
        "peekOfCode": "def test__postprocess_parsed_nodes_include_metadata():\n    np = _TestNodeParser()\n    nodes = []\n    for i in range(3):\n        node = TextNode(text=f\"I am Node number {i}\")\n        node.metadata = {\"node_number\": i}\n        nodes.append(node)\n    ret = np._postprocess_parsed_nodes(nodes, {})\n    for i, node in enumerate(ret):\n        assert node.metadata == {\"node_number\": i}",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_node_parser",
        "documentation": {}
    },
    {
        "label": "test__postprocess_parsed_nodes_include_metadata_parent_doc",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_node_parser",
        "description": "reference_code.llama-index-core.tests.node_parser.test_node_parser",
        "peekOfCode": "def test__postprocess_parsed_nodes_include_metadata_parent_doc():\n    np = _TestNodeParser()\n    doc = Document(text=\"I am root\")\n    doc.metadata = {\"document_type\": \"root\"}\n    nodes = []\n    for i in range(3):\n        node = TextNode(text=f\"I am Node number {i}\")\n        node.metadata = {\"node_number\": i}\n        node.relationships = {NodeRelationship.SOURCE: doc.as_related_node_info()}\n        nodes.append(node)",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_node_parser",
        "documentation": {}
    },
    {
        "label": "test_number_of_returned_nodes",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_semantic_double_merging_splitter",
        "description": "reference_code.llama-index-core.tests.node_parser.test_semantic_double_merging_splitter",
        "peekOfCode": "def test_number_of_returned_nodes() -> None:\n    nodes = splitter.get_nodes_from_documents([doc])\n    assert len(nodes) == 2\n@pytest.mark.skipif(not spacy_available, reason=\"Spacy model not available\")\ndef test_creating_initial_chunks() -> None:\n    text = doc.text\n    sentences = splitter.sentence_splitter(text)\n    initial_chunks = splitter._create_initial_chunks(sentences)\n    assert len(initial_chunks) == 4\n@pytest.mark.skipif(not spacy_available, reason=\"Spacy model not available\")",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_semantic_double_merging_splitter",
        "documentation": {}
    },
    {
        "label": "test_creating_initial_chunks",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_semantic_double_merging_splitter",
        "description": "reference_code.llama-index-core.tests.node_parser.test_semantic_double_merging_splitter",
        "peekOfCode": "def test_creating_initial_chunks() -> None:\n    text = doc.text\n    sentences = splitter.sentence_splitter(text)\n    initial_chunks = splitter._create_initial_chunks(sentences)\n    assert len(initial_chunks) == 4\n@pytest.mark.skipif(not spacy_available, reason=\"Spacy model not available\")\ndef test_config_models() -> None:\n    with pytest.raises(ValueError):\n        LanguageConfig(language=\"polish\")\n    with pytest.raises(ValueError):",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_semantic_double_merging_splitter",
        "documentation": {}
    },
    {
        "label": "test_config_models",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_semantic_double_merging_splitter",
        "description": "reference_code.llama-index-core.tests.node_parser.test_semantic_double_merging_splitter",
        "peekOfCode": "def test_config_models() -> None:\n    with pytest.raises(ValueError):\n        LanguageConfig(language=\"polish\")\n    with pytest.raises(ValueError):\n        LanguageConfig(language=\"polish\", spacy_model=\"en_core_web_md\")\n    with pytest.raises(ValueError):\n        LanguageConfig(language=\"french\", spacy_model=\"en_core_web_md\")\n    with pytest.raises(ValueError):\n        LanguageConfig(language=\"empty\", spacy_model=\"empty\")\n    LanguageConfig(language=\"english\", spacy_model=\"en_core_web_md\")",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_semantic_double_merging_splitter",
        "documentation": {}
    },
    {
        "label": "test_chunk_size_1",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_semantic_double_merging_splitter",
        "description": "reference_code.llama-index-core.tests.node_parser.test_semantic_double_merging_splitter",
        "peekOfCode": "def test_chunk_size_1() -> None:\n    splitter.max_chunk_size = 0\n    nodes = splitter.get_nodes_from_documents([doc])\n    # length of each sentence\n    assert len(nodes) == 13\n    assert len(nodes[0].get_content()) == 111\n    assert len(nodes[1].get_content()) == 72\n    assert len(nodes[2].get_content()) == 91\n    assert len(nodes[3].get_content()) == 99\n    assert len(nodes[4].get_content()) == 100",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_semantic_double_merging_splitter",
        "documentation": {}
    },
    {
        "label": "test_chunk_size_2",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_semantic_double_merging_splitter",
        "description": "reference_code.llama-index-core.tests.node_parser.test_semantic_double_merging_splitter",
        "peekOfCode": "def test_chunk_size_2() -> None:\n    splitter.max_chunk_size = 200\n    nodes = splitter.get_nodes_from_documents([doc])\n    for node in nodes:\n        assert len(node.get_content()) < 200\n@pytest.mark.skipif(not spacy_available, reason=\"Spacy model not available\")\ndef test_chunk_size_3() -> None:\n    splitter.max_chunk_size = 500\n    nodes = splitter.get_nodes_from_documents([doc_same])\n    for node in nodes:",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_semantic_double_merging_splitter",
        "documentation": {}
    },
    {
        "label": "test_chunk_size_3",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_semantic_double_merging_splitter",
        "description": "reference_code.llama-index-core.tests.node_parser.test_semantic_double_merging_splitter",
        "peekOfCode": "def test_chunk_size_3() -> None:\n    splitter.max_chunk_size = 500\n    nodes = splitter.get_nodes_from_documents([doc_same])\n    for node in nodes:\n        assert len(node.get_content()) < 500",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_semantic_double_merging_splitter",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_semantic_double_merging_splitter",
        "description": "reference_code.llama-index-core.tests.node_parser.test_semantic_double_merging_splitter",
        "peekOfCode": "doc = Document(\n    text=\"Warsaw: Warsaw, the capital city of Poland, is a bustling metropolis located on the banks of the Vistula River. \"\n    \"It is known for its rich history, vibrant culture, and resilient spirit. Warsaw's skyline is characterized by a mix of historic architecture and modern skyscrapers. \"\n    \"The Old Town, with its cobblestone streets and colorful buildings, is a UNESCO World Heritage Site.\\n\\n\"\n    \"Football: Football, also known as soccer, is a popular sport played by millions of people worldwide. \"\n    \"It is a team sport that involves two teams of eleven players each. The objective of the game is to score goals by kicking the ball into the opposing team's goal. \"\n    \"Football matches are typically played on a rectangular field called a pitch, with goals at each end. \"\n    \"The game is governed by a set of rules known as the Laws of the Game. Football is known for its passionate fanbase and intense rivalries between clubs and countries. \"\n    \"The FIFA World Cup is the most prestigious international football tournament.\\n\\n\"\n    \"Mathematics: Mathematics is a fundamental discipline that deals with the study of numbers, quantities, and shapes. \"",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_semantic_double_merging_splitter",
        "documentation": {}
    },
    {
        "label": "doc_same",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_semantic_double_merging_splitter",
        "description": "reference_code.llama-index-core.tests.node_parser.test_semantic_double_merging_splitter",
        "peekOfCode": "doc_same = Document(\n    text=\"Krakow is one of the oldest and largest cities in Poland, located in the southern part of the country on the Vistula River. \"\n    * 20\n)\ntry:\n    splitter = SemanticDoubleMergingSplitterNodeParser(\n        initial_threshold=0.7,\n        appending_threshold=0.8,\n        merging_threshold=0.7,\n        max_chunk_size=1000,",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_semantic_double_merging_splitter",
        "documentation": {}
    },
    {
        "label": "MockEmbedding",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_semantic_splitter",
        "description": "reference_code.llama-index-core.tests.node_parser.test_semantic_splitter",
        "peekOfCode": "class MockEmbedding(BaseEmbedding):\n    @classmethod\n    def class_name(cls) -> str:\n        return \"MockEmbedding\"\n    async def _aget_query_embedding(self, query: str) -> List[float]:\n        del query\n        return [0, 0, 1, 0, 0]\n    async def _aget_text_embedding(self, text: str) -> List[float]:\n        text = text.strip()\n        # assume dimensions are 5",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_semantic_splitter",
        "documentation": {}
    },
    {
        "label": "test_grouped_semantically",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_semantic_splitter",
        "description": "reference_code.llama-index-core.tests.node_parser.test_semantic_splitter",
        "peekOfCode": "def test_grouped_semantically() -> None:\n    document = Document(\n        text=\"They're taking the Hobbits to Isengard! I can't carry it for you. But I can carry you!\"\n    )\n    embeddings = MockEmbedding()\n    node_parser = SemanticSplitterNodeParser.from_defaults(embeddings)\n    nodes = node_parser.get_nodes_from_documents([document])\n    assert len(nodes) == 1\n    assert (\n        nodes[0].get_content()",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_semantic_splitter",
        "documentation": {}
    },
    {
        "label": "test_split_and_permutated",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_semantic_splitter",
        "description": "reference_code.llama-index-core.tests.node_parser.test_semantic_splitter",
        "peekOfCode": "def test_split_and_permutated() -> None:\n    document = Document(\n        text=\"They're taking the Hobbits to Isengard! I can't carry it for you. But I can carry you!\"\n    )\n    embeddings = MockEmbedding()\n    node_parser = SemanticSplitterNodeParser.from_defaults(embeddings)\n    text_splits = node_parser.sentence_splitter(document.text)\n    sentences = node_parser._build_sentence_groups(text_splits)\n    assert len(sentences) == 3\n    assert sentences[0][\"sentence\"] == \"They're taking the Hobbits to Isengard! \"",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_semantic_splitter",
        "documentation": {}
    },
    {
        "label": "test_html_table_extraction",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.node_parser.test_unstructured",
        "description": "reference_code.llama-index-core.tests.node_parser.test_unstructured",
        "peekOfCode": "def test_html_table_extraction() -> None:\n    test_data = Document(\n        text=\"\"\"\n    <!DOCTYPE html>\n    <html>\n    <head>\n        <title>Test Page</title>\n    </head>\n    <body>\n        <table>",
        "detail": "reference_code.llama-index-core.tests.node_parser.test_unstructured",
        "documentation": {}
    },
    {
        "label": "test_object_index",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.objects.test_base",
        "description": "reference_code.llama-index-core.tests.objects.test_base",
        "peekOfCode": "def test_object_index() -> None:\n    \"\"\"Test object index.\"\"\"\n    object_mapping = SimpleObjectNodeMapping.from_objects([\"a\", \"b\", \"c\"])\n    obj_index = ObjectIndex.from_objects(\n        [\"a\", \"b\", \"c\"], object_mapping, index_cls=SummaryIndex\n    )\n    # should just retrieve everything\n    assert obj_index.as_retriever().retrieve(\"test\") == [\"a\", \"b\", \"c\"]\n    # test adding an object\n    obj_index.insert_object(\"d\")",
        "detail": "reference_code.llama-index-core.tests.objects.test_base",
        "documentation": {}
    },
    {
        "label": "test_object_index_default_mapping",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.objects.test_base",
        "description": "reference_code.llama-index-core.tests.objects.test_base",
        "peekOfCode": "def test_object_index_default_mapping() -> None:\n    \"\"\"Test object index.\"\"\"\n    obj_index = ObjectIndex.from_objects([\"a\", \"b\", \"c\"], index_cls=SummaryIndex)\n    # should just retrieve everything\n    assert obj_index.as_retriever().retrieve(\"test\") == [\"a\", \"b\", \"c\"]\n    # test adding an object\n    obj_index.insert_object(\"d\")\n    assert obj_index.as_retriever().retrieve(\"test\") == [\"a\", \"b\", \"c\", \"d\"]\ndef test_object_index_fn_mapping() -> None:\n    \"\"\"Test object index.\"\"\"",
        "detail": "reference_code.llama-index-core.tests.objects.test_base",
        "documentation": {}
    },
    {
        "label": "test_object_index_fn_mapping",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.objects.test_base",
        "description": "reference_code.llama-index-core.tests.objects.test_base",
        "peekOfCode": "def test_object_index_fn_mapping() -> None:\n    \"\"\"Test object index.\"\"\"\n    objects = {obj: obj for obj in [\"a\", \"b\", \"c\", \"d\"]}\n    print(objects)\n    def to_node_fn(obj: str) -> TextNode:\n        return TextNode(id_=obj, text=obj)\n    def from_node_fn(node: TextNode) -> str:\n        return objects[node.id_]\n    obj_index = ObjectIndex.from_objects(\n        [\"a\", \"b\", \"c\"],",
        "detail": "reference_code.llama-index-core.tests.objects.test_base",
        "documentation": {}
    },
    {
        "label": "test_object_index_persist",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.objects.test_base",
        "description": "reference_code.llama-index-core.tests.objects.test_base",
        "peekOfCode": "def test_object_index_persist() -> None:\n    \"\"\"Test object index persist/load.\"\"\"\n    object_mapping = SimpleObjectNodeMapping.from_objects([\"a\", \"b\", \"c\"])\n    obj_index = ObjectIndex.from_objects(\n        [\"a\", \"b\", \"c\"], object_mapping, index_cls=SummaryIndex\n    )\n    obj_index.persist()\n    reloaded_obj_index = ObjectIndex.from_persist_dir()\n    assert obj_index._index.index_id == reloaded_obj_index._index.index_id\n    assert obj_index._index.index_struct == reloaded_obj_index._index.index_struct",
        "detail": "reference_code.llama-index-core.tests.objects.test_base",
        "documentation": {}
    },
    {
        "label": "test_object_index_with_tools",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.objects.test_base",
        "description": "reference_code.llama-index-core.tests.objects.test_base",
        "peekOfCode": "def test_object_index_with_tools() -> None:\n    \"\"\"Test object index with tools.\"\"\"\n    tool1 = FunctionTool.from_defaults(fn=lambda x: x, name=\"test_tool\")\n    tool2 = FunctionTool.from_defaults(fn=lambda x, y: x + y, name=\"test_tool2\")\n    object_mapping = SimpleToolNodeMapping.from_objects([tool1, tool2])\n    obj_retriever = ObjectIndex.from_objects(\n        [tool1, tool2], object_mapping, index_cls=SummaryIndex\n    )\n    assert obj_retriever.as_retriever().retrieve(\"test\") == [tool1, tool2]",
        "detail": "reference_code.llama-index-core.tests.objects.test_base",
        "documentation": {}
    },
    {
        "label": "_TestObject",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.objects.test_node_mapping",
        "description": "reference_code.llama-index-core.tests.objects.test_node_mapping",
        "peekOfCode": "class _TestObject(BaseModel):\n    \"\"\"Test object for node mapping.\"\"\"\n    __test__ = False\n    name: str\n    def __hash__(self) -> int:\n        return hash(self.name)\n    def __str__(self) -> str:\n        return f\"_TestObject(name='{self.name}')\"\nclass _TestSQLDatabase(SQLDatabase):\n    \"\"\"Test object for SQL Table Schema Node Mapping.\"\"\"",
        "detail": "reference_code.llama-index-core.tests.objects.test_node_mapping",
        "documentation": {}
    },
    {
        "label": "_TestSQLDatabase",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.objects.test_node_mapping",
        "description": "reference_code.llama-index-core.tests.objects.test_node_mapping",
        "peekOfCode": "class _TestSQLDatabase(SQLDatabase):\n    \"\"\"Test object for SQL Table Schema Node Mapping.\"\"\"\n    def __init__(self) -> None:\n        pass\ndef test_simple_object_node_mapping() -> None:\n    \"\"\"Test simple object node mapping.\"\"\"\n    strs = [\"a\", \"b\", \"c\"]\n    node_mapping = SimpleObjectNodeMapping.from_objects(strs)\n    assert node_mapping.to_node(\"a\").text == \"a\"\n    assert node_mapping.from_node(node_mapping.to_node(\"a\")) == \"a\"",
        "detail": "reference_code.llama-index-core.tests.objects.test_node_mapping",
        "documentation": {}
    },
    {
        "label": "test_simple_object_node_mapping",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.objects.test_node_mapping",
        "description": "reference_code.llama-index-core.tests.objects.test_node_mapping",
        "peekOfCode": "def test_simple_object_node_mapping() -> None:\n    \"\"\"Test simple object node mapping.\"\"\"\n    strs = [\"a\", \"b\", \"c\"]\n    node_mapping = SimpleObjectNodeMapping.from_objects(strs)\n    assert node_mapping.to_node(\"a\").text == \"a\"\n    assert node_mapping.from_node(node_mapping.to_node(\"a\")) == \"a\"\n    objects = [_TestObject(name=\"a\"), _TestObject(name=\"b\"), _TestObject(name=\"c\")]\n    node_mapping = SimpleObjectNodeMapping.from_objects(objects)\n    assert node_mapping.to_node(objects[0]).text == \"_TestObject(name='a')\"\n    assert node_mapping.from_node(node_mapping.to_node(objects[0])) == objects[0]",
        "detail": "reference_code.llama-index-core.tests.objects.test_node_mapping",
        "documentation": {}
    },
    {
        "label": "test_simple_object_node_mapping_persist",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.objects.test_node_mapping",
        "description": "reference_code.llama-index-core.tests.objects.test_node_mapping",
        "peekOfCode": "def test_simple_object_node_mapping_persist() -> None:\n    \"\"\"Test persist/load.\"\"\"\n    strs = [\"a\", \"b\", \"c\"]\n    node_mapping = SimpleObjectNodeMapping.from_objects(strs)\n    node_mapping.persist()\n    loaded_node_mapping = SimpleObjectNodeMapping.from_persist_dir()\n    assert node_mapping.obj_node_mapping == loaded_node_mapping.obj_node_mapping\ndef test_tool_object_node_mapping() -> None:\n    \"\"\"Test tool object node mapping.\"\"\"\n    tool1 = FunctionTool.from_defaults(",
        "detail": "reference_code.llama-index-core.tests.objects.test_node_mapping",
        "documentation": {}
    },
    {
        "label": "test_tool_object_node_mapping",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.objects.test_node_mapping",
        "description": "reference_code.llama-index-core.tests.objects.test_node_mapping",
        "peekOfCode": "def test_tool_object_node_mapping() -> None:\n    \"\"\"Test tool object node mapping.\"\"\"\n    tool1 = FunctionTool.from_defaults(\n        fn=lambda x: x,\n        name=\"test_tool\",\n        description=\"test\",\n    )\n    tool2 = FunctionTool.from_defaults(\n        fn=lambda x, y: x + y, name=\"test_tool2\", description=\"test\"\n    )",
        "detail": "reference_code.llama-index-core.tests.objects.test_node_mapping",
        "documentation": {}
    },
    {
        "label": "test_sql_table_node_mapping_to_node",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.objects.test_node_mapping",
        "description": "reference_code.llama-index-core.tests.objects.test_node_mapping",
        "peekOfCode": "def test_sql_table_node_mapping_to_node(mocker: MockerFixture) -> None:\n    \"\"\"Test to add node for sql table node mapping object to ensure no 'None' values in metadata output to avoid issues with nulls when upserting to indexes.\"\"\"\n    mocker.patch(\n        \"llama_index.core.utilities.sql_wrapper.SQLDatabase.get_single_table_info\",\n        return_value=\"\",\n    )\n    # Define two table schemas with one that does not have context str defined\n    table1 = SQLTableSchema(table_name=\"table1\")\n    table2 = SQLTableSchema(table_name=\"table2\", context_str=\"stuff here\")\n    tables = [table1, table2]",
        "detail": "reference_code.llama-index-core.tests.objects.test_node_mapping",
        "documentation": {}
    },
    {
        "label": "test_lc_output_parser",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.output_parsers.test_base",
        "description": "reference_code.llama-index-core.tests.output_parsers.test_base",
        "peekOfCode": "def test_lc_output_parser() -> None:\n    \"\"\"Test langchain output parser.\"\"\"\n    class MockOutputParser(LCOutputParser):\n        \"\"\"\n        Mock output parser.\n        Similar to langchain's StructuredOutputParser, but better for testing.\n        \"\"\"\n        response_schema: ResponseSchema\n        def get_format_instructions(self) -> str:\n            \"\"\"Get format instructions.\"\"\"",
        "detail": "reference_code.llama-index-core.tests.output_parsers.test_base",
        "documentation": {}
    },
    {
        "label": "AttrDict",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.output_parsers.test_pydantic",
        "description": "reference_code.llama-index-core.tests.output_parsers.test_pydantic",
        "peekOfCode": "class AttrDict(BaseModel):\n    test_attr: str\n    foo: int\nclass TestModel(BaseModel):\n    __test__ = False\n    title: str\n    attr_dict: AttrDict\ndef test_pydantic() -> None:\n    \"\"\"Test pydantic output parser.\"\"\"\n    output = \"\"\"\\",
        "detail": "reference_code.llama-index-core.tests.output_parsers.test_pydantic",
        "documentation": {}
    },
    {
        "label": "TestModel",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.output_parsers.test_pydantic",
        "description": "reference_code.llama-index-core.tests.output_parsers.test_pydantic",
        "peekOfCode": "class TestModel(BaseModel):\n    __test__ = False\n    title: str\n    attr_dict: AttrDict\ndef test_pydantic() -> None:\n    \"\"\"Test pydantic output parser.\"\"\"\n    output = \"\"\"\\\n    Here is the valid JSON:\n    {\n        \"title\": \"TestModel\",",
        "detail": "reference_code.llama-index-core.tests.output_parsers.test_pydantic",
        "documentation": {}
    },
    {
        "label": "test_pydantic",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.output_parsers.test_pydantic",
        "description": "reference_code.llama-index-core.tests.output_parsers.test_pydantic",
        "peekOfCode": "def test_pydantic() -> None:\n    \"\"\"Test pydantic output parser.\"\"\"\n    output = \"\"\"\\\n    Here is the valid JSON:\n    {\n        \"title\": \"TestModel\",\n        \"attr_dict\": {\n            \"test_attr\": \"test_attr\",\n            \"foo\": 2\n        }",
        "detail": "reference_code.llama-index-core.tests.output_parsers.test_pydantic",
        "documentation": {}
    },
    {
        "label": "test_pydantic_format",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.output_parsers.test_pydantic",
        "description": "reference_code.llama-index-core.tests.output_parsers.test_pydantic",
        "peekOfCode": "def test_pydantic_format() -> None:\n    \"\"\"Test pydantic format.\"\"\"\n    query = \"hello world\"\n    parser = PydanticOutputParser(output_cls=AttrDict)\n    formatted_query = parser.format(query)\n    assert \"hello world\" in formatted_query\ndef test_pydantic_format_with_blocks() -> None:\n    \"\"\"Test pydantic format with blocks.\"\"\"\n    parser = PydanticOutputParser(output_cls=AttrDict)\n    messages = [",
        "detail": "reference_code.llama-index-core.tests.output_parsers.test_pydantic",
        "documentation": {}
    },
    {
        "label": "test_pydantic_format_with_blocks",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.output_parsers.test_pydantic",
        "description": "reference_code.llama-index-core.tests.output_parsers.test_pydantic",
        "peekOfCode": "def test_pydantic_format_with_blocks() -> None:\n    \"\"\"Test pydantic format with blocks.\"\"\"\n    parser = PydanticOutputParser(output_cls=AttrDict)\n    messages = [\n        ChatMessage(\n            role=\"user\",\n            blocks=[\n                TextBlock(text=\"hello world\"),\n                ImageBlock(\n                    url=\"https://pbs.twimg.com/media/GVhGD1PXkAANfPV?format=jpg&name=4096x4096\"",
        "detail": "reference_code.llama-index-core.tests.output_parsers.test_pydantic",
        "documentation": {}
    },
    {
        "label": "output_parser",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.output_parsers.test_selection",
        "description": "reference_code.llama-index-core.tests.output_parsers.test_selection",
        "peekOfCode": "def output_parser() -> SelectionOutputParser:\n    return SelectionOutputParser()\ndef test_format(output_parser: SelectionOutputParser) -> None:\n    test_template = \"Test prompt template with some {field} to fill in.\"\n    new_test_template = output_parser.format(test_template)\n    new_test_template.format(field=\"field\")\n@pytest.mark.parametrize(\n    (\"output\", \"num_match\"),\n    [\n        pytest.param(",
        "detail": "reference_code.llama-index-core.tests.output_parsers.test_selection",
        "documentation": {}
    },
    {
        "label": "test_format",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.output_parsers.test_selection",
        "description": "reference_code.llama-index-core.tests.output_parsers.test_selection",
        "peekOfCode": "def test_format(output_parser: SelectionOutputParser) -> None:\n    test_template = \"Test prompt template with some {field} to fill in.\"\n    new_test_template = output_parser.format(test_template)\n    new_test_template.format(field=\"field\")\n@pytest.mark.parametrize(\n    (\"output\", \"num_match\"),\n    [\n        pytest.param(\n            \"\"\"[\n    {\"choice\": 1, \"reason\": \"just because\"},",
        "detail": "reference_code.llama-index-core.tests.output_parsers.test_selection",
        "documentation": {}
    },
    {
        "label": "test_parse",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.output_parsers.test_selection",
        "description": "reference_code.llama-index-core.tests.output_parsers.test_selection",
        "peekOfCode": "def test_parse(\n    output_parser: SelectionOutputParser, output: str, num_match: int\n) -> None:\n    parsed = output_parser.parse(output=output)\n    assert isinstance(parsed, StructuredOutput)\n    assert isinstance(parsed.parsed_output, list)\n    assert len(parsed.parsed_output) == num_match\n    assert parsed.parsed_output[0].choice == 1\n    assert parsed.parsed_output[0].reason == \"just because\"\ndef test_failed_parse(output_parser: SelectionOutputParser) -> None:",
        "detail": "reference_code.llama-index-core.tests.output_parsers.test_selection",
        "documentation": {}
    },
    {
        "label": "test_failed_parse",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.output_parsers.test_selection",
        "description": "reference_code.llama-index-core.tests.output_parsers.test_selection",
        "peekOfCode": "def test_failed_parse(output_parser: SelectionOutputParser) -> None:\n    no_json_in_response = (\n        \" Based on the given choices, the most relevant choice for the question\"\n        \" 'What are the <redacted>?' is:\\n\\n(1) <redacted>.\\n\\nThe reason for\"\n        \" this choice is that <redacted>. Therefore, choosing option (1) would\"\n        \" provide the most relevant information for finding the <redacted>.\"\n    )\n    with pytest.raises(ValueError, match=\"Failed to convert*\") as exc_info:\n        output_parser.parse(output=no_json_in_response)",
        "detail": "reference_code.llama-index-core.tests.output_parsers.test_selection",
        "documentation": {}
    },
    {
        "label": "test_extract_json_str",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.output_parsers.test_utils",
        "description": "reference_code.llama-index-core.tests.output_parsers.test_utils",
        "peekOfCode": "def test_extract_json_str() -> None:\n    input = \"\"\"\\\nHere is the valid JSON:\n{\n    \"title\": \"TestModel\",\n    \"attr_dict\": {\n        \"test_attr\": \"test_attr\",\n        \"foo\": 2\n    }\n}\\",
        "detail": "reference_code.llama-index-core.tests.output_parsers.test_utils",
        "documentation": {}
    },
    {
        "label": "MockEmbedding",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.playground.test_base",
        "description": "reference_code.llama-index-core.tests.playground.test_base",
        "peekOfCode": "class MockEmbedding(BaseEmbedding):\n    @classmethod\n    def class_name(cls) -> str:\n        return \"MockEmbedding\"\n    async def _aget_query_embedding(self, query: str) -> List[float]:\n        del query\n        return [0, 0, 1, 0, 0]\n    async def _aget_text_embedding(self, text: str) -> List[float]:\n        text = text.strip()\n        # assume dimensions are 5",
        "detail": "reference_code.llama-index-core.tests.playground.test_base",
        "documentation": {}
    },
    {
        "label": "test_get_set_compare",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.playground.test_base",
        "description": "reference_code.llama-index-core.tests.playground.test_base",
        "peekOfCode": "def test_get_set_compare(patch_llm_predictor, patch_token_text_splitter) -> None:\n    \"\"\"Test basic comparison of indices.\"\"\"\n    documents = [Document(text=\"They're taking the Hobbits to Isengard!\")]\n    indices = [\n        VectorStoreIndex.from_documents(\n            documents=documents, embed_model=MockEmbedding()\n        ),\n        SummaryIndex.from_documents(documents),\n        TreeIndex.from_documents(documents=documents),\n    ]",
        "detail": "reference_code.llama-index-core.tests.playground.test_base",
        "documentation": {}
    },
    {
        "label": "test_from_docs",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.playground.test_base",
        "description": "reference_code.llama-index-core.tests.playground.test_base",
        "peekOfCode": "def test_from_docs(patch_llm_predictor, patch_token_text_splitter) -> None:\n    \"\"\"Test initialization via a list of documents.\"\"\"\n    documents = [\n        Document(text=\"I can't carry it for you.\"),\n        Document(text=\"But I can carry you!\"),\n    ]\n    playground = Playground.from_docs(documents=documents)\n    assert len(playground.indices) == len(DEFAULT_INDEX_CLASSES)\n    assert len(playground.retriever_modes) == len(DEFAULT_MODES)\n    with pytest.raises(ValueError):",
        "detail": "reference_code.llama-index-core.tests.playground.test_base",
        "documentation": {}
    },
    {
        "label": "test_validation",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.playground.test_base",
        "description": "reference_code.llama-index-core.tests.playground.test_base",
        "peekOfCode": "def test_validation() -> None:\n    \"\"\"Test validation of indices and modes.\"\"\"\n    with pytest.raises(ValueError):\n        _ = Playground(indices=[\"VectorStoreIndex\"])  # type: ignore\n    with pytest.raises(ValueError):\n        _ = Playground(\n            indices=[VectorStoreIndex, SummaryIndex, TreeIndex]  # type: ignore\n        )\n    with pytest.raises(ValueError):\n        _ = Playground(indices=[])  # type: ignore",
        "detail": "reference_code.llama-index-core.tests.playground.test_base",
        "documentation": {}
    },
    {
        "label": "test_forward_back_processor",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_base",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_base",
        "peekOfCode": "def test_forward_back_processor(tmp_path: Path) -> None:\n    \"\"\"Test forward-back processor.\"\"\"\n    nodes = [\n        TextNode(text=\"Hello world.\", id_=\"3\"),\n        TextNode(text=\"This is a test.\", id_=\"2\"),\n        TextNode(text=\"This is another test.\", id_=\"1\"),\n        TextNode(text=\"This is a test v2.\", id_=\"4\"),\n        TextNode(text=\"This is a test v3.\", id_=\"5\"),\n    ]\n    nodes_with_scores = [NodeWithScore(node=node) for node in nodes]",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_base",
        "documentation": {}
    },
    {
        "label": "test_fixed_recency_postprocessor",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_base",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_base",
        "peekOfCode": "def test_fixed_recency_postprocessor() -> None:\n    \"\"\"Test fixed recency processor.\"\"\"\n    # try in metadata\n    nodes = [\n        TextNode(\n            text=\"Hello world.\",\n            id_=\"1\",\n            metadata={\"date\": \"2020-01-01\"},\n            excluded_embed_metadata_keys=[\"date\"],\n        ),",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_base",
        "documentation": {}
    },
    {
        "label": "test_embedding_recency_postprocessor",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_base",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_base",
        "peekOfCode": "def test_embedding_recency_postprocessor() -> None:\n    \"\"\"Test fixed recency processor.\"\"\"\n    # try in node info\n    nodes = [\n        TextNode(\n            text=\"Hello world.\",\n            id_=\"1\",\n            metadata={\"date\": \"2020-01-01\"},\n            excluded_embed_metadata_keys=[\"date\"],\n        ),",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_base",
        "documentation": {}
    },
    {
        "label": "test_time_weighted_postprocessor",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_base",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_base",
        "peekOfCode": "def test_time_weighted_postprocessor() -> None:\n    \"\"\"Test time weighted processor.\"\"\"\n    key = \"__last_accessed__\"\n    # try in metadata\n    nodes = [\n        TextNode(text=\"Hello world.\", id_=\"1\", metadata={key: 0}),\n        TextNode(text=\"This is a test.\", id_=\"2\", metadata={key: 1}),\n        TextNode(text=\"This is another test.\", id_=\"3\", metadata={key: 2}),\n        TextNode(text=\"This is a test v2.\", id_=\"4\", metadata={key: 3}),\n    ]",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_base",
        "documentation": {}
    },
    {
        "label": "test_keyword_postprocessor",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_base",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_base",
        "peekOfCode": "def test_keyword_postprocessor() -> None:\n    \"\"\"Test keyword processor.\"\"\"\n    key = \"__last_accessed__\"\n    # try in metadata\n    nodes = [\n        TextNode(text=\"Hello world.\", id_=\"1\", metadata={key: 0}),\n        TextNode(text=\"This is a test.\", id_=\"2\", metadata={key: 1}),\n        TextNode(text=\"This is another test.\", id_=\"3\", metadata={key: 2}),\n        TextNode(text=\"This is a test v2.\", id_=\"4\", metadata={key: 3}),\n    ]",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_base",
        "documentation": {}
    },
    {
        "label": "test_keyword_postprocessor_for_non_english",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_base",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_base",
        "peekOfCode": "def test_keyword_postprocessor_for_non_english() -> None:\n    \"\"\"Test keyword processor for non English.\"\"\"\n    try:\n        key = \"__last_accessed__\"\n        # try in metadata\n        nodes = [\n            TextNode(text=\"\", id_=\"1\", metadata={key: 0}),\n            TextNode(text=\"\", id_=\"2\", metadata={key: 1}),\n            TextNode(text=\"\", id_=\"3\", metadata={key: 2}),\n            TextNode(text=\"v2\", id_=\"4\", metadata={key: 3}),",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_base",
        "documentation": {}
    },
    {
        "label": "spacy_installed",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_base",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_base",
        "peekOfCode": "spacy_installed = bool(find_spec(\"spacy\"))\ndef test_forward_back_processor(tmp_path: Path) -> None:\n    \"\"\"Test forward-back processor.\"\"\"\n    nodes = [\n        TextNode(text=\"Hello world.\", id_=\"3\"),\n        TextNode(text=\"This is a test.\", id_=\"2\"),\n        TextNode(text=\"This is another test.\", id_=\"1\"),\n        TextNode(text=\"This is a test v2.\", id_=\"4\"),\n        TextNode(text=\"This is a test v3.\", id_=\"5\"),\n    ]",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_base",
        "documentation": {}
    },
    {
        "label": "mock_llmpredictor_predict",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_llm_rerank",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_llm_rerank",
        "peekOfCode": "def mock_llmpredictor_predict(\n    self: Any, prompt: BasePromptTemplate, **prompt_args: Any\n) -> str:\n    \"\"\"Patch llm predictor predict.\"\"\"\n    context_str = prompt_args[\"context_str\"]\n    node_strs = context_str.split(\"\\n\")\n    node_to_choice_and_score = {\n        \"Test\": (True, \"1\"),\n        \"Test2\": (False, \"0\"),\n        \"Test3\": (True, \"3\"),",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_llm_rerank",
        "documentation": {}
    },
    {
        "label": "mock_format_node_batch_fn",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_llm_rerank",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_llm_rerank",
        "peekOfCode": "def mock_format_node_batch_fn(nodes: List[BaseNode]) -> str:\n    \"\"\"Mock format node batch fn.\"\"\"\n    return \"\\n\".join([node.get_content() for node in nodes])\n@patch.object(\n    MockLLM,\n    \"predict\",\n    mock_llmpredictor_predict,\n)\ndef test_llm_rerank() -> None:\n    \"\"\"Test LLM rerank.\"\"\"",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_llm_rerank",
        "documentation": {}
    },
    {
        "label": "test_llm_rerank",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_llm_rerank",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_llm_rerank",
        "peekOfCode": "def test_llm_rerank() -> None:\n    \"\"\"Test LLM rerank.\"\"\"\n    nodes = [\n        TextNode(text=\"Test\"),\n        TextNode(text=\"Test2\"),\n        TextNode(text=\"Test3\"),\n        TextNode(text=\"Test4\"),\n        TextNode(text=\"Test5\"),\n        TextNode(text=\"Test6\"),\n        TextNode(text=\"Test7\"),",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_llm_rerank",
        "documentation": {}
    },
    {
        "label": "test_metadata_replacement",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_metadata_replacement",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_metadata_replacement",
        "peekOfCode": "def test_metadata_replacement() -> None:\n    node = TextNode(\n        text=\"This is a test 1.\", metadata={\"key\": \"This is a another test.\"}\n    )\n    nodes = [NodeWithScore(node=node, score=1.0)]\n    postprocessor = MetadataReplacementPostProcessor(target_metadata_key=\"key\")\n    nodes = postprocessor.postprocess_nodes(nodes)\n    assert len(nodes) == 1\n    assert nodes[0].node.get_content() == \"This is a another test.\"",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_metadata_replacement",
        "documentation": {}
    },
    {
        "label": "mock_tokenizer_fn",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_optimizer",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_optimizer",
        "peekOfCode": "def mock_tokenizer_fn(text: str) -> List[str]:\n    \"\"\"Mock tokenizer function.\"\"\"\n    # split by words\n    return text.split(\" \")\ndef mock_tokenizer_fn2(text: str) -> List[str]:\n    \"\"\"Mock tokenizer function.\"\"\"\n    # split by words\n    return text.split(\",\")\ndef mock_get_text_embedding(text: str) -> List[float]:\n    \"\"\"Mock get text embedding.\"\"\"",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_optimizer",
        "documentation": {}
    },
    {
        "label": "mock_tokenizer_fn2",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_optimizer",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_optimizer",
        "peekOfCode": "def mock_tokenizer_fn2(text: str) -> List[str]:\n    \"\"\"Mock tokenizer function.\"\"\"\n    # split by words\n    return text.split(\",\")\ndef mock_get_text_embedding(text: str) -> List[float]:\n    \"\"\"Mock get text embedding.\"\"\"\n    # assume dimensions are 5\n    if text == \"hello\":\n        return [1, 0, 0, 0, 0]\n    elif text == \"world\":",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_optimizer",
        "documentation": {}
    },
    {
        "label": "mock_get_text_embedding",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_optimizer",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_optimizer",
        "peekOfCode": "def mock_get_text_embedding(text: str) -> List[float]:\n    \"\"\"Mock get text embedding.\"\"\"\n    # assume dimensions are 5\n    if text == \"hello\":\n        return [1, 0, 0, 0, 0]\n    elif text == \"world\":\n        return [0, 1, 0, 0, 0]\n    elif text == \"foo\":\n        return [0, 0, 1, 0, 0]\n    elif text == \"bar\":",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_optimizer",
        "documentation": {}
    },
    {
        "label": "mock_get_text_embeddings",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_optimizer",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_optimizer",
        "peekOfCode": "def mock_get_text_embeddings(texts: List[str]) -> List[List[float]]:\n    \"\"\"Mock get text embeddings.\"\"\"\n    return [mock_get_text_embedding(text) for text in texts]\ndef mock_get_text_embedding_chinese(text: str) -> List[float]:\n    \"\"\"Mock get text embedding.\"\"\"\n    # assume dimensions are 5\n    if text == \"\":\n        return [1, 0, 0, 0, 0]\n    elif text == \"\":\n        return [0, 1, 0, 0, 0]",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_optimizer",
        "documentation": {}
    },
    {
        "label": "mock_get_text_embedding_chinese",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_optimizer",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_optimizer",
        "peekOfCode": "def mock_get_text_embedding_chinese(text: str) -> List[float]:\n    \"\"\"Mock get text embedding.\"\"\"\n    # assume dimensions are 5\n    if text == \"\":\n        return [1, 0, 0, 0, 0]\n    elif text == \"\":\n        return [0, 1, 0, 0, 0]\n    elif text == \"\":\n        return [0, 0, 1, 0, 0]\n    elif text == \"\":",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_optimizer",
        "documentation": {}
    },
    {
        "label": "mock_get_text_embeddings_chinese",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_optimizer",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_optimizer",
        "peekOfCode": "def mock_get_text_embeddings_chinese(texts: List[str]) -> List[List[float]]:\n    \"\"\"Mock get text embeddings.\"\"\"\n    return [mock_get_text_embedding_chinese(text) for text in texts]\n@patch.object(MockEmbedding, \"_get_text_embedding\", side_effect=mock_get_text_embedding)\n@patch.object(\n    MockEmbedding, \"_get_text_embeddings\", side_effect=mock_get_text_embeddings\n)\ndef test_optimizer(_mock_embeds: Any, _mock_embed: Any) -> None:\n    \"\"\"Test optimizer.\"\"\"\n    optimizer = SentenceEmbeddingOptimizer(",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_optimizer",
        "documentation": {}
    },
    {
        "label": "test_optimizer",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_optimizer",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_optimizer",
        "peekOfCode": "def test_optimizer(_mock_embeds: Any, _mock_embed: Any) -> None:\n    \"\"\"Test optimizer.\"\"\"\n    optimizer = SentenceEmbeddingOptimizer(\n        embed_model=MockEmbedding(embed_dim=5),\n        tokenizer_fn=mock_tokenizer_fn,\n        percentile_cutoff=0.5,\n        context_before=0,\n        context_after=0,\n    )\n    query = QueryBundle(query_str=\"hello\", embedding=[1, 0, 0, 0, 0])",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_optimizer",
        "documentation": {}
    },
    {
        "label": "mock_rankgpt_chat",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_rankgpt_rerank",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_rankgpt_rerank",
        "peekOfCode": "def mock_rankgpt_chat(self: Any, messages, **kwargs: Any) -> ChatResponse:\n    return ChatResponse(\n        message=ChatMessage(role=MessageRole.SYSTEM, content=\"[2] > [1] > [3]\")\n    )\nasync def mock_rankgpt_achat(self, messages, **kwargs: Any) -> ChatResponse:\n    # Mock api call\n    await asyncio.sleep(1)\n    return ChatResponse(\n        message=ChatMessage(role=MessageRole.SYSTEM, content=\"[2] > [1] > [3]\")\n    )",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_rankgpt_rerank",
        "documentation": {}
    },
    {
        "label": "test_rankgpt_rerank",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_rankgpt_rerank",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_rankgpt_rerank",
        "peekOfCode": "def test_rankgpt_rerank():\n    rankgpt_rerank = RankGPTRerank(\n        top_n=2,\n        llm=MockLLM(),\n    )\n    result = rankgpt_rerank.postprocess_nodes(nodes_with_score, query_str=\"Test query\")\n    assert len(result) == 2\n    assert result[0].node.get_content() == \"Test2\"\n    assert result[1].node.get_content() == \"Test\"\n@patch.object(",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_rankgpt_rerank",
        "documentation": {}
    },
    {
        "label": "nodes",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_rankgpt_rerank",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_rankgpt_rerank",
        "peekOfCode": "nodes = [\n    TextNode(text=\"Test\"),\n    TextNode(text=\"Test2\"),\n    TextNode(text=\"Test3\"),\n]\nnodes_with_score = [NodeWithScore(node=n) for n in nodes]\n@patch.object(\n    MockLLM,\n    \"chat\",\n    mock_rankgpt_chat,",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_rankgpt_rerank",
        "documentation": {}
    },
    {
        "label": "nodes_with_score",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_rankgpt_rerank",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_rankgpt_rerank",
        "peekOfCode": "nodes_with_score = [NodeWithScore(node=n) for n in nodes]\n@patch.object(\n    MockLLM,\n    \"chat\",\n    mock_rankgpt_chat,\n)\ndef test_rankgpt_rerank():\n    rankgpt_rerank = RankGPTRerank(\n        top_n=2,\n        llm=MockLLM(),",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_rankgpt_rerank",
        "documentation": {}
    },
    {
        "label": "MockFunctionCallingLLM",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_structured_llm_rerank",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_structured_llm_rerank",
        "peekOfCode": "class MockFunctionCallingLLM(MockLLM):\n    @property\n    def metadata(self) -> LLMMetadata:\n        return super().metadata.model_copy(update={\"is_function_calling_model\": True})\n@patch.object(\n    MockFunctionCallingLLM,\n    \"structured_predict\",\n    mock_llmpredictor_structured_predict,\n)\ndef test_llm_rerank() -> None:",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_structured_llm_rerank",
        "documentation": {}
    },
    {
        "label": "mock_llmpredictor_structured_predict",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_structured_llm_rerank",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_structured_llm_rerank",
        "peekOfCode": "def mock_llmpredictor_structured_predict(\n    self: Any, prompt: BasePromptTemplate, **prompt_args: Any\n) -> DocumentRelevanceList:\n    \"\"\"Patch llm predictor predict.\"\"\"\n    context_str = prompt_args[\"context_str\"]\n    node_strs = context_str.split(\"\\n\")\n    node_to_choice_and_score = {\n        \"Test\": (True, \"1\"),\n        \"Test2\": (False, \"0\"),\n        \"Test3\": (True, \"3\"),",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_structured_llm_rerank",
        "documentation": {}
    },
    {
        "label": "mock_format_node_batch_fn",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_structured_llm_rerank",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_structured_llm_rerank",
        "peekOfCode": "def mock_format_node_batch_fn(nodes: List[BaseNode]) -> str:\n    \"\"\"Mock format node batch fn.\"\"\"\n    return \"\\n\".join([node.get_content() for node in nodes])\nclass MockFunctionCallingLLM(MockLLM):\n    @property\n    def metadata(self) -> LLMMetadata:\n        return super().metadata.model_copy(update={\"is_function_calling_model\": True})\n@patch.object(\n    MockFunctionCallingLLM,\n    \"structured_predict\",",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_structured_llm_rerank",
        "documentation": {}
    },
    {
        "label": "test_llm_rerank",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_structured_llm_rerank",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_structured_llm_rerank",
        "peekOfCode": "def test_llm_rerank() -> None:\n    \"\"\"Test LLM rerank.\"\"\"\n    nodes = [\n        TextNode(text=\"Test\"),\n        TextNode(text=\"Test2\"),\n        TextNode(text=\"Test3\"),\n        TextNode(text=\"Test4\"),\n        TextNode(text=\"Test5\"),\n        TextNode(text=\"Test6\"),\n        TextNode(text=\"Test7\"),",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_structured_llm_rerank",
        "documentation": {}
    },
    {
        "label": "mock_errored_structured_predict",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_structured_llm_rerank",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_structured_llm_rerank",
        "peekOfCode": "def mock_errored_structured_predict(\n    self: Any, prompt: BasePromptTemplate, **prompt_args: Any\n) -> str:\n    return \"fake error\"\n@patch.object(\n    MockFunctionCallingLLM,\n    \"structured_predict\",\n    mock_errored_structured_predict,\n)\n@pytest.mark.parametrize(\"raise_on_failure\", [True, False])",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_structured_llm_rerank",
        "documentation": {}
    },
    {
        "label": "test_llm_rerank_errored_structured_predict",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.postprocessor.test_structured_llm_rerank",
        "description": "reference_code.llama-index-core.tests.postprocessor.test_structured_llm_rerank",
        "peekOfCode": "def test_llm_rerank_errored_structured_predict(raise_on_failure: bool) -> None:\n    \"\"\"Test LLM rerank with errored structured predict.\"\"\"\n    nodes = [\n        TextNode(text=\"Test\"),\n        TextNode(text=\"Test2\"),\n        TextNode(text=\"Test3\"),\n        TextNode(text=\"Test4\"),\n    ]\n    nodes_with_score = [NodeWithScore(node=n) for n in nodes]\n    llm = MockFunctionCallingLLM()",
        "detail": "reference_code.llama-index-core.tests.postprocessor.test_structured_llm_rerank",
        "documentation": {}
    },
    {
        "label": "MockSong",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.program.test_function_program",
        "description": "reference_code.llama-index-core.tests.program.test_function_program",
        "peekOfCode": "class MockSong(BaseModel):\n    \"\"\"Mock Song class.\"\"\"\n    title: str\nclass MockAlbum(BaseModel):\n    title: str\n    artist: str\n    songs: List[MockSong]\nMOCK_ALBUM = MockAlbum(\n    title=\"hello\",\n    artist=\"world\",",
        "detail": "reference_code.llama-index-core.tests.program.test_function_program",
        "documentation": {}
    },
    {
        "label": "MockAlbum",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.program.test_function_program",
        "description": "reference_code.llama-index-core.tests.program.test_function_program",
        "peekOfCode": "class MockAlbum(BaseModel):\n    title: str\n    artist: str\n    songs: List[MockSong]\nMOCK_ALBUM = MockAlbum(\n    title=\"hello\",\n    artist=\"world\",\n    songs=[MockSong(title=\"song1\"), MockSong(title=\"song2\")],\n)\nMOCK_ALBUM_2 = MockAlbum(",
        "detail": "reference_code.llama-index-core.tests.program.test_function_program",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.program.test_function_program",
        "description": "reference_code.llama-index-core.tests.program.test_function_program",
        "peekOfCode": "class MockLLM(MagicMock):\n    def predict_and_call(\n        self,\n        tools: List[\"BaseTool\"],\n        user_msg: Optional[Union[str, ChatMessage]] = None,\n        chat_history: Optional[List[ChatMessage]] = None,\n        verbose: bool = False,\n        allow_parallel_tool_calls: bool = False,\n        **kwargs: Any,\n    ) -> \"AgentChatResponse\":",
        "detail": "reference_code.llama-index-core.tests.program.test_function_program",
        "documentation": {}
    },
    {
        "label": "test_function_program",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_function_program",
        "description": "reference_code.llama-index-core.tests.program.test_function_program",
        "peekOfCode": "def test_function_program() -> None:\n    \"\"\"Test Function program.\"\"\"\n    prompt_template_str = \"\"\"This is a test album with {topic}\"\"\"\n    llm_program = FunctionCallingProgram.from_defaults(\n        output_cls=MockAlbum,\n        prompt_template_str=prompt_template_str,\n        llm=MockLLM(),\n    )\n    obj_output = llm_program(topic=\"songs\")\n    assert isinstance(obj_output, MockAlbum)",
        "detail": "reference_code.llama-index-core.tests.program.test_function_program",
        "documentation": {}
    },
    {
        "label": "test_function_program_multiple",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_function_program",
        "description": "reference_code.llama-index-core.tests.program.test_function_program",
        "peekOfCode": "def test_function_program_multiple() -> None:\n    \"\"\"Test Function program multiple.\"\"\"\n    prompt_template_str = \"\"\"This is a test album with {topic}\"\"\"\n    llm_program = FunctionCallingProgram.from_defaults(\n        output_cls=MockAlbum,\n        prompt_template_str=prompt_template_str,\n        llm=MockLLM(),\n        allow_parallel_tool_calls=True,\n    )\n    obj_outputs = llm_program(topic=\"songs\")",
        "detail": "reference_code.llama-index-core.tests.program.test_function_program",
        "documentation": {}
    },
    {
        "label": "MOCK_ALBUM",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.program.test_function_program",
        "description": "reference_code.llama-index-core.tests.program.test_function_program",
        "peekOfCode": "MOCK_ALBUM = MockAlbum(\n    title=\"hello\",\n    artist=\"world\",\n    songs=[MockSong(title=\"song1\"), MockSong(title=\"song2\")],\n)\nMOCK_ALBUM_2 = MockAlbum(\n    title=\"hello2\",\n    artist=\"world2\",\n    songs=[MockSong(title=\"song3\"), MockSong(title=\"song4\")],\n)",
        "detail": "reference_code.llama-index-core.tests.program.test_function_program",
        "documentation": {}
    },
    {
        "label": "MOCK_ALBUM_2",
        "kind": 5,
        "importPath": "reference_code.llama-index-core.tests.program.test_function_program",
        "description": "reference_code.llama-index-core.tests.program.test_function_program",
        "peekOfCode": "MOCK_ALBUM_2 = MockAlbum(\n    title=\"hello2\",\n    artist=\"world2\",\n    songs=[MockSong(title=\"song3\"), MockSong(title=\"song4\")],\n)\ndef _get_mock_album_response(\n    allow_parallel_tool_calls: bool = False,\n) -> AgentChatResponse:\n    \"\"\"Get mock album.\"\"\"\n    if allow_parallel_tool_calls:",
        "detail": "reference_code.llama-index-core.tests.program.test_function_program",
        "documentation": {}
    },
    {
        "label": "MockLLM",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.program.test_llm_program",
        "description": "reference_code.llama-index-core.tests.program.test_llm_program",
        "peekOfCode": "class MockLLM(MagicMock):\n    def complete(self, prompt: str) -> CompletionResponse:\n        test_object = {\"hello\": \"world\"}\n        text = json.dumps(test_object)\n        return CompletionResponse(text=text)\n    @property\n    def metadata(self) -> LLMMetadata:\n        return LLMMetadata()\nclass MockChatLLM(MagicMock):\n    def chat(self, prompt: str) -> ChatResponse:",
        "detail": "reference_code.llama-index-core.tests.program.test_llm_program",
        "documentation": {}
    },
    {
        "label": "MockChatLLM",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.program.test_llm_program",
        "description": "reference_code.llama-index-core.tests.program.test_llm_program",
        "peekOfCode": "class MockChatLLM(MagicMock):\n    def chat(self, prompt: str) -> ChatResponse:\n        test_object = {\"hello\": \"chat\"}\n        text = json.dumps(test_object)\n        return ChatResponse(\n            message=ChatMessage(role=MessageRole.ASSISTANT, content=text)\n        )\n    @property\n    def metadata(self) -> LLMMetadata:\n        metadata = LLMMetadata()",
        "detail": "reference_code.llama-index-core.tests.program.test_llm_program",
        "documentation": {}
    },
    {
        "label": "TestModel",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.program.test_llm_program",
        "description": "reference_code.llama-index-core.tests.program.test_llm_program",
        "peekOfCode": "class TestModel(BaseModel):\n    __test__ = False\n    hello: str\ndef test_llm_program() -> None:\n    \"\"\"Test LLM program.\"\"\"\n    output_parser = PydanticOutputParser(output_cls=TestModel)\n    llm_program = LLMTextCompletionProgram.from_defaults(\n        output_parser=output_parser,\n        prompt_template_str=\"This is a test prompt with a {test_input}.\",\n        llm=MockLLM(),",
        "detail": "reference_code.llama-index-core.tests.program.test_llm_program",
        "documentation": {}
    },
    {
        "label": "test_llm_program",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_llm_program",
        "description": "reference_code.llama-index-core.tests.program.test_llm_program",
        "peekOfCode": "def test_llm_program() -> None:\n    \"\"\"Test LLM program.\"\"\"\n    output_parser = PydanticOutputParser(output_cls=TestModel)\n    llm_program = LLMTextCompletionProgram.from_defaults(\n        output_parser=output_parser,\n        prompt_template_str=\"This is a test prompt with a {test_input}.\",\n        llm=MockLLM(),\n    )\n    # mock llm\n    obj_output = llm_program(test_input=\"hello\")",
        "detail": "reference_code.llama-index-core.tests.program.test_llm_program",
        "documentation": {}
    },
    {
        "label": "test_llm_program_with_messages",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_llm_program",
        "description": "reference_code.llama-index-core.tests.program.test_llm_program",
        "peekOfCode": "def test_llm_program_with_messages() -> None:\n    \"\"\"Test LLM program.\"\"\"\n    messages = [ChatMessage(role=MessageRole.USER, content=\"Test\")]\n    prompt = ChatPromptTemplate(message_templates=messages)\n    output_parser = PydanticOutputParser(output_cls=TestModel)\n    llm_program = LLMTextCompletionProgram.from_defaults(\n        output_parser=output_parser,\n        prompt=prompt,\n        llm=MockLLM(),\n    )",
        "detail": "reference_code.llama-index-core.tests.program.test_llm_program",
        "documentation": {}
    },
    {
        "label": "test_llm_program_with_messages_and_chat",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_llm_program",
        "description": "reference_code.llama-index-core.tests.program.test_llm_program",
        "peekOfCode": "def test_llm_program_with_messages_and_chat() -> None:\n    \"\"\"Test LLM program.\"\"\"\n    messages = [ChatMessage(role=MessageRole.USER, content=\"Test\")]\n    prompt = ChatPromptTemplate(message_templates=messages)\n    output_parser = PydanticOutputParser(output_cls=TestModel)\n    llm_program = LLMTextCompletionProgram.from_defaults(\n        output_parser=output_parser,\n        prompt=prompt,\n        llm=MockChatLLM(),\n    )",
        "detail": "reference_code.llama-index-core.tests.program.test_llm_program",
        "documentation": {}
    },
    {
        "label": "MagicLLM",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.program.test_multi_modal_llm_program",
        "description": "reference_code.llama-index-core.tests.program.test_multi_modal_llm_program",
        "peekOfCode": "class MagicLLM(MagicMock):\n    def chat(self, messages: Sequence[ChatMessage]) -> ChatResponse:\n        test_object = {\"hello\": \"world\"}\n        text = json.dumps(test_object)\n        return ChatResponse(message=ChatMessage(role=\"assistant\", content=text))\n    @property\n    def metadata(self) -> LLMMetadata:\n        return LLMMetadata()\nclass TestModel(BaseModel):\n    __test__ = False",
        "detail": "reference_code.llama-index-core.tests.program.test_multi_modal_llm_program",
        "documentation": {}
    },
    {
        "label": "TestModel",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.program.test_multi_modal_llm_program",
        "description": "reference_code.llama-index-core.tests.program.test_multi_modal_llm_program",
        "peekOfCode": "class TestModel(BaseModel):\n    __test__ = False\n    hello: str\ndef test_multi_modal_llm_program(image_url: str) -> None:\n    \"\"\"Test Multi Modal LLM Pydantic program.\"\"\"\n    output_parser = PydanticOutputParser(output_cls=TestModel)\n    multi_modal_llm_program = MultiModalLLMCompletionProgram.from_defaults(\n        output_parser=output_parser,\n        prompt_template_str=\"This is a test prompt with a {test_input}.\",\n        multi_modal_llm=MagicLLM(),",
        "detail": "reference_code.llama-index-core.tests.program.test_multi_modal_llm_program",
        "documentation": {}
    },
    {
        "label": "image_url",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_multi_modal_llm_program",
        "description": "reference_code.llama-index-core.tests.program.test_multi_modal_llm_program",
        "peekOfCode": "def image_url() -> str:\n    return \"https://astrabert.github.io/hophop-science/images/whale_doing_science.png\"\nclass MagicLLM(MagicMock):\n    def chat(self, messages: Sequence[ChatMessage]) -> ChatResponse:\n        test_object = {\"hello\": \"world\"}\n        text = json.dumps(test_object)\n        return ChatResponse(message=ChatMessage(role=\"assistant\", content=text))\n    @property\n    def metadata(self) -> LLMMetadata:\n        return LLMMetadata()",
        "detail": "reference_code.llama-index-core.tests.program.test_multi_modal_llm_program",
        "documentation": {}
    },
    {
        "label": "test_multi_modal_llm_program",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_multi_modal_llm_program",
        "description": "reference_code.llama-index-core.tests.program.test_multi_modal_llm_program",
        "peekOfCode": "def test_multi_modal_llm_program(image_url: str) -> None:\n    \"\"\"Test Multi Modal LLM Pydantic program.\"\"\"\n    output_parser = PydanticOutputParser(output_cls=TestModel)\n    multi_modal_llm_program = MultiModalLLMCompletionProgram.from_defaults(\n        output_parser=output_parser,\n        prompt_template_str=\"This is a test prompt with a {test_input}.\",\n        multi_modal_llm=MagicLLM(),\n        image_documents=[ImageBlock(url=image_url)],\n    )\n    # mock Multi Modal llm",
        "detail": "reference_code.llama-index-core.tests.program.test_multi_modal_llm_program",
        "documentation": {}
    },
    {
        "label": "Joke",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "description": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "peekOfCode": "class Joke(BaseModel):\n    \"\"\"Test joke model.\"\"\"\n    setup: str\n    punchline: Optional[str] = None\nclass Show(BaseModel):\n    \"\"\"Test show model with jokes list.\"\"\"\n    title: str = \"\"\n    jokes: List[Joke] = Field(default_factory=list)\nclass Person(BaseModel):\n    \"\"\"Test person model.\"\"\"",
        "detail": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "documentation": {}
    },
    {
        "label": "Show",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "description": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "peekOfCode": "class Show(BaseModel):\n    \"\"\"Test show model with jokes list.\"\"\"\n    title: str = \"\"\n    jokes: List[Joke] = Field(default_factory=list)\nclass Person(BaseModel):\n    \"\"\"Test person model.\"\"\"\n    name: str\n    age: Optional[int] = None\n    hobbies: List[str] = Field(default_factory=list)\ndef test_process_streaming_content_incremental_complete_json():",
        "detail": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "documentation": {}
    },
    {
        "label": "Person",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "description": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "peekOfCode": "class Person(BaseModel):\n    \"\"\"Test person model.\"\"\"\n    name: str\n    age: Optional[int] = None\n    hobbies: List[str] = Field(default_factory=list)\ndef test_process_streaming_content_incremental_complete_json():\n    \"\"\"Test processing complete JSON content.\"\"\"\n    response = ChatResponse(\n        message=ChatMessage(\n            role=MessageRole.ASSISTANT,",
        "detail": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "documentation": {}
    },
    {
        "label": "test_process_streaming_content_incremental_complete_json",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "description": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "peekOfCode": "def test_process_streaming_content_incremental_complete_json():\n    \"\"\"Test processing complete JSON content.\"\"\"\n    response = ChatResponse(\n        message=ChatMessage(\n            role=MessageRole.ASSISTANT,\n            content='{\"name\": \"John\", \"age\": 30, \"hobbies\": [\"reading\", \"coding\"]}',\n        )\n    )\n    result = process_streaming_content_incremental(response, Person)\n    assert isinstance(result, Person)",
        "detail": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "documentation": {}
    },
    {
        "label": "test_process_streaming_content_incremental_incomplete_json",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "description": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "peekOfCode": "def test_process_streaming_content_incremental_incomplete_json():\n    \"\"\"Test processing incomplete JSON content.\"\"\"\n    response = ChatResponse(\n        message=ChatMessage(\n            role=MessageRole.ASSISTANT,\n            content='{\"name\": \"John\", \"age\": 30',\n        )\n    )\n    result = process_streaming_content_incremental(response, Person)\n    # Should handle incomplete JSON gracefully",
        "detail": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "documentation": {}
    },
    {
        "label": "test_process_streaming_content_incremental_with_current_object",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "description": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "peekOfCode": "def test_process_streaming_content_incremental_with_current_object():\n    \"\"\"Test processing with existing current object.\"\"\"\n    current_person = Person(name=\"Jane\", age=25)\n    response = ChatResponse(\n        message=ChatMessage(\n            role=MessageRole.ASSISTANT,\n            content='{\"name\": \"John\", \"age\": 30}',\n        )\n    )\n    result = process_streaming_content_incremental(response, Person, current_person)",
        "detail": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "documentation": {}
    },
    {
        "label": "test_process_streaming_content_incremental_empty_content",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "description": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "peekOfCode": "def test_process_streaming_content_incremental_empty_content():\n    \"\"\"Test processing empty content.\"\"\"\n    response = ChatResponse(\n        message=ChatMessage(\n            role=MessageRole.ASSISTANT,\n            content=\"\",\n        )\n    )\n    result = process_streaming_content_incremental(response, Person)\n    # Should return FlexibleModel instance when no content",
        "detail": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "documentation": {}
    },
    {
        "label": "test_process_streaming_content_incremental_with_list",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "description": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "peekOfCode": "def test_process_streaming_content_incremental_with_list():\n    \"\"\"Test processing content with list structures.\"\"\"\n    response = ChatResponse(\n        message=ChatMessage(\n            role=MessageRole.ASSISTANT,\n            content='{\"title\": \"Comedy Show\", \"jokes\": [{\"setup\": \"Why did the chicken cross the road?\", \"punchline\": \"To get to the other side!\"}]}',\n        )\n    )\n    result = process_streaming_content_incremental(response, Show)\n    assert isinstance(result, Show)",
        "detail": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "documentation": {}
    },
    {
        "label": "test_process_streaming_content_incremental_malformed_json",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "description": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "peekOfCode": "def test_process_streaming_content_incremental_malformed_json():\n    \"\"\"Test processing malformed JSON with current object.\"\"\"\n    current_show = Show(\n        title=\"Comedy Show\",\n        jokes=[Joke(setup=\"First joke\", punchline=\"First punchline\")],\n    )\n    response = ChatResponse(\n        message=ChatMessage(\n            role=MessageRole.ASSISTANT,\n            content='{\"jokes\": [{\"setup\": \"Second joke\", \"punchline\": \"Second punchline\"}',",
        "detail": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "documentation": {}
    },
    {
        "label": "test_extract_partial_list_progress_valid",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "description": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "peekOfCode": "def test_extract_partial_list_progress_valid():\n    \"\"\"Test extracting partial list progress with valid content.\"\"\"\n    content = '{\"jokes\": [{\"setup\": \"Why did the chicken cross the road?\", \"punchline\": \"To get to the other side!\"}]'\n    current_show = Show(title=\"Comedy Show\")\n    from llama_index.core.program.utils import create_flexible_model\n    partial_cls = create_flexible_model(Show)\n    result = _extract_partial_list_progress(content, Show, current_show, partial_cls)\n    if result is not None:\n        assert hasattr(result, \"jokes\")\ndef test_extract_partial_list_progress_no_current():",
        "detail": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "documentation": {}
    },
    {
        "label": "test_extract_partial_list_progress_no_current",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "description": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "peekOfCode": "def test_extract_partial_list_progress_no_current():\n    \"\"\"Test extracting partial list progress without current object.\"\"\"\n    content = '{\"jokes\": [{\"setup\": \"Why did the chicken cross the road?\"}]'\n    from llama_index.core.program.utils import create_flexible_model\n    partial_cls = create_flexible_model(Show)\n    result = _extract_partial_list_progress(content, Show, None, partial_cls)\n    assert result is None\ndef test_extract_partial_list_progress_invalid_content():\n    \"\"\"Test extracting partial list progress with invalid content.\"\"\"\n    content = \"invalid json content\"",
        "detail": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "documentation": {}
    },
    {
        "label": "test_extract_partial_list_progress_invalid_content",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "description": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "peekOfCode": "def test_extract_partial_list_progress_invalid_content():\n    \"\"\"Test extracting partial list progress with invalid content.\"\"\"\n    content = \"invalid json content\"\n    current_show = Show(title=\"Comedy Show\")\n    from llama_index.core.program.utils import create_flexible_model\n    partial_cls = create_flexible_model(Show)\n    result = _extract_partial_list_progress(content, Show, current_show, partial_cls)\n    assert result is None\ndef test_parse_partial_list_items_complete_objects():\n    \"\"\"Test parsing complete objects from list content.\"\"\"",
        "detail": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "documentation": {}
    },
    {
        "label": "test_parse_partial_list_items_complete_objects",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "description": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "peekOfCode": "def test_parse_partial_list_items_complete_objects():\n    \"\"\"Test parsing complete objects from list content.\"\"\"\n    list_content = '{\"setup\": \"Why did the chicken cross the road?\", \"punchline\": \"To get to the other side!\"}, {\"setup\": \"Second joke\", \"punchline\": \"Second punchline\"}'\n    result = _parse_partial_list_items(list_content, \"jokes\", Show)\n    assert isinstance(result, list)\n    assert len(result) == 2\n    assert result[0][\"setup\"] == \"Why did the chicken cross the road?\"\n    assert result[0][\"punchline\"] == \"To get to the other side!\"\n    assert result[1][\"setup\"] == \"Second joke\"\n    assert result[1][\"punchline\"] == \"Second punchline\"",
        "detail": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "documentation": {}
    },
    {
        "label": "test_parse_partial_list_items_incomplete_objects",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "description": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "peekOfCode": "def test_parse_partial_list_items_incomplete_objects():\n    \"\"\"Test parsing incomplete objects from list content.\"\"\"\n    list_content = '{\"setup\": \"Why did the chicken cross the road?\", \"punchline\": \"To get to the other side!\"}, {\"setup\": \"Second joke\"'\n    result = _parse_partial_list_items(list_content, \"jokes\", Show)\n    assert isinstance(result, list)\n    # Should get at least the complete object\n    assert len(result) >= 1\n    assert result[0][\"setup\"] == \"Why did the chicken cross the road?\"\ndef test_parse_partial_list_items_invalid_content():\n    \"\"\"Test parsing invalid content returns empty list.\"\"\"",
        "detail": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "documentation": {}
    },
    {
        "label": "test_parse_partial_list_items_invalid_content",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "description": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "peekOfCode": "def test_parse_partial_list_items_invalid_content():\n    \"\"\"Test parsing invalid content returns empty list.\"\"\"\n    list_content = \"completely invalid content\"\n    result = _parse_partial_list_items(list_content, \"jokes\", Show)\n    assert isinstance(result, list)\n    assert len(result) == 0\ndef test_parse_partial_list_items_empty_content():\n    \"\"\"Test parsing empty content returns empty list.\"\"\"\n    list_content = \"\"\n    result = _parse_partial_list_items(list_content, \"jokes\", Show)",
        "detail": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "documentation": {}
    },
    {
        "label": "test_parse_partial_list_items_empty_content",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "description": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "peekOfCode": "def test_parse_partial_list_items_empty_content():\n    \"\"\"Test parsing empty content returns empty list.\"\"\"\n    list_content = \"\"\n    result = _parse_partial_list_items(list_content, \"jokes\", Show)\n    assert isinstance(result, list)\n    assert len(result) == 0\ndef test_parse_partial_list_items_malformed_json():\n    \"\"\"Test parsing malformed JSON objects.\"\"\"\n    list_content = '{\"setup\": \"Why did the chicken cross the road?\", \"punchline\": \"To get to the other side!\"}, {\"setup\": \"Second joke\", invalid'\n    result = _parse_partial_list_items(list_content, \"jokes\", Show)",
        "detail": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "documentation": {}
    },
    {
        "label": "test_parse_partial_list_items_malformed_json",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "description": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "peekOfCode": "def test_parse_partial_list_items_malformed_json():\n    \"\"\"Test parsing malformed JSON objects.\"\"\"\n    list_content = '{\"setup\": \"Why did the chicken cross the road?\", \"punchline\": \"To get to the other side!\"}, {\"setup\": \"Second joke\", invalid'\n    result = _parse_partial_list_items(list_content, \"jokes\", Show)\n    assert isinstance(result, list)\n    # Should get the complete object, ignore the malformed one\n    assert len(result) >= 1\n    assert result[0][\"setup\"] == \"Why did the chicken cross the road?\"\ndef test_process_streaming_content_incremental_none_message():\n    \"\"\"Test processing when message content is None.\"\"\"",
        "detail": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "documentation": {}
    },
    {
        "label": "test_process_streaming_content_incremental_none_message",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "description": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "peekOfCode": "def test_process_streaming_content_incremental_none_message():\n    \"\"\"Test processing when message content is None.\"\"\"\n    response = ChatResponse(\n        message=ChatMessage(\n            role=MessageRole.ASSISTANT,\n            content=None,\n        )\n    )\n    result = process_streaming_content_incremental(response, Person)\n    # Should return FlexibleModel instance when no content",
        "detail": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "documentation": {}
    },
    {
        "label": "test_process_streaming_content_incremental_progressive_list_building",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "description": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "peekOfCode": "def test_process_streaming_content_incremental_progressive_list_building():\n    \"\"\"Test progressive list building with incremental updates.\"\"\"\n    # Start with empty show\n    current_show = Show(title=\"Comedy Show\", jokes=[])\n    # First update - add one joke\n    response1 = ChatResponse(\n        message=ChatMessage(\n            role=MessageRole.ASSISTANT,\n            content='{\"title\": \"Comedy Show\", \"jokes\": [{\"setup\": \"First joke\", \"punchline\": \"First punchline\"}]}',\n        )",
        "detail": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "documentation": {}
    },
    {
        "label": "test_process_streaming_content_incremental_validation_error_fallback",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "description": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "peekOfCode": "def test_process_streaming_content_incremental_validation_error_fallback():\n    \"\"\"Test fallback when validation to target class fails.\"\"\"\n    # Create content that validates to FlexibleModel but not to strict Person\n    response = ChatResponse(\n        message=ChatMessage(\n            role=MessageRole.ASSISTANT,\n            content='{\"name\": \"John\", \"unknown_field\": \"value\"}',\n        )\n    )\n    result = process_streaming_content_incremental(response, Person)",
        "detail": "reference_code.llama-index-core.tests.program.test_streaming_utils",
        "documentation": {}
    },
    {
        "label": "Person",
        "kind": 6,
        "importPath": "reference_code.llama-index-core.tests.program.test_utils",
        "description": "reference_code.llama-index-core.tests.program.test_utils",
        "peekOfCode": "class Person(BaseModel):\n    name: str\n    age: Optional[int] = None\n    hobbies: List[str] = Field(default_factory=list)\ndef test_repair_incomplete_json() -> None:\n    \"\"\"Test JSON repair function.\"\"\"\n    # Test adding missing quotes\n    assert _repair_incomplete_json('{\"name\": \"John') == '{\"name\": \"John\"}'\n    # Test adding missing braces\n    assert _repair_incomplete_json('{\"name\": \"John\"') == '{\"name\": \"John\"}'",
        "detail": "reference_code.llama-index-core.tests.program.test_utils",
        "documentation": {}
    },
    {
        "label": "test_repair_incomplete_json",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_utils",
        "description": "reference_code.llama-index-core.tests.program.test_utils",
        "peekOfCode": "def test_repair_incomplete_json() -> None:\n    \"\"\"Test JSON repair function.\"\"\"\n    # Test adding missing quotes\n    assert _repair_incomplete_json('{\"name\": \"John') == '{\"name\": \"John\"}'\n    # Test adding missing braces\n    assert _repair_incomplete_json('{\"name\": \"John\"') == '{\"name\": \"John\"}'\n    # Test empty string\n    assert _repair_incomplete_json(\"\") == \"{}\"\n    # Test already valid JSON\n    valid_json = '{\"name\": \"John\", \"age\": 30}'",
        "detail": "reference_code.llama-index-core.tests.program.test_utils",
        "documentation": {}
    },
    {
        "label": "test_process_streaming_objects",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_utils",
        "description": "reference_code.llama-index-core.tests.program.test_utils",
        "peekOfCode": "def test_process_streaming_objects() -> None:\n    \"\"\"Test processing streaming objects.\"\"\"\n    # Test processing complete object\n    response = ChatResponse(\n        message=ChatMessage(\n            role=MessageRole.ASSISTANT,\n            content='{\"name\": \"John\", \"age\": 30}',\n        )\n    )\n    result = process_streaming_objects(response, Person)",
        "detail": "reference_code.llama-index-core.tests.program.test_utils",
        "documentation": {}
    },
    {
        "label": "test_num_valid_fields",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_utils",
        "description": "reference_code.llama-index-core.tests.program.test_utils",
        "peekOfCode": "def test_num_valid_fields() -> None:\n    \"\"\"Test counting valid fields.\"\"\"\n    # Test simple object\n    person = Person(name=\"John\", age=None, hobbies=[])\n    assert num_valid_fields(person) == 1  # Only name is non-None\n    # Test with more fields\n    person = Person(name=\"John\", age=30, hobbies=[\"reading\"])\n    assert num_valid_fields(person) == 3  # All fields are non-None\n    # Test list of objects\n    people = [",
        "detail": "reference_code.llama-index-core.tests.program.test_utils",
        "documentation": {}
    },
    {
        "label": "test_create_flexible_model",
        "kind": 2,
        "importPath": "reference_code.llama-index-core.tests.program.test_utils",
        "description": "reference_code.llama-index-core.tests.program.test_utils",
        "peekOfCode": "def test_create_flexible_model() -> None:\n    \"\"\"Test creating flexible model.\"\"\"\n    FlexiblePerson = create_flexible_model(Person)\n    # Should accept partial data\n    flexible_person = FlexiblePerson(name=\"John\")\n    assert flexible_person.name == \"John\"\n    assert flexible_person.age is None\n    # Should accept extra fields\n    flexible_person = FlexiblePerson(\n        name=\"John\", extra_field=\"value\", another_field=123",
        "detail": "reference_code.llama-index-core.tests.program.test_utils",
        "documentation": {}
    }
]